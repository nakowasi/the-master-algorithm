<html>

<head>

<meta content="text/html;charset=utf-8" http-equiv="Content-Type"/>

<link rel="stylesheet" href="./style.css">

<title>The Master Algorithm

</title>

</head>

<body>

<div id="babilu_link-316">

<section id="babilu_link-340">

<div><a id="babilu_link-358"></a>

<div>

<!-- <img alt="Book Title Page" src="images/000030.jpg"/> -->

</div>

</div>

</section>

</div>

<div id="babilu_link-7">

<div>

<section id="babilu_link-341">

<h1><a id="babilu_link-359"></a> <a href="#babilu_link-8">Copyright</a></h1>

<p class="noindent">Copyright © 2015 by Pedro Domingos</p>

<p class="noindent">Hardcover first published in 2015 by Basic Books, an imprint of Perseus Books, LLC, a subsidiary of Hachette Book Group, Inc.</p>

<p class="noindent">The Basic Books name and logo is a trademark of the Hachette Book Group.</p>

<p class="noindent">First paperback edition: February 2018</p>

<p class="noindent">Basic Books</p>

<p class="noindent">Hachette Book Group</p>

<p class="noindent">1290 Avenue of the Americas, New York, NY 10104</p>

<p class="noindent"><a href="http://www.basicbooks.com">www.basicbooks.com</a></p>

<p class="noindent">Hachette Book Group supports the right to free expression and the value of copyright. The purpose of copyright is to encourage writers and artists to produce the creative works that enrich our culture.</p>

<p class="noindent">The scanning, uploading, and distribution of this book without permission is a theft of the author’s intellectual property. If you would like permission to use material from the book (other than for review purposes), please contact permissions@hbgusa.com. Thank you for your support of the author’s rights.</p>

<p class="noindent">The Hachette Speakers Bureau provides a wide range of authors for speaking events. To find out more, go to www.hachettespeakersbureau.com or call (866) 376-6591.</p>

<p class="noindent">The publisher is not responsible for websites (or their content) that are not owned by the publisher.</p>

<p class="noindent">The Library of Congress has cataloged the hardcover as follows:</p>

<p class="noindent">Domingos, Pedro.</p>

<p class="noindent">The master algorithm : how the quest for the ultimate learning machine will remake our world / Pedro Domingos.</p>

<p class="noindent">  pages cm</p>

<p class="noindent">Includes index.</p>

<p class="noindent">ISBN 978-0-465-06570-7 (hardcover)—ISBN 978-0-465-06192-1 (ebook) 1. Knowledge representation (Information theory) 2. Artifi cial intelligence—Social aspects. 3. Artifi cial intelligence—Philosophy. 4. Cognitive science—Mathematics. 5. Algorithms. I. Title.</p>

<p class="noindent">Q387.D66 2015</p>

<p class="noindent">003’54—dc23</p>

<p class="noindent">ISBN 978-0-465-09427-1 (paperback) <span>2015007615</span></p>

<p class="noindent">E3-20200513-JV-PC-DPU</p>

</section>

</div>

</div>

<div id="babilu_link-339">

<nav id="babilu_link-352">

<h1><a id="babilu_link-360"></a> Contents</h1>

<h1><a id="babilu_link-360"></a> 内容</h1>

<ol>

<li class="noindent english" id="babilu_link-361" value="1"><a href="Image00001.jpg">Cover</a></li>

<li class="noindent chinese" id="babilu_link-361" value="1"><a href="Image00001.jpg">覆盖</a></li>

<li class="noindent english" id="babilu_link-362" value="2"><a href="#babilu_link-340">Title Page</a></li>

<li class="noindent chinese" id="babilu_link-362" value="2"><a href="#babilu_link-340">标题页</a></li>

<li class="noindent english" id="babilu_link-8" value="3"><a href="#babilu_link-341">Copyright</a></li>

<li class="noindent chinese" id="babilu_link-8" value="3"><a href="#babilu_link-341">版权声明</a></li>

<li class="noindent english" id="babilu_link-363" value="4"><a href="#babilu_link-342">Dedication</a></li>

<li class="noindent chinese" id="babilu_link-363" value="4"><a href="#babilu_link-342">献身精神</a></li>

<li class="noindent english" id="babilu_link-364" value="5"><a href="#babilu_link-343">Epigraph</a></li>

<li class="noindent chinese" id="babilu_link-364" value="5"><a href="#babilu_link-343">书记</a></li>

<li class="noindent english" id="babilu_link-354" value="6"><a href="#babilu_link-344">Prologue</a></li>

<li class="noindent chinese" id="babilu_link-354" value="6"><a href="#babilu_link-344">序言</a></li>

<li class="noindent english" id="babilu_link-328" value="7"><a href="#babilu_link-14"><i>Chapter 1</i> <span>The Machine-Learning Revolution</span></a></li>

<li class="noindent chinese" id="babilu_link-328" value="7"><a href="#babilu_link-14"><i>第一章</i> <span>机器学习革命</span></a></li>

<li class="noindent english" id="babilu_link-319" value="8"><a href="#babilu_link-337"><i>Chapter 2</i> <span>The Master Algorithm</span></a></li>

<li class="noindent chinese" id="babilu_link-319" value="8"><a href="#babilu_link-337"><i>第二章</i> <span>主算法</span></a></li>

<li class="noindent english" id="babilu_link-11" value="9"><a href="#babilu_link-4"><i>Chapter 3</i> <span>Hume’s Problem of Induction</span></a></li>

<li class="noindent chinese" id="babilu_link-11" value="9"><a href="#babilu_link-4"><i>第三章</i> <span>休谟的归纳问题</span></a></li>

<li class="noindent english" id="babilu_link-356" value="10"><a href="#babilu_link-338"><i>Chapter 4</i> <span>How Does Your Brain Learn?</span></a></li>

<li class="noindent chinese" id="babilu_link-356" value="10"><a href="#babilu_link-338"><i>第四章</i> <span>你的大脑是如何学习的？</span></a></li>

<li class="noindent english" id="babilu_link-330" value="11"><a href="#babilu_link-315"><i>Chapter 5</i> <span>Evolution: Nature’s Learning Algorithm</span></a></li>

<li class="noindent chinese" id="babilu_link-330" value="11"><a href="#babilu_link-315"><i>第五章</i> <span>进化：自然界的学习算法</span></a></li>

<li class="noindent english" id="babilu_link-323" value="12"><a href="#babilu_link-6"><i>Chapter 6</i> <span>In the Church of the Reverend Bayes</span></a></li>

<li class="noindent chinese" id="babilu_link-323" value="12"><a href="#babilu_link-6"><i>第六章</i> <span>在贝叶斯牧师的教堂里</span></a></li>

<li class="noindent english" id="babilu_link-1" value="13"><a href="#babilu_link-13"><i>Chapter 7</i> <span>You Are What You Resemble</span></a></li>

<li class="noindent chinese" id="babilu_link-1" value="13"><a href="#babilu_link-13"><i>第七章</i> <span>你是你所喜欢的人</span></a></li>

<li class="noindent english" id="babilu_link-314" value="14"><a href="#babilu_link-5"><i>Chapter 8</i> <span>Learning Without a Teacher</span></a></li>

<li class="noindent chinese" id="babilu_link-314" value="14"><a href="#babilu_link-5"><i>第八章</i> <span>没有老师的学习</span></a></li>

<li class="noindent english" id="babilu_link-3" value="15"><a href="#babilu_link-12"><i>Chapter 9</i> <span>The Pieces of the Puzzle Fall into Place</span></a></li>

<li class="noindent chinese" id="babilu_link-3" value="15"><a href="#babilu_link-12"><i>第九章</i> <span>拼图的碎片落到了地上</span></a></li>

<li class="noindent english" id="babilu_link-336" value="16"><a href="#babilu_link-345"><i>Chapter 10</i> <span>This Is the World on Machine Learning</span></a></li>

<li class="noindent chinese" id="babilu_link-336" value="16"><a href="#babilu_link-345"><i>第十章</i> <span>这就是机器学习的世界</span></a></li>

<li class="noindent english" id="babilu_link-332" value="17"><a href="#babilu_link-346">Epilogue</a></li>

<li class="noindent chinese" id="babilu_link-332" value="17"><a href="#babilu_link-346">后记</a></li>

<li class="noindent english" id="babilu_link-321" value="18"><a href="#babilu_link-347">Acknowledgments</a></li>

<li class="noindent chinese" id="babilu_link-321" value="18"><a href="#babilu_link-347">鸣谢</a></li>

<li class="noindent english" id="babilu_link-365" value="19"><a href="#babilu_link-15">Discover More</a></li>

<li class="noindent chinese" id="babilu_link-365" value="19"><a href="#babilu_link-15">发现更多</a></li>

<li class="noindent english" id="babilu_link-366" value="20"><a href="#babilu_link-348">About the Author</a></li>

<li class="noindent chinese" id="babilu_link-366" value="20"><a href="#babilu_link-348">关于作者</a></li>

<li class="noindent english" id="babilu_link-334" value="21"><a href="#babilu_link-349">Praise for The Master Algorithm</a></li>

<li class="noindent chinese" id="babilu_link-334" value="21"><a href="#babilu_link-349">对 “主算法“ 的赞誉</a></li>

<li class="noindent english" id="babilu_link-325" value="22"><a href="#babilu_link-350">Further Readings</a></li>

<li class="noindent chinese" id="babilu_link-325" value="22"><a href="#babilu_link-350">进一步阅读</a></li>

<li class="noindent english" id="babilu_link-17" value="23"><a href="#babilu_link-351">Index</a></li>

<li class="noindent chinese" id="babilu_link-17" value="23"><a href="#babilu_link-351">索引</a></li>

</ol>

</nav>

</div>

<div id="babilu_link-326">

<div>

<section id="babilu_link-342">

<p class="noindent english"> T<span>O THE MEMORY OF MY SISTER</span> R<span>ITA, WHO LOST HER BATTLE WITH CANCER WHILE</span> I <span>WAS WRITING THIS BOOK</span></p>

<p class="noindent chinese"><span>纪念我的妹妹</span><span>丽塔，她</span>在我<span>写这本书</span><span>时失去了与癌症的斗争</span>。</p>

</section>

</div>

</div>

<div id="babilu_link-317">

<div>

<div>

<div>

<p class="noindent english"><strong><b>Explore book giveaways, sneak peeks, deals, and more.</b></strong></p>

<p class="noindent chinese"><strong><b>探索图书赠品、偷拍、交易和更多。</b></strong></p>

</div>

<div><a href="https://discover.hachettebookgroup.com/?ref=9780465061921&amp;discp=0">Tap here to learn more</a> .

</div>

</div>

<div>

<img alt="Basic Books logo" src="images/000013.jpg"/></p>

</div>

</div>

</div>

<div id="babilu_link-9">

<div>

<section id="babilu_link-343">

<div>

<p class="noindent english"><a id="babilu_link-368"></a> The grand aim of science is to cover the greatest number of experimental facts by logical deduction from the smallest number of hypotheses or axioms.</p>

<p class="noindent chinese">科学的宏伟目标是通过从最少的假设或公理的逻辑推导来涵盖最大数量的实验事实。</p>

<p class="noindent english">—<i>Albert Einstein</i></p>

<p class="noindent chinese">-阿尔伯特-<i>爱因斯坦</i></p>

</div>

<div>

<p class="noindent english">Civilization advances by extending the number of important operations we can perform without thinking about them.</p>

<p class="noindent chinese">文明的进步在于扩大了我们可以不假思索地进行的重要操作的数量。</p>

<p class="noindent english">—<i>Alfred North Whitehead</i></p>

<p class="noindent chinese">-<i>阿尔弗雷德·诺斯·怀特海</i></p>

</div>

</section>

</div>

</div>

<div id="babilu_link-353">

<div>

<section id="babilu_link-344">

<h1><a id="babilu_link-369"></a> <a href="#babilu_link-354">Prologue</a></h1>

<h1><a id="babilu_link-369"></a> <a href="#babilu_link-354">序言</a></h1>

<p class="noindent english">You may not know it, but machine learning is all around you. When you type a query into a search engine, it’s how the engine figures out which results to show you (and which ads, as well). When you read your e-mail, you don’t see most of the spam, because machine learning filtered it out. Go to Amazon.com to buy a book or Netflix to watch a video, and a machine-learning system helpfully recommends some you might like. Facebook uses machine learning to decide which updates to show you, and Twitter does the same for tweets. Whenever you use a computer, chances are machine learning is involved somewhere.</p>

<p class="noindent chinese">你可能不知道，但机器学习就在你身边。当你在搜索引擎上输入查询信息时，引擎就是这样计算出要向你展示哪些结果（以及哪些广告）。当你阅读你的电子邮件时，你不会看到大部分的垃圾邮件，因为机器学习把它过滤掉了。去亚马逊网站买书或去 Netflix 看视频，机器学习系统会帮助你推荐一些你可能喜欢的东西。Facebook 使用机器学习来决定向你展示哪些更新，而 Twitter 对推文也是如此。只要你使用计算机，就有可能在某处涉及到机器学习。</p>

<p class="noindent english">Traditionally, the only way to get a computer to do something—from adding two numbers to flying an airplane—was to write down an algorithm explaining how, in painstaking detail. But machine-learning algorithms, also known as learners, are different: they figure it out on their own, by making inferences from data. And the more data they have, the better they get. Now we don’t have to program computers; they program themselves.</p>

<p class="noindent chinese">传统上，让计算机做某件事 —— 从两个数字相加到驾驶飞机 —— 的唯一方法是写下一个算法，详细解释如何做。但机器学习算法，也被称为学习者，是不同的：它们通过从数据中进行推理，自己找出答案。他们拥有的数据越多，他们就越好。现在我们不必对计算机进行编程，而是由它们自己编程。</p>

<p class="noindent english">It’s not just in cyberspace, either: your whole day, from the moment you wake up to the moment you fall asleep, is suffused with machine learning.</p>

<p class="noindent chinese">这也不仅仅是在网络空间：你的整个一天，从你醒来的那一刻到你睡着的那一刻，都充满了机器学习。</p>

<p class="noindent english"><a id="babilu_link-370"></a> Your clock radio goes off at 7:00 a.m. It’s playing a song you haven’t heard before, but you really like it. Courtesy of Pandora, it’s been learning your tastes in music, like your own personal radio jock. Perhaps the song itself was produced with the help of machine learning. You eat breakfast and read the morning paper. It came off the printing press a few hours earlier, the printing process carefully adjusted to avoid streaking using a learning algorithm. The temperature in your house is just right, and your electricity bill noticeably down, since you installed a Nest learning thermostat.</p>

<p class="noindent chinese">你的时钟收音机在早上 7 点响起。它正在播放一首你以前没有听过的歌曲，但你非常喜欢它。由潘多拉提供，它一直在学习你的音乐品味，就像你自己的个人电台主持人。也许这首歌本身就是在机器学习的帮助下产生的。你吃着早餐，读着晨报。它是在几个小时前从印刷机上下来的，印刷过程经过仔细调整，以避免出现使用学习算法的痕迹。自从你安装了 Nest 学习型恒温器后，你家里的温度恰到好处，电费也明显下降。</p>

<p class="noindent english">As you drive to work, your car continually adjusts fuel injection and exhaust recirculation to get the best gas mileage. You use Inrix, a traffic prediction system, to shorten your rush-hour commute, not to mention lowering your stress level. At work, machine learning helps you combat information overload. You use a data cube to summarize masses of data, look at it from every angle, and drill down on the most important bits. You have a decision to make: Will layout A or B bring more business to your website? A web-learning system tries both out and reports back. You need to check out a potential supplier’s website, but it’s in a foreign language. No problem: Google automatically translates it for you. Your e-mail conveniently sorts itself into folders, leaving only the most important messages in the inbox. Your word processor checks your grammar and spelling. You find a flight for an upcoming trip, but hold off on buying the ticket because Bing Travel predicts its price will go down soon. Without realizing it, you accomplish a lot more, hour by hour, than you would without the help of machine learning.</p>

<p class="noindent chinese">当你开车去上班时，你的汽车不断地调整燃油喷射和排气再循环，以获得最佳的汽油里程。你使用 Inrix，一个交通预测系统，以缩短高峰期的通勤时间，更不用说降低你的压力水平了。在工作中，机器学习帮助你对抗信息过载。你使用一个数据立方体来总结大量的数据，从各个角度看它，并深入研究最重要的部分。你有一个决定要做。布局 A 或 B 会给你的网站带来更多的业务吗？一个网络学习系统对这两种布局都进行了尝试并作出了报告。你需要查看一个潜在供应商的网站，但它是用外语写的。没问题：谷歌自动为你翻译。你的电子邮件很方便地被分到文件夹里，只留下最重要的信息在收件箱里。你的文字处理器检查你的语法和拼写。你为即将到来的旅行找到了一个航班，但却迟迟不买票，因为 Bing Travel 预测其价格很快就会下降。在不知不觉中，你每小时完成的工作比没有机器学习帮助的时候要多得多。</p>

<p class="noindent english">During a break you check on your mutual funds. Most of them use learning algorithms to help pick stocks, and one of them is completely run by a learning system. At lunchtime you walk down the street, smart phone in hand, looking for a place to eat. Yelp’s learning system helps you find it. Your cell phone is chock-full of learning algorithms. They’re hard at work correcting your typos, understanding your spoken commands, reducing transmission errors, recognizing bar codes, and much else. Your phone can even anticipate what you’re going to do next and advise you accordingly. For example, as you’re finishing lunch, it <a id="babilu_link-371"></a> discreetly alerts you that your afternoon meeting with an out-of-town visitor will have to start late because her flight has been delayed.</p>

<p class="noindent chinese">休息时，你检查一下你的共同基金。他们中的大多数使用学习算法来帮助挑选股票，其中一个完全由学习系统运行。在午餐时间，你走在街上，手里拿着智能手机，寻找一个吃饭的地方。Yelp 的学习系统帮助你找到它。你的手机里装满了学习算法。它们正在努力工作，纠正你的错别字，理解你的口语命令，减少传输错误，识别条形码，以及其他许多方面。你的手机甚至可以预测你接下来要做什么，并给你相应的建议。例如，当你吃完午餐时，它谨慎地提醒你，你下午与一位外地访客的会议将不得不推迟开始，因为她的航班被延误了。</p>

<p class="noindent english">Night has fallen by the time you get off work. Machine learning helps keep you safe as you walk to your car, monitoring the video feed from the surveillance camera in the parking lot and alerting off-site security staff if it detects suspicious activity. On your way home, you stop at the supermarket, where you walk down aisles that were laid out with the help of learning algorithms: which goods to stock, which end-of-aisle displays to set up, whether to put the salsa in the sauce section or next to the tortilla chips. You pay with a credit card. A learning algorithm decided to send you the offer for that card and approved your application. Another one continually looks for suspicious transactions and alerts you if it thinks your card number was stolen. A third one tries to estimate how happy you are with this card. If you’re a good customer but seem dissatisfied, you get a sweetened offer before you switch to another one.</p>

<p class="noindent chinese">当你下班的时候，夜幕已经降临了。当你走向你的车时，机器学习帮助你保持安全，监测停车场的监控摄像头的视频资料，如果发现可疑活动，就向场外的安保人员发出警报。在回家的路上，你在超市停了下来，在那里你走过的过道都是在学习算法的帮助下布置好的：哪些商品需要库存，哪些过道末端的展示需要设置，是把莎莎酱放在酱料区还是放在玉米片旁边。你用信用卡付款。一个学习算法决定向你发送该卡的报价并批准你的申请。另一个不断地寻找可疑的交易，如果它认为你的卡号被盗，就会提醒你。第三种试图估计你对这张卡的满意程度。如果你是一个好客户，但似乎不满意，在你转到另一张卡之前，你会得到一个更优惠的报价。</p>

<p class="noindent english">You get home and walk to the mailbox. You have a letter from a friend, routed to you by a learning algorithm that can read handwritten addresses. There’s also the usual junk, selected for you by other learning algorithms (oh, well). You stop for a moment to take in the cool night air. Crime in your city is noticeably down since the police started using statistical learning to predict where crimes are most likely to occur and concentrating beat officers there. You eat dinner with your family. The mayor is in the news. You voted for him because he personally called you on election day, after a learning algorithm pinpointed you as a key undecided voter. After dinner, you watch the ball game. Both teams selected their players with the help of statistical learning. Or perhaps you play games on your Xbox with your kids, and Kinect’s learning algorithm figures out where you are and what you’re doing. Before going to sleep, you take your medicine, which was designed and tested with the help of yet more learning algorithms. Your doctor, too, may have used machine learning to help diagnose you, from interpreting X-rays to figuring out an unusual set of symptoms.</p>

<p class="noindent chinese">你回到家，走到邮筒前。你有一封朋友的信，是由一个能读懂手写地址的学习算法转给你的。还有一些常见的垃圾，由其他学习算法为你选择（哦，好吧）。你停下脚步，感受一下夜晚的凉爽空气。自从警方开始使用统计学习来预测最可能发生犯罪的地方，并将巡逻人员集中在那里，你所在城市的犯罪率明显下降。你和你的家人吃晚饭。市长上了新闻。你把票投给了他，因为他在选举日亲自给你打了电话，此前，一个学习算法把你确定为一个关键的未决定的选民。晚餐后，你观看了球赛。两支球队都在统计学习的帮助下选择了他们的球员。或者你和你的孩子在 Xbox 上玩游戏，Kinect 的学习算法找出了你的位置和你在做什么。睡觉前，你吃药，这是在更多学习算法的帮助下设计和测试的。你的医生也可能使用机器学习来帮助诊断你，从解释 X 射线到弄清一组不寻常的症状。</p>

<p class="noindent english">Machine learning plays a part in every stage of your life. If you studied online for the SAT college admission exam, a learning algorithm <a id="babilu_link-372"></a> graded your practice essays. And if you applied to business school and took the GMAT exam recently, one of your essay graders was a learning system. Perhaps when you applied for your job, a learning algorithm picked your résumé from the virtual pile and told your prospective employer: here’s a strong candidate; take a look. Your latest raise may have come courtesy of another learning algorithm. If you’re looking to buy a house, Zillow.com will estimate what each one you’re considering is worth. When you’ve settled on one, you apply for a home loan, and a learning algorithm studies your application and recommends accepting it (or not). Perhaps most important, if you’ve used an online dating service, machine learning may even have helped you find the love of your life.</p>

<p class="noindent chinese">机器学习在你生活的每个阶段都发挥着作用。如果你在网上为 SAT 大学入学考试学习，一个学习算法为你的练习作文打分。如果你最近申请商学院并参加了 GMAT 考试，你的论文评分员之一就是一个学习系统。也许当你申请工作时，一个学习算法从虚拟的简历堆中挑选出你的简历，并告诉你的未来雇主：这是一个强有力的候选人；看看吧。你最近的加薪可能是由另一个学习算法提供的。如果你想买房子，Zillow.com 会估计你考虑的每一个房子的价值。当你确定了一个，你就申请房屋贷款，一个学习算法会研究你的申请并建议接受它（或不接受）。也许最重要的是，如果你使用过在线约会服务，机器学习甚至可能已经帮助你找到了你的爱情。</p>

<p class="noindent english">Society is changing, one learning algorithm at a time. Machine learning is remaking science, technology, business, politics, and war. Satellites, DNA sequencers, and particle accelerators probe nature in ever-finer detail, and learning algorithms turn the torrents of data into new scientific knowledge. Companies know their customers like never before. The candidate with the best voter models wins, like Obama against Romney. Unmanned vehicles pilot themselves across land, sea, and air. No one programmed your tastes into the Amazon recommendation system; a learning algorithm figured them out on its own, by generalizing from your past purchases. Google’s self-driving car taught itself how to stay on the road; no engineer wrote an algorithm instructing it, step-by-step, how to get from A to B. No one knows how to program a car to drive, and no one needs to, because a car equipped with a learning algorithm picks it up by observing what the driver does.</p>

<p class="noindent chinese">社会正在改变，一次一个学习算法。机器学习正在重新塑造科学、技术、商业、政治和战争。卫星、DNA 测序仪和粒子加速器以更精细的方式探测大自然，而学习算法则将大量的数据转化为新的科学知识。公司以前所未有的方式了解他们的客户。拥有最佳选民模型的候选人获胜，如奥巴马对罗姆尼。无人驾驶车辆在陆地、海洋和空中自行驾驶。没有人把你的口味编入亚马逊的推荐系统；一个学习算法从你过去的购买中归纳出了这些口味。谷歌的自动驾驶汽车自己学会了如何在路上行驶；没有工程师编写算法，一步一步地指导它如何从 A 地到 B 地。没有人知道如何为汽车编程，也没有人需要这样做，因为配备了学习算法的汽车通过观察司机的行为来学习。</p>

<p class="noindent english">Machine learning is something new under the sun: a technology that builds itself. Ever since our remote ancestors started sharpening stones into tools, humans have been designing artifacts, whether they’re hand built or mass produced. But learning algorithms are artifacts that design other artifacts. “Computers are useless,” said Picasso. “They can only give you answers.” Computers aren’t supposed to be creative; they’re supposed to do what you tell them to. If what you tell them to do is be creative, you get machine learning. A learning algorithm is like <a id="babilu_link-373"></a> a master craftsman: every one of its productions is different and exquisitely tailored to the customer’s needs. But instead of turning stone into masonry or gold into jewelry, learners turn data into algorithms. And the more data they have, the more intricate the algorithms can be.</p>

<p class="noindent chinese">机器学习是阳光下的新事物：一种能自我构建的技术。自从我们遥远的祖先开始将石头磨成工具以来，人类一直在设计人工制品，无论是手工制造还是大规模生产。但学习算法是设计其他人工制品的人工制品。“计算机是无用的”，毕加索说，“它们只能给你答案。” 计算机不应该是创造性的；它们应该做你告诉它们的事情。如果你让它们做的事情是创造性的，你就会得到机器学习。一个学习算法就像一个工匠大师：它的每一个产品都是不同的，都是根据客户的需求精致地定制的。但是，学习者不是把石头变成砖石或把黄金变成珠宝，而是把数据变成算法。而他们拥有的数据越多，算法就越复杂。</p>

<p class="noindent english"><i>Homo sapiens</i> is the species that adapts the world to itself instead of adapting itself to the world. Machine learning is the newest chapter in this million-year saga: with it, the world senses what you want and changes accordingly, without you having to lift a finger. Like a magic forest, your surroundings—virtual today, physical tomorrow—rearrange themselves as you move through them. The path you picked out between the trees and bushes grows into a road. Signs pointing the way spring up in the places where you got lost.</p>

<p class="noindent chinese"><i>智人</i>是让世界适应自己而不是让自己适应世界的物种。机器学习是这个百万年传奇的最新篇章：有了它，世界可以感知你想要的东西，并作出相应的改变，而不需要你动一根手指。就像一个神奇的森林，你周围的环境 —— 今天是虚拟的，明天是实体的 —— 会随着你的移动而重新排列。你在树木和灌木丛之间挑出的小路会成长为一条道路。在你迷路的地方出现了指路的标志。</p>

<p class="noindent english">These seemingly magical technologies work because, at its core, machine learning is about prediction: predicting what we want, the results of our actions, how to achieve our goals, how the world will change. Once upon a time we relied on shamans and soothsayers for this, but they were much too fallible. Science’s predictions are more trustworthy, but they are limited to what we can systematically observe and tractably model. Big data and machine learning greatly expand that scope. Some everyday things can be predicted by the unaided mind, from catching a ball to carrying on a conversation. Some things, try as we might, are just unpredictable. For the vast middle ground between the two, there’s machine learning.</p>

<p class="noindent chinese">这些看似神奇的技术之所以奏效，是因为机器学习的核心是预测：预测我们想要什么，我们行动的结果，如何实现我们的目标，世界将如何变化。从前我们依靠萨满和占卜师来做这件事，但他们太容易犯错了。科学的预测更值得信赖，但它们局限于我们可以系统地观察和切实可行的模型。大数据和机器学习大大扩展了这个范围。一些日常事物可以通过无助的头脑来预测，从接球到进行对话。有些事情，尽管我们努力尝试，但就是无法预测。对于这两者之间的巨大中间地带，有机器学习。</p>

<p class="noindent english">Paradoxically, even as they open new windows on nature and human behavior, learning algorithms themselves have remained shrouded in mystery. Hardly a day goes by without a story in the media involving machine learning, whether it’s Apple’s launch of the Siri personal assistant, IBM’s Watson beating the human <i>Jeopardy!</i> champion, Target finding out a teenager is pregnant before her parents do, or the NSA looking for dots to connect. But in each case the learning algorithm driving the story is a black box. Even books on big data skirt around what really happens when the computer swallows all those terabytes and magically comes up with new insights. At best, we’re left with the impression that learning algorithms just find correlations between pairs of events, such <a id="babilu_link-374"></a> as googling “flu medicine” and having the flu. But finding correlations is to machine learning no more than bricks are to houses, and people don’t live in bricks.</p>

<p class="noindent chinese">矛盾的是，即使它们为自然和人类行为打开了新的窗口，学习算法本身仍然笼罩在神秘之中。媒体上几乎每天都有涉及机器学习的报道，无论是苹果公司推出的 Siri 个人助理、IBM 的 Watson 击败人类的 <i>Jeopardy！</i>冠军、Target 公司在其父母之前发现一名少女怀孕，还是国家安全局在寻找连接点。但在每个案例中，驱动故事的学习算法都是一个黑盒子。即使是关于大数据的书籍，也会绕开当计算机吞下所有这些兆字节并神奇地提出新的见解时真正发生了什么。在最好的情况下，我们留下的印象是，学习算法只是在成对的事件之间寻找相关性，例如如搜索 “流感药” 和患流感。但是，寻找相关性对于机器学习来说，就像砖头对于房子一样，而人们并不住在砖头里。</p>

<p class="noindent english">When a new technology is as pervasive and game changing as machine learning, it’s not wise to let it remain a black box. Opacity opens the door to error and misuse. Amazon’s algorithm, more than any one person, determines what books are read in the world today. The NSA’s algorithms decide whether you’re a potential terrorist. Climate models decide what’s a safe level of carbon dioxide in the atmosphere. Stock-picking models drive the economy more than most of us do. You can’t control what you don’t understand, and that’s why you need to understand machine learning—as a citizen, a professional, and a human being engaged in the pursuit of happiness.</p>

<p class="noindent chinese">当一项新技术像机器学习一样普遍存在并改变游戏规则时，让它保持一个黑盒子是不明智的。不透明性为错误和误用打开了方便之门。亚马逊的算法，比任何一个人都更能决定今天世界上有哪些书被阅读。国家安全局的算法决定你是否是一个潜在的恐怖分子。气候模型决定什么是大气中二氧化碳的安全水平。挑选股票的模型比我们大多数人更能推动经济发展。你无法控制你不了解的东西，这就是为什么你需要了解机器学习 —— 作为一个公民，一个专业人士，一个从事追求幸福的人。</p>

<p class="noindent english">This book’s first goal is to let you in on the secrets of machine learning. Only engineers and mechanics need to know how a car’s engine works, but every driver needs to know that turning the steering wheel changes the car’s direction and stepping on the brake brings it to a stop. Few people today know what the corresponding elements of a learner even are, let alone how to use them. The psychologist Don Norman coined the term <i>conceptual model</i> to refer to the rough knowledge of a technology we need to have in order to use it effectively. This book provides you with a conceptual model of machine learning.</p>

<p class="noindent chinese">本书的第一个目标是让你了解机器学习的秘密。只有工程师和机械师才需要知道汽车的发动机是如何工作的，但每个司机都需要知道转动方向盘可以改变汽车的方向，踩下刹车可以让汽车停下来。今天很少有人知道学习者的相应要素甚至是什么，更不用说如何使用它们了。心理学家唐·诺曼创造了<i>概念模型</i>这一术语，指的是我们为了有效使用一项技术而需要掌握的粗略知识。本书为你提供了一个机器学习的概念模型。</p>

<p class="noindent english">Not all learning algorithms work the same, and the differences have consequences. Take Amazon’s and Netflix’s recommenders, for example. If each were guiding you through a physical bookstore, trying to determine what’s “right for you,” Amazon would be more likely to walk you over to shelves you’ve frequented previously; Netflix would take you to unfamiliar and seemingly odd sections of the store but lead you to stuff you’d end up loving. In this book we’ll see the different kinds of algorithms that companies like Amazon and Netflix use. Netflix’s algorithm has a deeper (even if still quite limited) understanding of your tastes than Amazon’s, but ironically that doesn’t mean Amazon would be better off using it. Netflix’s business model depends on driving demand into the long tail of obscure movies and TV shows, which cost <a id="babilu_link-375"></a> it little, and away from the blockbusters, which your subscription isn’t enough to pay for. Amazon has no such problem; although it’s well placed to take advantage of the long tail, it’s equally happy to sell you more expensive popular items, which also simplify its logistics. And we, as customers, are more willing to take a chance on an odd item if we have a subscription than if we have to pay for it separately.</p>

<p class="noindent chinese">并非所有的学习算法的工作原理都是一样的，而且这些差异会产生影响。以亚马逊和 Netflix 的推荐器为例。如果他们各自引导你穿过一家实体书店，试图确定什么是 “适合你的”，亚马逊会更倾向于把你带到你以前经常光顾的书架上；Netflix 会把你带到商店里不熟悉的、看起来很奇怪的区域，但会引导你找到你最终会喜欢的东西。在这本书中，我们将看到亚马逊和 Netflix 等公司使用的不同种类的算法。Netflix 的算法比亚马逊的算法对你的口味有更深的了解（即使仍然相当有限），但具有讽刺意味的是，这并不意味着亚马逊会更好地使用它。Netflix 的商业模式取决于将需求引向晦涩难懂的电影和电视节目的长尾部分，这些电影和电视节目的成本而远离大片，你的订阅不足以支付这些影片。亚马逊没有这样的问题；尽管它很好地利用了长尾效应，但它同样乐意向你出售更昂贵的流行商品，这也简化了它的物流。而且，作为顾客，如果我们有一个订阅，比起我们必须单独付款，我们更愿意在一个奇怪的项目上抓住机会。</p>

<p class="noindent english">Hundreds of new learning algorithms are invented every year, but they’re all based on the same few basic ideas. These are what this book is about, and they’re all you really need to know to understand how machine learning is changing the world. Far from esoteric, and quite aside even from their use in computers, they are answers to questions that matter to all of us: How do we learn? Is there a better way? What can we predict? Can we trust what we’ve learned? Rival schools of thought within machine learning have very different answers to these questions. The main ones are five in number, and we’ll devote a chapter to each. Symbolists view learning as the inverse of deduction and take ideas from philosophy, psychology, and logic. Connectionists reverse engineer the brain and are inspired by neuroscience and physics. Evolutionaries simulate evolution on the computer and draw on genetics and evolutionary biology. Bayesians believe learning is a form of probabilistic inference and have their roots in statistics. Analogizers learn by extrapolating from similarity judgments and are influenced by psychology and mathematical optimization. Driven by the goal of building learning machines, we’ll tour a good chunk of the intellectual history of the last hundred years and see it in a new light.</p>

<p class="noindent chinese">每年都有数以百计的新学习算法被发明出来，但它们都是基于同样的几个基本想法。这些就是本书的内容，也是你真正需要知道的，以了解机器学习如何改变世界。它们远非深奥，甚至与它们在计算机中的应用完全无关，它们是对我们所有人都很重要的问题的回答：我们如何学习？是否有更好的方法？我们能预测什么？我们能相信我们学到的东西吗？机器学习中的对立学派对这些问题有非常不同的答案。主要有五种，我们将用一章的篇幅来介绍。符号学派将学习视为演绎的逆向，并从哲学、心理学和逻辑学中汲取思想。连接主义者对大脑进行逆向工程，其灵感来自于神经科学和物理学。进化论者在计算机上模拟进化，并借鉴了遗传学和进化生物学。贝叶斯主义者认为学习是一种概率推理的形式，他们的根基在统计学。类比者通过对相似性判断的推断来学习，并受到心理学和数学优化的影响。在建造学习机器这一目标的驱动下，我们将参观过去一百年的知识历史中的很大一部分，并以新的眼光看待它。</p>

<p class="noindent english">Each of the five tribes of machine learning has its own master algorithm, a general-purpose learner that you can in principle use to discover knowledge from data in any domain. The symbolists’ master algorithm is inverse deduction, the connectionists’ is backpropagation, the evolutionaries’ is genetic programming, the Bayesians’ is Bayesian inference, and the analogizers’ is the support vector machine. In practice, however, each of these algorithms is good for some things but not others. What we really want is a single algorithm combining the key features of all of them: the ultimate master algorithm. For some this is <a id="babilu_link-376"></a> an unattainable dream, but for many of us in machine learning, it’s what puts a twinkle in our eye and keeps us working late into the night.</p>

<p class="noindent chinese">机器学习的五个部落都有自己的主算法，一个通用的学习者，原则上你可以用它来从任何领域的数据中发现知识。符号派的主算法是逆推法，连接派的主算法是反向传播，进化派的主算法是遗传编程，贝叶斯派的主算法是贝叶斯推理，而模拟派的主算法是支持向量机。然而，在实践中，这些算法中的每一种都对某些事情有好处，但对其他事情却没有好处。我们真正想要的是一种结合了所有这些算法的关键特征的单一算法：终极主算法。对于一些人来说，这是一个无法实现的梦想，但对于我们许多从事机器学习的人来说，这是让我们眼睛一亮并一直工作到深夜的原因。</p>

<p class="noindent english">If it exists, the Master Algorithm can derive all knowledge in the world—past, present, and future—from data. Inventing it would be one of the greatest advances in the history of science. It would speed up the progress of knowledge across the board, and change the world in ways that we can barely begin to imagine. The Master Algorithm is to machine learning what the Standard Model is to particle physics or the Central Dogma to molecular biology: a unified theory that makes sense of everything we know to date, and lays the foundation for decades or centuries of future progress. The Master Algorithm is our gateway to solving some of the hardest problems we face, from building domestic robots to curing cancer.</p>

<p class="noindent chinese">如果它存在，主算法可以从数据中得出世界上的所有知识 —— 过去、现在和未来。发明它将是科学史上最伟大的进步之一。它将加速知识的全面进步，并以我们几乎无法想象的方式改变世界。主算法对于机器学习来说，就像标准模型对于粒子物理学或中央教条对于分子生物学一样：一个统一的理论，使我们迄今所知的一切都有意义，并为未来几十年或几百年的进步奠定基础。主算法是我们解决一些我们面临的最困难问题的途径，从制造家用机器人到治愈癌症。</p>

<p class="noindent english">Take cancer. Curing it is hard because cancer is not one disease, but many. Tumors can be triggered by a dizzying array of causes, and they mutate as they metastasize. The surest way to kill a tumor is to sequence its genome, figure out which drugs will work against it—without harming you, given <i>your</i> genome and medical history—and perhaps even design a new drug specifically for your case. No doctor can master all the knowledge required for this. Sounds like a perfect job for machine learning: in effect, it’s a more complicated and challenging version of the searches that Amazon and Netflix do every day, except it’s looking for the right treatment for you instead of the right book or movie. Unfortunately, while today’s learning algorithms can diagnose many diseases with superhuman accuracy, curing cancer is well beyond their ken. If we succeed in our quest for the Master Algorithm, it will no longer be.</p>

<p class="noindent chinese">以癌症为例。治愈它是困难的，因为癌症不是一种疾病，而是很多。肿瘤可由一系列令人眼花缭乱的原因引发，并在转移过程中发生变异。杀死肿瘤的最可靠方法是对其基因组进行测序，找出哪些药物会对其起作用 —— 鉴于<i>你的</i>基因组和病史，在不伤害你的情况下 —— 甚至可能专门为你的情况设计一种新药。没有一个医生能掌握这方面所需的所有知识。听起来像是机器学习的完美工作：实际上，它是亚马逊和 Netflix 每天做的搜索的一个更复杂和更具挑战性的版本，只是它在寻找适合你的治疗方法，而不是合适的书或电影。不幸的是，虽然今天的学习算法能够以超人的准确度诊断许多疾病，但治愈癌症却远远超出了他们的能力。如果我们在寻求主算法的过程中取得成功，它将不再是。</p>

<p class="noindent english">The second goal of this book is thus to enable <i>you</i> to invent the Master Algorithm. You’d think this would require heavy-duty mathematics and severe theoretical work. On the contrary, what it requires is stepping back from the mathematical arcana to see the overarching pattern of learning phenomena; and for this the layman, approaching the forest from a distance, is in some ways better placed than the specialist, already deeply immersed in the study of particular trees. Once we have the conceptual solution, we can fill in the mathematical details; but that is not for this book, and not the most important part. Thus, as we visit <a id="babilu_link-377"></a> each tribe, our goal is to gather its piece of the puzzle and understand where it fits, mindful that none of the blind men can see the whole elephant. In particular, we’ll see what each tribe can contribute to curing cancer, and also what it’s missing. Then, step-by-step, we’ll assemble all the pieces into the solution—or rather, <i>a</i> solution that is not yet the Master Algorithm, but is the closest anyone has come, and hopefully makes a good launch pad for your imagination. And we’ll preview the use of this algorithm as a weapon in the fight against cancer. As you read the book, feel free to skim or skip any parts you find troublesome; it’s the big picture that matters, and you’ll probably get more out of those parts if you revisit them after the puzzle is assembled.</p>

<p class="noindent chinese">因此，本书的第二个目标是使<i>你</i>能够发明主算法。你会认为这需要繁重的数学和严格的理论工作。恰恰相反，这需要从数学奥义中抽身出来，看到学习现象的总体模式；为此，从远处接近森林的普通人，在某些方面比已经深深沉浸于特定树木研究的专家更有优势。一旦我们有了概念上的解决方案，我们就可以填补数学上的细节；但这不是本书的内容，也不是最重要的部分。因此，当我们访问每个部落时，我们的目标是收集它的一块拼图，了解它的位置，注意没有一个盲人能看到整个大象。特别是，我们将看到每个部落能够为治疗癌症做出什么贡献，以及它缺少什么。然后，我们将一步一步地把所有的碎片组装成解决方案 —— 或者说，<i>一个</i>还不是主算法的解决方案，但却是最接近的，希望能为你的想象力提供一个良好的启动平台。我们还将预览这一算法作为抗癌武器的使用情况。当你阅读这本书时，你可以自由地略过或跳过任何你认为麻烦的部分；重要的是大局，如果你在拼图完成后重新审视这些部分，你可能会从这些部分中得到更多。</p>

<p class="noindent english">I’ve been a machine-learning researcher for more than twenty years. My interest in it was sparked by a book with an odd title I saw in a bookstore when I was a senior in college: <i>Artificial Intelligence</i> . It had only a short chapter on machine learning, but on reading it, I immediately became convinced that learning was the key to solving AI and that the state of the art was so primitive that maybe I could contribute something. Shelving plans for an MBA, I entered the PhD program at the University of California, Irvine. Machine learning was then a small, obscure field, and UCI had one of the few sizable research groups anywhere. Some of my classmates dropped out because they didn’t see much of a future in it, but I persisted. To me nothing could have more impact than teaching computers to learn: if we could do that, we would get a leg up on every other problem. By the time I graduated five years later, the data-mining explosion was under way, and so was my path to this book. My doctoral dissertation unified symbolic and analogical learning. I’ve spent much of the last ten years unifying symbolism and Bayesianism, and more recently those two with connectionism. It’s time to go the next step and attempt a synthesis of all five paradigms.</p>

<p class="noindent chinese">我从事机器学习研究已经二十多年了。我对它的兴趣是由我在大学四年级时在书店看到的一本标题奇怪的书引发的：《<i>人工智能</i>》。这本书只有一个关于机器学习的简短章节，但读完之后，我立即相信学习是解决人工智能的关键，而且技术水平非常原始，也许我可以做出一些贡献。搁置了读 MBA 的计划，我进入了加州大学欧文分校的博士课程。当时，机器学习是一个小而晦涩的领域，UCI 有一个为数不多的有规模的研究小组。我的一些同学退学了，因为他们认为这没有什么前途，但我坚持了下来。对我来说，没有什么比教计算机学习更有影响力的了：如果我们能做到这一点，我们就能在其他所有问题上获得优势。当我五年后毕业时，数据挖掘的爆炸性增长正在进行中，而我写这本书的道路也是如此。我的博士论文统一了符号学习和类比学习。在过去的十年里，我花了很多时间来统一符号学和贝叶斯主义，最近又将这两者与连接主义结合起来。现在是时候迈出下一步，尝试对所有五个范式进行综合。</p>

<p class="noindent english">I had a number of different but overlapping audiences in mind when writing this book.</p>

<p class="noindent chinese">在写这本书时，我心中有许多不同但重叠的读者。</p>

<p class="noindent english">If you’re curious what all the hubbub surrounding big data and machine learning is about and suspect that there’s something deeper going <a id="babilu_link-378"></a> on than what you see in the papers, you’re right! This book is your guide to the revolution.</p>

<p class="noindent chinese">如果你对围绕大数据和机器学习的所有喧嚣感到好奇，并怀疑有比你在报纸上看到的更深层次的东西你就对了！这本书是你的革命指南。这本书就是你的革命指南。</p>

<p class="noindent english">If your main interest is in the business uses of machine learning, this book can help you in at least six ways: to become a savvier consumer of analytics; to make the most of your data scientists; to avoid the pitfalls that kill so many data-mining projects; to discover what you can automate without the expense of hand-coded software; to reduce the rigidity of your information systems; and to anticipate some of the new technology that’s coming your way. I’ve seen too much time and money wasted trying to solve a problem with the wrong learning algorithm, or misinterpreting what the algorithm said. It doesn’t take much to avoid these fiascoes. In fact, all it takes is to read this book.</p>

<p class="noindent chinese">如果你的主要兴趣是机器学习的商业用途，这本书至少可以在六个方面帮助你：成为一个更精明的分析消费者；充分利用你的数据科学家；避免扼杀许多数据挖掘项目的陷阱；发现你可以在不花费手工编码软件的情况下实现自动化；减少你信息系统的僵化；以及预测一些即将到来的新技术。我见过太多的时间和金钱被浪费在试图用错误的学习算法来解决问题上，或者误解了算法的内容。要避免这些惨败并不需要太多。事实上，只需要阅读这本书就可以了。</p>

<p class="noindent english">If you’re a citizen or policy maker concerned with the social and political issues raised by big data and machine learning, this book will give you a primer on the technology—what it is, where it’s taking us, what it does and doesn’t make possible—without boring you with all the ins and outs. From privacy to the future of work and the ethics of roboticized warfare, we’ll see where the real issues are and how to think about them.</p>

<p class="noindent chinese">如果你是一个关注大数据和机器学习所带来的社会和政治问题的公民或政策制定者，这本书将为你提供关于这项技术的入门知识 —— 它是什么，它将把我们带到哪里，它能做什么，不能做什么 —— 而不会让你对所有的内幕感到厌倦。从隐私到未来的工作和机器人化战争的伦理，我们将看到真正的问题在哪里，以及如何去思考它们。</p>

<p class="noindent english">If you’re a scientist or engineer, machine learning is a powerful armory that you don’t want to be without. The old, tried-and-true statistical tools don’t get you far in the age of big (or even medium) data. You need machine learning’s nonlinear chops to accurately model most phenomena, and it brings with it a new scientific worldview. The expression <i>paradigm shift</i> is used too casually these days, but I believe it’s not an exaggeration to say that that’s what this book describes.</p>

<p class="noindent chinese">如果你是一个科学家或工程师，机器学习是一个强大的武器库，你不回希望没有它。在大数据（甚至是中数据）时代，老的、久经考验的统计工具并不能让你走得更远。你需要机器学习的非线性能力来准确地模拟大多数现象，它带来了一个新的科学世界观。<i>范式转变</i>这一表述如今被用得太随意了，但我相信，说这就是本书所描述的，并不夸张。</p>

<p class="noindent english">If you’re a machine-learning expert, you’re already familiar with much of what the book covers, but you’ll also find in it many fresh ideas, historical nuggets, and useful examples and analogies. Most of all, I hope the book will provide a new perspective on machine learning and maybe even start you thinking in new directions. Low-hanging fruit is all around us, and it behooves us to pick it, but we also shouldn’t lose sight of the bigger rewards that lie just beyond. (Apropos of which, <a id="babilu_link-379"></a> I hope you’ll forgive my poetic license in using the term <i>master algorithm</i> to refer to a general-purpose learner.)</p>

<p class="noindent chinese">如果你是一个机器学习专家，你已经熟悉了书中的大部分内容，但你也会在书中发现许多新鲜的想法、历史的金句以及有用的例子和类比。最重要的是，我希望这本书能够为机器学习提供一个新的视角，甚至可能让你开始思考新的方向。低垂的果实就在我们身边，我们应该去采摘它，但我们也不应该忽视后面更大的收获。（关于这一点，我希望你能原谅我用<i>主算法</i>这个词来指代通用学习器的诗意许可）。</p>

<p class="noindent english">If you’re a student of any age—a high schooler wondering what to major in, a college undergraduate deciding whether to go into research, or a seasoned professional considering a career change—my hope is that this book will spark in you an interest in this fascinating field. The world has a dire shortage of machine-learning experts, and if you decide to join us, you can look forward to not only exciting times and material rewards but also a unique opportunity to serve society. And if you’re already studying machine learning, I hope the book will help you get the lay of the land; if in your travels you chance upon the Master Algorithm, that alone makes it worth writing.</p>

<p class="noindent chinese">如果你是任何年龄段的学生 —— 想知道主修什么的高中生，决定是否进入研究领域的大学本科生，或者考虑改变职业的经验丰富的专业人士 —— 我希望这本书能激发你对这个迷人领域的兴趣。世界上极度缺乏机器学习专家，如果你决定加入我们，你不仅可以期待激动人心的时刻和物质回报，还可以期待一个独特的机会来服务社会。如果你已经在学习机器学习，我希望这本书能帮助你了解这个领域；如果你在旅行中偶然发现了 “主算法“ ，仅此一点就值得一写。</p>

<p class="noindent english">Last but not least, if you have an appetite for wonder, machine learning is an intellectual feast, and you’re invited—RSVP!</p>

<p class="noindent chinese">最后但并非最不重要的是，如果你对奇迹有兴趣，机器学习是一场智力盛宴，你是被邀请的 RSVP!</p>

</section>

</div>

</div>

<div id="babilu_link-327">

<div>

<section id="babilu_link-14">

<h1><a id="babilu_link-41"></a> <a href="#babilu_link-328">CHAPTER ONE</a></h1>

<h1><a id="babilu_link-41"></a> <a href="#babilu_link-328">第一章</a></h1>

<h1><a href="#babilu_link-328">The Machine-Learning Revolution</a></h1>

<h1><a href="#babilu_link-328">机器学习革命</a></h1>

<p class="noindent english">We live in the age of algorithms. Only a generation or two ago, mentioning the word <i>algorithm</i> would have drawn a blank from most people. Today, algorithms are in every nook and cranny of civilization. They are woven into the fabric of everyday life. They’re not just in your cell phone or your laptop but in your car, your house, your appliances, and your toys. Your bank is a gigantic tangle of algorithms, with humans turning the knobs here and there. Algorithms schedule flights and then fly the airplanes. Algorithms run factories, trade and route goods, cash the proceeds, and keep records. If every algorithm suddenly stopped working, it would be the end of the world as we know it.</p>

<p class="noindent chinese">我们生活在算法的时代。仅仅在一两代人之前，提到<i>算法</i>这个词，大多数人都会感到茫然。今天，算法存在于文明的每个角落。它们被交织在日常生活的结构中。它们不仅存在于你的手机或笔记本电脑中，还存在于你的汽车、房子、电器和玩具中。你的银行是一个巨大的算法的纠结，人类在这里和那里转动旋钮。算法安排航班，然后驾驶飞机。算法运行工厂，交易和运送货物，兑现收益，并保持记录。如果每个算法突然停止工作，这将是我们所知的世界末日。</p>

<p class="noindent english">An algorithm is a sequence of instructions telling a computer what to do. Computers are made of billions of tiny switches called transistors, and algorithms turn those switches on and off billions of times per second. The simplest algorithm is: flip a switch. The state of one transistor is one bit of information: one if the transistor is on, and zero if it’s off. One bit somewhere in your bank’s computers says whether your account is overdrawn or not. Another bit somewhere in the Social Security Administration’s computers says whether you’re alive or dead. The second simplest algorithm is: combine two bits. Claude Shannon, <a id="babilu_link-73"></a> better known as the father of information theory, was the first to realize that what transistors are doing, as they switch on and off in response to other transistors, is reasoning. (That was his master’s thesis at MIT—the most important master’s thesis of all time.) If transistor A turns on only when transistors B and C are both on, it’s doing a tiny piece of logical reasoning. If A turns on when either B or C is on, that’s another tiny logical operation. And if A turns on whenever B is off, and vice versa, that’s a third operation. Believe it or not, every algorithm, no matter how complex, can be reduced to just these three operations: AND, OR, and NOT. Simple algorithms can be represented by diagrams, using different symbols for the AND, OR, and NOT operations. For example, if a fever can be caused by influenza or malaria, and you should take Tylenol for a fever and a headache, this can be expressed as follows:</p>

<p class="noindent chinese">算法是一连串的指令，告诉计算机要做什么。计算机是由数十亿个被称为晶体管的微小开关组成的，而算法每秒钟将这些开关打开和关闭数十亿次。最简单的算法是：翻转一个开关。一个晶体管的状态是一个比特的信息：如果晶体管是打开的，就是 1，如果是关闭的，就是 0。在你的银行电脑的某个地方，有一个比特说你的账户是否透支。在社会安全局的计算机中的某处的另一个比特说你是活着还是死了。第二个最简单的算法是：结合两个比特。克劳德·香农，作为信息理论之父而闻名，他是第一个意识到晶体管在做什么的人，当它们响应其他晶体管的开关时，是在进行推理。（那是他在麻省理工学院的硕士论文 —— 有史以来最重要的硕士论文）。如果晶体管 A 只有在晶体管 B 和 C 都打开时才打开，它就在做一个小小的逻辑推理。如果 A 在 B 或 C 开启时打开，这是另一个微小的逻辑运算。如果 A 在 B 关闭时打开，反之亦然，这就是第三个操作。信不信由你，每一种算法，无论多么复杂，都可以简化为这三种操作。AND, OR, 和 NOT。简单的算法可以用图来表示，用不同的符号表示 AND、OR 和 NOT 操作。例如，如果发烧可能是由流感或疟疾引起的，而你应该服用泰诺治疗发烧和头痛，这可以表示为：</p>

<div>

<div>

<img alt="image" src="images/000031.jpg"/>

</div>

</div>

<p class="noindent english">By combining many such operations, we can carry out very elaborate chains of logical reasoning. People often think computers are all about numbers, but they’re not. Computers are all about logic. Numbers and arithmetic are made of logic, and so is everything else in a computer. Want to add two numbers? There’s a combination of transistors that does that. Want to beat the human <i>Jeopardy!</i> champion? There’s a combination of transistors for that too (much bigger, naturally).</p>

<p class="noindent chinese">通过结合许多这样的操作，我们可以进行非常复杂的逻辑推理链。人们经常认为计算机都是关于数字的，但事实并非如此。计算机都是关于逻辑的。数字和算术是由逻辑构成的，而计算机中的其他东西也是如此。想把两个数字相加？有一个晶体管的组合可以做到这一点。想打败人类的 <i>“危险游戏</i>” 冠军吗？也有一个晶体管组合可以做到这一点（自然要大得多）。</p>

<p class="noindent english">It would be prohibitively expensive, though, if we had to build a new computer for every different thing we want to do. Rather, a modern computer is a vast assembly of transistors that can do many different things, depending on which transistors are activated. Michelangelo said that all he did was see the statue inside the block of marble and carve away the excess stone until the statue was revealed. Likewise, an algorithm carves away the excess transistors in the computer until the <a id="babilu_link-311"></a> intended function is revealed, whether it’s an airliner’s autopilot or a new Pixar movie.</p>

<p class="noindent chinese">不过，如果我们不得不为每一件我们想做的不同事情建造一台新的计算机，那将是非常昂贵的。相反，现代计算机是一个巨大的晶体管组件，可以做许多不同的事情，这取决于哪些晶体管被激活。米开朗基罗说，他所做的就是看到大理石块里面的雕像，然后雕刻掉多余的石头，直到雕像露出来。同样，一个算法将计算机中多余的晶体管雕刻掉，直到露出无论是客机的自动驾驶仪还是皮克斯的新电影，都是如此。</p>

<p class="noindent english">An algorithm is not just any set of instructions: they have to be precise and unambiguous enough to be executed by a computer. For example, a cooking recipe is not an algorithm because it doesn’t exactly specify what order to do things in or exactly what each step is. Exactly how much sugar is a spoonful? As everyone who’s ever tried a new recipe knows, following it may result in something delicious or a mess. In contrast, an algorithm always produces the same result. Even if a recipe specifies precisely half an ounce of sugar, we’re still not out of the woods because the computer doesn’t know what sugar is, or an ounce. If we wanted to program a kitchen robot to make a cake, we would have to tell it how to recognize sugar from video, how to pick up a spoon, and so on. (We’re still working on that.) The computer has to know how to execute the algorithm all the way down to turning specific transistors on and off. So a cooking recipe is very far from an algorithm.</p>

<p class="noindent chinese">算法不是任何一组指令：它们必须足够精确和不含糊，才能被计算机执行。例如，一个烹饪食谱不是一个算法，因为它没有确切地规定做事的顺序或每一步的具体内容。一勺糖到底是多少？每个尝试过新菜谱的人都知道，按照它做的结果可能是好吃的东西，也可能是一团糟。相比之下，一个算法总是产生相同的结果。即使一个食谱精确地规定了半盎司的糖，我们仍然没有摆脱困境，因为计算机不知道什么是糖，也不知道一盎司。如果我们想给厨房机器人编程来制作蛋糕，我们就必须告诉它如何从视频中识别糖，如何拿起勺子，等等。（我们仍在努力。）计算机必须知道如何执行算法，一直到打开和关闭特定的晶体管。因此，一个烹饪食谱与一个算法相差甚远。</p>

<p class="noindent english">On the other hand, the following is an algorithm for playing tic-tac-toe:</p>

<p class="noindent chinese">另一方面，下面是一个玩井字游戏的算法。</p>

<div>

<p class="noindent english"><i>If you or your opponent has two in a row, play on the remaining square.</i></p>

<p class="noindent chinese"><i>如果你或你的对手连续有两个，就在剩下的方格中下。</i></p>

<p class="noindent english"><i>Otherwise, if there’s a move that creates two lines of two in a row, play that.</i></p>

<p class="noindent chinese"><i>否则，如果有一步棋可以连续创造出两行两列，就下这一步。</i></p>

<p class="noindent english"><i>Otherwise, if the center square is free, play there.</i></p>

<p class="noindent chinese"><i>否则，如果中间的格子是空的，就在那里下。</i></p>

<p class="noindent english"><i>Otherwise, if your opponent has played in a corner, play in the opposite corner.</i></p>

<p class="noindent chinese"><i>否则，如果你的对手在一个角落下棋，就在对面的角落下棋。</i></p>

<p class="noindent english"><i>Otherwise, if there’s an empty corner, play there.</i></p>

<p class="noindent chinese"><i>否则，如果有一个空角落，就在那里玩。</i></p>

<p class="noindent english"><i>Otherwise, play on any empty square.</i></p>

<p class="noindent chinese"><i>否则，就在任何一个空位上玩。</i></p>

</div>

<p class="noindent english">This algorithm has the nice property that it never loses! Of course, it’s still missing many details, like how the board is represented in the computer’s memory and how this representation is changed by a move. For example, we could have two bits for each square, with the value 00 <a id="babilu_link-42"></a> if the square is empty, which changes to 01 if it has a naught and 10 if it has a cross. But it’s precise and unambiguous enough that any competent programmer could fill in the blanks. It also helps that we don’t really have to specify an algorithm ourselves all the way down to individual transistors; we can use preexisting algorithms as building blocks, and there’s a huge number of them to choose from.</p>

<p class="noindent chinese">这个算法有一个很好的特性，那就是它永远不会输！当然，它仍然缺少许多细节，比如棋盘在计算机内存中是如何表示的，以及这种表示是如何被移动所改变的。例如，我们可以为每个方格设置两个比特，如果方格是空的，则数值为 00 如果有一个无，则变为 01，如果有一个十字，则变为 10。但它足够精确和不含糊，任何有能力的程序员都可以填补这些空白。它还有助于我们不必真正自己指定一个算法，一直到单个晶体管；我们可以使用预先存在的算法作为构建块，而且有大量的算法可供选择。</p>

<p class="noindent english">Algorithms are an exacting standard. It’s often said that you don’t really understand something until you can express it as an algorithm. (As Richard Feynman said, “What I cannot create, I do not understand.”) Equations, the bread and butter of physicists and engineers, are really just a special kind of algorithm. For example, Newton’s second law, arguably the most important equation of all time, tells you to compute the net force on an object by multiplying its mass by its acceleration. It also tells you implicitly that the acceleration is the force divided by the mass, but making that explicit is itself an algorithmic step. In any area of science, if a theory cannot be expressed as an algorithm, it’s not entirely rigorous. (Not to mention you can’t use a computer to solve it, which really limits what you can do with it.) Scientists make theories, and engineers make devices. Computer scientists make algorithms, which are both theories and devices.</p>

<p class="noindent chinese">算法是一个严格的标准。人们常说，只有当你能把某件事情表达为一种算法时，你才真正理解它。（正如理查德·费曼所说，“我不能创造的东西，我就不明白。”）方程，物理学家和工程师的面包和黄油，实际上只是一种特殊的算法。例如，牛顿第二定律，可以说是有史以来最重要的方程，告诉你通过将物体的质量乘以其加速度来计算物体的净力。它还隐含地告诉你，加速度是力除以质量，但明确这一点本身就是一个算法步骤。在任何科学领域，如果一个理论不能被表达为一种算法，它就不是完全严谨的。（更不用说你不能用计算机来解决它，这确实限制了你能用它做什么）。科学家制造理论，而工程师制造设备。计算机科学家制造算法，它既是理论也是设备。</p>

<p class="noindent english">Designing an algorithm is not easy. Pitfalls abound, and nothing can be taken for granted. Some of your intuitions will turn out to have been wrong, and you’ll have to find another way. On top of designing the algorithm, you have to write it down in a language computers can understand, like Java or Python (at which point it’s called a program). Then you have to debug it: find every error and fix it until the computer runs your program without screwing up. But once you have a program that does what you want, you can really go to town. Computers will do your bidding millions of times, at ultrahigh speed, without complaint. Everyone in the world can use your creation. The cost can be zero, if you so choose, or enough to make you a billionaire, if the problem you solved is important enough. A programmer—someone who creates algorithms and codes them up—is a minor god, creating universes at will. You could even say that the God of Genesis himself is a programmer: language, <a id="babilu_link-40"></a> not manipulation, is his tool of creation. Words become worlds. Today, sitting on the couch with your laptop, you too can be a god. Imagine a universe and make it real. The laws of physics are optional.</p>

<p class="noindent chinese">设计一个算法是不容易的。陷阱比比皆是，没有任何东西可以被视为理所当然。你的一些直觉将被证明是错误的，你将不得不找到另一种方法。在设计算法的基础上，你必须用计算机能够理解的语言将其写下来，比如 Java 或 Python（这时它被称为程序）。然后，你必须对它进行调试：找出每一个错误并加以修正，直到计算机在运行你的程序时不会出现问题。但是，一旦你有了一个能做你想做的事的程序，你就可以真正去做了。计算机会以超高的速度为你做几百万次的工作，毫无怨言。世界上的每个人都可以使用你的创造。如果你选择，成本可以是零，或者足以让你成为亿万富翁，如果你解决的问题足够重要。一个程序员 —— 一些创造算法并将其编码的人 —— 是一个小神，随意创造宇宙。你甚至可以说《创世纪》的上帝本身就是一个程序员：语言，而不是操纵，是他创造的工具。言语成为世界。今天，坐在沙发上用你的笔记本电脑，你也可以成为一个神。想象一个宇宙并使之成为现实。物理学定律是可有可无的。</p>

<p class="noindent english">Over time, computer scientists build on each other’s work and invent algorithms for new things. Algorithms combine with other algorithms to use the results of other algorithms, in turn producing results for still more algorithms. Every second, billions of transistors in billions of computers switch billions of times. Algorithms form a new kind of ecosystem—ever growing, comparable in richness only to life itself.</p>

<p class="noindent chinese">随着时间的推移，计算机科学家们在彼此的工作基础上，发明了新事物的算法。算法与其他算法相结合，使用其他算法的结果，反过来为更多的算法产生结果。每一秒钟，数十亿台计算机中的数十亿个晶体管都会切换数十亿次。算法形成了一种新的生态系统 —— 不断增长，其丰富性仅可与生命本身相提并论。</p>

<p class="noindent english">Inevitably, however, there is a serpent in this Eden. It’s called the complexity monster. Like the Hydra, the complexity monster has many heads. One of them is space complexity: the number of bits of information an algorithm needs to store in the computer’s memory. If the algorithm needs more memory than the computer can provide, it’s useless and must be discarded. Then there’s the evil sister, time complexity: how long the algorithm takes to run, that is, how many steps of using and reusing the transistors it has to go through before it produces the desired results. If it’s longer than we can wait, the algorithm is again useless. But the scariest face of the complexity monster is human complexity. When algorithms become too intricate for our poor human brains to understand, when the interactions between different parts of the algorithm are too many and too involved, errors creep in, we can’t find them and fix them, and the algorithm doesn’t do what we want. Even if we somehow make it work, it winds up being needlessly complicated for the people using it and doesn’t play well with other algorithms, storing up trouble for later.</p>

<p class="noindent chinese">然而，不可避免的是，在这个伊甸园里有一条蛇。它被称为复杂性怪物。像九头蛇一样，复杂性怪物有很多头。其中之一是空间复杂性：一个算法需要存储在计算机内存中的信息比特数。如果算法需要的内存超过了计算机所能提供的，它就是无用的，必须被丢弃。然后还有一个邪恶的姐妹，时间复杂度：算法需要运行多长时间，也就是说，在产生预期结果之前，它必须经历多少个使用和重复使用晶体管的步骤。如果它比我们能等待的时间长，那么这个算法又是无用的。但是，复杂性怪物的最可怕的面孔是人类的复杂性。当算法变得过于复杂，我们可怜的人类大脑无法理解时，当算法的不同部分之间的相互作用太多、太复杂时，错误就会悄悄出现，我们无法发现并修复它们，而算法就不会做我们想要的事情。即使我们以某种方式使其工作，它最终也会对使用它的人造成不必要的复杂，并且不能与其他算法很好地配合，为以后储存了麻烦。</p>

<p class="noindent english">Every computer scientist does battle with the complexity monster every day. When computer scientists lose the battle, complexity seeps into our lives. You’ve probably noticed that many a battle has been lost. Nevertheless, we continue to build our tower of algorithms, with greater and greater difficulty. Each new generation of algorithms has to be built on top of the previous ones and has to deal with their complexities in addition to its own. The tower grows taller and taller, and it covers the whole world, but it’s also increasingly fragile, like a house of cards <a id="babilu_link-198"></a> waiting to collapse. One tiny error in an algorithm and a billion-dollar rocket explodes, or the power goes out for millions. Algorithms interact in unexpected ways, and the stock market crashes.</p>

<p class="noindent chinese">每个计算机科学家每天都在与复杂的怪物战斗。当计算机科学家在战斗中失败时，复杂性就会渗入我们的生活。你可能已经注意到，许多战斗已经失败了。尽管如此，我们仍在继续建造我们的算法之塔，而且难度越来越大。每一代新的算法都必须建立在以前的算法之上，除了自身的复杂性外，还必须处理它们的复杂性。这座塔越来越高，覆盖了整个世界，但它也越来越脆弱，就像一座纸牌屋等待着倒塌。算法中的一个微小错误，就会使价值数十亿美元的火箭爆炸，或者使数百万人停电。算法以意想不到的方式相互作用，股票市场就会崩溃。</p>

<p class="noindent english">If programmers are minor gods, the complexity monster is the devil himself. Little by little, it’s winning the war.</p>

<p class="noindent chinese">如果程序员是小神，那么复杂性怪物就是魔鬼本身。渐渐地，它正在赢得这场战争。</p>

<p class="noindent english">There has to be a better way.</p>

<p class="noindent chinese">一定有更好的方法。</p>

<h1 id="babilu_link-380"><b>Enter the learner</b></h1>

<h1 id="babilu_link-380"><b>输入学习者</b></h1>

<p class="noindent english">Every algorithm has an input and an output: the data goes into the computer, the algorithm does what it will with it, and out comes the result. Machine learning turns this around: in goes the data and the desired result and out comes the algorithm that turns one into the other. Learning algorithms—also known as learners—are algorithms that make other algorithms. With machine learning, computers write their own programs, so we don’t have to.</p>

<p class="noindent chinese">每个算法都有一个输入和一个输出：数据进入计算机，算法对其进行处理，然后得出结果。机器学习把这一点扭转过来：输入的是数据和所需的结果，然后得出的是把一个变成另一个的算法。学习算法 —— 也被称为学习者 —— 是制造其他算法的算法。有了机器学习，计算机就可以编写自己的程序，所以我们不必这样做。</p>

<p class="noindent english">Wow.</p>

<p class="noindent chinese">哇！</p>

<p class="noindent english"><i>Computers write their own programs.</i> Now that’s a powerful idea, maybe even a little scary. If computers start to program themselves, how will we control them? Turns out we can control them quite well, as we’ll see. A more immediate objection is that perhaps this sounds too good to be true. Surely writing algorithms requires intelligence, creativity, problem-solving chops—things that computers just don’t have? How is machine learning distinguishable from magic? Indeed, as of today people can write many programs that computers can’t learn. But, more surprisingly, computers can learn programs that people can’t write. We know how to drive cars and decipher handwriting, but these skills are subconscious; we’re not able to explain to a computer how to do these things. If we give a learner a sufficient number of examples of each, however, it will happily figure out how to do them on its own, at which point we can turn it loose. That’s how the post office reads zip codes, and that’s why self-driving cars are on the way.</p>

<p class="noindent chinese"><i>计算机编写自己的程序。</i>现在，这是一个强大的想法，也许甚至有点吓人。如果计算机开始为自己编程，我们将如何控制它们？事实证明，我们可以很好地控制它们，正如我们将看到的。一个更直接的反对意见是，也许这听起来好得不像真的。当然，编写算法需要智慧、创造力和解决问题的能力 —— 这些都是计算机所不具备的？机器学习如何与魔术区分开来？的确，到今天为止，人们可以编写许多计算机无法学习的程序。但是，更令人惊讶的是，计算机可以学习人们写不出的程序。我们知道如何驾驶汽车和破译笔迹，但这些技能是下意识的；我们无法向计算机解释如何做这些事情。然而，如果我们给学习者提供足够数量的例子，它就会很高兴地自己想出如何做这些事情，这时我们就可以把它放开。这就是邮局读取邮政编码的方式，也是自动驾驶汽车即将到来的原因。</p>

<p class="noindent english">The power of machine learning is perhaps best explained by a low-tech analogy: farming. In an industrial society, goods are made in <a id="babilu_link-137"></a> factories, which means that engineers have to figure out exactly how to assemble them from their parts, how to make those parts, and so on—all the way to raw materials. It’s a lot of work. Computers are the most complex goods ever invented, and designing them, the factories that make them, and the programs that run on them is a ton of work. But there’s another, much older way in which we can get some of the things we need: by letting nature make them. In farming, we plant the seeds, make sure they have enough water and nutrients, and reap the grown crops. Why can’t technology be more like this? It can, and that’s the promise of machine learning. Learning algorithms are the seeds, data is the soil, and the learned programs are the grown plants. The machine-learning expert is like a farmer, sowing the seeds, irrigating and fertilizing the soil, and keeping an eye on the health of the crop but otherwise staying out of the way.</p>

<p class="noindent chinese">机器学习的力量也许可以用一个低技术含量的比喻来解释：耕作。在工业社会中，商品是在工厂中制造的，这意味着工程师必须准确地找出如何从零件中组装起来，如何制造这些零件，等等，一直到原材料。这是一项艰巨的工作。计算机是有史以来最复杂的商品，设计它们，制造它们的工厂，以及在它们上面运行的程序都是大量的工作。但还有另一种更古老的方式，我们可以获得一些我们需要的东西：让大自然来制造它们。在农业方面，我们种植种子，确保它们有足够的水和养分，然后收获成长的作物。为什么技术不能更像这样？它可以，这就是机器学习的承诺。学习算法是种子，数据是土壤，而学习的程序是生长的植物。机器学习专家就像一个农民，播种、灌溉和施肥，并关注作物的健康状况，但在其他方面不插手。</p>

<p class="noindent english">Once we look at machine learning this way, two things immediately jump out. The first is that the more data we have, the more we can learn. No data? Nothing to learn. Big data? Lots to learn. That’s why machine learning has been turning up everywhere, driven by exponentially growing mountains of data. If machine learning was something you bought in the supermarket, its carton would say: “Just add data.”</p>

<p class="noindent chinese">一旦我们以这种方式看待机器学习，有两件事就会立即跳出来。第一件事是，我们拥有的数据越多，我们能学到的东西就越多。没有数据？就没有东西可学。大数据？有很多可以学习。这就是为什么在指数级增长的数据山的推动下，机器学习已经到处出现。如果机器学习是你在超市里买的东西，它的包装盒上会写着：“只需添加数据。” </p>

<p class="noindent english">The second thing is that machine learning is a sword with which to slay the complexity monster. Given enough data, a learning program that’s only a few hundred lines long can easily generate a program with millions of lines, and it can do this again and again for different problems. The reduction in complexity for the programmer is phenomenal. Of course, like the Hydra, the complexity monster sprouts new heads as soon as we cut off the old ones, but they start off smaller and take a while to grow, so we still get a big leg up.</p>

<p class="noindent chinese">第二件事是，机器学习是一把剑，可以用来杀死复杂的怪物。给予足够的数据，一个只有几百行的学习程序可以很容易地生成一个有几百万行的程序，而且它可以针对不同的问题反复地这样做。对程序员来说，复杂性的降低是惊人的。当然，就像九头蛇一样，一旦我们砍掉旧的头，复杂度怪物就会萌生新的头，但它们开始时比较小，需要一段时间才能长大，所以我们仍然可以获得很大的优势。</p>

<p class="noindent english">We can think of machine learning as the inverse of programming, in the same way that the square root is the inverse of the square, or integration is the inverse of differentiation. Just as we can ask “What number squared gives 16?” or “What is the function whose derivative is <i>x</i> + 1?” we can ask, “What is the algorithm that produces this output?” <a id="babilu_link-33"></a> We will soon see how to turn this insight into concrete learning algorithms.</p>

<p class="noindent chinese">我们可以把机器学习看作是编程的逆过程，就像平方根是平方的逆过程，或者积分是微分的逆过程一样。正如我们可以问 “什么数字的平方可以得到 16？” 或 “导数为 <i>x</i> + 1 的函数是什么？” 我们可以问，“产生这个输出的算法是什么？”我们很快就会看到如何将这种洞察力转化为具体的学习算法。</p>

<p class="noindent english">Some learners learn knowledge, and some learn skills. “All humans are mortal” is a piece of knowledge. Riding a bicycle is a skill. In machine learning, knowledge is often in the form of statistical models, because most knowledge is statistical: all humans are mortal, but only 4 percent are Americans. Skills are often in the form of procedures: if the road curves left, turn the wheel left; if a deer jumps in front of you, slam on the brakes. (Unfortunately, as of this writing Google’s self-driving cars still confuse windblown plastic bags with deer.) Often, the procedures are quite simple, and it’s the knowledge at their core that’s complex. If you can tell which e-mails are spam, you know which ones to delete. If you can tell how good a board position in chess is, you know which move to make (the one that leads to the best position).</p>

<p class="noindent chinese">有些学习者学习知识，有些学习技能。“人类都是凡人” 是一种知识。骑自行车是一种技能。在机器学习中，知识通常以统计模型的形式出现，因为大多数知识都是统计学的：所有的人都是凡人，但只有 4% 是美国人。技能往往是程序的形式：如果道路向左弯曲，就向左打轮；如果有鹿跳到你面前，就踩下刹车。（不幸的是，截至本文写作时，谷歌的自动驾驶汽车仍将被风吹起的塑料袋与鹿混淆。）通常情况下，这些程序是非常简单的，而其核心的知识才是复杂的。如果你能分辨出哪些邮件是垃圾邮件，你就知道该删除哪些邮件。如果你能知道国际象棋中的一个棋盘位置有多好，你就知道该走哪一步（导致最佳位置的那一步）。</p>

<p class="noindent english">Machine learning takes many different forms and goes by many different names: pattern recognition, statistical modeling, data mining, knowledge discovery, predictive analytics, data science, adaptive systems, self-organizing systems, and more. Each of these is used by different communities and has different associations. Some have a long half-life, some less so. In this book I use the term <i>machine learning</i> to refer broadly to all of them.</p>

<p class="noindent chinese">机器学习有许多不同的形式，也有许多不同的名字：模式识别、统计建模、数据挖掘、知识发现、预测分析、数据科学、适应性系统、自组织系统等等。其中每一个都被不同的社区所使用，并有不同的关联。有些有很长的半衰期，有些则不那么长。在本书中，我用<i>机器学习</i>这个词来广义地指代所有这些东西。</p>

<p class="noindent english">Machine learning is sometimes confused with artificial intelligence (or AI for short). Technically, machine learning is a subfield of AI, but it’s grown so large and successful that it now eclipses its proud parent. The goal of AI is to teach computers to do what humans currently do better, and learning is arguably the most important of those things: without it, no computer can keep up with a human for long; with it, the rest follows.</p>

<p class="noindent chinese">机器学习有时会与人工智能（简称 AI）相混淆。从技术上讲，机器学习是人工智能的一个子领域，但它已经发展得如此庞大和成功，以至于现在让它引以为傲的母体黯然失色。人工智能的目标是教计算机做人类目前做得更好的事情，而学习可以说是这些事情中最重要的：没有它，没有计算机可以长期跟上人类的步伐；有了它，其他的事情也就随之而来。</p>

<p class="noindent english">In the information-processing ecosystem, learners are the superpredators. Databases, crawlers, indexers, and so on are the herbivores, patiently munging on endless fields of data. Statistical algorithms, online analytical processing, and so on are the predators. Herbivores are necessary, since without them the others couldn’t exist, but superpredators have a more exciting life. A crawler is like a cow, the web is its worldwide <a id="babilu_link-224"></a> meadow, each page is a blade of grass. When the crawler is done munging, a copy of the web is sitting on its hard disks. An indexer then makes a list of the pages where each word appears, much like the index at the end of a book. Databases, like elephants, are big and heavy and never forget. Among these patient beasts dart statistical and analytical algorithms, compacting and selecting, turning data into information. Learners eat up this information, digest it, and turn it into knowledge.</p>

<p class="noindent chinese">在信息处理的生态系统中，学习者是超级掠食者。数据库、爬虫、索引器等等是食草动物，它们耐心地在无尽的数据领域中耕耘。统计算法、在线分析处理等则是食肉动物。食草动物是必要的，因为没有它们，其他的就不可能存在，但超级捕食者的生活更令人兴奋。爬虫就像一头牛，网络是它的全球每一页都是一片草叶。当爬虫完成采摘后，它的硬盘上就会有一份网络的副本。然后，索引器将每个词出现的页面列出来，很像书末的索引。数据库，像大象一样，又大又重，而且永远不会忘记。在这些耐心的野兽中，统计和分析算法飞速运转，压缩和选择，将数据变成信息。学习者吃掉这些信息，消化它，并把它变成知识。</p>

<p class="noindent english">Machine-learning experts (aka machine learners) are an elite priesthood even among computer scientists. Many computer scientists, particularly those of an older generation, don’t understand machine learning as well as they’d like to. This is because computer science has traditionally been all about thinking deterministically, but machine learning requires thinking statistically. If a rule for, say, labeling e-mails as spam is 99 percent accurate, that does not mean it’s buggy; it may be the best you can do and good enough to be useful. This difference in thinking is a large part of why Microsoft has had a lot more trouble catching up with Google than it did with Netscape. At the end of the day, a browser is just a standard piece of software, but a search engine requires a different mind-set.</p>

<p class="noindent chinese">机器学习专家（又称机器学习者）即使在计算机科学家中也是一个精英神职。许多计算机科学家，特别是那些老一代的计算机科学家，并不像他们希望的那样了解机器学习。这是因为计算机科学传统上都是确定性思维，但机器学习需要统计性思维。如果一个规则，比如说，将电子邮件标记为垃圾邮件的准确率为 99%，这并不意味着它有问题；它可能是你能做的最好的，而且好到可以使用。这种思维上的差异是微软在追赶谷歌时比追赶网景时困难得多的很大一部分原因。归根结底，浏览器只是一个标准的软件，但搜索引擎需要不同的思维方式。</p>

<p class="noindent english">The other reason machine learners are the über-geeks is that the world has far fewer of them than it needs, even by the already dire standards of computer science. According to tech guru Tim O’Reilly, “data scientist” is the hottest job title in Silicon Valley. The McKinsey Global Institute estimates that by 2018 the United States alone will need 140,000 to 190,000 more machine-learning experts than will be available, and 1.5 million more data-savvy managers. Machine learning’s applications have exploded too suddenly for education to keep up, and it has a reputation for being a difficult subject. Textbooks are liable to give you math indigestion. This difficulty is more apparent than real, however. All of the important ideas in machine learning can be expressed math-free. As you read this book, you may even find yourself inventing your own learning algorithms, with nary an equation in sight.</p>

<p class="noindent chinese">机器学习者是超凡脱俗的另一个原因是，世界上的机器学习者远远少于它的需求，即使按照计算机科学已经很糟糕的标准，也是如此。根据科技大师蒂姆·奥莱利的说法，“数据科学家” 是硅谷最热门的工作头衔。麦肯锡全球研究所估计，到 2018 年，仅美国就需要 14 万至 19 万名机器学习专家，比现有的多出 150 万名精通数据的管理人员。机器学习的应用爆发得太突然了，以至于教育无法跟上，而且它被誉为是一个困难的学科。教科书很可能会让你在数学上消化不良。然而，这种困难是显而易见的，而不是真实的。机器学习中的所有重要观点都可以用无数学的方式表达。当你阅读这本书时，你甚至会发现自己在发明自己的学习算法，而没有看到一个方程式。</p>

<p class="noindent english">The Industrial Revolution automated manual work and the Information Revolution did the same for mental work, but machine learning <a id="babilu_link-94"></a> automates automation itself. Without it, programmers become the bottleneck holding up progress. With it, the pace of progress picks up. If you’re a lazy and not-too-bright computer scientist, machine learning is the ideal occupation, because learning algorithms do all the work but let you take all the credit. On the other hand, learning algorithms could put us out of our jobs, which would only be poetic justice.</p>

<p class="noindent chinese">工业革命使手工劳动自动化，信息革命对脑力劳动也是如此，但机器学习使自动化本身自动化。没有它，程序员就成为阻碍进步的瓶颈。有了它，进步的步伐就会加快。如果你是一个懒惰和不太聪明的计算机科学家，机器学习是理想的职业，因为学习算法做所有的工作，但让你承担所有的功劳。另一方面，学习算法可能会让我们失业，这只会是诗意的正义。</p>

<p class="noindent english">By taking automation to new heights, the machine-learning revolution will cause extensive economic and social changes, just as the Internet, the personal computer, the automobile, and the steam engine did in their time. One area where these changes are already apparent is business.</p>

<p class="noindent chinese">通过将自动化提升到新的高度，机器学习革命将引起广泛的经济和社会变化，就像互联网、个人电脑、汽车和蒸汽机在其时代所做的那样。这些变化已经显现的一个领域是商业。</p>

<h1 id="babilu_link-381"><b>Why businesses embrace machine learning</b></h1>

<h1 id="babilu_link-381"><b>企业为何拥抱机器学习</b></h1>

<p class="noindent english">Why is Google worth so much more than Yahoo? They both make their money from showing ads on the web, and they’re both top destinations. Both use auctions to sell ads and machine learning to predict how likely a user is to click on an ad (the higher the probability, the more valuable the ad). But Google’s learning algorithms are much better than Yahoo’s. This is not the only reason for the difference in their market caps, of course, but it’s a big one. Every predicted click that doesn’t happen is a wasted opportunity for the advertiser and lost revenue for the website. With Google’s annual revenue of $50 billion, every 1 percent improvement in click prediction potentially means another half billion dollars in the bank, every year, for the company. No wonder Google is a big fan of machine learning, and Yahoo and others are trying hard to catch up.</p>

<p class="noindent chinese">为什么谷歌的价值比雅虎高那么多？它们都通过在网络上展示广告来赚钱，而且都是顶级的目的地。两者都使用拍卖来销售广告，并使用机器学习来预测用户点击广告的可能性（可能性越高，广告的价值越大）。但谷歌的学习算法要比雅虎的好得多。当然，这不是它们市值差异的唯一原因，但这是一个重要原因。每一次预测的点击没有发生，对广告商来说就是浪费了机会，对网站来说就是损失了收入。谷歌的年收入为 500 亿美元，点击预测每提高 1%，就可能意味着该公司每年又有 5 亿美元的收入。难怪谷歌是机器学习的忠实粉丝，而雅虎和其他公司正在努力追赶。</p>

<p class="noindent english">Web advertising is just one manifestation of a much larger phenomenon. In every market, producers and consumers need to connect before a transaction can happen. In pre-Internet days, the main obstacles to this were physical. You could only buy books from your local bookstore, and your local bookstore had limited shelf space. But when you can download any book to your e-reader any time, the problem becomes the overwhelming number of choices. How do you browse the shelves of a bookstore that has millions of titles for sale? The same goes <a id="babilu_link-52"></a> for other information goods: videos, music, news, tweets, blogs, plain old web pages. It also goes for every product and service that can be procured remotely: shoes, flowers, gadgets, hotel rooms, tutoring, investments. It even applies to people looking for a job or a date. How do you find each other? This is the defining problem of the Information Age, and machine learning is a big part of the solution.</p>

<p class="noindent chinese">网络广告只是一个更大的现象的一种表现形式。在每个市场上，生产者和消费者都需要在交易发生之前进行联系。在互联网之前的时代，实现这一目标的主要障碍是实物。你只能从当地书店买书，而当地书店的书架空间有限。但是，当你可以随时将任何书籍下载到你的电子阅读器时，问题就变成了压倒性的选择数量。你如何浏览一个有数百万种书出售的书店的书架？其他信息商品也是如此，：视频、音乐、新闻、推特、博客、普通网页。这也适用于所有可以远程采购的产品和服务：鞋子、鲜花、小工具、酒店房间、家教、投资。它甚至适用于寻找工作或约会的人。你们如何找到对方？这是信息时代的决定性问题，而机器学习是解决方案的一个重要部分。</p>

<p class="noindent english">As companies grow, they go through three phases. First, they do everything manually: the owners of a mom-and-pop store personally know their customers, and they order, display, and recommend items accordingly. This is nice, but it doesn’t scale. In the second and least happy phase, the company grows large enough that it needs to use computers. In come the programmers, consultants, and database managers, and millions of lines of code get written to automate all the functions of the company that can be automated. Many more people are served, but not as well: decisions are made based on coarse demographic categories, and computer programs are too rigid to match humans’ infinite versatility.</p>

<p class="noindent chinese">随着公司的发展，他们经历了三个阶段。首先，他们手动做所有的事情：一家母婴店的店主亲自了解他们的顾客，他们相应地订购、展示和推荐商品。这很好，但它没有规模。在第二个也是最不快乐的阶段，公司发展到足够大，需要使用计算机。程序员、顾问和数据库管理员都来了，数以百万计的代码被编写出来，以实现公司所有可以自动化的功能的自动化。更多的人得到了服务，但却不尽如人意：决策是基于粗略的人口统计学分类，而计算机程序过于僵化，无法与人类的无限多面性相匹配。</p>

<p class="noindent english">After a point, there just aren’t enough programmers and consultants to do all that’s needed, and the company inevitably turns to machine learning. Amazon can’t neatly encode the tastes of all its customers in a computer program, and Facebook doesn’t know how to write a program that will choose the best updates to show to each of its users. Walmart sells millions of products and has billions of choices to make every day; if the programmers at Walmart tried to write a program to make all of them, they would never be done. Instead, what these companies do is turn learning algorithms loose on the mountains of data they’ve accumulated and let them divine what customers want.</p>

<p class="noindent chinese">到了一定程度后，就是没有足够的程序员和顾问来做所有需要的事情，公司不可避免地转向机器学习。亚马逊无法将其所有客户的口味整齐地编码在一个计算机程序中，而 Facebook 也不知道如何编写一个程序，以选择最佳的更新来展示给每个用户。沃尔玛销售数百万种产品，每天有数十亿种选择；如果沃尔玛的程序员试图写一个程序来做所有的选择，他们将永远无法完成。相反，这些公司所做的是把学习算法放在他们所积累的大量数据上，让它们来决定客户想要什么。</p>

<p class="noindent english">Learning algorithms are the matchmakers: they find producers and consumers for each other, cutting through the information overload. If they’re smart enough, you get the best of both worlds: the vast choice and low cost of the large scale, with the personalized touch of the small. Learners are not perfect, and the last step of the decision is usually still for humans to make, but learners intelligently reduce the choices to something a human can manage.</p>

<p class="noindent chinese">学习算法是媒人：他们为对方找到生产者和消费者，穿过信息过载。如果他们足够聪明，你就能得到两个世界的最好结果：大规模的大量选择和低成本，以及小规模的个性化触摸。学习者并不完美，最后一步的决定通常还是由人类来做，但学习者智能地将选择减少到人类可以管理的程度。</p>

<p class="noindent english"><a id="babilu_link-53"></a> In retrospect, we can see that the progression from computers to the Internet to machine learning was inevitable: computers enable the Internet, which creates a flood of data and the problem of limitless choice; and machine learning uses the flood of data to help solve the limitless choice problem. The Internet by itself is not enough to move demand from “one size fits all” to the long tail of infinite variety. Netflix may have one hundred thousand DVD titles in stock, but if customers don’t know how to find the ones they like, they will default to choosing the hits. It’s only when Netflix has a learning algorithm to figure out your tastes and recommend DVDs that the long tail really takes off.</p>

<p class="noindent chinese">回过头来，我们可以看到，从计算机到互联网再到机器学习的进展是不可避免的：计算机使互联网成为可能，而互联网创造了大量的数据和无限选择的问题；机器学习利用大量的数据帮助解决无限选择的问题。互联网本身并不足以将需求从 “一刀切” 转向无限种类的长尾。Netflix 可能有十万种 DVD 的库存，但如果客户不知道如何找到他们喜欢的，他们会默认选择热门的。只有当 Netflix 有一个学习算法来弄清你的口味并推荐 DVD 时，长尾才会真正起飞。</p>

<p class="noindent english">Once the inevitable happens and learning algorithms become the middlemen, power becomes concentrated in them. Google’s algorithms largely determine what information you find, Amazon’s what products you buy, and Match.com’s who you date. The last mile is still yours—choosing from among the options the algorithms present you with—but 99.9 percent of the selection was done by them. The success or failure of a company now depends on how much the learners like its products, and the success of a whole economy—whether everyone gets the best products for their needs at the best price—depends on how good the learners are.</p>

<p class="noindent chinese">一旦不可避免地发生，学习算法成为中间人，权力就会集中在它们身上。谷歌的算法在很大程度上决定了你找到什么信息，亚马逊的算法决定了你购买什么产品，Match.com 决定了你和谁约会。最后一公里仍然是你的 —— 从算法呈现给你的选项中选择，但 99.9% 的选择是由它们完成的。现在，一家公司的成败取决于学习者对其产品的喜爱程度，而整个经济的成功 —— 每个人是否能以最优惠的价格获得符合其需求的最佳产品 —— 则取决于学习者的水平。</p>

<p class="noindent english">The best way for a company to ensure that learners like its products is to run them itself. Whoever has the best algorithms and the most data wins. A new type of network effect takes hold: whoever has the most customers accumulates the most data, learns the best models, wins the most new customers, and so on in a virtuous circle (or a vicious one, if you’re the competition). Switching from Google to Bing may be easier than switching from Windows to Mac, but in practice you don’t because Google, with its head start and larger market share, knows better what you want, even if Bing’s technology is just as good. And pity a new entrant into the search business, starting with zero data against engines with over a decade of learning behind them.</p>

<p class="noindent chinese">公司确保学习者喜欢其产品的最好方法是自己运行它们。谁拥有最好的算法和最多的数据，谁就赢了。一种新型的网络效应开始发挥作用：谁拥有最多的客户，谁就能积累最多的数据，学到最好的模型，赢得最多的新客户，如此循环往复，形成良性循环（如果你是竞争对手，则是恶性循环）。从谷歌转到必应可能比从 Windows 转到 Mac 更容易，但实际上你不会这样做，因为谷歌凭借其先发优势和更大的市场份额，更了解你想要什么，即使必应的技术同样出色。而可怜的是，一个新进入搜索行业的人，从零数据开始，与背后有超过十年学习经验的引擎对抗。</p>

<p class="noindent english">You might think that after a while more data is just more of the same, but that saturation point is nowhere in sight. The long tail keeps going. If you look at the recommendations Amazon or Netflix gives you, it’s <a id="babilu_link-222"></a> clear they’re still very crude, and Google’s search results still leave a lot to be desired. Every feature of a product, every corner of a website can potentially be improved using machine learning. Should the link at the bottom of a page be red or blue? Try them both and see which one gets the most clicks. Better still, keep the learners running and continuously adjust all aspects of the website.</p>

<p class="noindent chinese">你可能会认为，一段时间后，更多的数据只是更多的相同，但饱和点还没有看到。长尾效应一直在持续。如果你看看亚马逊或 Netflix 给你的推荐，很明显它们仍然非常粗糙，而谷歌的搜索结果仍然有很多需要改进的地方。一个产品的每一个功能，一个网站的每一个角落都有可能使用机器学习来改进。一个页面底部的链接应该是红色还是蓝色？两者都试试，看看哪一个能得到最多的点击。更好的是，保持学习者的运行，不断调整网站的各个方面。</p>

<p class="noindent english">The same dynamic happens in any market where there’s lots of choice and lots of data. The race is on, and whoever learns fastest wins. It doesn’t stop with understanding customers better: companies can apply machine learning to every aspect of their operations, provided data is available, and data is pouring in from computers, communication devices, and ever-cheaper and more ubiquitous sensors. “Data is the new oil” is a popular refrain, and as with oil, refining it is big business. IBM, as well plugged into the corporate world as anyone, has organized its growth strategy around providing analytics to companies. Businesses look at data as a strategic asset: What data do I have that my competitors don’t? How can I take advantage of it? What data do my competitors have that I don’t?</p>

<p class="noindent chinese">同样的动态发生在任何有大量选择和大量数据的市场。比赛开始了，谁学得最快，谁就赢了。它并不局限于更好地了解客户：只要有数据，公司就可以将机器学习应用于其业务的各个方面，而数据正从计算机、通信设备以及越来越便宜和越来越普遍的传感器中涌入。“数据是新的石油” 是一个流行的说法，就像石油一样，提炼它是大生意。IBM 公司和其他人一样融入企业界，围绕着为公司提供分析服务制定了其发展战略。企业将数据视为一种战略资产：我有哪些数据是我的竞争对手没有的？我怎样才能利用它？我的竞争对手有什么数据是我没有的？</p>

<p class="noindent english">In the same way that a bank without databases can’t compete with a bank that has them, a company without machine learning can’t keep up with one that uses it. While the first company’s experts write a thousand rules to predict what its customers want, the second company’s algorithms learn billions of rules, a whole set of them for each individual customer. It’s about as fair as spears against machine guns. Machine learning is a cool new technology, but that’s not why businesses embrace it. They embrace it because they have no choice.</p>

<p class="noindent chinese">就像没有数据库的银行无法与拥有数据库的银行竞争一样，没有机器学习的公司也无法跟上使用机器学习的公司。第一家公司的专家写了一千条规则来预测其客户的需求，而第二家公司的算法则学习了数十亿条规则，为每个客户制定了一整套的规则。这就像长矛对机关枪一样不公平。机器学习是一项很酷的新技术，但这并不是企业拥抱它的原因。他们拥抱它是因为他们别无选择。</p>

<h1 id="babilu_link-382"><b>Supercharging the scientific method</b></h1>

<h1 id="babilu_link-382"><b>对科学方法进行增压</b></h1>

<p class="noindent english">Machine learning is the scientific method on steroids. It follows the same process of generating, testing, and discarding or refining hypotheses. But while a scientist may spend his or her whole life coming up with and testing a few hundred hypotheses, a machine-learning system can do the same in a fraction of a second. Machine learning automates <a id="babilu_link-54"></a> discovery. It’s no surprise, then, that it’s revolutionizing science as much as it’s revolutionizing business.</p>

<p class="noindent chinese">机器学习是类固醇的科学方法。它遵循同样的生成、测试、放弃或改进假设的过程。但是，当一个科学家可能花费他或她的一生来提出和测试几百个假设时，机器学习系统可以在几分之一秒内完成同样的工作。机器学习使发现自动化。因此，它对科学的革命性影响和对商业的革命性影响一样大，这并不奇怪。</p>

<p class="noindent english">To make progress, every field of science needs to have data commensurate with the complexity of the phenomena it studies. This is why physics was the first science to take off: Tycho Brahe’s recordings of the positions of the planets and Galileo’s observations of pendulums and inclined planes were enough to infer Newton’s laws. It’s also why molecular biology, despite being younger than neuroscience, has outpaced it: DNA microarrays and high-throughput sequencing provide a volume of data that neuroscientists can only hope for. And it’s the reason why social science research is such an uphill battle: if all you have is a sample of a hundred people, with a dozen measurements apiece, all you can model is some very narrow phenomenon. But even this narrow phenomenon does not exist in isolation; it’s affected by a myriad others, which means you’re still far from understanding it.</p>

<p class="noindent chinese">为了取得进展，每个科学领域都需要有与其研究的现象的复杂性相称的数据。这就是为什么物理学是第一个起飞的科学。第谷·布拉赫对行星位置的记录和伽利略对钟摆和斜面的观察，足以推断出牛顿定律。这也是为什么分子生物学尽管比神经科学年轻，但已经超过了它。DNA 微阵列和高通量测序提供的数据量是神经科学家们只能望其项背的。这也是社会科学研究如此艰难的原因：如果你所拥有的只是一百个人的样本，每个人有十几个测量值，你所能模拟的就是一些非常狭窄的现象。但即使是这种狭窄的现象也不是孤立存在的；它受到无数其他现象的影响，这意味着你仍然远远没有理解它。</p>

<p class="noindent english">The good news today is that sciences that were once data-poor are now data-rich. Instead of paying fifty bleary-eyed undergraduates to perform some task in the lab, psychologists can get as many subjects as they want by posting the task on Amazon’s Mechanical Turk. (It makes for a more diverse sample too.) It’s getting hard to remember, but little more than a decade ago sociologists studying social networks lamented that they couldn’t get their hands on a network with more than a few hundred members. Now there’s Facebook, with over a billion. A good chunk of those members post almost blow-by-blow accounts of their lives too; it’s like having a live feed of social life on planet Earth. In neuroscience, connectomics and functional magnetic resonance imaging have opened an extraordinarily detailed window into the brain. In molecular biology, databases of genes and proteins grow exponentially. Even in “older” sciences like physics and astronomy, progress continues because of the flood of data pouring forth from particle accelerators and digital sky surveys.</p>

<p class="noindent chinese">今天的好消息是，曾经数据匮乏的科学现在数据丰富了。心理学家不需要花钱请 50 个眼花缭乱的本科生在实验室里执行某种任务，而是在亚马逊的 Mechanical Turk 上发布任务，就可以得到他们想要的许多研究对象（这也使得样本更加多样化）。现在已经很难记得了，但十多年前，研究社会网络的社会学家们哀叹他们无法掌握一个拥有几百个成员的网络。现在有了 Facebook，它有超过 10 亿人。这些成员中的很大一部分也会发布他们的生活，这就像拥有地球上社会生活的现场直播。在神经科学领域，连接组学和功能磁共振成像为了解大脑打开了一扇非常详细的窗口。在分子生物学中，基因和蛋白质的数据库成倍增长。即使在像物理学和天文学这样的 “老” 科学中，由于从粒子加速器和数字天空调查中涌出的大量数据，进步也在继续。</p>

<p class="noindent english">Big data is no use if you can’t turn it into knowledge, however, and there aren’t enough scientists in the world for the task. Edwin Hubble discovered new galaxies by poring over photographic plates, but you can <a id="babilu_link-140"></a> bet the half-billion sky objects in the Sloan Digital Sky Survey weren’t identified that way. It would be like trying to count the grains of sand on a beach by hand. You can write rules to distinguish galaxies from stars from noise objects (such as birds, planes, Superman), but they’re not very accurate. Instead, the SKICAT (sky image cataloging and analysis tool) project used a learning algorithm. Starting from plates where objects were labeled with the correct categories, it figured out what characterizes each one and applied the result to all the unlabeled plates. Even better, it could classify objects that were too faint for humans to label, and these comprise the majority of the survey.</p>

<p class="noindent chinese">然而，如果你不能将其转化为知识，大数据是没有用的，而且世界上没有足够的科学家来完成这项任务。埃德温·哈勃通过仔细研究摄影板发现了新的星系，但是你可以斯隆数字天空调查中的 5 亿个天空物体并不是以这种方式确定的。这就像试图用手去数沙滩上的沙粒一样。你可以写一些规则来区分星系和恒星与噪音物体（如鸟类、飞机、超人），但它们并不十分准确。相反，SKICAT（天空图像编目和分析工具）项目使用了一种学习算法。从物体被标记为正确类别的板块开始，它找出每个物体的特征，并将结果应用于所有未标记的板块。更妙的是，它可以对那些人类无法标记的物体进行分类，而这些物体占了调查的大部分。</p>

<p class="noindent english">With big data and machine learning, you can understand much more complex phenomena than before. In most fields, scientists have traditionally used only very limited kinds of models, like linear regression, where the curve you fit to the data is always a straight line. Unfortunately, most phenomena in the world are nonlinear. (Or fortunately, since otherwise life would be very boring—in fact, there would be no life.) Machine learning opens up a vast new world of nonlinear models. It’s like turning on the lights in a room where only a sliver of moonlight filtered before.</p>

<p class="noindent chinese">通过大数据和机器学习，你可以理解比以前复杂得多的现象。在大多数领域，科学家们传统上只使用非常有限的几种模型，比如线性回归，你拟合到数据上的曲线总是一条直线。不幸的是，世界上的大多数现象都是非线性的。（或者说是幸运的，因为否则生活会非常无聊 —— 事实上，就没有生命了）。机器学习为非线性模型打开了一个广阔的新世界。这就像在一个之前只有一丝月光过滤的房间里打开灯。</p>

<p class="noindent english">In biology, learning algorithms figure out where genes are located in a DNA molecule, where superfluous bits of RNA get spliced out before proteins are synthesized, how proteins fold into their characteristic shapes, and how different conditions affect the expression of different genes. Rather than testing thousands of new drugs in the lab, learners predict whether they will work, and only the most promising get tested. They also weed out molecules likely to have nasty side effects, like cancer. This avoids expensive failures, like candidate drugs being nixed only after human trials have begun.</p>

<p class="noindent chinese">在生物学中，学习算法找出基因在 DNA 分子中的位置，在合成蛋白质之前，RNA 的多余部分被剪掉，蛋白质如何折叠成其特有的形状，以及不同条件如何影响不同基因的表达。学习者不是在实验室里测试数以千计的新药，而是预测它们是否会起作用，只有最有希望的才会被测试。他们还剔除了可能会产生讨厌的副作用的分子，如癌症。这就避免了昂贵的失败，比如候选药物在人体试验开始后才被淘汰。</p>

<p class="noindent english">The biggest challenge, however, is assembling all this information into a coherent whole. What are all the things that affect your risk of heart disease, and how do they interact? All Newton needed was three laws of motion and one of gravitation, but a complete model of a cell, an organism, or a society is more than any one human can discover. As knowledge grows, scientists specialize ever more narrowly, but no one <a id="babilu_link-30"></a> is able to put the pieces together because there are far too many pieces. Scientists collaborate, but language is a very slow medium of communication. Scientists try to keep up with others’ research, but the volume of publications is so high that they fall farther and farther behind. Often, redoing an experiment is easier than finding the paper that reported it. Machine learning comes to the rescue, scouring the literature for relevant information, translating one area’s jargon into another’s, and even making connections that scientists weren’t aware of. Increasingly, machine learning acts as a giant hub, through which modeling techniques invented in one field make their way into others.</p>

<p class="noindent chinese">然而，最大的挑战是将所有这些信息组合成一个连贯的整体。影响你的心脏病风险的所有东西是什么，它们是如何相互作用的？牛顿只需要三个运动定律和一个万有引力定律，但一个完整的细胞、有机体或社会的模型是任何一个人都无法发现的。随着知识的增长，科学家们的专业范围越来越窄，但没有一个人因为有太多的碎片，所以能把这些碎片拼在一起。科学家们进行合作，但语言是一种非常缓慢的交流媒介。科学家们试图跟上别人的研究，但出版物的数量如此之多，以至于他们越来越落伍。通常情况下，重做一个实验比找到报道它的论文更容易。机器学习来了，它在文献中搜寻相关信息，将一个领域的行话翻译成另一个领域的行话，甚至建立科学家们没有意识到的联系。越来越多地，机器学习充当了一个巨大的枢纽，通过它，一个领域发明的建模技术可以进入其他领域。</p>

<p class="noindent english">If computers hadn’t been invented, science would have ground to a halt in the second half of the twentieth century. This might not have been immediately apparent to the scientists because they would have been focused on whatever limited progress they could still make, but the ceiling for that progress would have been much, much lower. Similarly, without machine learning, many sciences would face diminishing returns in the decades to come.</p>

<p class="noindent chinese">如果没有计算机的发明，科学在二十世纪下半叶就会停滞不前。科学家们可能不会立即意识到这一点，因为他们会专注于他们仍然可以取得的有限进展，但这种进展的上限会低很多很多。同样，如果没有机器学习，许多科学将在未来几十年内面临收益递减。</p>

<p class="noindent english">To see the future of science, take a peek inside a lab at the Manchester Institute of Biotechnology, where a robot by the name of Adam is hard at work figuring out which genes encode which enzymes in yeast. Adam has a model of yeast metabolism and general knowledge of genes and proteins. It makes hypotheses, designs experiments to test them, physically carries them out, analyzes the results, and comes up with new hypotheses until it’s satisfied. Today, human scientists still independently check Adam’s conclusions before they believe them, but tomorrow they’ll leave it to robot scientists to check each other’s hypotheses.</p>

<p class="noindent chinese">要想看到科学的未来，请窥视一下曼彻斯特生物技术研究所的一个实验室，那里有一个名为亚当的机器人正在努力工作，找出哪些基因在酵母中编码哪些酶。亚当拥有一个酵母新陈代谢的模型以及基因和蛋白质的一般知识。它提出假设，设计实验来测试它们，实际执行它们，分析结果，并提出新的假设，直到它满意为止。今天，人类科学家在相信亚当的结论之前仍然独立地检查它们，但明天他们将把它留给机器人科学家来检查彼此的假设。</p>

<h1 id="babilu_link-383"><b>A billion Bill Clintons</b></h1>

<h1 id="babilu_link-383"><b>十亿个比尔·克林顿</b></h1>

<p class="noindent english">Machine learning was the kingmaker in the 2012 presidential election. The factors that usually decide presidential elections—the economy, likability of the candidates, and so on—added up to a wash, and the outcome came down to a few key swing states. Mitt Romney’s campaign <a id="babilu_link-260"></a> followed a conventional polling approach, grouping voters into broad categories and targeting each one or not. Neil Newhouse, Romney’s pollster, said that “if we can win independents in Ohio, we can win this race.” Romney won them by 7 percent but still lost the state and the election.</p>

<p class="noindent chinese">在 2012 年的总统选举中，机器学习成为了决定性的因素。通常决定总统选举的因素 —— 经济、候选人的好感度等等 —— 加起来就像一场洗牌，结果只剩下几个关键的摇摆州。米特·罗姆尼的竞选团队遵循传统的民意调查方法，将选民分为大类，并针对每一类选民或不针对每一类。罗姆尼的民调专家尼尔·纽豪斯说，“如果我们能在俄亥俄州赢得独立人士，我们就能赢得这场比赛”。罗姆尼以 7% 的比例赢得了他们，但仍然输掉了该州和选举。</p>

<p class="noindent english">In contrast, President Obama hired Rayid Ghani, a machine-learning expert, as chief scientist of his campaign, and Ghani proceeded to put together the greatest analytics operation in the history of politics. They consolidated all voter information into a single database; combined it with what they could get from social networking, marketing, and other sources; and set about predicting four things for each individual voter: how likely he or she was to support Obama, show up at the polls, respond to the campaign’s reminders to do so, and change his or her mind about the election based on a conversation about a specific issue. Based on these voter models, every night the campaign ran 66,000 simulations of the election and used the results to direct its army of volunteers: whom to call, which doors to knock on, what to say.</p>

<p class="noindent chinese">相比之下，奥巴马总统聘请了机器学习专家雷伊德·加尼作为其竞选团队的首席科学家，加尼着手组建了政治史上最伟大的分析行动。他们将所有选民信息整合到一个数据库中；将其与他们可以从社交网络、营销和其他来源获得的信息结合起来；并着手预测每个选民的四件事：他或她支持奥巴马的可能性有多大，在投票时出现，对竞选活动的提醒作出回应，以及根据关于某个具体问题的谈话改变对选举的看法。基于这些选民模型，竞选团队每天晚上都会进行 66,000 次选举模拟，并利用模拟结果来指导其志愿者大军：给谁打电话，敲哪扇门，说什么。</p>

<p class="noindent english">In politics, as in business and war, there is nothing worse than seeing your opponent make moves that you don’t understand and don’t know what to do about until it’s too late. That’s what happened to the Romney campaign. They could see the other side buying ads in particular cable stations in particular towns but couldn’t tell why; their crystal ball was too fuzzy. In the end, Obama won every battleground state save North Carolina and by larger margins than even the most accurate pollsters had predicted. The most accurate pollsters, in turn, were the ones (like Nate Silver) who used the most sophisticated prediction techniques; they were less accurate than the Obama campaign because they had fewer resources. But they were a lot more accurate than the traditional pundits, whose predictions were based on their expertise.</p>

<p class="noindent chinese">在政治上，就像在商业和战争中一样，没有什么比看到你的对手做出你不理解的举动更糟糕的了，直到为时已晚才知道该怎么做。这就是发生在罗姆尼竞选团队身上的事情。他们可以看到对方在特定城镇的特定有线电视台购买广告，但却不知道为什么；他们的水晶球太模糊了。最后，奥巴马赢得了除北卡罗来纳州以外的每一个战场州，而且其幅度甚至超过了最准确的民调机构的预测。而最准确的民调机构又是那些使用最复杂的预测技术的人（如内特·西尔弗）；他们不如奥巴马竞选团队准确，因为他们的资源较少。但他们比传统的专家学者要准确得多，他们的预测是基于他们的专业知识。</p>

<p class="noindent english">You might think the 2012 election was a fluke: most elections are not close enough for machine learning to be the deciding factor. But machine learning will <i>cause</i> more elections to be close in the future. In politics, as in everything, learning is an arms race. In the days of Karl Rove, a former direct marketer and data miner, the Republicans were ahead. <a id="babilu_link-180"></a> By 2012, they’d fallen behind, but now they’re catching up again. We don’t know who’ll be ahead in the next election cycle, but both parties will be working hard to win. That means understanding the voters better and tailoring the candidates’ pitches—even choosing the candidates themselves—accordingly. The same applies to entire party platforms, during and between election cycles: if detailed voter models, based on hard data, say a party’s current platform is a losing one, the party will change it. As a result, major events aside, gaps between candidates in the polls will be smaller and shorter lived. Other things being equal, the candidates with the better voter models will win, and voters will be better served for it.</p>

<p class="noindent chinese">你可能认为 2012 年的选举是一种侥幸：大多数选举都没有接近到让机器学习成为决定性因素。但机器学习将<i>使</i> 未来更多的选举变得接近。在政治领域，就像在一切领域一样，学习是一场军备竞赛。在卡尔·罗夫的时代，共和党人领先。到了 2012 年，他们已经落后了，但现在他们又开始追赶了。我们不知道谁会在下一个选举周期中领先，但两党都会努力争取胜利。这意味着要更好地了解选民，并根据实际情况调整候选人的策略，甚至选择候选人本身。这同样适用于整个党的纲领，在选举周期期间和选举周期之间：如果基于硬数据的详细选民模型说一个党的当前纲领是一个失败的纲领，该党将改变它。因此，撇开重大事件不谈，候选人之间在民调中的差距会更小，时间也会更短。在其他条件相同的情况下，拥有更好的选民模型的候选人将获胜，而选民将因此得到更好的服务。</p>

<p class="noindent english">One of the greatest talents a politician can have is the ability to understand voters, individually or in small groups, and speak directly to them (or seem to). Bill Clinton is the paradigmatic example of this in recent memory. The effect of machine learning is like having a dedicated Bill Clinton for every voter. Each of these mini-Clintons is a far cry from the real one, but they have the advantage of numbers; even Bill Clinton can’t know what every single voter in America is thinking (although he’d surely like to). Learning algorithms are the ultimate retail politicians.</p>

<p class="noindent chinese">政治家最伟大的才能之一是能够理解选民，无论是个人还是小团体，并直接与他们交谈（或似乎如此）。在最近的记忆中，比尔·克林顿是这方面的典范例子。机器学习的效果就像为每个选民配备了一个专门的比尔·克林顿。这些迷你克林顿中的每一个都与真实的克林顿相差甚远，但他们有数量上的优势；即使是比尔·克林顿也不可能知道美国每个选民在想什么（尽管他肯定想知道）。学习算法是最终的零售政治家。</p>

<p class="noindent english">Of course, as with companies, politicians can put their machine-learned knowledge to bad uses as well as good ones. For example, they could make inconsistent promises to different voters. But voters, media, and watchdog organizations can do their own data mining and expose politicians who cross the line. The arms race is not just between candidates but among all participants in the democratic process.</p>

<p class="noindent chinese">当然，和公司一样，政治家们可以把机器学习的知识用于坏的方面，也可以用于好的方面。例如，他们可以对不同的选民作出不一致的承诺。但选民、媒体和监督组织可以做自己的数据挖掘，并揭露那些越界的政治家。军备竞赛不仅仅是在候选人之间，而是在民主进程的所有参与者之间。</p>

<p class="noindent english">The larger outcome is that democracy works better because the bandwidth of communication between voters and politicians increases enormously. In these days of high-speed Internet, the amount of information your elected representatives get from you is still decidedly nineteenth century: a hundred bits or so every two years, as much as fits on a ballot. This is supplemented by polling and perhaps the occasional e-mail or town-hall meeting, but that’s still precious little. Big data and machine learning change the equation. In the future, provided voter <a id="babilu_link-215"></a> models are accurate, elected officials will be able to ask voters what they want a thousand times a day and act accordingly—without having to pester the actual flesh-and-blood citizens.</p>

<p class="noindent chinese">更大的结果是，民主运作得更好，因为选民和政治家之间的沟通带宽大大增加。在互联网高速发展的今天，你的民选代表从你那里得到的信息量仍然是明显的十九世纪：每两年有 100 比特左右，与选票上的信息量一样多。这是由民意调查和可能偶尔的电子邮件或市民大会补充的，但这仍然是非常少的。大数据和机器学习改变了这个方程式。在未来，只要选民模型准确，当选官员将能够每天询问选民他们想要什么，并采取相应的行动，而不必纠缠真正有血有肉的公民。</p>

<h1 id="babilu_link-384"><b>One if by land, two if by Internet</b></h1>

<h1 id="babilu_link-384"><b>如果通过陆路，一个；如果通过互联网，两个</b></h1>

<p class="noindent english">Out in cyberspace, learning algorithms man the nation’s ramparts. Every day, foreign attackers attempt to break into computers at the Pentagon, defense contractors, and other companies and government agencies. Their tactics change continually; what worked against yesterday’s attacks is powerless against today’s. Writing code to detect and block each one would be as effective as the Maginot Line, and the Pentagon’s Cyber Command knows it. But machine learning runs into a problem if an attack is the first of its kind and there aren’t any previous examples of it to learn from. Instead, learners build models of normal behavior, of which there’s plenty, and flag anomalies. Then they call in the cavalry (aka system administrators). If cyberwar ever comes to pass, the generals will be human, but the foot soldiers will be algorithms. Humans are too slow and too few and would be quickly swamped by an army of bots. We need our own bot army, and machine learning is like West Point for bots.</p>

<p class="noindent chinese">在网络空间中，学习算法是国家的堡垒。每天，外国攻击者都试图侵入五角大楼、国防承包商以及其他公司和政府机构的计算机。他们的策略不断变化；对昨天的攻击有效的方法，对今天的攻击无能为力。编写代码来检测和阻止每一次攻击将像马奇诺防线一样有效，五角大楼的网络司令部知道这一点。但是，如果一个攻击是第一次发生，而且没有任何以前的例子可以学习，那么机器学习就会遇到一个问题。相反，学习者建立正常行为的模型，其中有很多，并标记异常情况。然后他们召集骑兵（又称系统管理员）。如果网络战争一旦发生，将军将是人类，但步兵将是算法。人类太慢，太少，很快就会被机器人军队淹没。我们需要自己的机器人军队，而机器学习就像机器人的西点军校。</p>

<p class="noindent english">Cyberwar is an instance of asymmetric warfare, where one side can’t match the other’s conventional military power but can still inflict grievous damage. A handful of terrorists armed with little more than box cutters can knock down the Twin Towers and kill thousands of innocents. All the biggest threats to US security today are in the realm of asymmetric warfare, and there’s an effective weapon against all of them: information. If the enemy can’t hide, he can’t survive. The good news is that we have plenty of information, and that’s also the bad news.</p>

<p class="noindent chinese">网络战争是不对称战争的一个例子，一方无法与另一方的常规军事力量相提并论，但仍然可以造成严重的破坏。一小撮恐怖分子只用一把裁纸刀就能推倒双子塔，杀死成千上万的无辜者。今天对美国安全的所有最大威胁都属于非对称战争的范畴，而且有一种有效的武器可以对付所有这些威胁：信息。如果敌人无法隐藏，他就无法生存。好消息是，我们有大量的信息，这也是坏消息。</p>

<p class="noindent english">The National Security Agency (NSA) has become infamous for its bottomless appetite for data: by one estimate, every day it intercepts over a billion phone calls and other communications around the globe. Privacy issues aside, however, it doesn’t have millions of staffers to eavesdrop on all these calls and e-mails or even just keep track of who’s <a id="babilu_link-256"></a> talking to whom. The vast majority of calls are perfectly innocent, and writing a program to pick out the few suspicious ones is very hard. In the old days, the NSA used keyword matching, but that’s easy to get around. (Just call the bombing a “wedding” and the bomb the “wedding cake.”) In the twenty-first century, it’s a job for machine learning. Secrecy is the NSA’s trademark, but its director has testified to Congress that mining of phone logs has already halted dozens of terrorism threats.</p>

<p class="noindent chinese">美国国家安全局（NSA）因其对数据的无底洞式的渴望而臭名昭著：根据一项估计，它每天在全球范围内拦截超过 10 亿个电话和其他通信。然而，撇开隐私问题不谈，它并没有数以百万计的工作人员来窃听所有这些电话和电子邮件，甚至只是跟踪谁在与谁交谈。绝大多数电话都是完全无辜的，编写一个程序来挑选出少数可疑的电话是非常困难的。在过去，国家安全局使用关键词匹配，但这很容易被绕过。（只要把爆炸案称为 “婚礼”，把炸弹称为 “婚礼蛋糕” 就可以了。保密是国家安全局的商标，但其局长在国会作证时表示，对电话记录的挖掘已经阻止了几十个恐怖主义威胁。</p>

<p class="noindent english">Terrorists can hide in the crowd at a football game, but learners can pick out their faces. They can make exotic bombs, but learners can sniff them out. Learners can also do something more subtle: connect the dots between events that individually seem harmless but add up to an ominous pattern. This approach could have prevented 9/11. There’s a further twist: once a learned program is deployed, the bad guys change their behavior to defeat it. This contrasts with the natural world, which always works the same way. The solution is to marry machine learning with game theory, something I’ve worked on in the past: don’t just learn to defeat what your opponent does now; learn to parry what he might do against your learner. Factoring in the costs and benefits of different actions, as game theory does, can also help strike the right balance between privacy and security.</p>

<p class="noindent chinese">恐怖分子可以躲在足球比赛的人群中，但学习者可以找出他们的脸。他们可以制造奇特的炸弹，但学习者可以嗅出它们。学习者还可以做一些更微妙的事情：把那些单独看起来无害，但加起来是不祥之兆的事件联系起来。这种方法可以防止 9/11 事件的发生。还有一个转折点：一旦部署了学习型程序，坏人就会改变他们的行为来打败它。这与自然界形成鲜明对比，后者总是以同样的方式运作。解决方案是将机器学习与博弈论结合起来，这是我过去一直在研究的问题：不要只是学习打败你的对手现在所做的事情；要学习抵御他可能对你的学习者所做的事情。正如博弈论所做的那样，将不同行动的成本和收益考虑在内，也有助于在隐私和安全之间取得适当的平衡。</p>

<p class="noindent english">During the Battle of Britain, the Royal Air Force held back the Luftwaffe despite being heavily outnumbered. German pilots couldn’t understand how, wherever they went, they always ran into the RAF. The British had a secret weapon: radar, which detected the German planes well before they crossed into Britain’s airspace. Machine learning is like having a radar that sees into the future. Don’t just react to your adversary’s moves; predict them and preempt them.</p>

<p class="noindent chinese">在不列颠战役期间，皇家空军在人数众多的情况下仍挡住了德国空军的进攻。德国飞行员无法理解，为什么他们无论走到哪里，都会遇到皇家空军。英国人有一个秘密武器：雷达，它在德国飞机进入英国领空之前就已经发现了它们。机器学习就像拥有一个能看到未来的雷达。不要只是对你的对手的举动做出反应；要预测它们并抢先一步。</p>

<p class="noindent english">An example of this closer to home is what’s known as predictive policing. By forecasting crime trends and strategically focusing patrols where they’re most likely to be needed, as well as taking other preventive measures, a city’s police force can effectively do the job of a much larger one. In many ways, law enforcement is similar to asymmetric warfare, and many of the same learning techniques apply, whether it’s in fraud detection, uncovering criminal networks, or plain old beat policing.</p>

<p class="noindent chinese">离家近的一个例子是所谓的预测性警务。通过预测犯罪趋势和战略性地将巡逻集中在最有可能需要的地方，以及采取其他预防措施，一个城市的警察部队可以有效地完成一个更大的警察部队的工作。在许多方面，执法与不对称战争相似，许多相同的学习技术也适用，无论是在欺诈侦查、揭露犯罪网络，还是普通的巡逻警务。</p>

<p class="noindent english"><a id="babilu_link-46"></a> Machine learning also has a growing role on the battlefield. Learners can help dissipate the fog of war, sifting through reconnaissance imagery, processing after-action reports, and piecing together a picture of the situation for the commander. Learning powers the brains of military robots, helping them keep their bearings, adapt to the terrain, distinguish enemy vehicles from civilian ones, and home in on their targets. DARPA’s AlphaDog carries soldiers’ gear for them. Drones can fly autonomously with the help of learning algorithms; although they are still partly controlled by human pilots, the trend is for one pilot to oversee larger and larger swarms. In the army of the future, learners will greatly outnumber soldiers, saving countless lives.</p>

<p class="noindent chinese">机器学习在战场上的作用也越来越大。学习者可以帮助驱散战争的迷雾，筛选侦察图像，处理行动后的报告，并为指挥官拼凑出一幅局势图。学习为军用机器人的大脑提供动力，帮助它们保持方位，适应地形，区分敌方车辆和民用车辆，并对其目标进行定位。DARPA 的 AlphaDog 为士兵携带装备。无人机可以在学习算法的帮助下自主飞行；尽管它们仍然部分地由人类飞行员控制，但趋势是由一名飞行员监督越来越大的机群。在未来的军队中，学习者的数量将大大超过士兵，拯救无数的生命。</p>

<h1 id="babilu_link-385"><b>Where are we headed?</b></h1>

<h1 id="babilu_link-385"><b>我们要去哪里？</b></h1>

<p class="noindent english">Technology trends come and go all the time. What’s unusual about machine learning is that, through all these changes, through boom and bust, it just keeps growing. Its first big hit was in finance, predicting stock ups and downs, starting in the late 1980s. The next wave was mining corporate databases, which by the mid-1990s were starting to grow quite large, and in areas like direct marketing, customer relationship management, credit scoring, and fraud detection. Then came the web and e-commerce, where automated personalization quickly became de rigueur. When the dot-com bust temporarily curtailed that, the use of learning for web search and ad placement took off. For better or worse, the 9/11 attacks put machine learning in the front line of the war on terror. Web 2.0 brought a swath of new applications, from mining social networks to figuring out what bloggers are saying about your products. In parallel, scientists of all stripes were increasingly turning to large-scale modeling, with molecular biologists and astronomers leading the charge. The housing bust barely registered; its main effect was a welcome transfer of talent from Wall Street to Silicon Valley. In 2011, the “big data” meme hit, putting machine learning squarely in the center of the global economy’s future. Today, there seems to be hardly an area of human endeavor untouched by machine <a id="babilu_link-257"></a> learning, including seemingly unlikely candidates like music, sports, and wine tasting.</p>

<p class="noindent chinese">技术趋势总是来来去去的。机器学习的不寻常之处在于，在所有这些变化中，在繁荣和萧条中，它只是不断地增长。它的第一个大热门是在金融领域，从 20 世纪 80 年代末开始预测股票涨跌。下一个浪潮是挖掘企业数据库，到 1990 年代中期，企业数据库开始变得相当庞大，并且在直销、客户关系管理、信用评分和欺诈检测等领域。然后是网络和电子商务，在那里，自动化的个性化迅速成为惯例。当网络公司倒闭后，网络搜索和广告投放的学习就开始兴起。无论好坏，9/11 袭击将机器学习推到了反恐战争的第一线。网络 2.0 带来了一系列新的应用，从挖掘社交网络到弄清博主对你的产品的评价。与此同时，各种类型的科学家越来越多地转向大规模建模，分子生物学家和天文学家引领了这一潮流。房地产萧条几乎没有被记录下来；其主要影响是人才从华尔街转移到硅谷，令人欣喜。2011 年，“大数据” 的流行，使机器学习成为全球经济未来的中心。今天，人类的努力似乎几乎没有一个领域没有被机器包括像音乐、体育和品酒这样看似不太可能的领域。</p>

<p class="noindent english">As remarkable as this growth is, it’s only a foretaste of what’s to come. Despite its usefulness, the generation of learning algorithms currently at work in industry is, in fact, quite limited. When the algorithms now in the lab make it to the front lines, Bill Gates’s remark that a breakthrough in machine learning would be worth ten Microsofts will seem conservative. And if the ideas that <i>really</i> put a glimmer in researchers’ eyes bear fruit, machine learning will bring about not just a new era of civilization, but a new stage in the evolution of life on Earth.</p>

<p class="noindent chinese">尽管这种增长令人瞩目，但它只是未来的一个预示。尽管它很有用，但目前在工业界工作的这一代学习算法实际上是相当有限的。当现在实验室里的算法进入前线时，比尔·盖茨所说的机器学习方面的突破将抵得上十个微软的说法将显得很保守。如果那些<i>真正</i>让研究人员眼前一亮的想法取得成果，机器学习不仅会带来一个新的文明时代，而且会带来地球上生命进化的一个新阶段。</p>

<p class="noindent english">What makes this possible? How do learning algorithms work? What can’t they currently do, and what will the next generation look like? How will the machine-learning revolution unfold? And what opportunities and dangers should you look out for? That’s what this book is about—read on!</p>

<p class="noindent chinese">是什么让这成为可能？学习算法是如何工作的？它们目前不能做什么，下一代会是什么样子？机器学习革命将如何展开？以及你应该注意哪些机会和危险？这就是本书的内容 —— 请继续阅读！</p>

</section>

</div>

</div>

<div id="babilu_link-318">

<div>

<section id="babilu_link-337">

<h1><a id="babilu_link-292"></a> <a href="#babilu_link-319">CHAPTER TWO</a></h1>

<h1><a id="babilu_link-292"></a> <a href="#babilu_link-319">第二章</a></h1>

<h1><a href="#babilu_link-319">The Master Algorithm</a></h1>

<h1><a href="#babilu_link-319">主算法</a></h1>

<p class="noindent english">Even more astonishing than the breadth of applications of machine learning is that it’s the <i>same</i> algorithms doing all of these different things. Outside of machine learning, if you have two different problems to solve, you need to write two different programs. They might use some of the same infrastructure, like the same programming language or the same database system, but a program to, say, play chess is of no use if you want to process credit-card applications. In machine learning, the same algorithm can do both, provided you give it the appropriate data to learn from. In fact, just a few algorithms are responsible for the great majority of machine-learning applications, and we’ll take a look at them in the next few chapters.</p>

<p class="noindent chinese">比机器学习应用的广泛性更令人吃惊的是，是<i>同样的</i>算法在做所有这些不同的事情。在机器学习之外，如果你有两个不同的问题要解决，你需要写两个不同的程序。它们可能使用一些相同的基础设施，如相同的编程语言或相同的数据库系统，但如果你想处理信用卡申请，一个下棋的程序是没有用的。在机器学习中，只要你给它适当的数据来学习，同一个算法就可以同时进行。事实上，只有少数算法负责绝大多数的机器学习应用，我们将在接下来的几章中对它们进行考察。</p>

<p class="noindent english">For example, consider Naïve Bayes, a learning algorithm that can be expressed as a single short equation. Given a database of patient records—their symptoms, test results, and whether or not they had some particular condition—Naïve Bayes can learn to diagnose the condition in a fraction of a second, often better than doctors who spent many years in medical school. It can also beat medical expert systems that took thousands of person-hours to build. The same algorithm is widely used to learn spam filters, a problem that at first sight has <a id="babilu_link-85"></a> nothing to do with medical diagnosis. Another simple learner, called the nearest-neighbor algorithm, has been used for everything from handwriting recognition to controlling robot hands to recommending books and movies you might like. And decision tree learners are equally apt at deciding whether your credit-card application should be accepted, finding splice junctions in DNA, and choosing the next move in a game of chess.</p>

<p class="noindent chinese">例如，考虑一下天真贝叶斯，一种可以表示为单一短方程的学习算法。给出一个病人记录的数据库 —— 他们的症状、测试结果以及他们是否患有某种特定疾病 —— 天真贝叶斯可以在几分之一秒内学会诊断病情，通常比在医学院学习多年的医生做得更好。它还能击败花费数千人小时建立的医学专家系统。同样的算法被广泛用于学习垃圾邮件过滤器，这个问题乍一看与医学诊断毫无关系，。另一种简单的学习器，称为最近邻算法，已被用于从手写识别到控制机器人的手，以及推荐你可能喜欢的书籍和电影。决策树学习器在决定是否接受你的信用卡申请、寻找 DNA 中的接合点以及在国际象棋游戏中选择下一步时也同样适用。</p>

<p class="noindent english">Not only can the same learning algorithms do an endless variety of different things, but they’re shockingly simple compared to the algorithms they replace. Most learners can be coded up in a few hundred lines, or perhaps a few thousand if you add a lot of bells and whistles. In contrast, the programs they replace can run in the hundreds of thousands or even millions of lines, and a single learner can induce an unlimited number of different programs.</p>

<p class="noindent chinese">同样的学习算法不仅可以做无穷无尽的不同事情，而且与它们所取代的算法相比，它们简单得令人震惊。大多数学习者可以在几百行内编码完成，如果你添加了很多的铃声和口哨，可能会有几千行。相比之下，它们所取代的程序可以运行几十万甚至几百万行，而且一个学习者可以诱发无限数量的不同程序。</p>

<p class="noindent english">If so few learners can do so much, the logical question is: Could one learner do everything? In other words, could a single algorithm learn all that can be learned from data? This is a very tall order, since it would ultimately include everything in an adult’s brain, everything evolution has created, and the sum total of all scientific knowledge. But in fact all the major learners—including nearest-neighbor, decision trees, and Bayesian networks, a generalization of Naïve Bayes—are universal in the following sense: if you give the learner enough of the appropriate data, it can approximate any function arbitrarily closely—which is math-speak for learning anything. The catch is that “enough data” could be infinite. Learning from finite data requires making assumptions, as we’ll see, and different learners make different assumptions, which makes them good for some things but not others.</p>

<p class="noindent chinese">如果这么少的学习者能做这么多，那么合乎逻辑的问题是：一个学习者能做所有的事情吗？换句话说，一个单一的算法能否学习所有可以从数据中学习的东西？这是一个非常高的要求，因为它最终将包括一个成年人大脑中的一切，进化所创造的一切，以及所有科学知识的总和。但事实上，所有主要的学习器 —— 包括最近邻、决策树和贝叶斯网络（天真贝叶斯的概括） —— 在以下意义上都是通用的：如果你给学习器足够多的适当数据，它可以任意地接近任何函数 —— 这就是数学上所说的学习任何东西。问题是，“足够的数据” 可能是无限的。正如我们将看到的，从有限的数据中学习需要做出假设，而不同的学习者做出不同的假设，这使得他们对某些事情有好处，但对其他事情则没有好处。</p>

<p class="noindent english">But what if instead of leaving these assumptions embedded in the algorithm we make them an explicit input, along with the data, and allow the user to choose which ones to plug in, perhaps even state new ones? Is there an algorithm that can take in any data and assumptions and output the knowledge that’s implicit in them? I believe so. Of course, we have to put some limits on what the assumptions can be, otherwise <a id="babilu_link-220"></a> we could cheat by giving the algorithm the entire target knowledge, or close to it, in the form of assumptions. But there are many ways to do this, from limiting the size of the input to requiring that the assumptions be no stronger than those of current learners.</p>

<p class="noindent chinese">但是，如果我们不把这些假设嵌入到算法中，而是把它们和数据一起作为明确的输入，并允许用户选择插入哪些假设，甚至是陈述新的假设呢？是否有一种算法可以接受任何数据和假设，并输出其中隐含的知识？我相信是的。当然，我们必须对假设的内容做一些限制，否则我们可以通过以假设的形式给算法提供整个目标知识，或接近目标知识的形式来作弊。但是有很多方法可以做到这一点，从限制输入的大小到要求假设不强于当前学习者的假设。</p>

<p class="noindent english">The question then becomes: How weak can the assumptions be and still allow all relevant knowledge to be derived from finite data? Notice the word <i>relevant</i> : we’re only interested in knowledge about our world, not about worlds that don’t exist. So inventing a universal learner boils down to discovering the deepest regularities in our universe, those that all phenomena share, and then figuring out a computationally efficient way to combine them with data. This requirement of computational efficiency precludes just using the laws of physics as the regularities, as we’ll see. It does not, however, imply that the universal learner has to be as efficient as more specialized ones. As so often happens in computer science, we’re willing to sacrifice efficiency for generality. This also applies to the amount of data required to learn a given target knowledge: a universal learner will generally need more data than a specialized one, but that’s OK provided we have the necessary amount—and the bigger data gets, the more likely this will be the case.</p>

<p class="noindent chinese">那么问题来了。假设能有多弱，仍然允许从有限的数据中得出所有相关的知识？注意这个词：我们只对<i>有关</i>我们世界的知识感兴趣，而不是对不存在的世界感兴趣。因此，发明一个通用的学习者可以归结为发现我们宇宙中最深层的规律性，那些所有现象都有的规律性，然后找出一种计算效率高的方法来把它们与数据结合起来。这种对计算效率的要求排除了仅仅使用物理定律作为规律性，正如我们将看到的。然而，这并不意味着通用学习器必须像更专业的学习器那样有效。正如计算机科学中经常发生的那样，我们愿意为通用性牺牲效率。这也适用于学习一个给定的目标知识所需的数据量：通用学习器通常比专业学习器需要更多的数据，但只要我们有必要的数据量就可以了 —— 数据越大，越有可能出现这种情况。</p>

<p class="noindent english">Here, then, is the central hypothesis of this book:</p>

<p class="noindent chinese">那么，这里就是本书的核心假设。</p>

<div>

<p class="noindent english"><i>All knowledge—past, present, and future—can be derived from data by a single, universal learning algorithm.</i></p>

<p class="noindent chinese"><i>所有的知识 —— 过去、现在和未来 —— 都可以通过一个单一的、通用的学习算法从数据中得到。</i></p>

</div>

<p class="noindent english">I call this learner the Master Algorithm. If such an algorithm is possible, inventing it would be one of the greatest scientific achievements of all time. In fact, the Master Algorithm is the last thing we’ll ever have to invent because, once we let it loose, it will go on to invent everything else that can be invented. All we need to do is provide it with enough of the right kind of data, and it will discover the corresponding knowledge. Give it a video stream, and it learns to see. Give it a library, and it learns to read. Give it the results of physics experiments, and it discovers the <a id="babilu_link-92"></a> laws of physics. Give it DNA crystallography data, and it discovers the structure of DNA.</p>

<p class="noindent chinese">我把这个学习者称为 “主算法”。如果这种算法是可能的，发明它将是有史以来最伟大的科学成就之一。事实上，主算法是我们最后需要发明的东西，因为一旦我们让它跑起来，它就会继续发明其他一切可以发明的东西。我们所要做的就是为它提供足够多的正确的数据，它就会发现相应的知识。给它一个视频流，它就学会了看。给它一个图书馆，它就学会了阅读。给它物理学实验的结果，它就会发现物理学定律。给它 DNA 晶体学数据，它就发现了 DNA 的结构。</p>

<p class="noindent english">This may sound far-fetched: How could one algorithm possibly learn so many different things and such difficult ones? But in fact many lines of evidence point to the existence of a Master Algorithm. Let’s see what they are.</p>

<p class="noindent chinese">这听起来可能很牵强。一个算法怎么可能学会这么多不同的东西，而且是这么难的东西？但事实上，许多证据表明存在着一个主算法。让我们来看看它们是什么。</p>

<h1 id="babilu_link-386"><b>The argument from neuroscience</b></h1>

<h1 id="babilu_link-386"><b>来自神经科学的论点</b></h1>

<p class="noindent english">In April 2000, a team of neuroscientists from MIT reported in <i>Nature</i> the results of an extraordinary experiment. They rewired the brain of a ferret, rerouting the connections from the eyes to the auditory cortex (the part of the brain responsible for processing sounds) and rerouting the connections from the ears to the visual cortex. You’d think the result would be a severely disabled ferret, but no: the auditory cortex learned to see, the visual cortex learned to hear, and the ferret was fine. In normal mammals, the visual cortex contains a map of the retina: neurons connected to nearby regions of the retina are close to each other in the cortex. Instead, the rewired ferrets developed a map of the retina in the auditory cortex. If the visual input is redirected instead to the somatosensory cortex, responsible for touch perception, it too learns to see. Other mammals also have this ability.</p>

<p class="noindent chinese">2000 年 4 月，麻省理工学院的一个神经科学家小组在《<i>自然</i>》杂志上报告了一项非凡的实验结果。他们对一只雪貂的大脑进行了重新布线，将眼睛与听觉皮层（大脑中负责处理声音的部分）的连接重新设置路由，并将耳朵与视觉皮层的连接重新设置路由。你会认为结果会是一只严重残疾的雪貂，但是没有：听觉皮层学会了看，视觉皮层学会了听，而雪貂则很好。在正常的哺乳动物中，视觉皮层包含一张视网膜的地图：与视网膜附近区域相连的神经元在皮层中相互靠近。相反，重新接线的雪貂在听觉皮层中形成了一个视网膜的地图。如果把视觉输入转到负责触摸感知的体感皮层，它也能学会看。其他哺乳动物也有这种能力。</p>

<p class="noindent english">In congenitally blind people, the visual cortex can take over other brain functions. In deaf ones, the auditory cortex does the same. Blind people can learn to “see” with their tongues by sending video images from a head-mounted camera to an array of electrodes placed on the tongue, with high voltages corresponding to bright pixels and low voltages to dark ones. Ben Underwood was a blind kid who taught himself to use echolocation to navigate, like bats do. By clicking his tongue and listening to the echoes, he could walk around without bumping into obstacles, ride a skateboard, and even play basketball. All of this is evidence that the brain uses the same learning algorithm throughout, with the areas dedicated to the different senses distinguished only by the different inputs they are connected to (e.g., eyes, ears, nose). In turn, the <a id="babilu_link-171"></a> associative areas acquire their function by being connected to multiple sensory regions, and the “executive” areas acquire theirs by connecting the associative areas and motor output.</p>

<p class="noindent chinese">在先天性失明的人中，视觉皮层可以接管其他大脑功能。在聋哑人中，听觉皮层也会这样做。盲人可以学习用舌头 “看” 东西，方法是将视频图像从头戴式摄像机发送到放在舌头上的电极阵列，高电压对应明亮的像素，低电压对应黑暗的像素。本·安德伍德是一个盲童，他教自己使用回声定位来导航，就像蝙蝠那样。通过点击他的舌头和听回声，他可以在不撞到障碍物的情况下走动，骑滑板，甚至打篮球。所有这些都证明，大脑自始至终使用相同的学习算法，专门用于不同感官的区域仅由它们所连接的不同输入（如眼睛、耳朵、鼻子）来区分。反过来，联想区通过与多个感觉区域相连而获得其功能，而 “执行” 区通过连接联想区和运动输出而获得其功能。</p>

<p class="noindent english">Examining the cortex under a microscope leads to the same conclusion. The same wiring pattern is repeated everywhere. The cortex is organized into columns with six distinct layers, feedback loops running to another brain structure called the thalamus, and a recurring pattern of short-range inhibitory connections and longer-range excitatory ones. A certain amount of variation is present, but it looks more like different parameters or settings of the same algorithm than different algorithms. Low-level sensory areas have more noticeable differences, but as the rewiring experiments show, these are not crucial. The cerebellum, the evolutionarily older part of the brain responsible for low-level motor control, has a clearly different and very regular architecture, built out of much smaller neurons, so it would seem that at least motor learning uses a different algorithm. If someone’s cerebellum is injured, however, the cortex takes over its function. Thus it seems that evolution kept the cerebellum around not because it does something the cortex can’t, but just because it’s more efficient.</p>

<p class="noindent chinese">在显微镜下检查大脑皮层会得出同样的结论。同样的布线模式到处重复。皮层被组织成具有六个不同层次的柱子，反馈回路运行到另一个称为丘脑的大脑结构，以及短程抑制性连接和长程兴奋性连接的重复模式。一定程度的变化是存在的，但它看起来更像是同一算法的不同参数或设置，而不是不同算法。低层次的感觉区域有更明显的差异，但正如重新布线实验所显示的，这些差异并不关键。小脑是大脑中负责低级运动控制的进化较早的部分，它有一个明显不同的、非常规则的结构，由小得多的神经元构建而成，所以看起来至少运动学习使用了不同的算法。然而，如果某人的小脑受伤，大脑皮层就会接管其功能。因此，似乎进化过程中保留小脑并不是因为它能做一些大脑皮层不能做的事情，而只是因为它的效率更高。</p>

<p class="noindent english">The computations taking place within the brain’s architecture are also similar throughout. All information in the brain is represented in the same way, via the electrical firing patterns of neurons. The learning mechanism is also the same: memories are formed by strengthening the connections between neurons that fire together, using a biochemical process known as long-term potentiation. All this is not just true of humans: different animals have similar brains. Ours is unusually large, but seems to be built along the same principles as other animals’.</p>

<p class="noindent chinese">在大脑结构中进行的计算也是全程相似的。大脑中的所有信息都以相同的方式表示，即通过神经元的电击模式。学习机制也是一样的：记忆是通过加强神经元之间的连接而形成的，使用的是被称为长期电位的生物化学过程。所有这些不仅仅是人类的真实情况：不同的动物有类似的大脑。我们的大脑异常庞大，但似乎是按照与其他动物相同的原则构建的。</p>

<p class="noindent english">Another line of argument for the unity of the cortex comes from what might be called the poverty of the genome. The number of connections in your brain is over a million times the number of letters in your genome, so it’s not physically possible for the genome to specify in detail how the brain is wired.</p>

<p class="noindent chinese">大脑皮层统一性的另一个论据来自于可能被称为基因组的贫困。你的大脑中的连接数是你的基因组中的字母数的 100 多万倍，所以基因组在物理上不可能详细说明大脑是如何接线的。</p>

<p class="noindent english">The most important argument for the brain being the Master Algorithm, however, is that it’s responsible for everything we can perceive <a id="babilu_link-97"></a> and imagine. If something exists but the brain can’t learn it, we don’t know it exists. We may just not see it or think it’s random. Either way, if we implement the brain in a computer, that algorithm can learn everything we can. Thus one route—arguably the most popular one—to inventing the Master Algorithm is to reverse engineer the brain. Jeff Hawkins took a stab at this in his book <i>On Intelligence</i> . Ray Kurzweil pins his hopes for the Singularity—the rise of artificial intelligence that greatly exceeds the human variety—on doing just that and takes a stab at it himself in his book <i>How to Create a Mind</i> . Nevertheless, this is only one of several possible approaches, as we’ll see. It’s not even necessarily the most promising one, because the brain is phenomenally complex, and we’re still in the very early stages of deciphering it. On the other hand, if we can’t figure out the Master Algorithm, the Singularity won’t happen any time soon.</p>

<p class="noindent chinese">然而，大脑是主算法的最重要论据是，它负责我们能感知的一切并想象。如果有些东西存在，但大脑无法学习它，我们就不知道它存在。我们可能只是没有看到它或认为它是随机的。无论怎样，如果我们在计算机中实现大脑，该算法可以学习我们能够学习的一切。因此，发明主算法的一条途径 —— 可以说是最受欢迎的一条途径，就是对大脑进行逆向工程。杰夫·霍金斯在他的《<i>论智能</i>》一书中对此做了尝试。雷·库兹韦尔将他对奇点的希望 —— 大大超过人类的人工智能的崛起 —— 寄托在这一点上，并在他的《<i>如何创造思想</i>》一书中亲自尝试。然而，这只是几种可能的方法之一，我们将看到。这甚至不一定是最有希望的方法，因为大脑是非常复杂的，而我们仍然处于破译它的早期阶段。另一方面，如果我们不能弄清主算法，奇点也不会很快发生。</p>

<p class="noindent english">Not all neuroscientists believe in the unity of the cortex; we need to learn more before we can be sure. The question of just what the brain can and can’t learn is also hotly debated. But if there’s something we know but the brain can’t learn, it must have been learned by evolution.</p>

<p class="noindent chinese">并非所有的神经科学家都相信大脑皮层的统一性；在我们能够确定之前，我们需要了解更多。关于大脑能和不能学习的问题也是争论激烈。但如果有一些我们知道但大脑不能学习的东西，它一定是通过进化学到的。</p>

<h1 id="babilu_link-387"><b>The argument from evolution</b></h1>

<h1 id="babilu_link-387"><b>进化论的论点</b></h1>

<p class="noindent english">Life’s infinite variety is the result of a single mechanism: natural selection. Even more remarkable, this mechanism is of a type very familiar to computer scientists: iterative search, where we solve a problem by trying many candidate solutions, selecting and modifying the best ones, and repeating these steps as many times as necessary. Evolution <i>is</i> an algorithm. Paraphrasing Charles Babbage, the Victorian-era computer pioneer, God created not species but the algorithm for creating species. The “endless forms most beautiful” Darwin spoke of in the conclusion of <i>The Origin of Species</i> belie a most beautiful unity: all of those forms are encoded in strings of DNA, and all of them come about by modifying and combining those strings. Who would have guessed, given only a description of this algorithm, that it could produce you and me? If evolution can learn us, it can conceivably also learn everything that can <a id="babilu_link-297"></a> be learned, provided we implement it on a powerful enough computer. Indeed, evolving programs by simulating natural selection is a popular endeavor in machine learning. Evolution, then, is another promising path to the Master Algorithm.</p>

<p class="noindent chinese">生命的无限多样性是一个单一机制的结果：自然选择。更难能可贵的是，这种机制是计算机科学家非常熟悉的类型：迭代搜索，我们通过尝试许多候选解决方案来解决问题，选择和修改最佳解决方案，并根据需要多次重复这些步骤。进化<i>是</i>一种算法。套用维多利亚时代的计算机先驱查尔斯·巴贝奇的话说，上帝创造的不是物种，而是创造物种的算法。达尔文在《<i>物种起源</i>》的结论中谈到的 “无尽的最美的形式” 掩盖了一个最美的统一性：所有这些形式都被编码在 DNA 串中，所有这些都是通过修改和组合这些串而产生的。仅仅是对这种算法的描述，谁会想到它能产生你和我？如果进化论可以学习我们，那么可以想象它也可以学习一切可以只要我们在一个足够强大的计算机上实现它。事实上，通过模拟自然选择来演化程序是机器学习中的一个流行的努力。因此，进化是通向主算法的另一条充满希望的道路。</p>

<p class="noindent english">Evolution is the ultimate example of how much a simple learning algorithm can achieve given enough data. Its input is the experience and fate of all living creatures that ever existed. (Now <i>that’s</i> big data.) On the other hand, it’s been running for over three billion years on the most powerful computer on Earth: Earth itself. A computer version of it had better be faster and less data intensive than the original. Which one is the better model for the Master Algorithm: evolution or the brain? This is machine learning’s version of the nature versus nurture debate. And, just as nature and nurture combine to produce us, perhaps the true Master Algorithm contains elements of both.</p>

<p class="noindent chinese">进化是一个最终的例子，说明一个简单的学习算法在给定足够数据的情况下可以取得多大的成就。它的输入是所有曾经存在的生物的经验和命运。（<i>这就是</i>大数据。）另一方面，它已经在地球上最强大的计算机上运行了 30 多亿年。地球本身。它的计算机版本最好比原版更快，数据密度更低。哪一个是更好的主算法模型：进化还是大脑？这是机器学习的自然与教养的辩论版本。而且，正如自然和培养相结合产生了我们一样，也许真正的主算法包含了两者的元素。</p>

<h1 id="babilu_link-388"><b>The argument from physics</b></h1>

<h1 id="babilu_link-388"><b>物理学的论点</b></h1>

<p class="noindent english">In a famous 1959 essay, the physicist and Nobel laureate Eugene Wigner marveled at what he called “the unreasonable effectiveness of mathematics in the natural sciences.” By what miracle do laws induced from scant observations turn out to apply far beyond them? How can the laws be many orders of magnitude more precise than the data they are based on? Most of all, why is it that the simple, abstract language of mathematics can accurately capture so much of our infinitely complex world? Wigner considered this a deep mystery, in equal parts fortunate and unfathomable. Nevertheless, it is so, and the Master Algorithm is a logical extension of it.</p>

<p class="noindent chinese">在 1959 年的一篇著名文章中，物理学家和诺贝尔奖获得者尤金·维格纳对他所谓的 “数学在自然科学中的不合理的有效性” 感到惊叹。通过什么奇迹，从稀少的观察中得出的定律竟然远远超出了它们的适用范围？这些定律怎么会比它们所依据的数据精确许多个数量级？最重要的是，为什么简单、抽象的数学语言能够准确地捕捉到我们无限复杂的世界的如此之多？维格纳认为这是一个深奥的谜，既有幸运的一面，也有深不可测的一面。然而，事实就是如此，“主算法” 是它的逻辑延伸。</p>

<p class="noindent english">If the world were just a blooming, buzzing confusion, there would be reason to doubt the existence of a universal learner. But if everything we experience is the product of a few simple laws, then it makes sense that a single algorithm can induce all that can be induced. All the Master Algorithm has to do is provide a shortcut to the laws’ consequences, replacing impossibly long mathematical derivations with much shorter ones based on actual observations.</p>

<p class="noindent chinese">如果世界只是一个绽放的、嗡嗡作响的混乱，我们就有理由怀疑普遍学习者的存在。但是，如果我们所经历的一切是一些简单定律的产物，那么，一个单一的算法可以诱导出所有可以诱导的东西，这就说得通。主算法所要做的就是为定律的结果提供一条捷径，用基于实际观察的更短的数学推导来取代不可能的长篇大论。</p>

<p class="noindent english"><a id="babilu_link-172"></a> For example, we believe that the laws of physics gave rise to evolution, but we don’t know how. Instead, we can induce natural selection directly from observations, as Darwin did. Countless wrong inferences could be drawn from those observations, but most of them never occur to us, because our inferences are influenced by our broad knowledge of the world, and that knowledge is consistent with the laws of nature.</p>

<p class="noindent chinese">例如，我们相信物理学定律产生了进化，但我们不知道如何产生。相反，我们可以直接从观察中诱发自然选择，正如达尔文所做的那样。从这些观察中可以得出无数错误的推论，但大多数推论从未发生在我们身上，因为我们的推论受到我们对世界的广泛知识的影响，而这些知识与自然规律是一致的。</p>

<p class="noindent english">How much of the character of physical law percolates up to higher domains like biology and sociology remains to be seen, but the study of chaos provides many tantalizing examples of very different systems with similar behavior, and the theory of universality explains them. The Mandelbrot set is a beautiful example of how a very simple iterative procedure can give rise to an inexhaustible variety of forms. If the mountains, rivers, clouds, and trees of the world are all the result of such procedures—and fractal geometry shows they are—perhaps those procedures are just different parametrizations of a single one that we can induce from them.</p>

<p class="noindent chinese">物理规律的特点在多大程度上渗透到生物学和社会学等更高的领域还有待观察，但混沌研究提供了许多具有类似行为的非常不同的系统的诱人的例子，而普遍性理论解释了这些例子。曼德布罗特集是一个美丽的例子，说明一个非常简单的迭代程序可以产生取之不尽的各种形式。如果世界上的山脉、河流、云彩和树木都是这种程序的结果 —— 分形几何学表明它们是 —— 也许这些程序只是一个单一程序的不同参数，我们可以从它们那里诱导出来。</p>

<p class="noindent english">In physics, the same equations applied to different quantities often describe phenomena in completely different fields, like quantum mechanics, electromagnetism, and fluid dynamics. The wave equation, the diffusion equation, Poisson’s equation: once we discover it in one field, we can more readily discover it in others; and once we’ve learned how to solve it in one field, we know how to solve it in all. Moreover, all these equations are quite simple and involve the same few derivatives of quantities with respect to space and time. Quite conceivably, they are all instances of a master equation, and all the Master Algorithm needs to do is figure out how to instantiate it for different data sets.</p>

<p class="noindent chinese">在物理学中，应用于不同量的相同方程往往描述了完全不同领域的现象，如量子力学、电磁学和流体动力学。波浪方程、扩散方程、泊松方程：一旦我们在一个领域发现了它，我们就能更容易地在其他领域发现它；一旦我们学会了如何在一个领域解决它，我们就知道如何在所有领域解决它。此外，所有这些方程都很简单，而且涉及相同的几个相对于空间和时间的量的导数。可以想象，它们都是一个主方程的实例，而主算法需要做的就是找出如何为不同的数据集实例化它。</p>

<p class="noindent english">Another line of evidence comes from optimization, the branch of mathematics concerned with finding the input to a function that produces its highest output. For example, finding the sequence of stock purchases and sales that maximizes your total returns is an optimization problem. In optimization, simple functions often give rise to surprisingly complex solutions. Optimization plays a prominent role in almost every field of science, technology, and business, including machine learning. Each field optimizes within the constraints defined by optimizations in <a id="babilu_link-130"></a> other fields. We try to maximize our happiness within economic constraints, which are firms’ best solutions within the constraints of the available technology—which in turn consists of the best solutions we could find within the constraints of biology and physics. Biology, in turn, is the result of optimization by evolution within the constraints of physics and chemistry, and the laws of physics themselves are solutions to optimization problems. Perhaps, then, everything that exists is the progressive solution of an overarching optimization problem, and the Master Algorithm follows from the statement of that problem.</p>

<p class="noindent chinese">另一个证据来自于优化，优化是数学的一个分支，涉及到为一个函数寻找能产生最高输出的输入。例如，找到使你的总收益最大化的股票购买和销售序列是一个优化问题。在优化中，简单的函数往往会产生令人惊讶的复杂解决方案。优化在科学、技术和商业的几乎每个领域都发挥着突出作用，包括机器学习。每个领域都在由其他领域的优化定义的约束条件下进行优化。我们试图在经济约束下最大化我们的幸福，而经济约束是企业在现有技术约束下的最佳解决方案，而现有技术又包括我们在生物学和物理学约束下所能找到的最佳解决方案。反过来，生物学是在物理学和化学的约束下进化的结果，而物理学的规律本身就是优化问题的解决方案。那么，也许所有存在的东西都是一个总体优化问题的渐进式解决方案，而主算法则来自于这个问题的陈述。</p>

<p class="noindent english">Physicists and mathematicians are not the only ones who find unexpected connections between disparate fields. In his book <i>Consilience</i> , the distinguished biologist E. O. Wilson makes an impassioned argument for the unity of all knowledge, from science to the humanities. The Master Algorithm is the ultimate expression of this unity: if all knowledge shares a common pattern, the Master Algorithm exists, and vice versa.</p>

<p class="noindent chinese">物理学家和数学家并不是唯一在不同的领域之间发现意外联系的人。杰出的生物学家威尔逊在他的书中为所有知识的<i>统一性</i>提出了慷慨激昂的论点，从科学到人文都是如此。主算法是这种统一性的最终体现：如果所有的知识都有一个共同的模式，主算法就存在，反之亦然。</p>

<p class="noindent english">Nevertheless, physics is unique in its simplicity. Outside physics and engineering, the track record of mathematics is more mixed. Sometimes it’s only reasonably effective, and sometimes its models are too oversimplified to be useful. This tendency to oversimplify stems from the limitations of the human mind, however, not from the limitations of mathematics. Most of the brain’s hardware (or rather, wetware) is devoted to sensing and moving, and to do math we have to borrow parts of it that evolved for language. Computers have no such limitations and can easily turn big data into very complex models. Machine learning is what you get when the unreasonable effectiveness of mathematics meets the unreasonable effectiveness of data. Biology and sociology will never be as simple as physics, but the method by which we discover their truths can be.</p>

<p class="noindent chinese">然而，物理学在其简单性方面是独一无二的。在物理学和工程学之外，数学的记录更加复杂。有时它只是合理地有效，有时它的模型过于简化而无用。然而，这种过度简化的趋势源于人类思维的局限性，而不是数学的局限性。大脑的大部分硬件（或者说，湿件）都致力于感知和移动，为了做数学，我们必须借用它为语言进化的部分。计算机没有这样的限制，可以轻松地将大数据转化为非常复杂的模型。机器学习就是当数学的不合理效力遇到数据的不合理效力时得到的东西。生物学和社会学永远不会像物理学那样简单，但我们发现其真理的方法可以。</p>

<h1 id="babilu_link-389"><b>The argument from statistics</b></h1>

<h1 id="babilu_link-389"><b>来自统计学的论点</b></h1>

<p class="noindent english">According to one school of statisticians, a single simple formula underlies all learning. Bayes’ theorem, as the formula is known, tells you how to <a id="babilu_link-199"></a> update your beliefs whenever you see new evidence. A Bayesian learner starts with a set of hypotheses about the world. When it sees a new piece of data, the hypotheses that are compatible with it become more likely, and the hypotheses that aren’t become less likely (or even impossible). After seeing enough data, a single hypothesis dominates, or a few do. For example, if I’m looking for a program that accurately predicts stock movements and a stock that a candidate program had predicted would go up instead goes down, that candidate loses credibility. After I’ve reviewed a number of candidates, only a few credible ones will remain, and they will encapsulate my new knowledge of the stock market.</p>

<p class="noindent chinese">根据一个统计学流派的说法，一个简单的公式是所有学习的基础。贝叶斯定理，正如人们所知道的那样，告诉你如何在看到新的证据时更新你的信念。一个贝叶斯学习者从一组关于世界的假设开始。当它看到一个新的数据时，与之相符的假设就会变得更有可能，而不相符的假设就会变得不太可能（甚至是不可能）。在看到足够多的数据后，一个假设会占主导地位，或者几个假设会占主导地位。例如，如果我正在寻找一个能准确预测股票走势的程序，而一个候选程序预测的股票会上涨，但却下跌了，那么这个候选程序就失去了可信度。在我审查了许多候选程序后，只剩下几个可信的程序，它们将囊括我对股市的新知识。</p>

<p class="noindent english">Bayes’ theorem is a machine that turns data into knowledge. According to Bayesian statisticians, it’s the <i>only</i> correct way to turn data into knowledge. If they’re right, either Bayes’ theorem is the Master Algorithm or it’s the engine that drives it. Other statisticians have serious reservations about the way Bayes’ theorem is used and prefer different ways to learn from data. In the days before computers, Bayes’ theorem could only be applied to very simple problems, and the idea of it as a universal learner would have seemed far-fetched. With big data and big computing to go with it, however, Bayes can find its way in vast hypothesis spaces and has spread to every conceivable field of knowledge. If there’s a limit to what Bayes can learn, we haven’t found it yet.</p>

<p class="noindent chinese">贝叶斯定理是一台将数据变成知识的机器。根据贝叶斯统计学家的说法，它是将数据变成知识的<i>唯一</i>正确方法。如果他们是对的，要么贝叶斯定理是主算法，要么是驱动它的引擎。其他统计学家对贝叶斯定理的使用方式有严重的保留，他们更喜欢用不同的方式来从数据中学习。在计算机之前的时代，贝叶斯定理只能应用于非常简单的问题，将其作为通用学习器的想法似乎很牵强。然而，随着大数据和大计算的出现，贝叶斯可以在巨大的假设空间中找到自己的方法，并且已经扩散到每一个可以想象的知识领域。如果说贝叶斯的学习能力有什么限制的话，我们还没有发现它。</p>

<h1 id="babilu_link-390"><b>The argument from computer science</b></h1>

<h1 id="babilu_link-390"><b>来自计算机科学的论点</b></h1>

<p class="noindent english">When I was a senior in college, I wasted a summer playing Tetris, a highly addictive video game where variously shaped pieces fall from above and which you try to pack as closely together as you can; the game is over when the pile of pieces reaches the top of the screen. Little did I know that this was my introduction to NP-completeness, the most important problem in theoretical computer science. Turns out that, far from an idle pursuit, mastering Tetris—<i>really</i> mastering it—is one of the most useful things you could ever do. If you can solve Tetris, you can solve thousands of the hardest and most important problems in science, technology, and management—all in one fell swoop. That’s <a id="babilu_link-296"></a> because at heart they are all the <i>same</i> problem. This is one of the most astonishing facts in all of science.</p>

<p class="noindent chinese">当我还是大四学生的时候，我浪费了一个暑假来玩俄罗斯方块，这是一个非常容易上瘾的视频游戏，各种形状的棋子从上面掉下来，你要尽可能地把它们紧密地放在一起；当棋子堆到屏幕的顶部时，游戏就结束。我不知道这是我对 NP 完备性的介绍，这是理论计算机科学中最重要的问题。事实证明，掌握俄罗斯方块 —— <i>真正</i>掌握它 —— 绝非空谈，而是你能做的最有用的事情之一。如果你能解决俄罗斯方块，你就能解决科学、技术和管理中数千个最难和最重要的问题 —— 所有这些都能一举解决。这是因为从本质上说，它们都是<i>同一个</i>问题。这是所有科学中最令人吃惊的事实之一。</p>

<p class="noindent english">Figuring out how proteins fold into their characteristic shapes; reconstructing the evolutionary history of a set of species from their DNA; proving theorems in propositional logic; detecting arbitrage opportunities in markets with transaction costs; inferring a three-dimensional shape from two-dimensional views; compressing data on a disk; forming a stable coalition in politics; modeling turbulence in sheared flows; finding the safest portfolio of investments with a given return, the shortest route to visit a set of cities, the best layout of components on a microchip, the best placement of sensors in an ecosystem, or the lowest energy state of a spin glass; scheduling flights, classes, and factory jobs; optimizing resource allocation, urban traffic flow, social welfare, and (most important) your Tetris score: these are all NP-complete problems, meaning that if you can efficiently solve one of them you can efficiently solve all problems in the class NP, including each other. Who would have guessed that all these problems, superficially so different, are really the same? But if they are, it makes sense that one algorithm could learn to solve all of them (or, more precisely, all efficiently solvable instances).</p>

<p class="noindent chinese">弄清蛋白质是如何折叠成其特征形状的；从 DNA 中重建一组物种的进化历史；证明命题逻辑中的定理；检测有交易成本的市场中的套利机会；从二维视图中推断出三维形状；压缩磁盘上的数据；形成政治中的稳定联盟；对剪切流中的湍流进行建模。寻找给定收益的最安全的投资组合，访问一组城市的最短路线，微芯片上元件的最佳布局，生态系统中传感器的最佳位置，或旋转玻璃的最低能量状态；安排航班、课程和工厂工作；优化资源分配、城市交通流量、社会福利，以及（最重要的）你的俄罗斯方块得分。这些都是 NP 完备的问题，这意味着如果你能有效地解决其中一个问题，你就能有效地解决 NP 类中的所有问题，包括彼此之间。谁会猜到所有这些表面上如此不同的问题其实是一样的？但如果它们是一样的，那么一个算法可以学会解决所有的问题（或者更准确地说，所有可有效解决的实例）也是有道理的。</p>

<p class="noindent english">P and NP are the two most important classes of problems in computer science. (The names are not very mnemonic, unfortunately.) A problem is in P if we can solve it efficiently, and it’s in NP if we can efficiently check its solution. The famous P = NP question is whether every efficiently checkable problem is also efficiently solvable. Because of NP-completeness, all it takes to answer it is to prove that <i>one</i> NP-complete problem is efficiently solvable (or not). NP is not the hardest class of problems in computer science, but it’s arguably the hardest “realistic” class: if you can’t even check a problem’s solution before the universe ends, what’s the point of trying to solve it? Humans are good at solving NP problems approximately, and conversely, problems that we find interesting (like Tetris) often have an “NP-ness” about them. One definition of artificial intelligence is that it consists of finding heuristic solutions to NP-complete problems. Often, we do this by reducing them to satisfiability, <a id="babilu_link-196"></a> the canonical NP-complete problem: Can a given logical formula ever be true, or is it self-contradictory? If we invent a learner that can learn to solve satisfiability, it has a good claim to being the Master Algorithm.</p>

<p class="noindent chinese">P 和 NP 是计算机科学中最重要的两类问题(不幸的是，这些名字不是很好记）。如果我们能有效地解决一个问题，那么这个问题就属于 P，如果我们能有效地检查它的解决方案，那么这个问题就属于 NP。著名的 P = NP 问题是，是否每个可有效检查的问题也是可有效解决的。由于 NP 的完备性，回答这个问题只需要证明<i>一个</i> NP 完备的问题是可有效解决的（或者不是）。NP 不是计算机科学中最难的一类问题，但它可以说是最难的 “现实” 类问题：如果你甚至不能在宇宙结束之前检查一个问题的解决方案，那么尝试解决它还有什么意义？人类大约擅长解决 NP 问题，反之，我们觉得有趣的问题（如俄罗斯方块）往往有一种 “NP 性”。人工智能的一个定义是，它包括寻找 NP-完整问题的启发式解决方案。通常情况下，我们通过将它们还原为可满足性来实现这一目标，典型的 NP-complete 问题：一个给定的逻辑公式能否为真，或者它是否自相矛盾？如果我们发明了一个可以学习解决可满足性的学习器，它就有资格成为主算法。</p>

<p class="noindent english">NP-completeness aside, the sheer existence of computers is itself a powerful sign that there is a Master Algorithm. If you could travel back in time to the early twentieth century and tell people that a soon-to-be-invented machine would solve problems in every realm of human endeavor—the <i>same</i> machine for <i>every</i> problem—no one would believe you. They would say that each machine can only do one thing: sewing machines don’t type, and typewriters don’t sew. Then in 1936 Alan Turing imagined a curious contraption with a tape and a head that read and wrote symbols on it, now known as a Turing machine. Every conceivable problem that can be solved by logical deduction can be solved by a Turing machine. Furthermore, a so-called universal Turing machine can simulate any other by reading its specification from the tape—in other words, it can be programmed to do anything.</p>

<p class="noindent chinese">撇开 NP 完备性不谈，计算机的存在本身就是一个强有力的信号，表明存在着一个主算法。如果你能穿越时空回到二十世纪初，告诉人们一台即将发明的机器将解决人类每个领域的问题 —— <i>每一个</i>问题都<i>是同一</i>台机器 —— 没有人会相信你。他们会说，每台机器只能做一件事：缝纫机不会打字，打字机不会缝纫。然后在 1936 年，阿兰·图灵想象了一个奇怪的装置，它有一盘磁带和一个可以在上面读写符号的头，现在被称为图灵机。每一个可以通过逻辑推理解决的可以想象的问题都可以通过图灵机解决。此外，一个所谓的通用图灵机可以通过从磁带上读取其规格来模拟任何其他机器，换句话说，它可以被编程为做任何事情。</p>

<p class="noindent english">The Master Algorithm is for induction, the process of learning, what the Turing machine is for deduction. It can learn to simulate any other algorithm by reading examples of its input-output behavior. Just as there are many models of computation equivalent to a Turing machine, there are probably many different equivalent formulations of a universal learner. The point, however, is to find the first such formulation, just as Turing found the first formulation of the general-purpose computer.</p>

<p class="noindent chinese">主算法是归纳法，是学习的过程，就像图灵机是演绎法一样。它可以通过阅读任何其他算法的输入输出行为的例子来学习模拟这些算法。正如有许多计算模型等同于图灵机一样，可能也有许多不同的通用学习器的等同表述。然而，关键是要找到第一个这样的表述，就像图灵找到了通用计算机的第一个表述一样。</p>

<h1 id="babilu_link-391"><b>Machine learners versus knowledge engineers</b></h1>

<h1 id="babilu_link-391"><b>机器学习者与知识工程师</b></h1>

<p class="noindent english">Of course, the Master Algorithm has at least as many skeptics as it has proponents. Doubt is in order when something looks like a silver bullet. The most determined resistance comes from machine learning’s perennial foe: knowledge engineering. According to its proponents, knowledge can’t be learned automatically; it must be programmed into the computer by human experts. Sure, learners can extract some things from data, but nothing you’d confuse with <i>real</i> knowledge. To knowledge engineers, big data is not the new oil; it’s the new snake oil.</p>

<p class="noindent chinese">当然，“主算法” 的怀疑者和支持者至少一样多。当某些东西看起来像一颗银弹时，怀疑是有必要的。最坚定的阻力来自于机器学习的常年敌人：知识工程。根据其支持者的说法，知识不能自动学习；必须由人类专家将其编入计算机。当然，学习者可以从数据中提取一些东西，但你不会把它们与<i>真正的</i>知识混淆。对知识工程师来说，大数据不是新的石油；它是新的蛇油。</p>

<p class="noindent english"><a id="babilu_link-79"></a> In the early days of AI, machine learning seemed like the obvious path to computers with humanlike intelligence; Turing and others thought it was the <i>only</i> plausible path. But then the knowledge engineers struck back, and by 1970 machine learning was firmly on the back burner. For a moment in the 1980s, it seemed like knowledge engineering was about to take over the world, with companies and countries making massive investments in it. But disappointment soon set in, and machine learning began its inexorable rise, at first quietly, and then riding a roaring wave of data.</p>

<p class="noindent chinese">在人工智能的早期，机器学习似乎是通往具有人类智能的计算机的明显途径；图灵和其他人认为这是<i>唯一</i>合理的途径。但随后知识工程师们进行了反击，到 1970 年，机器学习被牢牢地置于次要位置。在 20 世纪 80 年代的某一时刻，知识工程似乎即将占领世界，公司和国家在这方面进行了大量投资。但失望很快就来了，机器学习开始了它不可阻挡的崛起，起初是悄无声息的，后来则是乘着数据的浪潮轰轰烈烈的。</p>

<p class="noindent english">Despite machine learning’s successes, the knowledge engineers remain unconvinced. They believe that its limitations will soon become apparent, and the pendulum will swing back. Marvin Minsky, an MIT professor and AI pioneer, is a prominent member of this camp. Minsky is not just skeptical of machine learning as an alternative to knowledge engineering, he’s skeptical of <i>any</i> unifying ideas in AI. Minsky’s theory of intelligence, as expressed in his book <i>The Society of Mind</i> , could be unkindly characterized as “the mind is just one damn thing after another.” <i>The Society of Mind</i> is a laundry list of hundreds of separate ideas, each with its own vignette. The problem with this approach to AI is that it doesn’t work; it’s stamp collecting by computer. Without machine learning, the number of ideas needed to build an intelligent agent is infinite. If a robot had all the same capabilities as a human except learning, the human would soon leave it in the dust.</p>

<p class="noindent chinese">尽管机器学习取得了成功，知识工程师们仍然不以为然。他们认为，它的局限性很快就会显现出来，钟摆就会摆回来。麻省理工学院教授、人工智能先驱马文·明斯基是这一阵营的重要成员。明斯基不仅对机器学习作为知识工程的替代品持怀疑态度，他还对人工智能的<i>任何</i>统一思想持怀疑态度。明斯基的智能理论，正如他在《<i>心灵的社会</i>》一书中所表达的那样，可以不客气地描述为 “心灵只是一个又一个该死的东西”。《<i>心灵社会</i>》是一份由数百个独立想法组成的洗衣单，每个想法都有自己的小插曲。这种人工智能方法的问题在于它不起作用；它是用计算机收集邮票。如果没有机器学习，建立一个智能代理所需的想法数量是无限的。如果一个机器人除了学习之外拥有与人类相同的能力，那么人类很快就会把它甩在身后。</p>

<p class="noindent english">Minsky was an ardent supporter of the Cyc project, the most notorious failure in the history of AI. The goal of Cyc was to solve AI by entering into a computer all the necessary knowledge. When the project began in the 1980s, its leader, Doug Lenat, confidently predicted success within a decade. Thirty years later, Cyc continues to grow without end in sight, and commonsense reasoning still eludes it. Ironically, Lenat has belatedly embraced populating Cyc by mining the web, not because Cyc can read, but because there’s no other way.</p>

<p class="noindent chinese">明斯基是 Cyc 项目的热情支持者，这是人工智能历史上最臭名昭著的失败。Cyc 的目标是通过向计算机输入所有必要的知识来解决人工智能。当该项目在 20 世纪 80 年代开始时，其领导者道格·莱纳特自信地预测在十年内会取得成功。30 年后，Cyc 继续发展，看不到尽头，而常识性的推理仍然无法实现。具有讽刺意味的是，莱纳特迟迟没有接受通过挖掘网络来填充 Cyc，不是因为 Cyc 能够阅读，而是因为没有其他办法。</p>

<p class="noindent english">Even if by some miracle we managed to finish coding up all the necessary pieces, our troubles would be just beginning. Over the years, a number of research groups have attempted to build complete intelligent <a id="babilu_link-174"></a> agents by putting together algorithms for vision, speech recognition, language understanding, reasoning, planning, navigation, manipulation, and so on. Without a unifying framework, these attempts soon hit an insurmountable wall of complexity: too many moving parts, too many interactions, too many bugs for poor human software engineers to cope with. Knowledge engineers believe AI is just an engineering problem, but we have not yet reached the point where engineering can take us the rest of the way. In 1962, when Kennedy gave his famous moon-shot speech, going to the moon was an engineering problem. In 1662, it wasn’t, and that’s closer to where AI is today.</p>

<p class="noindent chinese">即使我们奇迹般地完成了所有必要部件的编码，我们的麻烦也才刚刚开始。多年来，一些研究小组试图通过将视觉、语音识别、语言理解、推理、计划、导航、操纵等算法放在一起，建立完整的智能。如果没有一个统一的框架，这些尝试很快就会遇到一堵无法逾越的复杂的墙：太多的活动部件，太多的互动，太多的错误，可怜的人类软件工程师无法应付。知识工程师认为人工智能只是一个工程问题，但我们还没有达到工程可以带我们走完剩下的路的程度。1962 年，当肯尼迪发表著名的射月演讲时，登月是一个工程问题。在 1662 年，它不是，而这更接近于人工智能今天的位置。</p>

<p class="noindent english">In industry, there’s no sign that knowledge engineering will ever be able to compete with machine learning outside of a few niche areas. Why pay experts to slowly and painfully encode knowledge into a form computers can understand, when you can extract it from data at a fraction of the cost? What about all the things the experts don’t know but you can discover from data? And when data is not available, the cost of knowledge engineering seldom exceeds the benefit. Imagine if farmers had to engineer each cornstalk in turn, instead of sowing the seeds and letting them grow: we would all starve.</p>

<p class="noindent chinese">在工业领域，没有迹象表明知识工程将能够在少数利基领域之外与机器学习竞争。如果你能以极低的成本从数据中提取知识，为什么还要花钱让专家慢慢地、痛苦地将知识编码成计算机可以理解的形式？那些专家们不知道但你可以从数据中发现的东西呢？而当数据不可用时，知识工程的成本很少超过收益。想象一下，如果农民不得不依次对每根玉米杆进行工程设计，而不是播下种子让它们生长：我们都会饿死。</p>

<p class="noindent english">Another prominent machine-learning skeptic is the linguist Noam Chomsky. Chomsky believes that language must be innate, because the examples of grammatical sentences children hear are not enough to learn a grammar. This only puts the burden of learning language on evolution, however; it does not argue against the Master Algorithm but only against it being something like the brain. Moreover, if a universal grammar exists (as Chomsky believes), elucidating it is a step toward elucidating the Master Algorithm. The only way this is not the case is if language has nothing in common with other cognitive abilities, which is implausible given its evolutionary recency.</p>

<p class="noindent chinese">另一位著名的机器学习怀疑论者是语言学家诺姆·乔姆斯基。乔姆斯基认为，语言必须是天生的，因为儿童听到的语法句子的例子不足以学习语法。然而，这只是把学习语言的责任推给了进化论；它并没有反对主算法，而只是反对它是类似大脑的东西。此外，如果一个通用的语法存在（正如乔姆斯基所相信的），阐明它是朝着阐明主算法迈出的一步。如果语言与其他认知能力没有任何共同之处，那就不是这种情况了，鉴于其进化的时间性，这是不可能的。</p>

<p class="noindent english">In any case, if we formalize Chomsky’s “poverty of the stimulus” argument, we find that it’s demonstrably false. In 1969, J. J. Horning proved that probabilistic context-free grammars can be learned from positive examples only, and stronger results have followed. (Context-free grammars are the linguist’s bread and butter, and the probabilistic version models <a id="babilu_link-217"></a> how likely each rule is to be used.) Besides, language learning doesn’t happen in a vacuum; children get all sorts of cues from their parents and the environment. If we’re able to learn language from a few years’ worth of examples, it’s partly because of the similarity between its structure and the structure of the world. This common structure is what we’re interested in, and we know from Horning and others that it suffices.</p>

<p class="noindent chinese">在任何情况下，如果我们把乔姆斯基的 “刺激的贫困” 论点正式化，我们会发现它明显是错误的。1969 年，J·J·霍宁证明了概率性无语境语法只能从正面的例子中学习，随后又有更有力的结果。（无语境语法是语言学家的面包和黄油，而版本的可能性限定了每条规则被使用的可能性有多大）。此外，语言学习并不是在真空中发生的；孩子们从他们的父母和环境中得到各种提示。如果我们能够从几年的例子中学习语言，部分原因是它的结构和世界的结构之间的相似性。这种共同的结构是我们感兴趣的，而且我们从霍宁和其他人那里知道，这就足够了。</p>

<p class="noindent english">More generally, Chomsky is critical of all statistical learning. He has a list of things statistical learners can’t do, but the list is fifty years out of date. Chomsky seems to equate machine learning with behaviorism, where animal behavior is reduced to associating responses with rewards. But machine learning is not behaviorism. Modern learning algorithms can learn rich internal representations, not just pairwise associations between stimuli.</p>

<p class="noindent chinese">更广泛地说，乔姆斯基对所有的统计学习都持批评态度。他有一份统计学习者不能做的事情的清单，但这份清单已经过时 50 年了。乔姆斯基似乎将机器学习等同于行为主义，在行为主义中，动物行为被简化为将反应与奖励联系起来。但机器学习不是行为主义。现代学习算法可以学习丰富的内部表征，而不仅仅是刺激之间的配对关联。</p>

<p class="noindent english">In the end, the proof is in the pudding. Statistical language learners work, and hand-engineered language systems don’t. The first eye-opener came in the 1970s, when DARPA, the Pentagon’s research arm, organized the first large-scale speech recognition project. To everyone’s surprise, a simple sequential learner of the type Chomsky derided handily beat a sophisticated knowledge-based system. Learners like it are now used in just about every speech recognizer, including Siri. Fred Jelinek, head of the speech group at IBM, famously quipped that “every time I fire a linguist, the recognizer’s performance goes up.” Stuck in the knowledge-engineering mire, computational linguistics had a near-death experience in the late 1980s. Since then, learning-based methods have swept the field, to the point where it’s hard to find a paper devoid of learning in a computational linguistics conference. Statistical parsers analyze language with accuracy close to that of humans, where hand-coded ones lagged far behind. Machine translation, spelling correction, part-of-speech tagging, word sense disambiguation, question answering, dialogue, summarization: the best systems in these areas all use learning. Watson, the <i>Jeopardy!</i> computer champion, would not have been possible without it.</p>

<p class="noindent chinese">最后，证据就在布丁中。统计语言学习器是有效的，而手工设计的语言系统则不然。第一次大开眼界是在 20 世纪 70 年代，当时五角大楼的研究部门 DARPA 组织了第一个大规模的语音识别项目。令所有人惊讶的是，一个被乔姆斯基嘲笑的简单的顺序学习器轻松地击败了一个复杂的基于知识的系统。像这样的学习器现在被用于几乎所有的语音识别器，包括 Siri。IBM 公司语音组的负责人弗雷德·耶利内克有句名言：“每当我解雇一位语言学家，识别器的性能就会上升。” 由于陷入知识工程的泥潭，计算语言学在 20 世纪 80 年代末有过一次濒临死亡的经历。从那时起，基于学习的方法已经席卷了整个领域，以至于在计算语言学会议上很难找到一篇没有学习的论文。统计分析器分析语言的准确性接近人类，而手工编码的分析器却远远落后于人类。机器翻译、拼写纠正、语篇标签、词义消歧、问题回答、对话、总结：这些领域最好的系统都使用了学习。如果没有学习，<i>Jeopardy！</i>计算机冠军 Watson 是不可能实现的。</p>

<p class="noindent english">To this Chomsky might reply that engineering successes are not proof of scientific validity. On the other hand, if your buildings collapse <a id="babilu_link-141"></a> and your engines don’t run, perhaps something is wrong with your theory of physics. Chomsky thinks linguists should focus on “ideal” speaker-listeners, as defined by him, and this gives him license to ignore things like the need for statistics in language learning. Perhaps it’s not surprising, then, that few experimentalists take his theories seriously any more.</p>

<p class="noindent chinese">对此，乔姆斯基可能会回答说，工程上的成功并不能证明科学的正确性。另一方面，如果你的建筑物倒塌你的发动机不运转，也许你的物理学理论出了问题。乔姆斯基认为语言学家应该专注于他所定义的 “理想的” 说话者·听话者，这让他有理由忽视诸如语言学习中需要统计学的东西。因此，很少有实验者认真对待他的理论，也许这并不奇怪。</p>

<p class="noindent english">Another potential source of objections to the Master Algorithm is the notion, popularized by the psychologist Jerry Fodor, that the mind is composed of a set of modules with only limited communication between them. For example, when you watch TV your “higher brain” knows that it’s only light flickering on a flat surface, but your visual system still sees three-dimensional shapes. Even if we believe in the modularity of mind, however, that does not imply that different modules use different learning algorithms. The same algorithm operating on, say, visual and verbal information may suffice.</p>

<p class="noindent chinese">反对 “主算法” 的另一个潜在来源是心理学家杰里·福多推广的概念，即心灵是由一组模块组成的，它们之间只有有限的交流。例如，当你看电视时，你的 “高级大脑” 知道这只是光线在平面上的闪烁，但你的视觉系统仍然看到三维的形状。然而，即使我们相信心灵的模块化，这并不意味着不同的模块使用不同的学习算法。比如说，对视觉和语言信息进行操作的相同算法可能就足够了。</p>

<p class="noindent english">Critics like Minsky, Chomsky, and Fodor once had the upper hand, but thankfully their influence has waned. Nevertheless, we should keep their criticisms in mind as we set out on the road to the Master Algorithm for two reasons. The first is that knowledge engineers faced many of the same problems machine learners do, and even if they didn’t succeed, they learned many valuable lessons. The second is that learning and knowledge are intertwined in surprisingly subtle ways, as we’ll soon find out. Unfortunately, the two camps often talk past each other. They speak different languages: machine learning speaks probability, and knowledge engineering speaks logic. Later in the book we’ll see what to do about this.</p>

<p class="noindent chinese">像明斯基（Minsky）、乔姆斯基（Chomsky）和福多（Fodor）这样的批评家曾经占了上风，但幸好他们的影响已经减弱。然而，在我们踏上通往主算法的道路时，我们应该牢记他们的批评，原因有二。第一，知识工程师面临着许多与机器学习者相同的问题，即使他们没有成功，他们也学到了许多宝贵的经验。第二是学习和知识以令人惊讶的微妙方式交织在一起，正如我们很快就会发现的。不幸的是，这两个阵营经常互相对骂。他们说着不同的语言：机器学习说的是概率，而知识工程说的是逻辑。在本书的后面，我们将看到该如何处理这个问题。</p>

<h1 id="babilu_link-392"><b>Swan bites robot</b></h1>

<h1 id="babilu_link-392"><b>天鹅咬伤机器人</b></h1>

<p class="noindent english">“No matter how smart your algorithm, there are some things it just can’t learn.” Outside of AI and cognitive science, the most common objections to machine learning are variants of this claim. Nassim Taleb hammered on it forcefully in his book <i>The Black Swan</i> . Some events are simply not predictable. If you’ve only ever seen white swans, you think <a id="babilu_link-149"></a> the probability of ever seeing a black one is zero. The financial meltdown of 2008 was a “black swan.”</p>

<p class="noindent chinese">“无论你的算法多么聪明，有些东西它就是学不会。” 在人工智能和认知科学之外，对机器学习最常见的反对意见是这种说法的变种。纳西姆·塔勒布在他的《<i>黑天鹅</i>》（The Black Swan）一书中对此进行了有力的抨击。有些事件是根本无法预测的。如果你只见过白天鹅，你会认为见到黑天鹅的概率为零。2008 年的金融风暴就是一只 “黑天鹅”。</p>

<p class="noindent english">It’s true that some things are predictable and some aren’t, and the first duty of the machine learner is to distinguish between them. But the goal of the Master Algorithm is to learn everything that <i>can</i> be known, and that’s a vastly wider domain than Taleb and others imagine. The housing bust was far from a black swan; on the contrary, it was widely predicted. Most banks’ models failed to see it coming, but that was due to well-understood limitations of those models, not limitations of machine learning in general. Learning algorithms are quite capable of accurately predicting rare, never-before-seen events; you could even say that that’s what machine learning is all about. What’s the probability of a black swan if you’ve never seen one? How about it’s the fraction of known species that belatedly turned out to have black specimens? This is only a crude example; we’ll see many deeper ones in this book.</p>

<p class="noindent chinese">的确，有些事情是可以预测的，有些则不是，机器学习者的首要职责就是区分它们。但是，主算法的目标是学习一切<i>可以</i>知道的东西，而这是一个比塔勒布和其他人想象的要广泛得多的领域。房地产萧条远非黑天鹅；相反，它被广泛预测到了。大多数银行的模型没能预见到它的到来，但这是由于这些模型的众所周知的局限性，而不是一般的机器学习的限制。学习算法很有能力准确预测罕见的、从未见过的事件；你甚至可以说，这就是机器学习的意义所在。如果你从来没有见过黑天鹅，那么黑天鹅的概率是多少呢？它是已知物种中迟迟没有出现黑色标本的那一部分，怎么样？这只是一个粗略的例子，我们将在本书中看到许多更深入的例子。</p>

<p class="noindent english">A related, frequently heard objection is “Data can’t replace human intuition.” In fact, it’s the other way around: human intuition can’t replace data. Intuition is what you use when you don’t know the facts, and since you often don’t, intuition is precious. But when the evidence is before you, why would you deny it? Statistical analysis beats talent scouts in baseball (as Michael Lewis memorably documented in <i>Moneyball</i>), it beats connoisseurs at wine tasting, and every day we see new examples of what it can do. Because of the influx of data, the boundary between evidence and intuition is shifting rapidly, and as with any revolution, entrenched ways have to be overcome. If I’m the expert on X at company Y, I don’t like to be overridden by some guy with data. There’s a saying in industry: “Listen to your customers, not to the HiPPO,” HiPPO being short for “highest paid person’s opinion.” If you want to be tomorrow’s authority, ride the data, don’t fight it.</p>

<p class="noindent chinese">一个相关的、经常听到的反对意见是 “数据不能取代人类的直觉”。事实上，情况恰恰相反：人类的直觉不能取代数据。直觉是你在不知道事实时使用的东西，由于你经常不知道，所以直觉很珍贵。但当证据摆在你面前时，你为什么要否认它呢？统计分析在棒球比赛中击败了球探（正如迈克尔·刘易斯在《<i>钱球</i>》中令人难忘的记录），它在品酒中击败了行家，而且每天我们都能看到它能做的新例子。由于数据的涌入，证据和直觉之间的界限正在迅速转变，正如任何革命一样，根深蒂固的方式必须被克服。如果我是 Y 公司的 X 专家，我不喜欢被一些有数据的人推翻。在工业界有一句话。“听从你的客户，而不是听从 HiPPO。” HiPPO 是 “最贵咨询费的人的意见” 的缩写。如果你想成为明天的权威，就要驾驭数据，不要与之对抗。</p>

<p class="noindent english">OK, some say, machine learning can find statistical regularities in data, but it will never discover anything deep, like Newton’s laws. It arguably hasn’t yet, but I bet it will. Stories of falling apples notwithstanding, deep scientific truths are not low-hanging fruit. Science goes through three phases, which we can call the Brahe, Kepler, and Newton <a id="babilu_link-138"></a> phases. In the Brahe phase, we gather lots of data, like Tycho Brahe patiently recording the positions of the planets night after night, year after year. In the Kepler phase, we fit empirical laws to the data, like Kepler did to the planets’ motions. In the Newton phase, we discover the deeper truths. Most science consists of Brahe- and Kepler-like work; Newton moments are rare. Today, big data does the work of billions of Brahes, and machine learning the work of millions of Keplers. If—let’s hope so—there are more Newton moments to be had, they are as likely to come from tomorrow’s learning algorithms as from tomorrow’s even more overwhelmed scientists, or at least from a combination of the two. (Of course, the Nobel prizes will go to the scientists, whether they have the key insights or just push the button. Learning algorithms have no ambitions of their own.) We’ll see in this book what those algorithms might look like and speculate about what they might discover—such as a cure for cancer.</p>

<p class="noindent chinese">好吧，有人说，机器学习可以找到数据中的统计规律性，但它永远不会发现任何深层次的东西，比如牛顿定律。可以说，它还没有，但我打赌它会的。尽管有苹果掉落的故事，但深奥的科学真理并不是垂手可得的水果。科学经历了三个阶段，我们可以称之为布拉赫、开普勒和牛顿阶段。在布拉赫阶段，我们收集大量的数据，就像第谷·布拉赫年复一年、夜复一夜耐心地记录行星的位置。在开普勒阶段，我们将经验法则与数据相结合，就像开普勒对行星运动所做的那样。在牛顿阶段，我们发现更深层次的真理。大多数科学由布拉赫和开普勒式的工作组成；牛顿时刻是罕见的。今天，大数据完成了数十亿布拉赫的工作，而机器学习完成了数百万开普勒的工作。如果 —— 让我们希望如此 —— 有更多的牛顿时刻，它们很可能来自于明天的学习算法和明天更加不堪重负的科学家，或者至少来自于两者的结合。（当然，诺贝尔奖将颁给科学家，无论他们是有关键的见解还是只是按下按钮。学习算法没有自己的野心）。我们将在本书中看到这些算法可能是什么样子的，并推测它们可能会发现什么 —— 比如治疗癌症的方法。</p>

<h1 id="babilu_link-393"><b>Is the Master Algorithm a fox or a hedgehog?</b></h1>

<h1 id="babilu_link-393"><b>主算法是一只狐狸还是一只刺猬？</b></h1>

<p class="noindent english">We need to consider one more potential objection to the Master Algorithm, perhaps the most serious one of all. It comes not from knowledge engineers or disgruntled experts, but from the machine-learning practitioners themselves. Putting that hat on for a moment, I might say: “But the Master Algorithm does not look like my daily life. I try hundreds of variations of many different learning algorithms on any given problem, and different algorithms do better on different problems. How could a single algorithm replace them all?”</p>

<p class="noindent chinese">我们需要考虑对 “主算法” 的另一个潜在反对意见，也许是最严重的一个。它不是来自知识工程师或心怀不满的专家，而是来自机器学习从业者本身。暂时戴上这顶帽子，我可能会说。“但主算法并不像我的日常生活。我在任何给定的问题上都会尝试许多不同的学习算法的数百种变化，不同的算法在不同的问题上表现更好。一个单一的算法怎么可能取代它们？”</p>

<p class="noindent english">To which the answer is: indeed. Wouldn’t it be nice if, instead of trying hundreds of variations of many algorithms, we just had to try hundreds of variations of a single one? If we can figure out what’s important and not so important in each one, what the important parts have in common and how they complement each other, we can, indeed, synthesize a Master Algorithm from them. That’s what we’re going to do in this book, or as close to it as we can. Perhaps you, dear reader, will have some ideas of your own as you read it.</p>

<p class="noindent chinese">对此，答案是：确实如此。如果我们不需要尝试许多算法的数百种变化，而只需要尝试单一算法的数百种变化，这不是很好吗？如果我们能够弄清楚每一种算法中什么是重要的，什么是不重要的，重要的部分有什么共同点，以及它们如何相互补充，我们确实可以从它们中合成一个主算法。这就是我们在这本书中要做的，或者说尽可能地接近它。也许你，亲爱的读者，在你阅读本书时，会有一些自己的想法。</p>

<p class="noindent english"><a id="babilu_link-134"></a> How complex will the Master Algorithm be? Thousands of lines of code? Millions? We don’t know yet, but machine learning has a delightful history of simple algorithms unexpectedly beating very fancy ones. In a famous passage of his book <i>The Sciences of the Artificial</i> , AI pioneer and Nobel laureate Herbert Simon asked us to consider an ant laboriously making its way home across a beach. The ant’s path is complex, not because the ant itself is complex but because the environment is full of dunelets to climb and pebbles to get around. If we tried to model the ant by programming in every possible path, we’d be doomed. Similarly, in machine learning the complexity is in the data; all the Master Algorithm has to do is assimilate it, so we shouldn’t be surprised if it turns out to be simple. The human hand is simple—four fingers, one opposable thumb—and yet it can make and use an infinite variety of tools. The Master Algorithm is to algorithms what the hand is to pens, swords, screwdrivers, and forks.</p>

<p class="noindent chinese">主算法将有多复杂？几千行的代码？还是几百万行？我们还不知道，但机器学习有一个令人愉快的历史，即简单的算法意外地击败了非常花哨的算法。人工智能先驱、诺贝尔奖获得者赫伯特·西蒙在其著作《<i>人工科学</i>》（The Sciences of the Artificial）中的一个著名段落中，要求我们考虑一只蚂蚁费力地穿过海滩回家的方式。蚂蚁的路径很复杂，不是因为蚂蚁本身很复杂，而是因为环境中充满了需要攀登的沙丘和需要绕行的卵石。如果我们试图通过对每一条可能的路径进行编程来模拟蚂蚁，我们将注定失败。同样地，在机器学习中，复杂性在于数据；主算法所要做的就是吸收它，所以如果它被证明是简单的，我们不应该感到惊讶。人类的手很简单 —— 四根手指，一个对立的拇指 —— 但它却能制造和使用无限多的工具。主算法对于算法来说，就像手对于笔、剑、螺丝刀和叉子一样。</p>

<p class="noindent english">As Isaiah Berlin memorably noted, some thinkers are foxes—they know many small things—and some are hedgehogs—they know one big thing. The same is true of learning algorithms. I hope the Master Algorithm is a hedgehog, but even if it’s a fox, we can’t catch it soon enough. The biggest problem with today’s learning algorithms is not that they are plural; it’s that, useful as they are, they still don’t do everything we’d like them to. Before we can discover deep truths with machine learning, we have to discover deep truths about machine learning.</p>

<p class="noindent chinese">正如以赛亚·伯林令人难忘地指出的那样，有些思想家是狐狸 —— 他们知道许多小东西，有些是刺猬 —— 他们知道一件大东西。学习算法的情况也是如此。我希望主算法是一只刺猬，但即使它是一只狐狸，我们也不能很快抓住它。今天的学习算法的最大问题不是它们是复数；而是，尽管它们很有用，但它们仍然不能做我们希望它们做的一切。在我们能够用机器学习发现深刻的真理之前，我们必须先发现关于机器学习的深刻真理。</p>

<h1 id="babilu_link-394"><b>What’s at stake</b></h1>

<h1 id="babilu_link-394"><b>关键是什么</b></h1>

<p class="noindent english">Suppose you’ve been diagnosed with cancer, and the traditional treatments—surgery, chemotherapy, and radiation therapy—have failed. What happens next will determine whether you live or die. The first step is to get the tumor’s genome sequenced. Companies like Foundation Medicine in Cambridge, Massachusetts, will do that for you: send them a sample of the tumor and they will send back a list of the known cancer-related mutations in its genome. This is needed because every cancer is different, and no single drug is likely to work for all. Cancers <a id="babilu_link-55"></a> mutate as they spread through your body, and by natural selection, the mutations most resistant to the drugs you’re taking are the most likely to grow. The right drug for you may be one that works for only 5 percent of patients, or you may need a combination of drugs that has never been tried before. Perhaps it will take a new drug designed specifically for your cancer, or a sequence of drugs to parry the cancer’s adaptations. Yet these drugs may have side effects that are deadly for you but not most other people. No doctor can keep track of all the information needed to predict the best treatment for you, given your medical history and your cancer’s genome. It’s an ideal job for machine learning, and yet today’s learners aren’t up to it. Each has some of the needed capabilities but is missing others. The Master Algorithm is the complete package. Applying it to vast amounts of patient and drug data, combined with knowledge mined from the biomedical literature, is how we will cure cancer.</p>

<p class="noindent chinese">假设你被诊断出患有癌症，而传统的治疗方法 —— 手术、化疗和放疗 —— 已经失败。接下来发生的事情将决定你是生是死。第一步是对肿瘤的基因组进行测序。像马萨诸塞州剑桥市的 Foundation Medicine 公司将为你做这件事：给他们发送一份肿瘤样本，他们将发回一份其基因组中已知的癌症相关突变的清单。之所以需要这样做，是因为每一种癌症都是不同的，而且没有一种药物可能对所有的癌症都有效。癌症因为它们在你的身体中传播，通过自然选择，对你正在服用的药物最有抵抗力的突变最有可能增长。适合你的药物可能是只对 5% 的病人有效的药物，或者你可能需要一种以前从未尝试过的药物组合。也许需要一种专门为你的癌症设计的新药，或一连串的药物来抵御癌症的适应性。然而，这些药物可能有副作用，对你来说是致命的，但对其他大多数人来说不是。鉴于你的病史和你的癌症基因组，没有一个医生能够跟踪所有需要预测对你最好治疗的信息。这是机器学习的理想工作，然而今天的学习者并不能胜任。每个人都有一些所需的能力，但却缺少其他能力。主算法是完整的软件包。将其应用于大量的病人和药物数据，结合从生物医学文献中挖掘的知识，是我们治愈癌症的方法。</p>

<p class="noindent english">A universal learner is sorely needed in many other areas, from life-and-death to mundane situations. Picture the ideal recommender system, one that recommends the books, movies, and gadgets you would pick for yourself if you had the time to check them all out. Amazon’s algorithm is a very far cry from it. That’s partly because it doesn’t have enough data—mainly it just knows which items you previously bought from Amazon—but if you went hog wild and gave it access to your complete stream of consciousness from birth, it wouldn’t know what to do with it. How do you transmute the kaleidoscope of your life, the myriad different choices you’ve made, into a coherent picture of who you are and what you want? This is well beyond the ken of today’s learners, but given enough data, the Master Algorithm should be able to understand you roughly as well as your best friend.</p>

<p class="noindent chinese">在许多其他领域，从生死攸关的情况到平凡的情况，都非常需要一个通用的学习者。想象一下理想的推荐系统，它可以推荐你自己会选择的书籍、电影和小工具，如果你有时间把它们都看完。亚马逊的算法与此相差甚远。这部分是因为它没有足够的数据 —— 主要是它只知道你以前在亚马逊买过哪些商品 —— 但如果你大肆宣传，让它获得你从出生开始的全部意识流，它就不知道该怎么做了。你如何将你生活中的万花筒，你所做的无数不同的选择，转化为你是谁和你想要什么的连贯画面？这远远超出了今天的学习者的能力，但如果有足够的数据，主算法应该能够像你最好的朋友一样大致了解你。</p>

<p class="noindent english">Someday there’ll be a robot in every house, doing the dishes, making the beds, even looking after the children while the parents work. How soon depends on how hard finding the Master Algorithm turns out to be. If the best we can do is combine many different learners, each of which solves only a small part of the AI problem, we’ll soon run into the <a id="babilu_link-192"></a> complexity wall. This piecemeal approach worked for <i>Jeopardy!</i> , but few believe tomorrow’s housebots will be Watson’s grandchildren. It’s not that the Master Algorithm will single-handedly crack AI; there’ll still be great feats of engineering to perform, and Watson is a good preview of them. But the 80/20 rule applies: the Master Algorithm will be 80 percent of the solution and 20 percent of the work, so it’s surely the best place to start.</p>

<p class="noindent chinese">有一天，每家每户都会有一个机器人，洗碗、铺床，甚至在父母工作时照顾孩子。多长时间取决于找到主算法有多难。如果我们能做的最好的事情是结合许多不同的学习者，每一个学习者只解决人工智能问题的一小部分，我们很快就会遇到复杂性的墙。这种零敲碎打的方法在《<i>危险游戏</i>》中很有效，但很少有人相信明天的家庭机器人会是沃森的孙子。这并不是说主算法将单枪匹马地破解人工智能；仍将有伟大的工程壮举需要完成，沃森就是一个很好的预告。但 80/20 法则适用：主算法将是 80% 的解决方案和 20% 的工作，所以它肯定是最好的起点。</p>

<p class="noindent english">The Master Algorithm’s impact on technology will not be limited to AI. A universal learner is a phenomenal weapon against the complexity monster. Systems that today are too complex to build will no longer be. Computers will do more with less help from us. They will not repeat the same mistakes over and over again, but learn with practice, like people do. Sometimes, like the butlers of legend, they’ll even guess what we want before we express it. If computers make us smarter, computers running the Master Algorithm will make us feel like geniuses. Technological progress will noticeably speed up, not just in computer science but in many different fields. This in turn will add to economic growth and speed poverty’s decline. With the Master Algorithm to help synthesize and distribute knowledge, the intelligence of an organization will be more than the sum of its parts, not less. Routine jobs will be automated and replaced by more interesting ones. Every job will be done better than it is today, whether by a better-trained human, a computer, or a combination of the two. Stock-market crashes will be fewer and smaller. With a fine grid of sensors covering the globe and learned models to make sense of its output moment by moment, we will no longer be flying blind; the health of our planet will take a turn for the better. A model of you will negotiate the world on your behalf, playing elaborate games with other people’s and entities’ models. And as a result of all this, our lives will be longer, happier, and more productive.</p>

<p class="noindent chinese">主算法对技术的影响将不仅限于人工智能。一个通用的学习者是对抗复杂性怪物的一个惊人的武器。今天太过复杂而无法构建的系统将不再是这样。计算机将在我们较少的帮助下做更多的事情。它们将不会重复同样的错误，而是像人一样在实践中学习。有时，就像传说中的管家一样，他们甚至会在我们表达之前猜到我们想要什么。如果计算机使我们更聪明，那么运行主算法的计算机将使我们感到自己是天才。技术进步将明显加快，不仅仅是在计算机科学领域，而是在许多不同的领域。这反过来又会增加经济增长，加速贫困的减少。有了主算法来帮助综合和分配知识，一个组织的智慧将超过其各部分的总和，而不是更少。常规工作将被自动化，并被更有趣的工作所取代。每项工作都将比现在做得更好，无论是由训练有素的人、计算机还是两者的结合。股票市场的崩溃将更少更小。有了覆盖全球的传感器的精细网格和学习过的模型，我们将不再盲目飞行；我们星球的健康将转好。一个你的模型将代表你与世界进行谈判，与其他人和实体的模型进行精心的游戏。而作为这一切的结果，我们的生命将更长久，更快乐，更有成效。</p>

<p class="noindent english">Because the potential impact is so great, it would behoove us to try to invent the Master Algorithm even if the odds of success were low. And even if it takes a long time, searching for a universal learner has many immediate benefits. One is the better understanding of machine <a id="babilu_link-84"></a> learning that a unified view enables. Too many business decisions are made with scant understanding of the analytics underpinning them, but it doesn’t have to be that way. To use a technology, we don’t need to master its inner workings, but we do need to have a good conceptual model of it. We need to know how to find a station on the radio, or change the volume. Today, those of us who aren’t machine-learning experts have no conceptual model of what a learner does. The algorithms we drive when we use Google, Facebook, or the latest analytics suite are a bit like a black limo with tinted windows that mysteriously shows up at our door one night: Should we get in? Where will it take us? It’s time to get in the driver’s seat. Knowing the assumptions that different learners make will help us pick the right one for the job, instead of going with a random one that fell into our lap—and then suffering with it for years, painfully rediscovering what we should have known from the start. By knowing what learners optimize, we can make certain they optimize what we care about, rather than what came in the box. Perhaps most important, once we know how a particular learner arrives at its conclusions, we’ll know what to make of that information—what to believe, what to return to the manufacturer, and how to get a better result next time around. And with the universal learner we’ll develop in this book as the conceptual model, we can do all this without cognitive overload. Machine learning is simple at heart; we just need to peel away the layers of math and jargon to reveal the innermost Russian doll.</p>

<p class="noindent chinese">由于潜在的影响是如此之大，即使成功的几率很低，我们也应该尝试发明主算法。而且，即使需要很长的时间，寻找一个通用的学习者也有许多直接的好处。一个是统一的观点所带来的对机器学习的更好理解。太多的商业决策是在对支撑它们的分析方法了解甚少的情况下做出的，但它不一定是这样的。为了使用一项技术，我们不需要掌握它的内部工作原理，但我们确实需要有一个良好的概念性模型。我们需要知道如何在收音机上找到一个电台，或改变音量。今天，我们这些不是机器学习专家的人，对学习者的工作没有概念模型。当我们使用谷歌、Facebook 或最新的分析套件时，我们驱动的算法有点像一辆黑色的豪华轿车，带着有色的窗户，某天晚上神秘地出现在我们的门口。我们应该上车吗？它将带我们去哪里？现在是时候进入驾驶座了。了解不同学习者的假设可以帮助我们挑选合适的学习者，而不是随意选择一个落入我们手中的学习者 —— 然后忍受多年，痛苦地重新发现我们一开始就应该知道的东西。通过了解学习者优化的内容，我们可以确定他们优化的是我们所关心的，而不是盒子里的东西。也许最重要的是，一旦我们知道一个特定的学习者是如何得出结论的，我们就会知道该如何处理这些信息 —— 相信什么，把什么还给制造商，以及如何在下一次获得更好的结果。有了我们将在本书中开发的通用学习器作为概念模型，我们就可以在没有认知过载的情况下完成这一切。机器学习的本质是简单的；我们只需要剥去一层层的数学和行话，就可以看到最里面的俄罗斯娃娃。</p>

<p class="noindent english">These benefits apply in both our personal and professional lives. How do I make the best of the trail of data that my every step in the modern world leaves? Every transaction works on two levels: what it accomplishes for you and what it teaches the system you just interacted with. Being aware of this is the first step to a happy life in the twenty-first century. Teach the learners, and they will serve you; but first you need to understand them. What in my job can be done by a learning algorithm, what can’t, and—most important—how can I take advantage of machine learning to do it better? The computer is your tool, not your adversary. Armed with machine learning, a manager becomes a <a id="babilu_link-195"></a> supermanager, a scientist a superscientist, an engineer a superengineer. The future belongs to those who understand at a very deep level how to combine their unique expertise with what algorithms do best.</p>

<p class="noindent chinese">这些好处在我们的个人和职业生活中都适用。我怎样才能最好地利用我在现代世界的每一步所留下的数据痕迹？每笔交易都在两个层面上发挥作用：它为你完成了什么，以及它给你刚刚与之互动的系统带来了什么。意识到这一点是在二十一世纪获得幸福生活的第一步。教导学习者，他们就会为你服务；但首先你需要了解他们。在我的工作中，哪些可以由学习算法完成，哪些不能，以及 —— 最重要的是 —— 我如何利用机器学习的优势来做得更好？计算机是你的工具，而不是你的对手。在机器学习的武装下，一个经理人成为超级经理人，一个科学家成为超级科学家，一个工程师成为超级工程师。未来属于那些在很深的层次上了解如何将他们独特的专业知识与算法的最佳表现相结合的人。</p>

<p class="noindent english">But perhaps the Master Algorithm is a Pandora’s box best left closed. Will computers enslave us or even exterminate us? Will machine learning be the handmaiden of dictators or evil corporations? Knowing where machine learning is headed will help us to understand what to worry about, what not, and what to do about it. The <i>Terminator</i> scenario, where a super-AI becomes sentient and subdues mankind with a robot army, has no chance of coming to pass with the kinds of learning algorithms we’ll meet in this book. Just because computers can learn doesn’t mean they magically acquire a will of their own. Learners learn to achieve the goals we set them; they don’t get to change the goals. Rather, we need to worry about them trying to serve us in ways that do more harm than good because they don’t know any better, and the cure for that is to teach them better.</p>

<p class="noindent chinese">但也许主算法是一个潘多拉的盒子，最好不要打开。计算机会不会奴役我们，甚至灭绝我们？机器学习会成为独裁者或邪恶公司的婢女吗？了解机器学习的发展方向将有助于我们了解什么需要担心，什么不需要担心，以及该如何应对。《<i>终结者</i>》（Terminator）的情景，即一个超级人工智能变得有知觉并以机器人军队征服人类，在本书中我们将见到的各种学习算法中，没有机会成为现实。计算机能够学习并不意味着它们会神奇地获得自己的意志。学习者通过学习来实现我们给他们设定的目标；他们并不能改变目标。相反，我们需要担心的是，它们试图以弊大于利的方式为我们服务，因为它们不知道有什么更好的办法，而解决这个问题的办法就是教会它们更好的办法。</p>

<p class="noindent english">Most of all, we have to worry about what the Master Algorithm could do in the wrong hands. The first line of defense is to make sure the good guys get it first—or, if it’s not clear who the good guys are, to make sure it’s open-sourced. The second is to realize that, no matter how good the learning algorithm is, it’s only as good as the data it gets. He who controls the data controls the learner. Your reaction to the datafication of life should not be to retreat to a log cabin—the woods, too, are full of sensors—but to aggressively seek control of the data that matters to you. It’s good to have recommenders that find what you want and bring it to you; you’d feel lost without them. But they should bring you what <i>you</i> want, not what someone else wants you to have. Control of data and ownership of the models learned from it is what many of the twenty-first century’s battles will be about—between governments, corporations, unions, and individuals. But you also have an ethical duty to share data for the common good. Machine learning alone will not cure cancer; cancer patients will, by sharing their data for the benefit of future patients.</p>

<p class="noindent chinese">最重要的是，我们必须担心主算法落入坏人之手会有什么后果。第一道防线是确保好人先得到它 —— 或者，如果不清楚谁是好人，确保它是开源的。第二道防线是要认识到，无论学习算法有多好，它的好坏只取决于它得到的数据。谁控制了数据，谁就控制了学习者。你对生活数据化的反应不应该是退缩到一个木屋里 —— 森林里也充满了传感器，而是要积极地寻求对你重要的数据的控制。有推荐人找到你想要的东西并把它带给你是件好事；没有他们你会感到迷失。但他们应该给你带来 <i>你</i>想要的东西，而不是别人希望你拥有的东西。对数据的控制和从数据中学习到的模型的所有权，是 21 世纪政府、公司、工会和个人之间的许多争斗的内容。但你也有道德义务为公共利益分享数据。仅仅依靠机器学习是无法治愈癌症的；癌症患者可以通过分享他们的数据来造福未来的患者。</p>

<h1 id="babilu_link-395"><b><a id="babilu_link-223"><b></b></a> A different theory of everything</b></h1>

<h1 id="babilu_link-395"><b><a id="babilu_link-223"><b></b></a>关于万物的不同理论</b></h1>

<p class="noindent english">Science today is thoroughly balkanized, a Tower of Babel where each subcommunity speaks its own jargon and can see only into a few adjacent subcommunities. The Master Algorithm would provide a unifying view of all of science and potentially lead to a new theory of everything. At first this may seem like an odd claim. What machine learning does is induce theories from data. How could the Master Algorithm itself grow into a theory? Isn’t string theory the theory of everything, and the Master Algorithm nothing like it?</p>

<p class="noindent chinese">今天的科学是彻底的巴尔干化，是一座巴别塔，每个子社区都说自己的行话，只能看到相邻的几个子社区。主算法将为所有的科学提供一个统一的观点，并有可能导致一个新的万物理论。乍一看，这似乎是一个奇怪的说法。机器学习所做的是从数据中引出理论。主算法本身怎么可能发展成为一种理论呢？弦理论不就是万物理论吗，而 “主算法” 和它完全不同？</p>

<p class="noindent english">To answer these questions, we have to first understand what a scientific theory is and is not. A theory is a set of constraints on what the world could be, not a complete description of it. To obtain the latter, you have to combine the theory with data. For example, consider Newton’s second law. It says that force equals mass times acceleration, or <i>F</i> = <i>ma</i> . It does not say what the mass or acceleration of any object are, or the forces acting on it. It only requires that, if the mass of an object is <i>m</i> and its acceleration is <i>a</i> , then the total force on it must be <i>ma</i> . It removes some of the universe’s degrees of freedom, but not all. The same is true of all other physical theories, including relativity, quantum mechanics, and string theory, which are, in effect, refinements of Newton’s laws.</p>

<p class="noindent chinese">要回答这些问题，我们首先要了解科学理论是什么，不是什么。一个理论是对世界可能是什么的一组约束，而不是对世界的完整描述。为了获得后者，你必须将理论与数据相结合。例如，考虑牛顿的第二定律。它说，力等于质量乘以加速度，或 <i>F</i> = <i>ma</i>。它并没有说任何物体的质量或加速度是什么，也没有说作用在它身上的力。它只要求，如果一个物体的质量是 <i>m</i>，其加速度是 <i>a</i>，那么它所受的总力必须是 <i>ma</i>。它取消了宇宙的一些自由度，但不是全部。所有其他的物理理论也是如此，包括相对论、量子力学和弦理论，它们实际上都是对牛顿定律的改进。</p>

<p class="noindent english">The power of a theory lies in how much it simplifies our description of the world. Armed with Newton’s laws, we only need to know the masses, positions, and velocities of all objects at one point in time; their positions and velocities at all times follow. So Newton’s laws reduce our description of the world by a factor of the number of distinguishable instants in the history of the universe, past and future. Pretty amazing! Of course, Newton’s laws are only an approximation of the true laws of physics, so let’s replace them with string theory, ignoring all its problems and the question of whether it can ever be empirically validated. Can we do better? Yes, for two reasons.</p>

<p class="noindent chinese">一个理论的力量在于它在多大程度上简化了我们对世界的描述。有了牛顿定律，我们只需要知道所有物体在一个时间点上的质量、位置和速度；它们在所有时间的位置和速度都是如此。因此，牛顿定律将我们对世界的描述减少了一个因素，那就是宇宙历史上可区分的时刻的数量，过去和未来。相当惊人当然，牛顿定律只是真正的物理学定律的近似值，所以让我们用弦理论来取代它，忽略它的所有问题以及它是否能被经验验证的问题。我们能做得更好吗？是的，原因有二。</p>

<p class="noindent english">The first is that, in reality, we never have enough data to completely determine the world. Even ignoring the uncertainty principle, precisely <a id="babilu_link-267"></a> knowing the positions and velocities of all particles in the world at some point in time is not remotely feasible. And because the laws of physics are chaotic, uncertainty compounds over time, and pretty soon they determine very little indeed. To accurately describe the world, we need a fresh batch of data at regular intervals. In effect, the laws of physics only tell us what happens locally. This drastically reduces their power.</p>

<p class="noindent chinese">首先是，在现实中，我们永远没有足够的数据来完全确定这个世界。即使忽略不确定性原则，准确地知道世界上所有粒子在某个时间点的位置和速度也是不可行的。而且，由于物理学定律是混乱的，不确定性随着时间的推移而加剧，很快他们就会确定很少的东西。为了准确地描述这个世界，我们需要定期获得一批新的数据。实际上，物理定律只告诉我们局部发生了什么。这极大地降低了它们的力量。</p>

<p class="noindent english">The second problem is that, even if we had complete knowledge of the world at some point in time, the laws of physics would still not allow us to determine its past and future. This is because the sheer amount of computation required to make those predictions would be beyond the capabilities of any imaginable computer. In effect, to perfectly simulate the universe we would need another, identical universe. This is why string theory is mostly irrelevant outside of physics. The theories we have in biology, psychology, sociology, or economics are not corollaries of the laws of physics; they had to be created from scratch. We assume that they are approximations of what the laws of physics would predict when applied at the scale of cells, brains, and societies, but there’s no way to know.</p>

<p class="noindent chinese">第二个问题是，即使我们在某个时间点对世界有完整的了解，物理学定律仍然不允许我们确定其过去和未来。这是因为进行这些预测所需的巨大计算量将超出任何可想象的计算机的能力。实际上，为了完美地模拟宇宙，我们需要另一个相同的宇宙。这就是为什么弦理论在物理学之外大多是不相关的。我们在生物学、心理学、社会学或经济学方面的理论不是物理学定律的推论；它们必须从头开始创建。我们假设它们是物理学定律在应用于细胞、大脑和社会规模时的近似预测，但我们没有办法知道。</p>

<p class="noindent english">Unlike the theories of a given field, which only have power within that field, the Master Algorithm has power across all fields. Within field X, it has less power than field X’s prevailing theory, but across all fields—when we consider the whole world—it has vastly more power than any other theory. The Master Algorithm is the germ of every theory; all we need to add to it to obtain theory X is the minimum amount of data required to induce it. (In the case of physics, that would be just the results of perhaps a few hundred key experiments.) The upshot is that, pound for pound, the Master Algorithm may well be the best starting point for a theory of everything we’ll ever have. <i>Pace</i> Stephen Hawking, it may ultimately tell us more about the mind of God than string theory.</p>

<p class="noindent chinese">与某一领域的理论不同的是，主算法只在该领域内具有影响力，而主算法在所有领域都具有影响力。在 X 领域内，它的力量不如 X 领域的主流理论，但在所有领域中 —— 当我们考虑整个世界时 —— 它的力量远远超过任何其他理论。主算法是每一个理论的萌芽；我们需要加入它以获得 X 理论的，只是诱发它所需的最小数据量。（在物理学的情况下，这只是几百个关键实验的结果）。结果是，主算法很可能是我们将拥有的万物理论的最佳起点。<i>按照</i>霍金的说法，它最终可能比弦理论更能告诉我们关于上帝的思想。</p>

<p class="noindent english">Some may say that seeking a universal learner is the epitome of techno-hubris. But dreaming is not hubris. Maybe the Master Algorithm will take its place among the great chimeras, alongside the philosopher’s stone and the perpetual motion machine. Or perhaps it will be more like finding the longitude at sea, given up as too difficult until <a id="babilu_link-268"></a> a lone genius solved it. More likely, it will be the work of generations, raised stone by stone like a cathedral. The only way to find out is to get up early one day and set out on the journey.</p>

<p class="noindent chinese">有些人可能会说，寻求一个普遍的学习者是技术性狂妄的缩影。但梦想并不是狂妄。也许 “主算法” 将在伟大的奇美拉中占有一席之地，与哲学家的石头和永动机并列。或者，也许它将更像在海上寻找经度，因为太难而被放弃，直到一个孤独的天才解决了它。更有可能的是，它将是几代人的工作，像大教堂一样一石一石垒起来。唯一的办法是在某一天早起，踏上旅程。</p>

<h1 id="babilu_link-396"><b>Candidates that don’t make the cut</b></h1>

<h1 id="babilu_link-396"><b>未能入选的候选人</b></h1>

<p class="noindent english">So, if the Master Algorithm exists, what is it? A seemingly obvious candidate is memorization: just remember everything you’ve seen; after a while you’ll have seen everything there is to see, and therefore know everything there is to know. The problem with this is that, as Heraclitus said, you never step in the same river twice. There’s far more to see than you ever could. No matter how many snowflakes you’ve examined, the next one will be different. Even if you had been present at the Big Bang and everywhere since, you would still have seen only a tiny fraction of what you could see in the future. If you had witnessed life on Earth up to ten thousand years ago, that would not have prepared you for what was to come. Someone who grew up in one city doesn’t become paralyzed when they move to another, but a robot capable only of memorization would. Besides, knowledge is not just a long list of facts. Knowledge is general, and has structure. “All humans are mortal” is much more succinct than seven billion statements of mortality, one for each human. Memorization gives us none of these things.</p>

<p class="noindent chinese">那么，如果主算法存在，它是什么？一个看似明显的候选者是记忆：只要记住你所看到的一切；一段时间后，你就会看到所有可以看到的东西，因此知道所有可以知道的东西。这样做的问题是，正如赫拉克利特所说，你永远不会在同一条河里踩两次。要看的东西远比你能看到的多。无论你检查过多少片雪花，下一片都会不同。即使你在宇宙大爆炸时和之后的任何地方都在场，你所看到的仍然只是你在未来能看到的东西的极小部分。如果你在一万年前见证了地球上的生命，那也不会让你对即将到来的事情有所准备。一个在一个城市长大的人在搬到另一个城市时不会变得瘫痪，但一个只懂得记忆的机器人会。此外，知识不只是一长串的事实。知识是一致的，而且有结构。“所有的人都是凡人” 比 70 亿条关于每个人都有一个的凡人的声明要简洁得多。背诵没有给我们带来这些东西。</p>

<p class="noindent english">Another candidate Master Algorithm is the microprocessor. After all, the one in your computer can be viewed as a single algorithm whose job is to execute other algorithms, like a universal Turing machine; and it can run any imaginable algorithm, up to its limits of memory and speed. In effect, to a microprocessor an algorithm is just another kind of data. The problem here is that, by itself, the microprocessor doesn’t know how to do anything; it just sits there idle all day. Where do the algorithms it runs come from? If they were coded up by a human programmer, no learning is involved. Nevertheless, there’s a sense in which the microprocessor is a good analog for the Master Algorithm. A microprocessor is not the best hardware for running any particular <a id="babilu_link-81"></a> algorithm. That would be an ASIC (application-specific integrated circuit) designed very precisely for that algorithm. Yet microprocessors are what we use for almost all applications, because their flexibility trumps their relative inefficiency. If we had to build an ASIC for every new application, the Information Revolution would never have happened. Similarly, the Master Algorithm is not the best algorithm for learning any particular piece of knowledge; that would be an algorithm that already encodes most of that knowledge (or all of it, making the data superfluous). The point, however, is to induce the knowledge from data, because it’s easier and costs less; so the more general the learning algorithm, the better.</p>

<p class="noindent chinese">另一个候选主算法是微处理器。毕竟，你电脑中的那个可以被看作是一个单一的算法，其工作是执行其他算法，就像一个通用的图灵机；它可以运行任何可以想象的算法，直到其内存和速度的极限。实际上，对微处理器来说，算法只是另一种数据。这里的问题是，微处理器本身并不知道如何做任何事情；它只是整天坐在那里闲着。它所运行的算法从何而来？如果它们是由人类程序员编出来的，就不涉及学习。然而，从某种意义上说，微处理器是主算法的一个很好的模拟。微处理器不是运行任何特定算法的最佳硬件。那将是一个为该算法非常精确设计的 ASIC（特定应用集成电路）。然而，微处理器是我们用于几乎所有应用的东西，因为其灵活性胜过其相对的低效率。如果我们必须为每一个新的应用建立一个 ASIC，信息革命就不会发生。同样，主算法并不是学习任何特定知识的最佳算法；那将是一个已经编码了大部分知识（或全部知识，使数据变得多余）的算法。然而，关键是要从数据中诱导出知识，因为这更容易，成本更低；所以学习算法越通用越好。</p>

<p class="noindent english">An even more extreme candidate is the humble NOR gate: a logic switch whose output is 1 only if its inputs are both 0. Recall that all computers are made of logic gates built out of transistors, and all computations can be reduced to combinations of AND, OR, and NOT gates. A NOR gate is just an OR gate followed by a NOT gate: the negation of a disjunction, as in “I’m happy as long as I’m not starving or sick.” AND, OR and NOT can all be implemented using NOR gates, so NOR can do everything, and in fact it’s all some microprocessors use. So why can’t it be the Master Algorithm? It’s certainly unbeatable for simplicity. Unfortunately, a NOR gate is not the Master Algorithm any more than a Lego brick is the universal toy. It can certainly be a universal building block for toys, but a pile of Legos doesn’t spontaneously assemble itself into a toy. The same applies to other simple computation schemes, like Petri nets or cellular automata.</p>

<p class="noindent chinese">一个更极端的候选者是简陋的 NOR 门：一个逻辑开关，只有当它的输入都是 0 时，其输出才是 1。回顾一下，所有的计算机都是由晶体管组成的逻辑门，所有的计算都可以简化为 AND、OR 和 NOT 门的组合。一个 NOR 门只是一个 OR 门，后面跟着一个 NOT 门：否定一个不连接，就像 “只要我不饿不病，我就很高兴”。AND、OR 和 NOT 都可以用 NOR 门来实现，所以 NOR 可以做任何事情，事实上，一些微处理器就使用它。那么为什么它不能成为主算法呢？它的简单性当然是无可匹敌的。不幸的是，NOR 门不是主算法，就像乐高砖是通用玩具一样。它当然可以成为玩具的通用积木，但一堆乐高积木并不能自发地组装成玩具。这同样适用于其他简单的计算方案，如 Petri 网或细胞自动机。</p>

<p class="noindent english">Moving on to more sophisticated alternatives, what about the queries that any good database engine can answer, or the simple algorithms in a statistical package? Aren’t those enough? These are bigger Lego bricks, but they’re still only bricks. A database engine never discovers anything new; it just tells you what it knows. Even if all the humans in a database are mortal, it doesn’t occur to it to generalize mortality to other humans. (Database engineers would blanch at the thought.) Much of statistics is about testing hypotheses, but someone has to formulate <a id="babilu_link-243"></a> them in the first place. Statistical packages can do linear regression and other simple procedures, but these have a very low limit on what they can learn, no matter how much data you feed them. The better packages cross into the gray zone between statistics and machine learning, but there are still many kinds of knowledge they can’t discover.</p>

<p class="noindent chinese">接着是更复杂的替代方案，那么任何好的数据库引擎都能回答的查询，或者统计软件包中的简单算法呢？难道这些还不够吗？这些是更大的乐高积木，但它们仍然只是积木。一个数据库引擎从来没有发现任何新的东西；它只是告诉你它所知道的东西。即使数据库中的所有人类都是死亡的，它也不会想到把死亡归纳为其他人类。（数据库工程师会对这种想法感到震惊。）统计学的大部分内容是关于测试假设的，但首先必须有人制定它们。统计软件包可以做线性回归和其他简单的程序，但是无论你给它们提供多少数据，它们能学到的东西都是非常有限的。更好的软件包跨越了统计学和机器学习之间的灰色地带，但仍有许多种类的知识它们无法发现。</p>

<p class="noindent english">OK, it’s time to come clean: the Master Algorithm is the equation <i>U(X)</i> = 0. Not only does it fit on a T-shirt; it fits on a postage stamp. Huh? <i>U(X)</i> = 0 just says that some (possibly very complex) function <i>U</i> of some (possibly very complex) variable <i>X</i> is equal to 0. Every equation can be reduced to this form; for example, <i>F = ma</i> is equivalent to <i>F</i> –<i>ma</i> = 0, so if you think of <i>F</i> –<i>ma</i> as a function <i>U</i> of <i>F</i> , voilà: <i>U(F)</i> = 0. In general, <i>X</i> could be any input and <i>U</i> could be any algorithm, so surely the Master Algorithm can’t be any more general than this; and since we’re looking for the most general algorithm we can find, this must be it. I’m just kidding, of course, but this particular failed candidate points to a real danger in machine learning: coming up with a learner that’s so general, it doesn’t have enough content to be useful.</p>

<p class="noindent chinese">好吧，是时候说清楚了：主算法是方程 <i>U（X）= 0</i>。它不仅适合在 T 恤衫上，还适合在邮票上。咦？<i>U（X）= 0</i> 只是说某个（可能非常复杂）变量 <i>X</i> 的某个（可能非常复杂）函数 <i>U</i> 等于 0。一般来说，<i>X</i> 可以是任何输入，<i>U</i> 可以是任何算法，所以主算法肯定不会比这更通用；既然我们在寻找我们能找到的最通用的算法，这一定是它。当然，我只是在开玩笑，但这个特别失败的候选者指出了机器学习中的一个真正的危险：想出的学习者太普遍了，它没有足够的内容来发挥作用。</p>

<p class="noindent english">So what’s the least content a learner can have in order to be useful? How about the laws of physics? After all, everything in the world obeys them (we believe), and they gave rise to evolution and (through it) the brain. Well, perhaps the Master Algorithm is implicit in the laws of physics, but if so, then we need to make it explicit. Just throwing data at the laws of physics won’t result in any new laws. Here’s one way to think about it: perhaps some field’s master theory is just the laws of physics compiled into a more convenient form for that field, but if so then we need an algorithm that finds a shortcut from that field’s data to its theory, and it’s not clear the laws of physics can be of any help with this. Another issue is that, if the laws of physics were different, the Master Algorithm would presumably still be able to discover them in many cases. Mathematicians like to say that God can disobey the laws of physics, but even he cannot defy the laws of logic. This may be so, but the laws of logic are for deduction; what we need is something equivalent, but for induction.</p>

<p class="noindent chinese">那么，一个学习者最不可能拥有的内容是什么，才是有用的？物理学定律如何？毕竟，世界上的一切都遵守它们（我们相信），而且它们催生了进化和（通过它）大脑。好吧，也许主算法是隐含在物理学定律中的，但如果是这样，那么我们就需要把它明确化。仅仅向物理学定律投掷数据不会产生任何新的定律。这里有一种思考方式：也许某些领域的主理论只是将物理学定律编译成对该领域更方便的形式，但如果是这样，那么我们就需要一种算法，从该领域的数据到其理论中找到一条捷径，而且不清楚物理学定律在这方面有什么帮助。另一个问题是，如果物理定律不同，主算法大概仍能在许多情况下发现它们。数学家们喜欢说，上帝可以不服从物理学定律，但即使是他也不能违背逻辑定律。这也许是真的，但逻辑定律是用于演绎的；我们需要的是等同的东西，但用于归纳。</p>

<h1 id="babilu_link-397"><b><a id="babilu_link-61"><b></b></a> The five tribes of machine learning</b></h1>

<h1 id="babilu_link-397"><b><a id="babilu_link-61"><b></b></a>机器学习的五个部落</b></h1>

<p class="noindent english">Of course, we don’t have to start from scratch in our hunt for the Master Algorithm. We have a few decades of machine learning research to draw on. Some of the smartest people on the planet have devoted their lives to inventing learning algorithms, and some would even claim that they already have a universal learner in hand. We will stand on the shoulders of these giants, but take such claims with a grain of salt. Which raises the question: how will we know when we’ve found the Master Algorithm? When the same learner, with only parameter changes and minimal input aside from the data, can understand video and text as well as humans, and make significant new discoveries in biology, sociology, and other sciences. Clearly, by this standard no learner has yet been demonstrated to be the Master Algorithm, even in the unlikely case one already exists.</p>

<p class="noindent chinese">当然，我们在寻找主算法的过程中不必从头开始。我们有几十年的机器学习研究可以借鉴。地球上一些最聪明的人已经把他们的生命献给了发明学习算法，有些人甚至会声称他们已经掌握了一个通用的学习者。我们将站在这些巨人的肩膀上，但对这种说法要慎重对待。这就提出了一个问题：我们将如何知道我们何时找到了主算法？当同一个学习者，除了数据之外，只需改变参数和最小的输入，就能像人类一样理解视频和文本，并在生物学、社会学和其他科学领域做出重大的新发现。显然，按照这个标准，还没有一个学习者被证明是主算法，即使在不太可能的情况下，一个主算法已经存在。</p>

<p class="noindent english">Crucially, the Master Algorithm is not required to start from scratch in each new problem. That bar is probably too high for <i>any</i> learner to meet, and it’s certainly very unlike what people do. For example, language does not exist in a vacuum; we couldn’t understand a sentence without our knowledge of the world it refers to. Thus, when learning to read, the Master Algorithm can rely on having previously learned to see, hear, and control a robot. Likewise, a scientist does not just blindly fit models to data; he can bring all his knowledge of the field to bear on the problem. Therefore, when making discoveries in biology, the Master Algorithm can first read all the biology it wants, relying on having previously learned to read. The Master Algorithm is not just a passive consumer of data; it can interact with its environment and actively seek the data it wants, like Adam, the robot scientist, or like any child exploring her world.</p>

<p class="noindent chinese">最重要的是，主算法不需要在每个新问题上从头开始。对<i>任何</i>学习者来说，这个标准可能太高了，而且这肯定与人们的工作非常不同。例如，语言并不存在于真空中；如果没有我们对它所指的世界的了解，我们就无法理解一个句子。因此，在学习阅读时，主算法可以依靠之前学习过的看、听和控制机器人。同样，科学家也不会盲目地将模型与数据相匹配；他可以将自己在该领域的所有知识用于解决这个问题。因此，当在生物学中做出发现时，主算法可以首先阅读它想要的所有生物学知识，这有赖于之前学会阅读。主算法不只是一个被动的数据消费者；它可以与环境互动，主动寻找它想要的数据，就像机器人科学家亚当，或者像任何探索她的世界的孩子。</p>

<p class="noindent english">Our search for the Master Algorithm is complicated, but also enlivened, by the rival schools of thought that exist within machine learning. The main ones are the symbolists, connectionists, evolutionaries, Bayesians, and analogizers. Each tribe has a set of core beliefs, and a particular problem that it cares most about. It has found a solution to <a id="babilu_link-98"></a> that problem, based on ideas from its allied fields of science, and it has a master algorithm that embodies it.</p>

<p class="noindent chinese">我们对主算法的探索是复杂的，但也是活跃的，因为在机器学习中存在着相互竞争的思想流派。主要有符号派、连接派、进化派、贝叶斯派和类比派。每个部落都有一套核心信念，和一个它最关心的特定问题。它已经找到了一个解决方案，以该问题，基于其联盟科学领域的想法，并有一个体现它的主算法。</p>

<p class="noindent english">For symbolists, all intelligence can be reduced to manipulating symbols, in the same way that a mathematician solves equations by replacing expressions by other expressions. Symbolists understand that you can’t learn from scratch: you need some initial knowledge to go with the data. They’ve figured out how to incorporate preexisting knowledge into learning, and how to combine different pieces of knowledge on the fly in order to solve new problems. Their master algorithm is inverse deduction, which figures out what knowledge is missing in order to make a deduction go through, and then makes it as general as possible.</p>

<p class="noindent chinese">对符号主义者来说，所有的智力都可以简化为操纵符号，就像数学家通过用其他表达式替换表达式来解决方程一样。符号主义者明白，你不可能从头开始学习：你需要一些初始知识来配合数据。他们已经想出了如何将已有的知识纳入学习，以及如何在飞行中结合不同的知识片段以解决新问题。他们的主算法是逆向推理，它找出为了使推理顺利进行而缺少的知识，然后使其尽可能地通用。</p>

<p class="noindent english">For connectionists, learning is what the brain does, and so what we need to do is reverse engineer it. The brain learns by adjusting the strengths of connections between neurons, and the crucial problem is figuring out which connections are to blame for which errors and changing them accordingly. The connectionists’ master algorithm is backpropagation, which compares a system’s output with the desired one and then successively changes the connections in layer after layer of neurons so as to bring the output closer to what it should be.</p>

<p class="noindent chinese">对于连接主义者来说，学习是大脑的工作，因此我们需要做的是对其进行反向工程。大脑通过调整神经元之间的连接强度来学习，关键的问题是弄清楚哪些连接是造成哪些错误的原因，并相应地改变它们。连接学家的主要算法是反向传播，它将一个系统的输出与所需的输出进行比较，然后连续改变一层又一层神经元的连接，以使输出更接近它应该是的样子。</p>

<p class="noindent english">Evolutionaries believe that the mother of all learning is natural selection. If it made us, it can make anything, and all we need to do is simulate it on the computer. The key problem that evolutionaries solve is learning structure: not just adjusting parameters, like backpropagation does, but creating the brain that those adjustments can then fine-tune. The evolutionaries’ master algorithm is genetic programming, which mates and evolves computer programs in the same way that nature mates and evolves organisms.</p>

<p class="noindent chinese">进化论者认为，所有学习的母亲是自然选择。如果它制造了我们，它就能制造任何东西，而我们需要做的就是在计算机上模拟它。进化论者解决的关键问题是学习结构：不只是调整参数，就像反向传播所做的那样，而是创造这些调整然后可以微调的大脑。进化者的主算法是遗传编程，它以自然界交配和进化生物的方式来交配和进化计算机程序。</p>

<p class="noindent english">Bayesians are concerned above all with uncertainty. All learned knowledge is uncertain, and learning itself is a form of uncertain inference. The problem then becomes how to deal with noisy, incomplete, and even contradictory information without falling apart. The solution is probabilistic inference, and the master algorithm is Bayes’ theorem and its derivates. Bayes’ theorem tells us how to incorporate new <a id="babilu_link-62"></a> evidence into our beliefs, and probabilistic inference algorithms do that as efficiently as possible.</p>

<p class="noindent chinese">贝叶斯主义者首先关注的是不确定性。所有学到的知识都是不确定的，而学习本身就是一种不确定的推理形式。那么，问题就变成了如何处理嘈杂的、不完整的、甚至是相互矛盾的信息而不至于崩溃。解决方案是概率推理，而主算法是贝叶斯定理及其派生算法。贝叶斯定理告诉我们如何将新的证据纳入我们的信念，而概率推理算法则尽可能有效地做到这一点。</p>

<p class="noindent english">For analogizers, the key to learning is recognizing similarities between situations and thereby inferring other similarities. If two patients have similar symptoms, perhaps they have the same disease. The key problem is judging how similar two things are. The analogizers’ master algorithm is the support vector machine, which figures out which experiences to remember and how to combine them to make new predictions.</p>

<p class="noindent chinese">对于类比者来说，学习的关键是认识到情况之间的相似性，从而推断出其他的相似性。如果两个病人有类似的症状，也许他们得的是同一种病。关键问题是判断两件事的相似程度。类比者的主算法是支持向量机，它找出要记住的经验，以及如何结合这些经验来进行新的预测。</p>

<p class="noindent english">Each tribe’s solution to its central problem is a brilliant, hard-won advance. But the true Master Algorithm must solve all five problems, not just one. For example, to cure cancer we need to understand the metabolic networks in the cell: which genes regulate which others, which chemical reactions the resulting proteins control, and how adding a new molecule to the mix would affect the network. It would be silly to try to learn all of this from scratch, ignoring all the knowledge that biologists have painstakingly accumulated over the decades. Symbolists know how to combine this knowledge with data from DNA sequencers, gene expression microarrays, and so on, to produce results that you couldn’t get with either alone. But the knowledge we obtain by inverse deduction is purely qualitative; we need to learn not just who interacts with whom, but how much, and backpropagation can do that. Nevertheless, both inverse deduction and backpropagation would be lost in space without some basic structure on which to hang the interactions and parameters they find, and genetic programming can discover it. At this point, if we had complete knowledge of the metabolism and all the data relevant to a given patient, we could figure out a treatment for her. But in reality the information we have is always very incomplete, and even incorrect in places; we need to make headway despite that, and that’s what probabilistic inference is for. In the hardest cases, the patient’s cancer looks very different from previous ones, and all our learned knowledge fails. Similarity-based algorithms can save the day by seeing analogies between superficially <a id="babilu_link-63"></a> very different situations, zeroing in on their essential similarities and ignoring the rest.</p>

<p class="noindent chinese">每个部落对其中心问题的解决方案都是一个辉煌的、来之不易的进步。但是，真正的主算法必须解决所有五个问题，而不仅仅是一个。例如，为了治愈癌症，我们需要了解细胞中的代谢网络：哪些基因调节其他基因，由此产生的蛋白质控制哪些化学反应，以及在混合中加入一个新分子会如何影响网络。如果试图从头开始学习这一切，无视生物学家几十年来辛苦积累的所有知识，那就太傻了。符号学家知道如何将这些知识与来自 DNA 测序仪、基因表达芯片等的数据结合起来，以产生你单独使用其中任何一种都无法得到的结果。但我们通过逆向演绎获得的知识是纯粹的定性的；我们需要了解的不仅仅是谁与谁的互动，还有多少，而反向传播可以做到这一点。尽管如此，如果没有一些基本的结构来悬挂它们所发现的相互作用和参数，反推法和反向传播都会在空间中迷失，而遗传编程可以发现它。在这一点上，如果我们拥有关于新陈代谢的完整知识和与某一特定病人相关的所有数据，我们就可以为她找出一种疗法。但在现实中，我们拥有的信息总是非常不完整，甚至有些地方是不正确的；尽管如此，我们还是需要取得进展，这就是概率推理的作用。在最困难的情况下，病人的癌症看起来与以前的非常不同，而我们所有学到的知识都会失效。基于相似性的算法可以通过看到表面上非常不同的情况之间的类比，将它们的基本相似性归零，而忽略其他的情况，从而挽救了这一切。</p>

<p class="noindent english">In this book we will synthesize a single algorithm with all these capabilities:</p>

<p class="noindent chinese">在本书中，我们将合成一个具有所有这些能力的单一算法。</p>

<div>

<div>

<img alt="image" src="images/000022.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-154"></a> Our quest will take us across the territory of each of the five tribes. The border crossings, where they meet, negotiate and skirmish, will be the trickiest part of the journey. Each tribe has a different piece of the puzzle, which we must gather. Machine learners, like all scientists, resemble the blind men and the elephant: one feels the trunk and thinks it’s a snake, another leans against the leg and thinks it’s a tree, yet another touches the tusk and thinks it’s a bull. Our aim is to touch each part without jumping to conclusions; and once we’ve touched all of them, we will try to picture the whole elephant. It’s far from obvious how to combine all the pieces into one solution—impossible, according to some—but this is what we will do.</p>

<p class="noindent chinese">我们的任务将带我们穿越五个部落中的每一个部落的领土。边境口岸是他们相遇、谈判和交战的地方，将是旅程中最棘手的部分。每个部落都有不同的拼图，我们必须收集这些拼图。机器学习者，像所有的科学家一样，就像盲人和大象一样：一个人摸着树干，认为它是一条蛇；另一个人靠着腿，认为它是一棵树；还有一个人摸着象牙，认为它是一头牛。我们的目的是触摸每一个部分，而不急于下结论；一旦我们触摸了所有的部分，我们将尝试想象整个大象。如何将所有的部分组合成一个解决方案是很明显的，有些人认为是不可能的，但这就是我们要做的。</p>

<p class="noindent english">The algorithm we’ll arrive at is not yet the Master Algorithm, for reasons we’ll see, but it’s the closest anyone has come. And we’ll gather enough riches along the way to make Croesus envious. Nevertheless, this book is only part one of the Master Algorithm saga. Part two’s protagonist is you, dear reader. Your mission, should you choose to accept it, is to go the rest of the way and bring back the prize. I will be your humble guide in part one, from here to the edge of the known world. Do I hear you protest that you don’t know enough, or algorithms are not your forte? Fear not. Computer science is still young, and unlike in physics or biology, you don’t need a PhD to start a revolution. (Just ask Bill Gates, Messrs. Sergey Brin and Larry Page, or Mark Zuckerberg.) Insight and persistence are what counts.</p>

<p class="noindent chinese">我们将得出的算法还不是主算法，原因我们会看到，但它是最接近的。我们将沿途收集足够的财富，让克罗伊索斯羡慕不已。尽管如此，这本书只是 “主算法“ 传奇的第一部分。第二部分的主角是你，亲爱的读者。你的任务，如果你选择接受的话，就是走完剩下的路，把奖品带回来。在第一部分中，我将是你卑微的向导，从这里到已知世界的边缘。我是否听到你抗议说你知道的不够多，或者算法不是你的强项？不要害怕。计算机科学还很年轻，与物理学或生物学不同的是，你不需要一个博士来发动一场革命。（问问比尔·盖茨、谢尔盖·布林和拉里·佩奇先生，或者马克·扎克伯格就知道了）。洞察力和毅力才是最重要的。</p>

<p class="noindent english">Are you ready? Our journey begins with a visit to the symbolists, the tribe with the oldest roots.</p>

<p class="noindent chinese">你准备好了吗？我们的旅程从访问象征主义者开始，这是一个有着最古老根基的部落。</p>

</section>

</div>

</div>

<div id="babilu_link-10">

<div>

<section id="babilu_link-4">

<h1><a id="babilu_link-242"></a> <a href="#babilu_link-11">CHAPTER THREE</a></h1>

<h1><a id="babilu_link-242"></a> <a href="#babilu_link-11">第三章</a></h1>

<h1><a href="#babilu_link-11">Hume’s Problem of Induction</a></h1>

<h1><a href="#babilu_link-11">休谟的归纳问题</a></h1>

<p class="noindent english">Are you a rationalist or an empiricist?</p>

<p class="noindent chinese">你是一个理性主义者还是一个经验主义者？</p>

<p class="noindent english">Rationalists believe that the senses deceive and that logical reasoning is the only sure path to knowledge. Empiricists believe that all reasoning is fallible and that knowledge must come from observation and experimentation. The French are rationalists; the Anglo-Saxons (as the French call them) are empiricists. Pundits, lawyers, and mathematicians are rationalists; journalists, doctors, and scientists are empiricists. <i>Murder, She Wrote</i> is a rationalist TV crime show; <i>CSI: Crime Scene Investigation</i> is an empiricist one. In computer science, theorists and knowledge engineers are rationalists; hackers and machine learners are empiricists.</p>

<p class="noindent chinese">理性主义者认为，感官会欺骗人，逻辑推理是获得知识的唯一可靠途径。经验主义者认为，所有的推理都是错误的，知识必须来自观察和实验。法国人是理性主义者；盎格鲁·撒克逊人（法国人叫他们）是经验主义者。学者、律师和数学家是理性主义者；记者、医生和科学家是经验主义者。《<i>她写谋杀</i>》是一个理性主义的电视犯罪节目；《<i>CSI：犯罪现场调查</i>》是一个经验主义的节目。在计算机科学领域，理论家和知识工程师是理性主义者；黑客和机器学习者是经验主义者。</p>

<p class="noindent english">The rationalist likes to plan everything in advance before making the first move. The empiricist prefers to try things and see how they turn out. I don’t know if there’s a gene for rationalism or one for empiricism, but looking at my computer scientist colleagues, I’ve observed time and again that they are almost like personality traits: some people are rationalistic to the core and could never have been otherwise; and others are empiricist through and through, and that’s what they’ll always be. The two sides can converse with each other and sometimes draw on each other’s results, but they can understand each other only so much. Deep <a id="babilu_link-75"></a> down each believes that what the other does is secondary, and not very interesting.</p>

<p class="noindent chinese">理性主义者喜欢在采取第一项行动之前提前计划一切。经验主义者更喜欢尝试事情，看看结果如何。我不知道是否有理性主义的基因或经验主义的基因，但看看我的计算机科学家同事，我一再观察到他们几乎就像性格特征一样：有些人是理性主义的核心，不可能是其他的；而另一些人则是经验主义的贯穿，这也是他们永远的特点。双方可以互相交流，有时也会借鉴对方的成果，但他们能理解对方的程度有限。在内心深处，每个人都认为对方所做的是次要的，而且不是很有趣。</p>

<p class="noindent english">Rationalists and empiricists have probably been around since the dawn of <i>Homo sapiens</i> . Before setting out on a hunt, Caveman Bob spent a long time sitting in his cave figuring out where the game would be. In the meantime, Cavewoman Alice was out systematically surveying the territory. Since both kinds are still with us, it’s probably safe to say that neither approach was better. You might think that machine learning is the final triumph of the empiricists, but the truth is more subtle, as we’ll soon see.</p>

<p class="noindent chinese">理性主义者和经验主义者可能从<i>智人</i>的黎明开始就已经存在了。在出发狩猎之前，穴居人鲍勃花了很长时间坐在他的山洞里，盘算着游戏的地点。与此同时，女穴居人爱丽丝则在外面系统地勘察领地。既然这两种人都还在我们身边，也许可以说这两种方法都不是更好。你可能会认为机器学习是经验主义者的最后胜利，但事实是更微妙的，我们很快就会看到。</p>

<p class="noindent english">Rationalism versus empiricism is a favorite question of philosophers. Plato was an early rationalist, and Aristotle an early empiricist. But the debate really took off during the Enlightenment, with a trio of great thinkers on each side: Descartes, Spinoza, and Leibniz were the leading rationalists; Locke, Berkeley, and Hume were their empiricist counterparts. Trusting in their powers of reasoning, the rationalists concocted theories of the universe that—to put it gently—did not stand the test of time, but they also invented fundamental mathematical techniques like calculus and analytical geometry. The empiricists were altogether more practical, and their influence is everywhere from the scientific method to the Constitution of the United States.</p>

<p class="noindent chinese">理性主义与经验主义是哲学家们最喜欢的一个问题。柏拉图是早期的理性主义者，而亚里士多德是早期的经验主义者。但这场辩论在启蒙运动期间真正开始了，双方都有三位伟大的思想家。笛卡尔、斯宾诺莎和莱布尼茨是主要的理性主义者；洛克、伯克利和休谟是他们的经验主义同行。理性主义者相信他们的推理能力，炮制了一些经不起时间考验的宇宙理论，但他们也发明了微积分和解析几何等基本数学技术。经验主义者则更为实际，从科学方法到美国宪法，他们的影响无处不在。</p>

<p class="noindent english">David Hume was the greatest of the empiricists and the greatest English-speaking philosopher of all time. Thinkers like Adam Smith and Charles Darwin count him among their key influences. You could also say he’s the patron saint of the symbolists. He was born in Scotland in 1711 and spent most of his life in eighteenth-century Edinburgh, a prosperous city full of intellectual ferment. A man of genial disposition, he was nevertheless an exacting skeptic who spent much of his time debunking the myths of his age. He also took the empiricist train of thought that Locke had started to its logical conclusion and asked a question that has since hung like a sword of Damocles over all knowledge, from the most trivial to the most advanced: How can we ever be justified in generalizing from what we’ve seen to what we <a id="babilu_link-280"></a> haven’t? Every learning algorithm is, in a sense, an attempt to answer this question.</p>

<p class="noindent chinese">大卫·休谟是最伟大的经验主义者，也是有史以来最伟大的英语哲学家。像亚当·斯密和查尔斯·达尔文这样的思想家把他算作他们的主要影响者之一。你也可以说他是象征主义者的守护神。他于 1711 年出生于苏格兰，在 18 世纪的爱丁堡度过了他的大部分生活，这是一个充满知识发酵的繁荣城市。他是一个性格和善的人，但却是一个严谨的怀疑论者，他花了很多时间来驳斥他那个时代的神话。他还将洛克所开创的经验主义思路推向了其逻辑结论，并提出了一个问题，这个问题从那时起就像一把达摩克利斯之剑悬挂在所有知识之上，从最微不足道的知识到最先进的知识：我们如何能够有理由从我们所看到的东西归纳出我们没有看到的东西？从某种意义上说，每一种学习算法都是对这个问题的尝试。</p>

<p class="noindent english">Hume’s question is also the departure point for our journey. We’ll start by illustrating it with an example from daily life and meeting its modern embodiment in the famous “no free lunch” theorem. Then we’ll see the symbolists’ answer to Hume. This leads us to the most important problem in machine learning: overfitting, or hallucinating patterns that aren’t really there. We’ll see how the symbolists solve it, and how machine learning is at heart a kind of alchemy, transmuting data into knowledge with the aid of a philosopher’s stone. For the symbolists, the philosopher’s stone is knowledge itself. In the next four chapters we’ll study the solutions of the other tribes’ alchemists.</p>

<p class="noindent chinese">休谟的问题也是我们旅程的出发点。我们将首先用日常生活中的一个例子来说明它，并在著名的 “没有免费的午餐” 定理中见到其现代体现。然后我们将看到象征主义者对休谟的回答。这将我们引向机器学习中最重要的问题：过度拟合，或者幻化出并不存在的模式。我们将看到象征主义者是如何解决这个问题的，以及机器学习在本质上是一种炼金术，借助哲学家的石头将数据转化为知识。对于象征主义者来说，哲学家的石头就是知识本身。在接下来的四章里，我们将研究其他部落的炼金术士的解决方案。</p>

<h1 id="babilu_link-398"><b>To date or not to date?</b></h1>

<h1 id="babilu_link-398"><b>约会还是不约会？</b></h1>

<p class="noindent english">You have a friend you really like, and you want to ask her out on a date. You have a hard time dealing with rejection, though, and you’re only going to ask her if you’re pretty sure she’ll say yes. It’s Friday evening, and there you sit with cell phone in hand, trying to decide whether or not to call her. You remember that the previous time you asked her, she said no. But why? Two times before that she said yes, and the one before those she said no. Maybe there are days she doesn’t like to go out? Or maybe she likes clubbing but not dinner dates? Being of an unusually systematic nature, you put down the phone and jot down what you can remember about those previous occasions:</p>

<p class="noindent chinese">你有一个你非常喜欢的朋友，你想约她出来约会。但你很难处理拒绝的问题，而且你只打算在你非常确定她会答应的情况下才约她。今天是星期五晚上，你坐在那里，手里拿着手机，试图决定是否给她打电话。你记得上次你问她时，她说不。但为什么？之前的两次她都说好，而之前的一次她说不。也许有些日子她不喜欢出去？或者她喜欢泡吧但不喜欢约会吃饭？作为一个异常有系统的人，你放下电话，记下你能记得的关于以前的那些场合。</p>

<div>

<p class="noindent english"><strong><b>Occasion</b></strong> : 1</p>

<p class="noindent chinese"><strong><b>场合</b></strong>: 1</p>

<p class="noindent english"><strong><b>Day of Week</b></strong> : Weekday</p>

<p class="noindent chinese"><strong><b>一周的日子</b></strong>: 平日</p>

<p class="noindent english"><strong><b>Type of Date</b></strong> : Dinner</p>

<p class="noindent chinese"><strong><b>日期类型</b></strong>: 晚餐</p>

<p class="noindent english"><strong><b>Weather</b></strong> : Warm</p>

<p class="noindent chinese"><strong><b>天气</b></strong>：温暖</p>

<p class="noindent english"><strong><b>TV Tonight</b></strong> : Bad</p>

<p class="noindent chinese"><strong><b>今晚的电视</b></strong>: 糟糕</p>

<p class="noindent english"><strong><b>Date?</b></strong> : No</p>

<p class="noindent chinese"><strong><b>日期？</b></strong>: 没有</p>

<p class="noindent english"><strong><b>Occasion</b></strong> : 2</p>

<p class="noindent chinese"><strong><b>场合</b></strong>: 2</p>

<p class="noindent english"><strong><b>Day of Week</b></strong> : Weekend</p>

<p class="noindent chinese"><strong><b>一周的日子</b></strong>: 周末</p>

<p class="noindent english"><strong><b>Type of Date</b></strong> : Club</p>

<p class="noindent chinese"><strong><b>日期类型</b></strong>: 俱乐部</p>

<p class="noindent english"><strong><b>Weather</b></strong> : Warm</p>

<p class="noindent chinese"><strong><b>天气</b></strong>: 温暖</p>

<p class="noindent english"><strong><b>TV Tonight</b></strong> : Bad</p>

<p class="noindent chinese"><strong><b>今晚的电视</b></strong>: 糟糕</p>

<p class="noindent english"><strong><b>Date?</b></strong> : Yes</p>

<p class="noindent chinese"><strong><b>日期？</b></strong>: 是</p>

<p class="noindent english"><strong><b>Occasion</b></strong> : 3</p>

<p class="noindent chinese"><strong><b>场合</b></strong>: 3</p>

<p class="noindent english"><strong><b>Day of Week</b></strong> : Weekend</p>

<p class="noindent chinese"><strong><b>一周的日子</b></strong>: 周末</p>

<p class="noindent english"><strong><b>Type of Date</b></strong> : Club</p>

<p class="noindent chinese"><strong><b>日期类型</b></strong>: 俱乐部</p>

<p class="noindent english"><strong><b>Weather</b></strong> : Warm</p>

<p class="noindent chinese"><strong><b>天气</b></strong>: 温暖</p>

<p class="noindent english"><strong><b>TV Tonight</b></strong> : Bad</p>

<p class="noindent chinese"><strong><b>今晚的电视</b></strong>: 坏</p>

<p class="noindent english"><strong><b>Date?</b></strong> : Yes</p>

<p class="noindent chinese"><strong><b>日期？</b></strong>: 是</p>

<p class="noindent english"><strong><b>Occasion</b></strong> : 4</p>

<p class="noindent chinese"><strong><b>场合</b></strong>: 4</p>

<p class="noindent english"><strong><b>Day of Week</b></strong> : Weekend</p>

<p class="noindent chinese"><strong><b>一周的日子</b></strong>: 周末</p>

<p class="noindent english"><strong><b>Type of Date</b></strong> : Club</p>

<p class="noindent chinese"><strong><b>日期类型</b></strong>: 俱乐部</p>

<p class="noindent english"><strong><b>Weather</b></strong> : Cold</p>

<p class="noindent chinese"><strong><b>天气</b></strong>: 寒冷</p>

<p class="noindent english"><strong><b>TV Tonight</b></strong> : Good</p>

<p class="noindent chinese"><strong><b>今晚的电视</b></strong>: 好</p>

<p class="noindent english"><strong><b>Date?</b></strong> : No</p>

<p class="noindent chinese"><strong><b>日期？</b></strong>: 没有</p>

<p class="noindent english"><strong><b>Occasion</b></strong> : Today</p>

<p class="noindent chinese"><strong><b>场合</b></strong>: 今天</p>

<p class="noindent english"><strong><b>Day of Week</b></strong> : Weekend</p>

<p class="noindent chinese"><strong><b>周的一天</b></strong>: 周末</p>

<p class="noindent english"><strong><b>Type of Date</b></strong> : Club</p>

<p class="noindent chinese"><strong><b>日期类型</b></strong>: 俱乐部</p>

<p class="noindent english"><strong><b>Weather</b></strong> : Cold</p>

<p class="noindent chinese"><strong><b>天气</b></strong>: 寒冷</p>

<p class="noindent english"><strong><b>TV Tonight</b></strong> : Bad</p>

<p class="noindent chinese"><strong><b>今晚的电视</b></strong>: 坏</p>

<p class="noindent english"><strong><b>Date?</b></strong> : ?</p>

<p class="noindent chinese"><strong><b>日期？</b></strong>: ?</p>

</div>

<p class="noindent english"><a id="babilu_link-258"></a> So… what shall it be? Date or no date? Is there a pattern that distinguishes the yeses from the nos? And, most important, what does that pattern say about today?</p>

<p class="noindent chinese">那么… 应该是什么呢？日期还是没有日期？是否有一种模式可以区分 “有” 和 “无”？而且，最重要的是，这种模式对今天有什么意义？</p>

<p class="noindent english">Clearly, there’s no single factor that correctly predicts the answer: on some weekends she likes to go out, and on some she doesn’t; sometimes she likes to go clubbing, and sometimes she doesn’t, and so on. What about a combination of factors? Maybe she likes to go clubbing on weekends? No, occasion number 4 crosses that one out. Or maybe she only likes to go out on warm weekend nights? Bingo! That works! In which case, looking at the frosty weather outside, tonight doesn’t look promising. But wait! What if she likes to go clubbing when there’s nothing good on TV? That also works, and that means today is a yes! Quick, call her before it gets too late. But wait a second. How do you know this is the right pattern? You’ve found two that agree with your previous experience, but they make opposite predictions. Come to think of it, what if she only goes clubbing when the weather is nice? Or she goes out on weekends when there’s nothing to watch on TV? Or—</p>

<p class="noindent chinese">显然，没有一个因素可以正确预测答案：在一些周末，她喜欢出去，而在一些周末她不喜欢；有时她喜欢去俱乐部，而有时她不喜欢，等等。如果是各种因素的组合呢？也许她喜欢在周末去夜总会？不，第 4 号场合将其排除。或者她只喜欢在温暖的周末晚上出去？没错！这就对了。这就对了！在这种情况下，看着外面的霜冻天气，今晚看起来并不乐观。但是，等等！如果她喜欢去俱乐部呢？如果她喜欢在电视上没有好节目的时候去泡吧呢？这也是可行的，这意味着今天是个好机会！快，在太晚之前给她打电话。但是等一下。你怎么知道这就是正确的模式？你已经找到了两个与你以前的经验一致的模式，但它们做出了相反的预测。仔细想想，如果她只在天气好的时候去泡吧呢？或者她在周末没有什么电视可看时才出去？或者 —— </p>

<p class="noindent english">At this point you crumple your notes in frustration and fling them into the wastebasket. There’s no way to know! What can you do? The ghost of Hume nods sadly over your shoulder. <i>You have no basis to pick one generalization over another.</i> Yes and no are equally legitimate answers to the question “What will she say?” And the clock is ticking. Bitterly, you fish out a quarter from your pocket and prepare to flip it.</p>

<p class="noindent chinese">在这一点上，你沮丧地揉碎了你的笔记，把它们扔进了废纸篓。没有办法知道！你能做什么？休谟的幽灵在你的肩膀上悲伤地点点头。<i>你没有依据去选择一种概括而不是另一种。</i>对于 “她会说什么？” 这个问题，“好” 和 “不” 都是同样合理的答案。而时间正在流逝。你痛苦地从口袋里摸出一个 25 美分，准备翻转它。</p>

<p class="noindent english">You’re not the only one in dire straits—so are we. We’ve only just set out on our road to the Master Algorithm and already we seem to have run into an insurmountable obstacle. Is there <i>any</i> way to learn something from the past that we can be confident will apply in the future? And if there isn’t, isn’t machine learning a hopeless enterprise? For that matter, isn’t all of science, even all of human knowledge, on rather shaky ground?</p>

<p class="noindent chinese">你不是唯一陷入困境的人，我们也是。我们才刚刚踏上通向算法大师的道路，就似乎遇到了不可逾越的障碍。有没有<i>什么</i>办法可以从过去学到一些我们可以确信会在未来应用的东西？如果没有，机器学习不就是一个无望的事业吗？就这个问题而言，所有的科学，甚至所有的人类知识，不都是在相当不稳定的基础上吗？</p>

<p class="noindent english">It’s not like big data would solve the problem. You could be super-Casanova and have dated millions of women thousands of times each, but your master database still wouldn’t answer the question of what <i>this</i> woman is going to say <i>this</i> time. Even if today is exactly like <a id="babilu_link-88"></a> some previous occasion when she said yes—same day of week, same type of date, same weather, and same shows on TV—that still doesn’t mean that this time she will say yes. For all you know, her answer is determined by some factor that you didn’t think of or don’t have access to. Or maybe there’s no rhyme or reason to her answers: they’re random, and you’re just spinning your wheels trying to find a pattern in them.</p>

<p class="noindent chinese">这并不是说大数据会解决这个问题。你可以是超级卡萨诺瓦，和数以百万计的女人各约会过数千次，但你的主数据库仍然不会回答<i>这个</i>女人<i>这次</i>会说什么的问题。即使今天和以前她说 “是” 的某个场合一模一样 —— 同样的日子，同样的约会类型，同样的天气，同样的电视节目 —— 但这仍然不意味着这次她会说 “是”。就你所知，她的回答是由一些你没有想到的或无法获得的因素决定的。或者，也许她的回答没有任何韵律或理由：它们是随机的，而你只是在旋转你的车轮，试图在其中找到一个模式。</p>

<p class="noindent english">Philosophers have debated Hume’s problem of induction ever since he posed it, but no one has come up with a satisfactory answer. Bertrand Russell liked to illustrate the problem with the story of the inductivist turkey. On his first morning at the farm, the turkey was fed at 9:00 a.m., but being a good inductivist, he didn’t jump to conclusions. He first collected many observations on many different days under many different circumstances. Having been fed consistently at 9:00 a.m. for many consecutive days, he finally concluded that yes, he would always be fed at 9:00 a.m. Then came the morning of Christmas eve, and his throat was cut.</p>

<p class="noindent chinese">自从休谟提出归纳法问题以来，哲学家们一直在争论这个问题，但没有人想出一个令人满意的答案。伯特兰·罗素喜欢用归纳法火鸡的故事来说明这个问题。在它到农场的第一个早晨，火鸡在 9 点被喂食，但作为一个优秀的归纳主义者，它并没有急于下结论。它首先在许多不同的日子里，在许多不同的情况下收集了许多观察结果。在连续许多天坚持在上午 9 点喂食后，它最终得出结论：是的，它总是在上午 9 点被喂食。然后，在圣诞节前夕的早晨，它的喉咙被割断了。</p>

<p class="noindent english">It would be nice if Hume’s problem was just a cute philosophical conundrum we could ignore, but we can’t. For example, Google’s business is based on guessing which web pages you’re looking for when you type some keywords into the search box. Their key asset is massive logs of search queries people have entered in the past and the links they clicked on in the corresponding results pages. But what do you do if someone types in a combination of keywords that’s not in the log? And even if it is, how can you be confident that the current user wants the same pages as the previous ones?</p>

<p class="noindent chinese">如果休谟的问题只是一个我们可以忽略的可爱的哲学难题就好了，但我们不能。例如，谷歌的业务是基于猜测当你在搜索框中输入一些关键词时你在寻找哪些网页。他们的关键资产是人们过去输入的搜索查询的大量日志，以及他们在相应结果页面上点击的链接。但是，如果有人输入的关键词组合不在日志中，你该怎么办？即使是这样，你怎么能确信当前的用户想要的页面与以前的用户相同呢？</p>

<p class="noindent english">How about we just <i>assume</i> that the future will be like the past? This is certainly a risky assumption. (It didn’t work for the inductivist turkey.) On the other hand, without it all knowledge is impossible, and so is life. We’d rather stay alive, even if precariously. Unfortunately, even with that assumption we’re not out of the woods. It takes care of the “trivial” cases: If I’m a doctor and patient B has exactly the same symptoms as patient A, I assume that the diagnosis is the same. But if patient B’s symptoms don’t exactly match anyone else’s, I’m still in the dark. <a id="babilu_link-275"></a> This is the machine-learning problem: generalizing to cases that <i>we haven’t seen before</i> .</p>

<p class="noindent chinese">我们就<i>假设</i>未来会像过去一样，怎么样？这当然是一个有风险的假设。（它对归纳主义的火鸡不起作用。）另一方面，没有它，所有的知识都是不可能的，生命也是如此。我们宁愿活着，即使是不稳定的。不幸的是，即使有了这个假设，我们也没有走出困境。它照顾到了 “微不足道” 的情况。如果我是一名医生，病人 B 的症状与病人 A 完全相同，我就假设诊断是一样的。但如果病人 B 的症状与其他人的症状不完全一致，我仍然是一头雾水。这就是机器学习的问题：对<i>我们以前没有见过</i>的案例进行归纳。</p>

<p class="noindent english">But perhaps that’s not such a big deal? With enough data, won’t most cases be in the “trivial” category? No. We saw in the previous chapter why memorization won’t work as a universal learner, but now we can make it more quantitative. Suppose you have a database with a trillion records, each with a thousand Boolean fields (i.e., each field is the answer to a yes/no question). That’s pretty big. What fraction of the possible cases have you seen? (Take a guess before you read on.) Well, the number of possible answers is two for each question, so for two questions it’s two times two (yes-yes, yes-no, no-yes, and no-no), for three questions it’s two cubed (2 × 2 × 2 = 2<sup>3</sup>), and for a thousand questions it’s two raised to the power of a thousand (2<sup>1000</sup>). The trillion records in your database are one-gazillionth of 1 percent of 2<sup>1000</sup> , where “gazillionth” means “zero point 286 zeros followed by 1.” Bottom line: no matter how much data you have—tera- or peta- or exa- or zetta- or yottabytes—you’ve basically seen <i>nothing</i> . The chances that the new case you need to make a decision on is already in the database are so vanishingly small that, without generalization, you won’t even get off the ground.</p>

<p class="noindent chinese">但也许这并不是什么大问题？有了足够的数据，大多数情况不就属于 “微不足道” 的范畴了吗？我们在上一章中看到了为什么背诵不能作为一个通用的学习者，但现在我们可以把它变得更加量化。假设你有一个有一万亿条记录的数据库，每条记录都有一千个布尔字段（也就是说，每个字段都是一个是/否问题的答案）。这是很庞大的。你见过的可能情况有多少？（在你继续阅读之前猜一猜。）好吧，每个问题的可能答案数是 2，所以对于两个问题来说，是 2 乘以 2（是·是、是·否、否·是和否），对于三个问题来说，是 2 的立方（2 × 2 × 2 = 2<sup>3</sup>），而对于一千个问题来说，是 2 提高到 1000 的幂（2<sup>1000</sup>）。你的数据库中的一万亿条记录是 2<sup>1000</sup> 的百分之一，其中 “Gazillionth” 意味着 “零点 286 个零，后面是 1”。一句话：不管你有多少数据 —— tera- 或 peta- 或 exa- 或 zetta- 或 yottabytes —— 你基本上<i>什么都没</i>看到。你需要做出决定的新案例已经出现在数据库中的几率非常小，以至于如果没有归纳，你甚至都无法起步。</p>

<p class="noindent english">If this all sounds a bit abstract, suppose you’re a major e-mail provider, and you need to label each incoming e-mail as spam or not spam. You may have a database of a trillion past e-mails, each already labeled as spam or not, but that won’t save you, since the chances that every new e-mail will be an exact copy of a previous one are just about zero. You have no choice but to try to figure out at a more general level what distinguishes spam from nonspam. And, according to Hume, there’s no way to do that.</p>

<p class="noindent chinese">如果这一切听起来有点抽象，假设你是一个主要的电子邮件供应商，你需要将每一封收到的电子邮件标记为垃圾邮件或非垃圾邮件。你可能有一个包含一万亿封过去邮件的数据库，每封邮件都已经被标记为垃圾邮件或非垃圾邮件，但这并不能拯救你，因为每封新邮件都是以前邮件的精确拷贝的可能性几乎为零。你别无选择，只能尝试在一个更普遍的层面上找出区分垃圾邮件和非垃圾邮件的原因。而且，根据 Hume 的说法，没有办法做到这一点。</p>

<h1 id="babilu_link-399"><b>The “no free lunch” theorem</b></h1>

<h1 id="babilu_link-399"><b>“没有免费的午餐” 定理</b></h1>

<p class="noindent english">Two hundred and fifty years after Hume set off his bombshell, it was given elegant mathematical form by David Wolpert, a physicist turned machine learner. His result, known as the “no free lunch” theorem, sets <a id="babilu_link-186"></a> a limit on how good a learner can be. The limit is pretty low: no learner can be better than random guessing! OK, we can go home: the Master Algorithm is just flipping coins. Seriously, though, how is it that no learner can beat coin flipping? And if that’s so, how come the world is full of highly successful learners, from spam filters to (any day now) self-driving cars?</p>

<p class="noindent chinese">在休谟放出重磅炸弹的 250 年后，物理学家大卫·沃尔珀特赋予了它优雅的数学形式，他是一位转为机器学习者。他的结果被称为 “没有免费的午餐” 定理，为一个学习者能有多好设定了一个限制。这个限制是相当低的：<i>没有一个学习者能比随机猜测的更好！</i>我们可以回家了。好了，我们可以回家了：主算法只是在抛硬币。不过，说真的，怎么会没有学习者能胜过抛硬币呢？如果是这样的话，为什么世界上有很多非常成功的学习者，从垃圾邮件过滤器到（随时可能出现的）自动驾驶汽车？</p>

<p class="noindent english">The “no free lunch” theorem is a lot like the reason Pascal’s wager fails. In his <i>Pensées</i> , published in 1669, Pascal said we should believe in the Christian God because if he exists that gains us eternal life, and if he doesn’t we lose very little. This was a remarkably sophisticated argument for the time, but as Diderot pointed out, an imam could make the same argument for believing in Allah. And if you pick the wrong god, the price you pay is eternal hell. On balance, considering the wide variety of possible gods, you’re no better off picking a particular one to believe in than you are picking any other. For every god that says “do this,” there’s another that says “no, do that.” You may as well just forget about god and enjoy life without religious constraints.</p>

<p class="noindent chinese">“没有免费的午餐” 定理很像帕斯卡尔的赌注失败的原因。在 1669 年出版的《<i>笔谈</i>》中，帕斯卡尔说我们应该相信基督教的上帝，因为如果他存在，我们就能获得永生，如果他不存在，我们的损失就很小。这在当时是一个非常复杂的论证，但正如狄德罗所指出的，一个伊玛目也可以为相信真主做同样的论证。而如果你选错了神，你付出的代价就是永远的地狱。总的来说，考虑到各种可能的神，你选择一个特定的神来信仰并不比选择任何其他神更好。每一个说 “做这个” 的神，都有另一个说 “不，做那个”。你还不如忘掉上帝，享受没有宗教约束的生活。</p>

<p class="noindent english">Replace “god” with “learning algorithm” and “eternal life” with “accurate prediction,” and you have the “no free lunch” theorem. Pick your favorite learner. (We’ll see many in this book.) For every world where it does better than random guessing, I, the devil’s advocate, will deviously construct one where it does worse by the same amount. All I have to do is flip the labels of all <i>unseen</i> instances. Since the labels of the observed ones agree, there’s no way your learner can distinguish between the world and the antiworld. On average over the two, it’s as good as random guessing. And therefore, on average over all possible worlds, pairing each world with its antiworld, your learner is equivalent to flipping coins.</p>

<p class="noindent chinese">用 “学习算法” 取代 “上帝”，用 “准确预测” 取代 “永生”，你就有了 “没有免费的午餐” 定理。挑选你最喜欢的学习者。（对于每一个它比随机猜测做得更好的世界，我这个魔鬼代言人都会狡猾地构建一个它做得更差的世界，而且是以同样的数量。我所要做的就是翻转所有<i>未见过的</i>实例的标签。由于观察到的标签一致，你的学习者不可能区分出世界和反世界。在这两者的平均水平上，它和随机猜测一样好。因此，在所有可能的世界中，将每个世界与它的反世界配对，你的学习者就相当于掷硬币。</p>

<p class="noindent english">Don’t give up on machine learning or the Master Algorithm just yet, though. We don’t care about all possible worlds, only the one we live in. If we know something about the world and incorporate it into our learner, it now has an advantage over random guessing. To this Hume would reply that that knowledge must itself have come from induction and is therefore fallible. That’s true, even if the knowledge was encoded <a id="babilu_link-76"></a> into our brains by evolution, but it’s a risk we’ll have to take. We can also ask whether there’s a nugget of knowledge so incontestable, so fundamental, that we can build all induction on top of it. (Something like Descartes’ “I think, therefore I am,” although it’s hard to see how to turn that one into a learning algorithm.) I think the answer is yes, and we’ll see what that nugget is in <a href="#babilu_link-12">Chapter 9</a> .</p>

<p class="noindent chinese">不过，现在还不要放弃机器学习或主算法。我们并不关心所有可能的世界，只关心我们所处的世界。如果我们对这个世界有所了解，并将其纳入我们的学习者，那么它现在比随机猜测有优势。对此，休谟会回答说，这种知识本身必须来自于归纳法，因此是有缺陷的。这是真的，即使这些知识是通过进化编码到我们的大脑中的但这是一个我们必须承担的风险。我们还可以问，是否有一个知识小块是如此无可争议，如此基本，以至于我们可以在它上面建立所有的归纳。（就像笛卡尔的 “我思故我在”，尽管很难看到如何把它变成一个学习算法）。我认为答案是肯定的，我们将在<a href="#babilu_link-12">第九章</a>中看到这个小块是什么。</p>

<p class="noindent english">In the meantime, the practical consequence of the “no free lunch” theorem is that there’s no such thing as learning without knowledge. Data alone is not enough. Starting from scratch will only get you to scratch. Machine learning is a kind of knowledge pump: we can use it to extract a lot of knowledge from data, but first we have to prime the pump.</p>

<p class="noindent chinese">同时，“没有免费的午餐” 定理的实际后果是，没有知识的学习是不存在的。仅有数据是不够的。从零开始，只能让你从头开始。机器学习是一种知识泵：我们可以用它来从数据中提取大量的知识，但首先我们必须给泵填料。</p>

<p class="noindent english">Machine learning is what mathematicians call an ill-posed problem: it doesn’t have a unique solution. Here’s a simple ill-posed problem: Which two numbers add up to 1,000? Assuming the numbers are positive, there are five hundred possible answers: 1 and 999, 2 and 998, and so on. The only way to solve an ill-posed problem is to introduce additional assumptions. If I tell you the second number is triple the first, bingo: the answer is 250 and 750.</p>

<p class="noindent chinese">机器学习就是数学家所说的不确定问题：它没有一个唯一的解决方案。这里有一个简单的问题：哪两个数字加起来是 1000？假设这两个数字是正数，有 500 个可能的答案。1 和 999，2 和 998，以此类推。解决一个欠妥问题的唯一方法是引入额外的假设。如果我告诉你第二个数字是第一个数字的三倍，bingo！答案是 250 和 750。</p>

<p class="noindent english">Tom Mitchell, a leading symbolist, calls it “the futility of bias-free learning.” In ordinary life, <i>bias</i> is a pejorative word: preconceived notions are bad. But in machine learning, preconceived notions are indispensable; you can’t learn without them. In fact, preconceived notions are also indispensable to human cognition, but they’re hardwired into the brain, and we take them for granted. It’s biases over and beyond those that are questionable.</p>

<p class="noindent chinese">汤姆·米切尔，一位著名的象征主义者，称其为 “无偏见的学习的徒劳”。在普通生活中，<i>偏见</i>是一个贬义词：先入为主的观念是不好的。但在机器学习中，先入为主的观念是不可或缺的；没有它们，你就无法学习。事实上，先入为主的观念对人类的认知也是不可或缺的，但它们被硬塞进了大脑，而我们认为这是理所当然的。值得怀疑的是超过这些的偏见。</p>

<p class="noindent english">Aristotle said that there is nothing in the intellect that was not first in the senses. Leibniz added, “Except the intellect itself.” The human brain is not a blank slate because it’s not a slate. A slate is passive, something you write on, but the brain actively processes the information it receives. Memory is the slate it writes on, and it does start out blank. On the other hand, a computer <i>is</i> a blank slate until you program it; the active process itself has to be written into memory before anything can happen. Our goal is to figure out the simplest program we can write <a id="babilu_link-203"></a> such that it will continue to write itself by reading data, without limit, until it knows everything there is to know.</p>

<p class="noindent chinese">亚里士多德说，智力中没有任何东西不是先在感官中出现的。莱布尼茨补充说，“除了智力本身”。人脑不是一块白板，因为它不是一块石板。石板是被动的，是你写在上面的东西，但大脑积极地处理它所收到的信息。记忆是它所写的石板，而且它一开始确实是空白的。另一方面，计算机<i>是</i>一块空白的石板，直到你对它进行编程；在任何事情发生之前，主动处理过程本身必须被写入记忆中。我们的目标是找出我们可以编写的最简单的程序，这样它就可以通过读取数据继续编写自己，没有限制，直到它知道所有要知道的东西。</p>

<p class="noindent english">Machine learning has an unavoidable element of gambling. In the first <i>Dirty Harry</i> movie, Clint Eastwood chases a bank robber, repeatedly firing at him. Finally, the robber is lying next to a loaded gun, unsure whether to spring for it. Did Harry fire six shots or only five? Harry sympathizes (so to speak): “You’ve got to ask yourself one question: ‘Do I feel lucky?’ Well, do you, punk?” That’s the question machine learners have to ask themselves every day when they go to work: Do I feel lucky today? Just like evolution, machine learning doesn’t get it right every time; in fact, errors are the rule, not the exception. But it’s OK, because we discard the misses and build on the hits, and the cumulative result is what matters. Once we acquire a new piece of knowledge, it becomes a basis for inducing yet more knowledge. The only question is where to begin.</p>

<p class="noindent chinese">机器学习有一个不可避免的赌博因素。在第一部《<i>肮脏的哈里</i>》电影中，克林特·伊斯特伍德追赶一个银行劫匪，反复向他开枪。最后，劫匪躺在一把上了膛的枪旁边，不确定是否要弹开。哈里是开了六枪还是只开了五枪？哈里很同情（可以这么说）。“你得问自己一个问题：‘我感到幸运吗？’ 好吧，你呢，废物？” 这就是机器学习者每天上班时要问自己的问题。我今天感到幸运吗？就像进化论一样，机器学习并不是每次都能做对；事实上，错误是规则，而不是例外。但这没关系，因为我们抛弃了失误，在成功的基础上继续努力，而累积的结果才是最重要的。一旦我们获得了新的知识，它就成为诱发更多知识的基础。唯一的问题是从哪里开始。</p>

<h1 id="babilu_link-400"><b>Priming the knowledge pump</b></h1>

<h1 id="babilu_link-400"><b>为知识泵打底</b></h1>

<p class="noindent english">In the <i>Principia</i> , along with his three laws of motion, Newton enunciates four rules of induction. Although these are much less well known than the physical laws, they are arguably as important. The key rule is the third one, which we can paraphrase thus:</p>

<p class="noindent chinese">在《<i>原理</i>》中，除了他的三个运动定律外，牛顿还阐述了四个归纳法则。尽管这些规则不如物理定律那样广为人知，但它们可以说同样重要。关键的规则是第三条，我们可以这样解读。</p>

<div>

<p class="noindent english"><i>Newton’s Principle: Whatever is true of everything we’ve seen is true of everything in the universe.</i></p>

<p class="noindent chinese"><i>牛顿原理：凡是我们所看到的东西都是真实的，宇宙中的所有东西都是真实的。</i></p>

</div>

<p class="noindent english">It’s not an exaggeration to say that this innocuous-sounding statement is at the heart of the Newtonian revolution and of modern science. Kepler’s laws applied to exactly six entities: the planets of the solar system known in his time. Newton’s laws apply to every last speck of matter in the universe. The leap in generality between the two is staggering, and it’s a direct consequence of Newton’s principle. This one principle is all by itself a knowledge pump of phenomenal power. Without it there would be no laws of nature, only a forever incomplete patchwork of small regularities.</p>

<p class="noindent chinese">毫不夸张地说，这个听起来无伤大雅的声明是牛顿革命和现代科学的核心。开普勒定律正好适用于六个实体：在他那个时代已知的太阳系的行星。牛顿定律适用于宇宙中的每一粒物质。两者之间通用性的飞跃是惊人的，这也是牛顿原理的直接后果。这条原则本身就是一个具有惊人力量的知识泵。没有它，就没有自然法则，只有永远不完整的小规律的拼凑。</p>

<p class="noindent english"><a id="babilu_link-74"></a> Newton’s principle is the first unwritten rule of machine learning. We induce the most widely applicable rules we can and reduce their scope only when the data forces us to. At first sight this may seem ridiculously overconfident, but it’s been working for science for over three hundred years. It’s certainly possible to imagine a universe so varied and capricious that Newton’s principle would systematically fail, but that’s not our universe.</p>

<p class="noindent chinese">牛顿原理是机器学习的第一个不成文的规则。我们诱导出最广泛适用的规则，只有在数据迫使我们这样做的时候才缩小其范围。乍一看，这似乎是可笑的过度自信，但它已经为科学工作了超过三百年。当然，我们有可能想象一个变化多端、反复无常的宇宙，以至于牛顿原理会系统地失效，但那不是我们的宇宙。</p>

<p class="noindent english">Newton’s principle is only the first step, however. We still need to figure out what is true of everything we’ve seen—how to extract the regularities from the raw data. The standard solution is to assume we know the <i>form</i> of the truth, and the learner’s job is to flesh it out. For example, in the dating problem you could assume that your friend’s answer is determined by a single factor, in which case learning just consists of checking each known factor (day of week, type of date, weather, and TV programming) to see if it correctly predicts her answer every time. The problem, of course, is that none of them do! You gambled and failed. So you relax your assumptions a bit. What if your friend’s answer is determined by a conjunction of two factors? With four factors, each with two possible values, there are twenty-four possibilities to check (six pairs of factors to pick from times two choices for each factor’s value). Now we have an embarrassment of riches: four conjunctions of two factors correctly predict the outcome! What to do? If you’re feeling lucky, you can just pick one of them and hope for the best. A more sensible option, though, is democracy: let them vote, and pick the winning prediction.</p>

<p class="noindent chinese">然而，牛顿原理只是第一步。我们仍然需要弄清楚我们所看到的一切的真实情况 —— 如何从原始数据中提取规律性的东西。标准的解决方案是假设我们知道真理的<i>形式</i>，而学习者的工作是充实它。例如，在约会问题中，你可以假设你朋友的答案是由单一因素决定的，在这种情况下，学习只是包括检查每个已知的因素（星期几、日期类型、天气和电视节目），看它是否每次都能正确地预测她的答案。当然，问题是，没有一个因素能做到这一点！你赌了一把，结果失败了。所以你把你的假设放宽一些。如果你朋友的答案是由两个因素共同决定的呢？有四个因素，每个因素有两个可能的值，就有二十四种可能性可供检查（六对因素可供挑选，每个因素的值有两个选择）。现在我们有了一个令人尴尬的财富：两个因素的四个连接点正确地预测了结果！怎么办？该怎么做呢？如果你觉得自己很幸运，你可以选择其中一个，并希望得到最好的结果。不过，一个更明智的选择是民主：让他们投票，并挑选出获胜的预测。</p>

<p class="noindent english">If all conjunctions of two factors fail, you can try all conjunctions of any number of factors. Machine learners and psychologists call these “conjunctive concepts.” Dictionary definitions are conjunctive concepts: a chair has a seat <i>and</i> a back <i>and</i> some number of legs. Remove any of these and it’s no longer a chair. A conjunctive concept is what Tolstoy had in mind when he wrote the opening sentence of <i>Anna Karenina</i> : “All happy families are alike; each unhappy family is unhappy in its own way.” The same is true of individuals. To be happy, you need health, love, friends, money, a job you like, and so on. Take any of these away, and misery ensues.</p>

<p class="noindent chinese">如果两个因素的所有连接都失败，你可以尝试任何数量的因素的所有连接。机器学习者和心理学家称这些为 “连接性概念”。词典中的定义是连接性概念：一把椅子有一个座位 <i>、</i> 一个靠背<i>和</i>一些腿的数量。去掉其中任何一个，它就不再是一把椅子。当托尔斯泰写下《<i>安娜·卡列尼娜</i>》的开篇时，他想到的就是一个联结概念：“所有幸福的家庭都是一样的；每个不幸福的家庭都以自己的方式不幸福。” 个人也是如此。要想幸福，你需要健康、爱情、朋友、金钱、一份你喜欢的工作，等等。剥夺其中任何一项，痛苦就会随之而来。</p>

<p class="noindent english"><a id="babilu_link-200"></a> In machine learning, examples of a concept are called positive examples, and counterexamples are called negative examples. If you’re trying to learn to recognize cats in images, images of cats are positive examples and images of dogs are negative ones. If you compiled a database of families from the world’s literature, the Karenins would be a negative example of a happy family, and there would be precious few positive examples.</p>

<p class="noindent chinese">在机器学习中，一个概念的例子被称为正面例子，反面例子被称为负面例子。如果你试图学习识别图像中的猫，猫的图像就是正例，狗的图像就是反例。如果你从世界文学作品中整理出一个家庭数据库，卡列宁家族将是一个幸福家庭的负面例子，而正面例子则少得可怜。</p>

<p class="noindent english">Starting with restrictive assumptions and gradually relaxing them if they fail to explain the data is typical of machine learning, and the process is usually carried out automatically by the learner, without any help from you. First, it tries all single factors, then all conjunctions of two factors, then all conjunctions of three, and so on. But now we run into a problem: there are <i>a lot</i> of conjunctive concepts and not enough time to try them all out.</p>

<p class="noindent chinese">从限制性假设开始，如果它们不能解释数据，就逐渐放宽它们，这是机器学习的典型做法，而且这个过程通常是由学习者自动进行的，不需要你的任何帮助。首先，它尝试所有的单一因素，然后是两个因素的所有连接，然后是三个因素的所有连接，以此类推。但现在我们遇到了一个问题：有<i>很多</i>连词概念，却没有足够的时间去尝试它们。</p>

<p class="noindent english">The dating example is a little deceptive because it’s very small (four variables and four examples). But suppose now that you run an online dating service and you need to figure out which couples to match. If each user of your system has filled out a questionnaire with answers to fifty yes/no questions, each potential match is characterized by one hundred attributes, fifty from each member of the prospective couple. Based on the couples that have gone on a date and reported the outcome, can you find a conjunctive definition for the concept of a “good match”? There are 3<sup>100</sup> possible definitions to try. (The three options for each attribute are yes, no, and not part of the concept.) Even with the fastest computer in the world, the couples will all be long gone—and your company bankrupt—by the time you’re done, unless you’re lucky and a very short definition hits the jackpot. So many rules, so little time. We need to do something smarter.</p>

<p class="noindent chinese">这个约会的例子有点欺骗性，因为它非常小（四个变量和四个例子）。但是，假设现在你经营一个在线约会服务，你需要弄清楚哪些情侣可以配对。如果你的系统的每个用户都填写了一份问卷，其中有 50 个是/否问题的答案，那么每个潜在的匹配对象都有 100 个属性，其中 50 个来自未来的情侣中的每个成员。根据那些已经约会并报告结果的情侣，你能为 “良好匹配” 的概念找到一个共轭的定义吗？有 3<sup>100</sup> 个可能的定义可以尝试。（每个属性的三个选项是 “是”、“不是” 和 “不属于这个概念”）。即使有世界上最快的计算机，在你完成的时候，这些夫妇都会消失，你的公司也会破产，除非你很幸运，一个非常短的定义中了大奖。这么多的规则，这么少的时间。我们需要做一些更聪明的事情。</p>

<p class="noindent english">Here’s one way. Suspend your disbelief and start by assuming that all matches are good. Then try excluding all matches that don’t have some attribute. Repeat this for each attribute, and choose the one that excludes the most bad matches and the fewest good ones. Your definition now looks something like, say, “It’s a good match only if he’s outgoing.” Now try adding every other attribute to that in turn, and choose the one <a id="babilu_link-201"></a> that excludes the most remaining bad matches and fewest remaining good ones. Perhaps the definition is now “It’s a good match only if he’s outgoing and so is she.” Try adding a third attribute to those two, and so on. Once you’ve excluded all the bad matches, you’re done: you have a definition of the concept that includes all the positive examples and excludes all the negative ones. For example: “A couple is a good match only if they’re both outgoing, he’s a dog person, and she’s not a cat person.” You can now throw away the data and keep only this definition, since it encapsulates all that’s relevant for your purposes. This algorithm is guaranteed to finish in a reasonable amount of time, and it’s also the first actual learner we meet in this book!</p>

<p class="noindent chinese">这里有一个方法。暂停你的怀疑，从假设所有的匹配都是好的开始。然后尝试排除所有不具备某种属性的匹配。对每个属性重复这个过程，然后选择一个排除了最多的坏比赛和最少的好比赛的属性。你的定义现在看起来像，比如说，“只有当他是外向的时候，才是一个好的匹配。” 现在试着依次增加其他属性，并选择一个以排除最多剩余的坏匹配和最少剩余的好匹配。也许现在的定义是 “只有当他是外向的，她也是外向的，才是好的匹配”。试着在这两个属性上增加第三个属性，以此类推。一旦你排除了所有不好的匹配，你就完成了：你有了一个概念的定义，其中包括所有正面的例子，排除了所有负面的。比如说。“一对夫妇只有在他们都是外向的，他是一个爱狗的人，而她不是一个爱猫的人时，才是一个好的匹配。” 现在你可以扔掉数据，只保留这个定义，因为它囊括了所有与你的目的相关的东西。这个算法可以保证在合理的时间内完成，而且它也是我们在本书中遇到的第一个实际的学习者！</p>

<h1 id="babilu_link-401"><b>How to rule the world</b></h1>

<h1 id="babilu_link-401"><b>如何统治世界</b></h1>

<p class="noindent english">Conjunctive concepts don’t get you very far, though. The problem is that, as Rudyard Kipling said, “There are nine and sixty ways of constructing tribal lays, and every one of them is right.” Real concepts are disjunctive. Chairs can have four legs or one, and sometimes none. You can win at chess in countless different ways. E-mails containing the word <i>Viagra</i> are probably spam, but so are e-mails containing “FREE!!!” Besides, all rules have exceptions. Some families manage to be dysfunctional yet happy. Birds fly, unless they’re penguins, ostriches, cassowaries, or kiwis (or they’ve broken a wing, or are locked in a cage, or…).</p>

<p class="noindent chinese">不过，连带的概念并不能让你走得很远。问题是，正如鲁德亚德·吉卜林所说，“有九百六十种构建部落谎言的方法，而每一种都是正确的。” 真正的概念是不连贯的。椅子可以有四条腿或一条腿，有时甚至没有。你可以用无数种不同的方式赢得国际象棋。含有<i>伟哥</i>一词的电子邮件可能是垃圾邮件，但含有 “免费！！” 的电子邮件也是如此。此外，所有规则都有例外。有些家庭能做到功能紊乱但却很幸福。鸟类会飞，除非它们是企鹅、鸵鸟、袋鼠或猕猴（或它们折断了翅膀，或被锁在笼子里，或… ）。</p>

<p class="noindent english">What we need is to learn concepts that are defined by a set of rules, not just a single rule, such as:</p>

<p class="noindent chinese">我们需要的是学习由一系列规则定义的概念，而不仅仅是单一的规则，例如：</p>

<div>

<p class="noindent english"><i>If you liked</i> Star Wars, <i>episodes IV–VI, you’ll like</i> Avatar.</p>

<p class="noindent chinese"><i>如果你喜欢</i>《星球大战》<i>第四集至第六集，你会喜欢</i>《阿凡达》。</p>

<p class="noindent english"><i>If you liked</i> Star Trek: The Next Generation <i>and</i> Titanic, <i>you’ll like</i> Avatar.</p>

<p class="noindent chinese"><i>如果你喜欢</i>《星际迷航：下一代》<i>和</i>《泰坦尼克号》，<i>你会喜欢</i>《阿凡达》。</p>

<p class="noindent english"><i>If you’re a member of the Sierra Club and read science-fiction books, you’ll like</i> Avatar.</p>

<p class="noindent chinese"><i>如果你是塞拉俱乐部的成员并阅读科幻书籍，你会喜欢</i>《阿凡达》。</p>

</div>

<p class="noindent english">Or:</p>

<p class="noindent chinese">或者：</p>

<div>

<p class="noindent english"><i><a id="babilu_link-132"><i></i></a> If your credit card was used in China, Canada, and Nigeria yesterday, it was stolen.</i></p>

<p class="noindent chinese"><i><a id="babilu_link-132"><i></i></a>如果你的信用卡昨天在中国、加拿大和尼日利亚被使用，那么它就是被盗了。</i></p>

<p class="noindent english"><i>If your credit card was used twice after 11:00 p.m. on a weekday, it was stolen.</i></p>

<p class="noindent chinese"><i>如果你的信用卡在工作日晚上 11 点后被使用了两次，那就是被盗了。</i></p>

<p class="noindent english"><i>If your credit card was used to purchase one dollar of gas, it was stolen.</i></p>

<p class="noindent chinese"><i>如果你的信用卡被用来购买一美元的汽油，那么它就是被盗了。</i></p>

</div>

<p class="noindent english">(If you’re wondering about the last rule, credit-card thieves used to routinely buy one dollar of gas to check that a stolen credit card was good before data miners caught on to the tactic.)</p>

<p class="noindent chinese">（如果你想知道最后一条规则，信用卡窃贼过去经常买一美元的汽油，以检查被盗的信用卡是否正常，然后数据挖掘者才发现这种策略。）</p>

<p class="noindent english">We can learn sets of rules like this one rule at a time, using the algorithm we saw before for learning conjunctive concepts. After we learn each rule, we discard the positive examples that it accounts for, so the next rule tries to account for as many of the remaining positive examples as possible, and so on until all are accounted for. It’s an example of “divide and conquer,” the oldest strategy in the scientist’s playbook. We can also improve the algorithm for finding a single rule by keeping some number <i>n</i> of hypotheses around, not just one, and at each step extending all of them in all possible ways and keeping the <i>n</i> best results.</p>

<p class="noindent chinese">我们可以使用我们之前看到的学习共轭概念的算法，一次学习这样的规则集。在我们学习完每条规则后，我们会丢弃它所占的正面例子，所以下一条规则会尽可能多地考虑到剩余的正面例子，以此类推，直到所有例子都被考虑到。这是一个 “分而治之” 的例子，是科学家游戏手册中最古老的策略。我们还可以改进寻找单一规则的算法，在周围保留一定数量<i>的</i>假设，而不仅仅是一个，并在每一步以所有可能的方式扩展所有的假设，并保留 <i>n</i> 个最佳结果。</p>

<p class="noindent english">Discovering rules in this way was the brainchild of Ryszard Michalski, a Polish computer scientist. Michalski’s hometown of Kalusz was successively part of Poland, Russia, Germany, and Ukraine, which may have left him more attuned than most to disjunctive concepts. After immigrating to the United States in 1970, he went on to found the symbolist school of machine learning, along with Tom Mitchell and Jaime Carbonell. He had an imperious personality. If you gave a talk at a machine-learning conference, the odds were good that at the end he’d raise his hand to point out that you had just rediscovered one of his old ideas.</p>

<p class="noindent chinese">以这种方式发现规则是波兰计算机科学家里夏德·米哈尔斯基的创意。米查尔斯基的家乡卡卢斯先后是波兰、俄罗斯、德国和乌克兰的一部分，这可能使他比大多数人更适应不连贯性的概念。1970 年移民美国后，他与汤姆·米切尔和海梅·卡博内尔一起创立了机器学习的符号主义学派。他的个性很强硬。如果你在机器学习会议上发表演讲，他很有可能在会议结束时举手指出，你刚刚重新发现了他的一个旧观点。</p>

<p class="noindent english">Sets of rules are popular with retailers who are deciding which goods to stock. Typically, they use a more exhaustive approach than “divide and conquer,” looking for all rules that strongly predict the purchase of each item. Walmart was a pioneer in this area. One of their early findings was that if you buy diapers you are also likely to buy beer. Huh? One interpretation of this is that Mom sends Dad to the supermarket <a id="babilu_link-221"></a> to buy diapers, and as emotional compensation, Dad buys a case of beer to go with them. Knowing this, the supermarket can now sell more beer by putting it next to the diapers, which would never have occurred to it without rule mining. The “beer and diapers” rule has acquired legendary status among data miners (although some claim the legend is of the urban variety). Either way, it’s a long way from the digital circuit design problems Michalski had in mind when he first started thinking about rule induction in the 1960s. When you invent a new learning algorithm, you can’t even begin to imagine all the things it will be used for.</p>

<p class="noindent chinese">成套的规则很受零售商的欢迎，因为他们要决定储存哪些商品。通常情况下，他们使用比 “分而治之” 更详尽的方法，寻找所有能有力预测每件商品购买的规则。沃尔玛是这一领域的先驱者。他们早期的发现之一是，如果你买尿布，你也可能会买啤酒。嗯？对此的一种解释是，妈妈让爸爸去超市去买尿布，而作为情感补偿，爸爸买了一箱啤酒来搭配。知道了这一点，超市现在可以通过把啤酒放在尿布旁边来卖出更多的啤酒，如果没有规则的挖掘，超市是不会想到这一点的。“啤酒和尿布” 的规则在数据挖掘者中获得了传奇性的地位（尽管有些人声称这是都市传奇而已）。不管怎么说，这与米哈尔斯基在 20 世纪 60 年代第一次开始思考规则归纳时想到的数字电路设计问题有很大的差距。当你发明一种新的学习算法时，你甚至无法想象它将被用于所有的事情。</p>

<p class="noindent english">My first direct experience of rule learning in action was when, having just moved to the United States to start graduate school, I applied for a credit card. The bank sent me a letter saying “We regret that your application has been rejected due to INSUFFICIENT-TIME-AT- CURRENT-ADDRESS and NO-PREVIOUS-CREDIT-HISTORY” (or some other all-cap words to that effect). I knew right then that there was much research left to do in machine learning.</p>

<p class="noindent chinese">我第一次直接体验到规则学习的作用是，当时我刚搬到美国开始读研究生，申请了一张信用卡。银行给我发了一封信，说 “我们很遗憾，你的申请被拒绝了，原因是没有足够的时间在目前的地址和没有以前的信用记录”（或者其他一些大意如此的全称）。我当时就知道，在机器学习方面还有很多研究要做。</p>

<h1 id="babilu_link-402"><b>Between blindness and hallucination</b></h1>

<h1 id="babilu_link-402"><b>在失明和幻觉之间</b></h1>

<p class="noindent english">Sets of rules are vastly more powerful than conjunctive concepts. They’re so powerful, in fact, that you can represent <i>any</i> concept using them. It’s not hard to see why. If you give me a complete list of all the instances of a concept, I can just turn each instance into a rule that specifies all attributes of that instance, and the set of all those rules is the definition of the concept. Going back to the dating example, one rule would be: <i>If it’s a warm weekend night, there’s nothing good on TV, and you propose going to a club, she’ll say yes</i> . The table only contains a few examples, but if it contained all 2 × 2 × 2 × 2 = 16 possible ones, with each labeled “Date” or “No date,” turning each positive example into a rule in this way would do the trick.</p>

<p class="noindent chinese">规则集比连接性概念强大得多。事实上，它们是如此强大，以至于你可以用它们来表示<i>任何</i>概念。这并不难理解。如果你给我一个概念的所有实例的完整列表，我可以把每个实例变成一个规则，指定该实例的所有属性，而所有这些规则的集合就是这个概念的定义。回到约会的例子，有一条规则是这样的。<i>如果这是一个温暖的周末夜晚，电视上没有什么好节目，而你提议去一个俱乐部，她会说好的</i>。这张表只包含几个例子，但如果它包含所有 2 × 2 × 2 × 2 = 16 个可能的例子，每个例子都标有 “约会” 或 “不约会”，以这种方式将每个正面的例子变成一个规则，就可以做到这一点。</p>

<p class="noindent english">The power of rule sets is a double-edged sword. On the upside, you know you can always find a rule set that perfectly matches the data. But before you start feeling lucky, realize that you’re at severe risk of finding a completely meaningless one. Remember the “no free lunch” theorem: <a id="babilu_link-146"></a> you can’t learn without knowledge. And assuming that the concept can be defined by a set of rules is tantamount to assuming nothing.</p>

<p class="noindent chinese">规则集的力量是一把双刃剑。从正面看，你知道你总是可以找到一个与数据完全匹配的规则集。但在你开始感到幸运之前，要意识到你面临着找到一个完全没有意义的规则集的严重风险。记住 “没有免费的午餐” 定理：没有知识你就无法学习。而假设这个概念可以由一套规则来定义，就等于什么都不假设了。</p>

<p class="noindent english">An example of a useless rule set is one that just covers the exact positive examples you’ve seen and nothing else. This rule set looks like it’s 100 percent accurate, but that’s an illusion: it will predict that every new example is negative, and therefore get every positive one wrong. If there are more positive than negative examples overall, this will be even worse than flipping coins. Imagine a spam filter that decides an e-mail is spam only if it’s an exact copy of a previously labeled spam message. It’s easy to learn and looks great on the labeled data, but you might as well have no spam filter at all. Unfortunately, our “divide and conquer” algorithm could easily learn a rule set like that.</p>

<p class="noindent chinese">一个无用的规则集的例子是，它只包括你已经看到的确切的正面例子，而没有其他的。这种规则集看起来是百分之百的准确，但这是一种错觉：它将预测每一个新的例子都是负面的，因此会把每一个正面的例子都搞错。如果正面的例子总体上多于负面的例子，这甚至会比掷硬币更糟糕。想象一下，有一个垃圾邮件过滤器，只有当它是以前被标记为垃圾邮件的精确拷贝时，才会判定它是垃圾邮件。这很容易学习，而且在标记的数据上看起来很好，但你还不如没有垃圾邮件过滤器。不幸的是，我们的 “分而治之” 的算法可以很容易地学习这样的规则集。</p>

<p class="noindent english">In his story “Funes the Memorious,” Jorge Luis Borges tells of meeting a youth with perfect memory. This might at first seem like a great fortune, but it is in fact an awful curse. Funes can remember the exact shape of the clouds in the sky at an arbitrary time in the past, but he has trouble understanding that a dog seen from the side at 3:14 p.m. is the same dog seen from the front at 3:15 p.m. His own face in the mirror surprises him every time he sees it. Funes can’t generalize; to him, two things are the same only if they look the same down to every last detail. An unrestricted rule learner is like Funes and is equally unable to function. Learning is forgetting the details as much as it is remembering the important parts. Computers are the ultimate idiot savants: they can remember everything with no trouble at all, but that’s not what we want them to do.</p>

<p class="noindent chinese">豪尔赫·路易斯·博尔赫斯在他的故事《记忆中的富内斯》中讲述了遇到一个拥有完美记忆的青年。这初看起来是一种巨大的财富，但实际上是一种可怕的诅咒。富内斯能记住过去某个任意时间天空中云朵的确切形状，但他很难理解下午 3 点 14 分从侧面看到的狗和下午 3 点 15 分从正面看到的狗是一样的。富内斯不能一概而论；对他来说，只有当两件东西在每个细节上看起来都一样时，它们才是相同的。一个不受限制的规则学习者就像富内斯一样，同样无法发挥作用。学习就是忘记细节，就像记住重要部分一样。计算机是最终的白痴救星：它们可以毫不费力地记住所有的东西，但这不是我们希望它们做的。</p>

<p class="noindent english">The problem is not limited to memorizing instances wholesale. Whenever a learner finds a pattern in the data that is not actually true in the real world, we say that it has overfit the data. Overfitting is the central problem in machine learning. More papers have been written about it than about any other topic. Every powerful learner, whether symbolist, connectionist, or any other, has to worry about hallucinating patterns. The only safe way to avoid it is to severely restrict what the learner can learn, for example by requiring that it be a short conjunctive concept. Unfortunately, that throws out the baby with the bathwater, <a id="babilu_link-77"></a> leaving the learner unable to see most of the true patterns that are visible in the data. Thus a good learner is forever walking the narrow path between blindness and hallucination.</p>

<p class="noindent chinese">这个问题并不限于全盘记忆实例。每当学习者在数据中发现一个在现实世界中并不真实的模式，我们就说它对数据进行了过度拟合。过度拟合是机器学习的核心问题。关于这个问题的论文比任何其他话题都要多。每一个强大的学习者，无论是符号主义、连接主义，还是其他任何一种，都不得不担心出现幻觉模式。避免这种情况的唯一安全方法是严格限制学习者可以学习的内容，例如要求它是一个简短的连接性概念。不幸的是，这样做会把婴儿和洗澡水一起扔掉，使学习者无法看到数据中可见的大多数真实模式。因此，一个好的学习者永远走在盲目和幻觉之间的狭窄道路上。</p>

<p class="noindent english">Humans are not immune to overfitting, either. You could even say that it’s the root cause of a lot of our evils. Consider the little white girl who, upon seeing a Latina baby at the mall, blurted out “Look, Mom, a baby maid!” (True event.) It’s not that she’s a natural-born bigot. Rather, she overgeneralized from the few Latina maids she has seen in her short life. The world is full of Latinas with other occupations, but she hasn’t met them yet. Our beliefs are based on our experience, which gives us a very incomplete picture of the world, and it’s easy to jump to false conclusions. Being smart and knowledgeable doesn’t immunize you against overfitting, either. Aristotle overfit when he said that it takes a force to keep an object moving. Galileo’s genius was to intuit that undisturbed objects keep moving without having visited outer space to witness it firsthand.</p>

<p class="noindent chinese">人类也不能幸免于过度拟合。你甚至可以说，它是我们很多罪恶的根源。想想那个白人小女孩吧，她在商场看到一个拉丁裔婴儿后，大声说：“看，妈妈，一个女佣宝宝！”（真实事件。）这并不是说她是一个天生的偏执者。相反，她从她短暂的一生中所见到的几个拉丁裔女仆中过度概括了。世界上有很多从事其他职业的拉美人，但她还没有见过他们。我们的信念是基于我们的经验，这让我们对世界有一个非常不完整的了解，而且很容易得出错误的结论。聪明和知识渊博也不能使你免于过度拟合。当亚里士多德说需要一种力量来保持物体的运动时，他就过度适应了。伽利略的天才之处在于，他在没有到过外太空亲眼目睹的情况下，就直觉到不受干扰的物体一直在移动。</p>

<p class="noindent english">Learning algorithms are particularly prone to overfitting, though, because they have an almost unlimited capacity to find patterns in data. In the time it takes a human to find one pattern, a computer can find millions. In machine learning, the computer’s greatest strength—its ability to process vast amounts of data and endlessly repeat the same steps without tiring—is also its Achilles’ heel. And it’s amazing what you can find if you search enough. <i>The Bible Code</i> , a 1998 bestseller, claimed that the Bible contains predictions of future events that you can find by skipping letters at regular intervals and assembling words from the letters you land on. Unfortunately, there are so many ways to do this that you’re guaranteed to find “predictions” in any sufficiently long text. Skeptics replied by finding them in <i>Moby Dick</i> and Supreme Court rulings, along with mentions of Roswell and UFOs in Genesis. John von Neumann, one of the founding fathers of computer science, famously said that “with four parameters I can fit an elephant, and with five I can make him wiggle his trunk.” Today we routinely learn models with millions of parameters, enough to give each elephant in the world his own <a id="babilu_link-189"></a> distinctive wiggle. It’s even been said that <i>data mining</i> means “torturing the data until it confesses.”</p>

<p class="noindent chinese">不过，学习算法特别容易过度拟合，因为它们有几乎无限的能力来寻找数据中的模式。在人类找到一个模式的时间里，计算机可以找到数百万个。在机器学习中，计算机最大的优势 —— 它能够处理大量的数据，并无休止地重复同样的步骤，而这也是其致命的弱点。而且，如果你搜索得够多，你能找到的东西也很惊人。1998 年的畅销书《<i>圣经密码</i>》声称，《圣经》中包含了对未来事件的预测，你可以通过定期跳过字母并从你落下的字母中组合出单词来找到。不幸的是，有很多方法可以做到这一点，以至于你可以保证在任何足够长的文本中找到 “预言”。怀疑论者的回答是在《<i>白鲸</i>》和最高法院的裁决中找到它们，以及在《创世纪》中提到的罗斯威尔和 UFO。约翰·冯·诺伊曼是计算机科学的奠基人之一，他有句名言：“用四个参数我就能装下一头大象，用五个参数我就能让它扭动躯干。” 今天，我们经常学习具有数百万个参数的模型，足以让世界上的每头大象都有自己的独特的扭动。甚至有人说，<i>数据挖掘</i>意味着 “折磨数据，直到它认罪”。</p>

<p class="noindent english">Overfitting is seriously exacerbated by noise. Noise in machine learning just means errors in the data, or random events that you can’t predict. Suppose that your friend really does like to go clubbing when there’s nothing interesting on TV, but you misremembered occasion number 3 and wrote down that there <i>was</i> something good on TV that night. If you now try to come up with a set of rules that makes an exception for that night, you’ll probably wind up with a worse answer than if you’d just ignored it. Or suppose that your friend had a hangover from going out the previous night and said no when ordinarily she would have said yes. Unless you know about the hangover, learning a set of rules that gets this example right is actually counterproductive: you’re better off “misclassifying” it as a no. It gets worse: noise can make it impossible to come up with <i>any</i> consistent set of rules. Notice that occasions 2 and 3 are in fact indistinguishable: they have exactly the same attributes. If your friend said yes on occasion 2 and no on occasion 3, there’s no rule that will get them both right.</p>

<p class="noindent chinese">过度拟合会因噪声而严重恶化。机器学习中的噪音只是指数据中的错误，或你无法预测的随机事件。假设你的朋友真的喜欢在电视上没有什么有趣的东西时去泡吧，但你记错了第 3 个场合，写下了那晚电视上<i>有</i>好东西。如果你现在试图想出一套规则，让那晚的情况例外，你的答案可能会比你忽略它更糟糕。或者假设你的朋友在前一天晚上出去后宿醉了，她说不去，而通常情况下她会说去。除非你知道宿醉的情况，否则学习一套规则来正确处理这个例子实际上是适得其反的：你最好把它 “错误” 地归类为 “不”。更糟的是：噪音会使你无法想出<i>任何</i>一致的规则集。请注意，场合 2 和场合 3 实际上是无法区分的：它们具有完全相同的属性。如果你的朋友在第 2 个场合说 “是”，而在第 3 个场合说 “不是”，那么没有任何规则能让他们都正确。</p>

<p class="noindent english">Overfitting happens when you have too many hypotheses and not enough data to tell them apart. The bad news is that even for the simple conjunctive learner, the number of hypotheses grows exponentially with the number of attributes. Exponential growth is a scary thing. An <i>E. coli</i> bacterium can divide into two roughly every fifteen minutes; given enough nutrients it can grow into a mass of bacteria the size of Earth in about a day. When the number of things an algorithm needs to do grows exponentially with the size of its input, computer scientists call it a combinatorial explosion and run for cover. In machine learning, the number of possible instances of a concept is an exponential function of the number of attributes: if the attributes are Boolean, each new attribute doubles the number of possible instances by taking each previous instance and extending it with a yes or no for that attribute. In turn, the number of possible concepts is an exponential function of the number of possible instances: since a concept labels each instance as <a id="babilu_link-204"></a> positive or negative, adding an instance doubles the number of possible concepts. As a result, the number of concepts is an exponential function of an exponential function of the number of attributes! In other words, machine learning is a combinatorial explosion of combinatorial explosions. Perhaps we should just give up and not waste our time on such a hopeless problem?</p>

<p class="noindent chinese">当你有太多的假设而没有足够的数据来区分它们时，就会发生过度拟合。坏消息是，即使是简单的联合学习器，假设的数量也会随着属性的数量呈指数级增长。指数级增长是一件可怕的事情。一个<i>大肠杆菌</i>大约每 15 分钟就能分裂成两个；如果有足够的养分，它可以在大约一天内成长为地球大小的细菌群。当一个算法需要做的事情的数量随着其输入的大小呈指数级增长时，计算机科学家称其为组合爆炸，并跑去躲避。在机器学习中，一个概念的可能实例的数量是属性数量的指数函数：如果属性是布尔值，每一个新的属性都会使可能实例的数量增加一倍，方法是将以前的每一个实例用该属性的是或否来扩展。反过来，可能的概念的数量是可能的实例数量的指数函数：因为一个概念将每个实例标记为正或负，增加一个实例会使可能的概念数量增加一倍。因此，概念的数量是属性数量的指数函数！换句话说，机器学习是一个组合爆炸的组合爆炸。也许我们应该放弃，不要把时间浪费在这样一个无望的问题上？</p>

<p class="noindent english">Fortunately, something happens in learning that kills off one of the exponentials, leaving only an “ordinary” singly exponential intractable problem. Suppose you have a bag full of concept definitions, each written on a piece of paper, and you take out a random one and see how well it matches the data. A bad definition is no more likely to get, say, all thousand examples in your data right than a coin is likely to come up heads a thousand times in a row. “A chair has four legs and is red or has a seat but no legs” will probably match some but not all chairs you’ve seen and also match some but not all other things. So if a random definition correctly matches a thousand examples, then it’s extremely unlikely to be the wrong definition, or at least it’s pretty close to the real one. And if the definition agrees with a million examples, then it’s practically certain to be the right one. How else would it get all those examples right?</p>

<p class="noindent chinese">幸运的是，在学习中会发生一些事情，杀死其中一个指数，只留下一个 “普通” 的单指数难以解决的问题。假设你有一个装满概念定义的袋子，每个都写在一张纸上，你随机拿出一个，看看它与数据的匹配程度。一个糟糕的定义不可能像一枚硬币连续一千次出现头像那样，把你的数据中所有的例子都说对。“一把椅子有四条腿，是红色的，或者有座位但没有腿” 可能会匹配一些但不是所有你见过的椅子，也会匹配一些但不是所有其他东西。因此，如果一个随机的定义正确地匹配了一千个例子，那么它就极不可能是错误的定义，或者至少它很接近真实的定义。如果这个定义与一百万个例子相吻合，那么它几乎可以肯定是正确的。否则，它怎么能把所有这些例子都弄对呢？</p>

<p class="noindent english">Of course, a real learning algorithm doesn’t just take a random definition from the bag: it tries a vast number of them, and they’re not chosen at random. The more definitions it tries, the more likely one of them will match all the examples just by chance. If you do more and more runs of a thousand coin flips, eventually it becomes practically certain that at least one run will come up all heads. And a learning algorithm doesn’t need to explicitly try all definitions one by one; when it finds the best one, the result is the same as if it had tried all others. Since the number of definitions the algorithm considers can be as large as a doubly exponential function of the number of attributes, at some point it’s practically guaranteed that the algorithm will find a bad hypothesis that looks good.</p>

<p class="noindent chinese">当然，真正的学习算法并不是从袋子里随机抽取一个定义：它尝试了大量的定义，而且这些定义不是随机选择的。它尝试的定义越多，其中一个就越有可能偶然地与所有的例子相匹配。如果你对一千枚硬币进行越来越多的投掷，最终实际上可以肯定，至少有一次投掷的结果都是正面。而且，学习算法不需要明确地逐一尝试所有的定义；当它找到最好的定义时，其结果与尝试所有其他的定义是一样的。由于算法考虑的定义数量可以大到与属性数量成双指数的函数，所以在某些时候，实际上可以保证算法会找到一个看起来不错的坏假设。</p>

<p class="noindent english">Bottom line: learning is a race between the amount of data you have and the number of hypotheses you consider. More data exponentially reduces the number of hypotheses that survive, but if you start with a lot of them, you may still have some bad ones left at the end. As a rule of <a id="babilu_link-23"></a> thumb, if the learner only considers an exponential number of hypotheses (for example, all possible conjunctive concepts), then the data’s exponential payoff cancels it and you’re OK, provided you have plenty of examples and not too many attributes. On the other hand, if it considers a doubly exponential number (for example, all possible rule sets), then the data cancels only one of the exponentials and you’re still in trouble. You can even figure out in advance how many examples you’ll need to be pretty sure that the learner’s chosen hypothesis is very close to the true one, provided it fits all the data; in other words, for the hypothesis to be probably approximately correct. Harvard’s Leslie Valiant received the Turing Award, the Nobel Prize of computer science, for inventing this type of analysis, which he describes in his book entitled, appropriately enough, <i>Probably Approximately Correct</i> .</p>

<p class="noindent chinese">一句话：学习是一场你所拥有的数据量和你所考虑的假设数量之间的竞赛。更多的数据会以指数形式减少存活的假设数量，但如果你开始时有很多假设，最后可能还会剩下一些不好的假设。作为经验法则，如果学习者只考虑指数数量的假设（例如，所有可能的共轭概念），那么数据的指数报酬就会抵消它，你就没问题了，只要你有大量的例子和不多的属性。另一方面，如果它考虑的是一个双指数的数字（例如，所有可能的规则集），那么数据只取消了其中一个指数，你仍然有麻烦。你甚至可以提前算出你需要多少个例子才能非常确定学习者选择的假设非常接近真实的假设，只要它符合所有的数据；换句话说，假设可能近似正确。哈佛大学的莱斯利·维拉提因为发明了这种类型的分析而获得了图灵奖，也就是计算机科学的诺贝尔奖，他在他的书中描述了这种分析，书名为《<i>大概</i>是正确的》，很恰当。</p>

<h1 id="babilu_link-403"><b>Accuracy you can believe in</b></h1>

<h1 id="babilu_link-403"><b>你可以相信的准确度</b></h1>

<p class="noindent english">In practice, Valiant-style analysis tends to be very pessimistic and to call for more data than you have. So how do you decide whether to believe what a learner tells you? Simple: you don’t believe anything until you’ve verified it on data that <i>the learner didn’t see</i> . If the patterns the learner hypothesized also hold true on new data, you can be pretty confident that they’re real. Otherwise you know the learner overfit. This is just the scientific method applied to machine learning: it’s not enough for a new theory to explain past evidence because it’s easy to concoct a theory that does that; the theory must also make new predictions, and you only accept it after they’ve been experimentally verified. (And even then only provisionally, because future evidence could still falsify it.)</p>

<p class="noindent chinese">在实践中，维拉提式的分析往往是非常悲观的，需要比你拥有更多的数据。那么，你如何决定是否相信一个学习者告诉你的东西？很简单：在你对<i>学习者没有看到的</i>数据进行验证之前，你不要相信任何东西。如果学习者假设的模式在新的数据上也是正确的，你就可以很有把握地认为它们是真的。否则你就知道学习者过度拟合了。这只是应用于机器学习的科学方法：一个新的理论仅仅解释过去的证据是不够的，因为很容易炮制出一个这样的理论；这个理论还必须做出新的预测，而且你只有在这些预测被实验验证之后才能接受它。（即便如此，也只是暂时的，因为未来的证据仍然可能使其失效）。</p>

<p class="noindent english">Einstein’s general relativity was only widely accepted once Arthur Eddington empirically confirmed its prediction that the sun bends the light of distant stars. But you don’t need to wait around for new data to arrive to decide whether you can trust your learner. Rather, you take the data you have and randomly divide it into a training set, which you give to the learner, and a test set, which you hide from it and use to verify its accuracy. Accuracy on held-out data is the gold standard in machine <a id="babilu_link-291"></a> learning. You can write a paper about a great new learning algorithm you’ve invented, but if your algorithm is not significantly more accurate than previous ones on held-out data, the paper is not publishable.</p>

<p class="noindent chinese">爱因斯坦的广义相对论只有在阿瑟·爱丁顿从经验上证实了它的预测，即太阳会弯曲远处恒星的光线后才被广泛接受。但你不需要等待新数据的到来来决定你是否可以信任你的学习者。相反，你把你拥有的数据随机分为训练集和测试集，前者是给学习者的，后者是你隐藏起来的，用来验证其准确性。在被隐藏的数据上的准确性是机器学习的黄金标准。你可以写一篇关于你所发明的伟大的新学习算法的论文，但是如果你的算法在保留数据上没有明显地比以前的算法更准确，这篇论文就不能发表。</p>

<p class="noindent english">Accuracy on previously unseen data is a pretty stringent test; so much so, in fact, that a lot of science fails it. That does not make it useless, because science is not just about prediction; it’s also about explanation and understanding. But ultimately, if your models don’t make accurate predictions on new data, you can’t be sure you’ve truly understood or explained the underlying phenomena. And for machine learning, testing on unseen data is indispensable because it’s the only way to tell whether the learner has overfit or not.</p>

<p class="noindent chinese">对以前未见过的数据的准确性是一个相当严格的测试；事实上，如此一来，很多科学都无法通过它。这并不意味着它毫无用处，因为科学不仅仅是预测；它也是关于解释和理解。但最终，如果你的模型不能对新数据做出准确的预测，你就不能确定你已经真正理解或解释了基本现象。而对于机器学习来说，对未见过的数据进行测试是必不可少的，因为这是判断学习者是否过度拟合的唯一方法。</p>

<p class="noindent english">Even test-set accuracy is not foolproof. According to legend, in an early military application a simple learner detected tanks with 100 percent accuracy in both the training set and the test set, each consisting of one hundred images. Amazing—or suspicious? Turns out all the tank images were lighter than the nontank ones, and that’s all the learner was picking up. These days we have larger data sets, but the quality of data collection isn’t necessarily better, so caveat emptor. Hard-nosed empirical evaluation played an important role in the growth of machine learning from a fledgling field into a mature one. Up to the late 1980s, researchers in each tribe mostly believed their own rhetoric, assumed their paradigm was fundamentally better, and communicated little with the other camps. Then symbolists like Ray Mooney and Jude Shavlik started to systematically compare the different algorithms on the same data sets and—surprise, surprise—no clear winner emerged. Today the rivalry continues, but there is much more cross-pollination. Having a common experimental framework and a large repository of data sets maintained by the machine-learning group at the University of California, Irvine, did wonders for progress. And as we’ll see, our best hope of creating a universal learner lies in synthesizing ideas from different paradigms.</p>

<p class="noindent chinese">即使测试集的准确性也不是万无一失的。据传说，在早期的军事应用中，一个简单的学习者在训练集和测试集中都能以 100% 的准确率检测到坦克，每个测试集由 100 张图像组成。令人惊讶或可疑？原来，所有的坦克图像都比非坦克图像要轻，这就是学习器所识别的全部。如今，我们有更大的数据集，但数据收集的质量并不一定更好，所以要注意。在机器学习从一个刚刚起步的领域成长为一个成熟的领域的过程中，严格的经验评估发挥了重要作用。直到 20 世纪 80 年代末，每个部落的研究人员都相信自己的说辞，认为自己的范式从根本上说是更好的，并且很少与其他阵营进行交流。然后，像雷·穆尼和裘德·沙威利这样的象征主义者开始在相同的数据集上系统地比较不同的算法，惊喜的是，没有明显的赢家出现了。今天，竞争仍在继续，但有更多的交叉授粉。有一个共同的实验框架和一个由加州大学欧文分校的机器学习小组维护的大型数据集库，对进展有很大的帮助。正如我们将看到的，我们创造一个通用学习者的最大希望在于综合不同范式的想法。</p>

<p class="noindent english">Of course, it’s not enough to be able to tell when you’re overfitting; we need to avoid it in the first place. That means stopping short of perfectly fitting the data even if we’re able to. One method is to use statistical <a id="babilu_link-235"></a> significance tests to make sure the patterns we’re seeing are really there. For example, a rule covering three hundred positive examples versus one hundred negatives and a rule covering three positives versus one negative are both 75 percent accurate on the training data, but the first rule is almost certainly better than coin flipping, while the second isn’t, since four flips of an unbiased coin could easily result in three heads. When constructing a rule, if at some point we can’t find any conditions that significantly improve its accuracy then we just stop, even if it still covers some negative examples. This reduces the rule’s training-set accuracy, but probably makes it a more accurate generalization, which is what we really care about.</p>

<p class="noindent chinese">当然，仅仅能够知道你什么时候过度拟合是不够的；我们需要首先避免过度拟合。这意味着即使我们能够完全拟合数据，也要停止。一种方法是使用统计学显著性测试，以确保我们所看到的模式真的存在。例如，一个涵盖三百个正面例子与一百个负面例子的规则和一个涵盖三个正面例子与一个负面例子的规则在训练数据上的准确率都是 75%，但第一个规则几乎肯定比抛硬币好，而第二个则不是，因为一个无偏见的硬币抛四次很容易得到三个头。在构建一个规则时，如果在某一点上我们找不到任何能显著提高其准确性的条件，那么我们就停止，即使它仍然涵盖一些负面的例子。这降低了规则的训练集准确性，但可能使其成为一个更准确的概括，这才是我们真正关心的。</p>

<p class="noindent english">We’re not home free yet, though. If I try one rule and it’s 75 percent accurate on four hundred examples, I can probably believe it. But if I try a million rules and the best one is 75 percent accurate on four hundred examples, I probably can’t, because that could easily happen by chance. This is the same problem you have when picking a mutual fund. The Clairvoyant Fund just beat the market ten years in a row. Wow, the manager must be a genius. Or not? If you have a thousand funds to choose from, the odds are better than even that one will beat the market ten years in a row, even if they’re all secretly run by dart-throwing monkeys. The scientific literature is also plagued by this problem. Significance tests are the gold standard for deciding whether a research result is publishable, but if several teams look for an effect and only one finds it, chances are it didn’t, even though you’d never guess that from reading their solid-looking paper. One solution would be to also publish negative results, so you’d know about all those failed attempts, but that hasn’t caught on. In machine learning, we can keep track of how many rules we’ve tried and adjust our significance tests accordingly, but then they tend to throw out a lot of good rules along with the bad ones. A better method is to realize that some false hypotheses will inevitably get through, but keep their number under control by rejecting enough low-significance ones, and then test the surviving hypotheses on further data.</p>

<p class="noindent chinese">不过，我们还没有回家。如果我试了一条规则，它在四百个例子中的准确率为 75%，我可能会相信它。但如果我尝试了一百万条规则，最好的一条在四百个例子中准确率达到 75%，我可能就不能相信了，因为这很可能是偶然发生的。这也是你在挑选共同基金时遇到的问题。千里眼基金刚刚连续十年击败了市场。哇，这个经理一定是个天才。或者不是？如果你有一千个基金可供选择，那么，有一个基金连续十年战胜市场的几率要大得多，即使它们都是由投掷飞镖的猴子秘密经营。科学文献也受到这个问题的困扰。显著性检验是决定一项研究结果是否可以发表的黄金标准，但是如果几个团队都在寻找一种效应，而只有一个团队发现了这种效应，那么很有可能它没有，尽管你从阅读他们看起来很牢固的论文中永远也猜不到。一个解决方案是同时公布负面结果，这样你就会知道所有这些失败的尝试，但这还没有流行起来。在机器学习中，我们可以跟踪我们尝试了多少条规则，并相应地调整我们的显著性测试，但这样他们往往会把很多好的规则和坏的规则一起扔掉。一个更好的方法是意识到一些错误的假设将不可避免地被通过，但通过拒绝足够多的低显著性的假设来控制其数量，然后在进一步的数据上测试幸存的假设。</p>

<p class="noindent english">Another popular method is to prefer simpler hypotheses. The “divide and conquer” algorithm implicitly prefers simpler rules because it stops <a id="babilu_link-135"></a> adding conditions to a rule as soon as it covers only positive examples and stops adding rules as soon as all positive examples are covered. But to combat overfitting, we need a stronger preference for simpler rules, one that will cause us to stop adding conditions even before all negative examples have been covered. For example, we can subtract a penalty proportional to the length of the rule from its accuracy and use that as an evaluation measure.</p>

<p class="noindent chinese">另一种流行的方法是倾向于更简单的假说。分割与征服 “算法隐含地倾向于更简单的规则，因为一旦规则只覆盖了正面的例子，它就停止一旦覆盖了所有正面的例子，就停止添加规则。但是，为了对抗过度拟合，我们需要对更简单的规则有更强的偏好，这种偏好会使我们在覆盖所有负面例子之前就停止添加条件。例如，我们可以从规则的准确性中减去与规则长度成正比的惩罚，并将其作为一种评估措施。</p>

<p class="noindent english">The preference for simpler hypotheses is popularly known as Occam’s razor, but in a machine-learning context this is somewhat misleading. “Entities should not be multiplied beyond necessity,” as the razor is often paraphrased, just means choosing the simplest theory that fits the data. Occam would probably have been perplexed by the notion that we should prefer a theory that does <i>not</i> perfectly account for the evidence on the grounds that it will generalize better. Simple theories are preferable because they incur a lower cognitive cost (for us) and a lower computational cost (for our algorithms), not because we necessarily expect them to be more accurate. On the contrary, even our most elaborate models are usually oversimplifications of reality. Even among theories that perfectly fit the data, we know from the “no free lunch” theorem that there’s no guarantee that the simplest one will generalize best, and in fact some of the best learning algorithms—like boosting and support vector machines—learn what appear to be gratuitously complex models. (We’ll see why they work in <a href="#babilu_link-13">Chapters 7</a> and <a href="#babilu_link-12">9</a> .)</p>

<p class="noindent chinese">对更简单假设的偏好被普遍称为奥卡姆剃刀，但在机器学习的背景下，这有点误导。正如奥卡姆剃刀经常被解读的那样，“如无必要勿增实体”，只是意味着选择适合数据的最简单理论。奥卡姆可能会对这样的概念感到困惑，即我们应该偏爱一个<i>不能</i>完美解释证据的理论，理由是它能更好地概括。简单的理论是可取的，因为它们（对我们来说）产生了较低的认知成本和较低的计算成本（对我们的算法来说），而不是因为我们一定期望它们更准确。相反，即使我们最精心设计的模型通常也是对现实的过度简化。即使在完全适合数据的理论中，我们从 “没有免费的午餐” 定理中知道，不能保证最简单的理论会有最好的概括性，事实上，一些最好的学习算法 —— 如提升和支持向量机 —— 学习的似乎是无端的复杂模型。（我们将在<a href="#babilu_link-13">第 7 章</a>和<a href="#babilu_link-12">第 9 章</a>中看到它们的作用）。</p>

<p class="noindent english">If your learner’s test-set accuracy disappoints, you need to diagnose the problem. Was it blindness or hallucination? In machine learning, the technical terms for these are <i>bias</i> and <i>variance</i> . A clock that’s always an hour late has high bias but low variance. If instead the clock alternates erratically between fast and slow but on average tells the right time, it has high variance but low bias. Suppose you’re down at the pub with some friends, drinking and playing darts. Unbeknownst to them, you’ve been practicing for years, and you’re a master of the game. All your darts go straight to the bull’s-eye. You have low bias and low variance, which is shown in the bottom left corner of this diagram:</p>

<p class="noindent chinese">如果你的学习者的测试集准确性令人失望，你需要诊断问题。是盲目性还是幻觉？在机器学习中，这些的技术术语是<i>偏差</i>和<i>方差</i>。一个总是迟到一小时的时钟有很高的偏差，但方差很低。如果时钟在快慢之间不稳定地交替，但平均来说是正确的时间，那么它就有高方差但低偏向。假设你和一些朋友在酒吧里喝酒和玩飞镖。他们不知道的是，你已经练习了很多年，你是一个游戏高手。你所有的飞镖都直奔靶心而去。你有低的偏差和低的变异，这在这个图的左下角显示。</p>

<div>

<div>

<img alt="image" src="images/000014.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-312"></a> Your friend Ben is also pretty good, but he’s had a bit too much to drink. His darts are all over, but he loudly points out that on average he’s hitting the bull’s-eye. (Maybe he should have been a statistician.) This is the low-bias, high-variance case, shown in the bottom right corner. Ben’s girlfriend, Ashley, is very steady, but she has a tendency to aim too high and to the right. She has low variance and high bias (top left corner). Cody, who’s visiting from out of town and has never played darts before, is both all over and off center. He has both high bias and high variance (top right).</p>

<p class="noindent chinese">你的朋友本也很厉害，但是他喝得有点多。他的飞镖到处都是，但是他大声地指出，平均来说，他击中了靶心。（也许他应该成为一名统计学家。）这就是右下角显示的低偏差、高变异的情况。本的女友阿什利非常稳重，但她有一种倾向，就是把目标定得太高，太偏向右边。她有低方差和高偏向（左上角）。科迪，从外地来参观，以前从来没有玩过飞镖，他既是全能的又是偏心的。他有高偏向和高方差（右上角）。</p>

<p class="noindent english">You can estimate the bias and variance of a learner by comparing its predictions after learning on random variations of the training set. If it keeps making the same mistakes, the problem is bias, and you need a more flexible learner (or just a different one). If there’s no pattern to the mistakes, the problem is variance, and you want to either try a less flexible learner or get more data. Most learners have a knob you can turn to make them more or less flexible, such as the threshold for significance tests or the penalty on the size of the model. Tweaking that knob is your first resort.</p>

<p class="noindent chinese">你可以通过比较一个学习者在训练集的随机变化上学习后的预测来估计它的偏差和方差。如果它一直在犯同样的错误，那么问题就出在偏差上，你需要一个更灵活的学习器（或者只是一个不同的学习器）。如果这些错误没有规律可循，那么问题就在于差异，你需要尝试一个不那么灵活的学习器或者获得更多的数据。大多数学习器都有一个旋钮，你可以转动它以使其更灵活或更不灵活，例如显著性检验的阈值或对模型大小的惩罚。调整这个旋钮是你的第一手段。</p>

<h1 id="babilu_link-404"><b><a id="babilu_link-159"><b></b></a> Induction is the inverse of deduction</b></h1>

<h1 id="babilu_link-404"><b><a id="babilu_link-159"><b></b></a>归纳法是演绎法的反面</b></h1>

<p class="noindent english">The deeper problem, however, is that most learners start out knowing too little, and no amount of knob-twiddling will get them to the finish line. Without the guidance of an adult brain’s worth of knowledge, they can easily go astray. Even though it’s what most learners do, just assuming you know the form of the truth (for example, that it’s a small set of rules) is not much to hang your hat on. A strict empiricist would say that that’s all a newborn has, encoded in her brain’s architecture, and indeed children overfit more than adults do, but we would like to learn faster than a child does. (Eighteen years is a long time, and that’s not counting college.) The Master Algorithm should be able to start with a large body of knowledge, whether it was provided by humans or learned in previous runs, and use it to guide new generalizations from data. That’s what scientists do, and it’s as far as it gets from a blank slate. The “divide and conquer” rule induction algorithm can’t do it, but there’s another way to learn rules that can.</p>

<p class="noindent chinese">然而，更深层次的问题是，大多数学习者一开始知道的东西太少了，无论怎么摆弄都无法让他们到达终点。如果没有一个成年人大脑的知识价值的指导，他们很容易误入歧途。尽管这是大多数学习者所做的，但仅仅假设你知道真理的形式（例如，它是一套小的规则），并没有多少东西可以挂在你的帽子上。一个严格的经验主义者会说，这就是一个新生儿所拥有的一切，在她的大脑架构中编码，而且确实儿童比成人更容易过度，但我们希望比儿童学习得更快。（18 年是一段很长的时间，这还不算大学。）主算法应该能够从大量的知识开始，无论这些知识是由人类提供的还是在以前的运行中学习的，并使用它来指导数据的新概括。这就是科学家所做的，而且是离白板最近的地方。“分而治之” 的规则归纳算法无法做到这一点，但有另一种方法可以学习规则。</p>

<p class="noindent english">The key is to realize that induction is just the inverse of deduction, in the same way that subtraction is the inverse of addition, or integration the inverse of differentiation. This idea was first proposed by William Stanley Jevons in the late 1800s. Steve Muggleton and Wray Buntine, an English Australian team, designed the first practical algorithm based on it in 1988. The strategy of taking a well-known operation and figuring out its inverse has a storied history in mathematics. Applying it to addition led to the invention of the integers, because without negative numbers, addition doesn’t always have an inverse (3–4 = –1). Similarly, applying it to multiplication led to the rationals, and applying it to squaring led to complex numbers. Let’s see if we can apply it to deduction. A classic example of deductive reasoning is:</p>

<p class="noindent chinese">关键是要认识到，归纳法只是演绎法的逆向，就像减法是加法的逆向，或者积分是微分的逆向一样。这个想法最早是由威廉·斯坦利·杰文斯在 19 世纪末提出的。史蒂夫·马格尔顿和韦恩·邦廷，一个英国澳大利亚团队，在 1988 年设计了第一个基于它的实用算法。从一个众所周知的运算中找出其逆运算的策略在数学中有着悠久的历史。将其应用于加法运算导致了整数的发明，因为如果没有负数，加法运算并不总是有一个逆运算（3 - 4 = -1）。同样，将其应用于乘法导致了有理数的出现，将其应用于平方导致了复数的出现。让我们看看是否可以把它应用于演绎法。一个演绎推理的经典例子是。</p>

<div>

<p class="noindent english"><i>Socrates is human.</i></p>

<p class="noindent chinese"><i>苏格拉底是人。</i></p>

<p class="noindent english"><i>All humans are mortal.</i></p>

<p class="noindent chinese"><i>人类都是凡人。</i></p>

<p class="noindent english"><i>Therefore…?…</i></p>

<p class="noindent chinese"><i>因此… ？… </i></p>

</div>

<p class="noindent english"><a id="babilu_link-236"></a> The first statement is a fact about Socrates, and the second is a general rule about humans. What follows? That Socrates is mortal, of course, by applying the rule to Socrates. In inductive reasoning we start instead with the initial and derived facts, and look for a rule that would allow us to infer the latter from the former:</p>

<p class="noindent chinese">第一个陈述是关于苏格拉底的事实，第二个陈述是关于人类的一般规则。接下来是什么？当然是通过对苏格拉底适用规则，得出苏格拉底是凡人的结论。在归纳推理中，我们从最初的事实和派生的事实开始，寻找一个可以让我们从前者推断出后者的规则。</p>

<div>

<p class="noindent english"><i>Socrates is human.</i></p>

<p class="noindent chinese"><i>苏格拉底是人。</i></p>

<p class="noindent english"><i>…?…</i></p>

<p class="noindent chinese"><i>…?…</i></p>

<p class="noindent english"><i>Therefore Socrates is mortal.</i></p>

<p class="noindent chinese"><i>因此，苏格拉底是凡人。</i></p>

</div>

<p class="noindent english">One such rule is: <i>If Socrates is human, then he’s mortal.</i> This does the job, but is not very useful because it’s specific to Socrates. But now we apply Newton’s principle and generalize the rule to all entities: <i>If an entity is human, then it’s mortal.</i> Or, more succinctly: <i>All humans are mortal.</i> Of course, it would be rash to induce this rule from Socrates alone, but we know similar facts about other humans:</p>

<p class="noindent chinese">其中一条规则是：<i>如果苏格拉底是人，那么他就是凡人。</i>这可以完成工作，但不是很有用，因为它是针对苏格拉底的。但是现在我们应用牛顿原理，把这个规则推广到所有实体。<i>如果一个实体是人，那么它就是凡人。</i>或者，更简洁地说。<i>所有人类都是必死的。</i>当然，仅从苏格拉底身上诱导出这一规则是草率的，但我们知道关于其他人类的类似事实。</p>

<div>

<p class="noindent english"><i>Plato is human. Plato is mortal.</i></p>

<p class="noindent chinese"><i>柏拉图是人。柏拉图是凡人。</i></p>

<p class="noindent english"><i>Aristotle is human. Aristotle is mortal.</i></p>

<p class="noindent chinese"><i>亚里士多德是人。亚里士多德是凡人。</i></p>

<p class="noindent english"><i>And so on.</i></p>

<p class="noindent chinese"><i>以此类推。</i></p>

</div>

<p class="noindent english">For each pair of facts, we construct the rule that allows us to infer the second fact from the first one and generalize it by Newton’s principle. When the same general rule is induced over and over again, we can have some confidence that it’s true.</p>

<p class="noindent chinese">对于每一对事实，我们构建规则，使我们能够从第一个事实推断出第二个事实，并通过牛顿原理对其进行概括。当相同的一般规则被反复诱导出来时，我们就可以有一些信心，认为它是真的。</p>

<p class="noindent english">So far we haven’t done anything that the “divide and conquer” algorithm couldn’t do. Suppose, however, that instead of knowing that Socrates, Plato, and Aristotle are human, we just know that they’re philosophers. We still want to conclude that they’re mortal, and we have previously induced or been told that all humans are mortal. What’s missing now? A different rule: <i>All philosophers are human.</i> This also a valid generalization (at least until we solve AI and robots start philosophizing), and it “fills the hole” in our reasoning:</p>

<p class="noindent chinese">到目前为止，我们还没有做任何 “分而治之” 的算法做不到的事情。然而，假设我们不知道苏格拉底、柏拉图和亚里士多德是人类，而只知道他们是哲学家。我们仍然想得出他们是凡人的结论，而我们之前已经诱导或被告知所有人类都是凡人。现在缺少什么呢？一个不同的规则。<i>所有的哲学家都是人类。</i>这也是一个有效的概括（至少在我们解决人工智能和机器人开始进行哲学研究之前），它 “填补” 了我们推理中的漏洞。</p>

<div>

<p class="noindent english"><i><a id="babilu_link-283"><i></i></a> Socrates is a philosopher.</i></p>

<p class="noindent chinese"><i><a id="babilu_link-283"><i></i></a>苏格拉底是一位哲学家。</i></p>

<p class="noindent english"><i>All philosophers are human.</i></p>

<p class="noindent chinese"><i>所有的哲学家都是人。</i></p>

<p class="noindent english"><i>All humans are mortal.</i></p>

<p class="noindent chinese"><i>人类都是凡人。</i></p>

<p class="noindent english"><i>Therefore Socrates is mortal.</i></p>

<p class="noindent chinese"><i>因此，苏格拉底是凡人。</i></p>

</div>

<p class="noindent english">We can also induce rules purely from other rules. If we know that all philosophers are human and mortal, we can induce that all humans are mortal. (We don’t induce that all mortals are human because we know other mortal creatures, like cats and dogs. On the other hand, scientists, artists, and so on are also human and mortal, reinforcing the rule.) In general, the more rules and facts we start out with, the more opportunities we have to induce new rules using “inverse deduction.” And the more rules we induce, the more rules we can induce. It’s a virtuous circle of knowledge creation, limited only by overfitting risk and computational cost. But here, too, having initial knowledge helps: if instead of one large hole we have many small ones to fill, our induction steps will be less risky and therefore less likely to overfit. (For example, given the same number of examples, inducing that all philosophers are human is less risky than inducing that all humans are mortal.)</p>

<p class="noindent chinese">我们也可以纯粹地从其他规则中诱导出规则。如果我们知道所有的哲学家都是人和凡人，我们就可以诱导出所有的人都是凡人。（我们不会诱导出所有的凡人都是人，因为我们知道其他的凡人生物，像猫和狗。另一方面，科学家、艺术家等等也是人，也是凡人，这就加强了这个规则）。一般来说，我们开始时的规则和事实越多，我们就有越多的机会使用 “反推法” 诱导出新的规则。而我们诱导的规则越多，我们可以诱导的规则也就越多。这是一个知识创造的良性循环，只受限于过拟合风险和计算成本。但在这里，拥有初始知识也是有帮助的：如果我们没有一个大洞，而是有许多小洞要填补，我们的归纳步骤就会减少风险，从而减少过度拟合的可能性。（例如，在相同数量的例子下，归纳出所有哲学家都是人类的风险比归纳出所有人类都是凡人的风险要小）。</p>

<p class="noindent english">Inverting an operation is often difficult because the inverse is not unique. For example, a positive number has two square roots, one positive and one negative (2<sup>2</sup> = (–2)<sup>2</sup> = 4). Most famously, integrating the derivative of a function only recovers the function up to a constant. The derivative of a function tells us how much that function goes up or down at each point. Adding up all those changes gives us the function back, except we don’t know where it started; we can “slide” the integrated function up or down without changing the derivative. To make life easy, we can “clamp down” the function by assuming the additive constant is zero. Inverse deduction has a similar problem, and Newton’s principle is one solution. For example, from <i>All Greek philosophers are human</i> and <i>All Greek philosophers are mortal</i> we can induce that <i>All humans are mortal</i> , or just that <i>All Greeks are mortal</i> . But why settle for the more modest generalization? Instead, we can assume that all humans <a id="babilu_link-163"></a> are mortal until we meet an exception. (Which, according to Ray Kurzweil, will be soon.)</p>

<p class="noindent chinese">反转一个操作往往是困难的，因为反转不是唯一的。例如，一个正数有两个平方根，一个是正数，一个是负数（2<sup>2</sup> = (-2)<sup>2</sup> = 4）。最有名的是，对一个函数的导数进行积分，只能恢复到一个常数的函数。一个函数的导数告诉我们这个函数在每个点上上升或下降了多少。把所有这些变化加起来，我们就能找回这个函数，只是我们不知道它从哪里开始；我们可以在不改变导数的情况下把积分函数向上或向下 “滑动”。为了使生活变得简单，我们可以通过假设加法常数为零来 “钳制” 该函数。反推法也有类似的问题，而牛顿原理是一种解决方案。例如，从 “<i>所有希腊哲学家都是人类</i>” 和 “<i>所有希腊哲学家都是凡人</i>” 中，我们可以得出 “<i>所有人类都是凡</i>人”，或者只是 “<i>所有希腊人都是凡人</i>”。但为什么要满足于更温和的概括呢？相反，我们可以假设所有人类直到我们遇到一个例外。（根据雷·库兹韦尔的说法，这种情况很快就会出现）。</p>

<p class="noindent english">In the meantime, one important application of inverse deduction is predicting whether new drugs will have harmful side effects. Failure during animal testing and clinical trials is the main reason new drugs take many years and billions of dollars to develop. By generalizing from known toxic molecular structures, we can form rules that quickly weed out many apparently promising compounds, greatly increasing the chances of successful trials on the remaining ones.</p>

<p class="noindent chinese">同时，反推法的一个重要应用是预测新药是否会产生有害的副作用。在动物试验和临床试验中的失败是新药开发需要多年时间和数十亿美元的主要原因。通过对已知的有毒分子结构进行归纳，我们可以形成规则，迅速剔除许多明显有希望的化合物，大大增加对剩余化合物进行试验的成功几率。</p>

<h1 id="babilu_link-405"><b>Learning to cure cancer</b></h1>

<h1 id="babilu_link-405"><b>学习治愈癌症</b></h1>

<p class="noindent english">More generally, inverse deduction is a great way to discover new knowledge in biology, and doing that is the first step in curing cancer. According to the Central Dogma, everything that happens in a living cell is ultimately controlled by its genes, via the proteins whose synthesis they initiate. In effect, a cell is like a tiny computer, and DNA is the program running on it: change the DNA, and a skin cell can become a neuron or a mouse cell can turn into a human one. In a computer program, all bugs are the programmer’s fault. But in a cell, bugs can arise spontaneously, when radiation or a copying error changes a gene into a different one, a gene is accidentally copied twice, and so on. Most of the time these mutations cause the cell to die silently, but sometimes the cell starts to grow and divide uncontrollably and a cancer is born.</p>

<p class="noindent chinese">更广泛地说，反推法是发现生物学新知识的一个好方法，而这样做是治愈癌症的第一步。根据中央教条，在一个活细胞中发生的一切最终都由其基因控制，通过其启动的蛋白质的合成。实际上，一个细胞就像一台小电脑，而 DNA 就是在它上面运行的程序：改变 DNA，一个皮肤细胞就能变成神经元，或者一个小鼠细胞就能变成人类。在计算机程序中，所有的错误都是程序员的错。但在细胞中，错误可能自发产生，当辐射或复制错误将一个基因改变成不同的基因，一个基因被意外地复制了两次，等等。大多数时候，这些突变导致细胞无声无息地死亡，但有时细胞开始不受控制地生长和分裂，癌症就诞生了。</p>

<p class="noindent english">Curing cancer means stopping the bad cells from reproducing without harming the good ones. That requires knowing how they differ, and in particular how their genomes differ, since all else follows from that. Luckily, gene sequencing is becoming routine and affordable. Using it, we can learn to predict which drugs will work against which cancer genes. This contrasts with traditional chemotherapy, which affects all cells indiscriminately. Learning which drugs work against which mutations requires a database of patients, their cancers’ genomes, the drugs tried, and the outcomes. The simplest rules encode one-to-one <a id="babilu_link-31"></a> correspondences between genes and drugs, such as <i>If the BCR-ABL gene is present, then use Gleevec</i> . (BCR-ABL causes a type of leukemia, and Gleevec cures it in nine out of ten patients.) Once sequencing cancer genomes and collating treatment outcomes becomes standard practice, many more rules like this will be discovered.</p>

<p class="noindent chinese">治愈癌症意味着在不伤害好细胞的情况下阻止坏细胞的繁殖。这需要了解它们的不同之处，特别是它们的基因组有何不同，因为其他一切都由此而来。幸运的是，基因测序正在成为常规的和可负担得起的。利用它，我们可以学会预测哪些药物会对哪些癌症基因起作用。这与传统的化疗形成鲜明对比，后者不加区分地影响所有细胞。学习哪些药物对哪些突变起作用，需要建立一个患者、他们的癌症基因组、尝试的药物和结果的数据库。最简单的规则对基因和药物之间的一对一对应进行编码，例如<i>如果存在 BCR-ABL 基因，则使用格列卫</i>。（BCR-ABL 会导致一种白血病，而格列卫能治愈 10 个病人中的 9 个）。一旦癌症基因组测序和整理治疗结果成为标准做法，将会发现更多像这样的规则。</p>

<p class="noindent english">That’s only the beginning, however. Most cancers involve a combination of mutations, or can only be cured by drugs that haven’t been discovered yet. The next step is to learn rules with more complex conditions, involving the cancer’s genome, the patient’s genome and medical history, known side effects of drugs, and so on. But ultimately what we need is a model of how the entire cell works, enabling us to simulate on the computer the effect of a specific patient’s mutations, as well as the effect of different combinations of drugs, existing or speculative. Our main sources of information for building such models are DNA sequencers, gene expression microarrays, and the biological literature. Combining these is where inverse deduction can shine.</p>

<p class="noindent chinese">然而，这仅仅是个开始。大多数癌症涉及突变的组合，或者只能由尚未发现的药物治愈。下一步是学习具有更复杂条件的规则，涉及癌症的基因组，病人的基因组和病史，药物的已知副作用，等等。但最终我们需要的是整个细胞如何工作的模型，使我们能够在计算机上模拟特定病人的突变的效果，以及现有或推测的不同药物组合的效果。我们建立这种模型的主要信息来源是 DNA 测序仪、基因表达微阵列和生物文献。将这些结合起来是逆向推理能够大放异彩的地方。</p>

<p class="noindent english">Adam, the robot scientist we met in <a href="#babilu_link-14">Chapter 1</a> , gives a preview. Adam’s goal is to figure out how yeast cells work. It starts with basic knowledge of yeast genetics and metabolism and a trove of gene expression data from yeast cells. It then uses inverse deduction to hypothesize which genes are expressed as which proteins, designs microarray experiments to test them, revises its hypotheses, and repeats. Whether each gene is expressed depends on other genes and conditions in the environment, and the resulting web of interactions can be represented as a set of rules, such as:</p>

<p class="noindent chinese">亚当，我们在<a href="#babilu_link-14">第一章</a>中遇到的机器人科学家，给出了一个预告。亚当的目标是弄清酵母细胞的工作原理。它从酵母遗传学和新陈代谢的基本知识以及酵母细胞的基因表达数据库开始。然后，它使用反推法来假设哪些基因表达为哪些蛋白质，设计微阵列实验来测试它们，修改其假设，并重复进行。每个基因是否表达取决于环境中的其他基因和条件，由此产生的相互作用网络可以表示为一组规则，如：。</p>

<div>

<p class="noindent english"><i>If the temperature is high, gene A is expressed.</i></p>

<p class="noindent chinese"><i>如果温度高，基因 A 就会表达。</i></p>

<p class="noindent english"><i>If gene A is expressed and gene B is not, gene C is expressed.</i></p>

<p class="noindent chinese"><i>如果基因 A 表达而基因 B 不表达，那么基因 C 就会表达。</i></p>

<p class="noindent english"><i>If gene C is expressed, gene D is not.</i></p>

<p class="noindent chinese"><i>如果基因 C 被表达，基因 D 就不会被表达。</i></p>

</div>

<p class="noindent english">If we knew the first and third rules but not the second, and we had microarray data where at a high temperature B and D were not expressed, we could induce the second rule by inverse deduction. Once we have that rule, and perhaps have verified it using a microarray <a id="babilu_link-228"></a> experiment, we can use it as the basis for further inductive inferences. In a similar manner, we can piece together the sequences of chemical reactions by which proteins do their work.</p>

<p class="noindent chinese">如果我们知道第一和第三条规则，但不知道第二条规则，而我们有微阵列数据，在高温下 B 和 D 不表达，我们可以通过反推法诱导出第二条规则。一旦我们有了这个规则，也许还用微阵列实验验证了它，我们就可以用它作为进一步归纳推理的基础。以类似的方式，我们可以拼凑出蛋白质进行工作的化学反应的序列。</p>

<p class="noindent english">Just knowing which genes regulate which genes and how proteins organize the cell’s web of chemical reactions is not enough, though. We also need to know how much of each molecular species is produced. DNA microarrays and other experiments can provide this type of quantitative information, but inverse deduction, with its “all or none” logical character, is not very good at dealing with it. For that we need the connectionist methods that we’ll meet in the next chapter.</p>

<p class="noindent chinese">不过，仅仅知道哪些基因调控哪些基因以及蛋白质如何组织细胞的化学反应网络是不够的。我们还需要知道每个分子物种产生的数量是多少。DNA 微阵列和其他实验可以提供这种类型的定量信息，但具有 “全部或没有” 逻辑特征的反推法并不擅长处理这种信息。为此，我们需要连接主义方法，我们将在下一章中见到。</p>

<h1 id="babilu_link-406"><b>A game of twenty questions</b></h1>

<h1 id="babilu_link-406"><b>20 个问题的游戏</b></h1>

<p class="noindent english">Another limitation of inverse deduction is that it’s very computationally intensive, which makes it hard to scale to massive data sets. For these, the symbolist algorithm of choice is decision tree induction. Decision trees can be viewed as an answer to the question of what to do if rules of more than one concept match an instance. How do we then decide which concept the instance belongs to? If we see a partly occluded object with a flat surface and four legs, how do we decide whether it is a table or a chair? One option is to order the rules, for example by decreasing accuracy, and choose the first one that matches. Another is to let the rules vote. Decision trees instead ensure a priori that each instance will be matched by exactly one rule. This will be the case if each pair of rules differs in at least one attribute test, and such a rule set can be organized into a decision tree. For example, consider these rules:</p>

<p class="noindent chinese">反推法的另一个限制是它的计算量非常大，这使得它很难扩展到大规模的数据集。对于这些，符号学算法的选择是决策树归纳法。决策树可以被看作是对 “如果有一个以上的概念规则与一个实例相匹配该怎么办” 这一问题的回答。那么，我们如何决定该实例属于哪个概念？如果我们看到一个部分遮挡的物体，有一个平面和四条腿，我们如何决定它是一张桌子还是一把椅子？一种选择是对规则进行排序，例如按照准确度的递减，然后选择第一个匹配的规则。另一种是让规则投票。决策树则是先验地确保每个实例都正好被一条规则所匹配。如果每对规则至少在一个属性测试中不同，就会出现这种情况，这样的规则集可以被组织成一棵决策树。例如，考虑这些规则。</p>

<div>

<p class="noindent english"><i>If you’re for cutting taxes and pro-life, you’re a Republican.</i></p>

<p class="noindent chinese"><i>如果你支持减税和支持生命，你就是一个共和党人。</i></p>

<p class="noindent english"><i>If you’re against cutting taxes, you’re a Democrat.</i></p>

<p class="noindent chinese"><i>如果你反对减税，你就是一个民主党人。</i></p>

<p class="noindent english"><i>If you’re for cutting taxes, pro-choice, and against gun control, you’re an independent.</i></p>

<p class="noindent chinese"><i>如果你支持减税，支持选择，反对枪支管制，你就是一个独立的人。</i></p>

<p class="noindent english"><i>If you’re for cutting taxes, pro-choice, and pro-gun control, you’re a Democrat.</i></p>

<p class="noindent chinese"><i>如果你支持减税、支持选择、支持枪支管制，你就是民主党人。</i></p>

</div>

<p class="noindent english"><a id="babilu_link-39"></a> These can be organized into the following decision tree:</p>

<p class="noindent chinese">这些可以组织成以下决策树。</p>

<div>

<div>

<img alt="image" src="images/000004.jpg"/>

</div>

</div>

<p class="noindent english">A decision tree is like playing a game of twenty questions with an instance. Starting at the root, each node asks about the value of one attribute, and depending on the answer, we follow one or another branch. When we arrive at a leaf, we read off the predicted concept. Each path from the root to a leaf corresponds to a rule. If this reminds you of those annoying phone menus you have to get through when you call customer service, it’s not an accident: a phone menu is a decision tree. The computer on the other end of the line is playing a game of twenty questions with you to figure out what you want, and each menu is a question.</p>

<p class="noindent chinese">决策树就像和一个实例玩 20 个问题的游戏。从根部开始，每个节点询问一个属性的值，根据答案，我们沿着一个或另一个分支走。当我们到达一个叶子时，我们读出预测的概念。从根到叶的每一条路径都对应着一条规则。如果这让你想起你打电话给客户服务时必须通过的那些恼人的电话菜单，这并不是一个意外：电话菜单是一个决策树。电话另一端的电脑正在和你玩一个 20 个问题的游戏，以弄清你想要什么，而每个菜单都是一个问题。</p>

<p class="noindent english">According to the decision tree above, you’re either a Republican, a Democrat, or an independent; you can’t be more than one, or none of the above. Sets of concepts with this property are called sets of classes, and the algorithm that predicts them is a classifier. A single concept implicitly defines two classes: the concept itself and its negation. (For <a id="babilu_link-24"></a> example, spam and nonspam.) Classifiers are the most widespread form of machine learning.</p>

<p class="noindent chinese">根据上面的决策树，你要么是共和党人，要么是民主党人，要么是独立人士；你不可能是一个以上的人，或者以上都不是。具有这种属性的概念集被称为类集，预测它们的算法是一个分类器。一个单一的概念隐含地定义了两个类：概念本身和它的否定。（对于垃圾邮件和非垃圾邮件。）分类器是机器学习中最广泛的形式。</p>

<p class="noindent english">We can learn decision trees using a variant of the “divide and conquer” algorithm. First we pick an attribute to test at the root. Then we focus on the examples that went down each branch and pick the next test for those. (For example, we check whether tax-cutters are pro-life or pro-choice.) We repeat this for each new node we induce until all the examples in a branch have the same class, at which point we label that branch with the class.</p>

<p class="noindent chinese">我们可以使用 “分而治之” 算法的一个变种来学习决策树。首先，我们在根部挑选一个要测试的属性。然后，我们把注意力放在每个分支下的例子上，并为这些例子挑选下一个测试。（例如，我们检查减税者是支持生命还是支持选择。）我们对每一个我们诱导的新节点重复这一步骤，直到一个分支中的所有例子都有相同的类别，在这一点上，我们给该分支贴上该类别的标签。</p>

<p class="noindent english">One salient question is how to pick the best attribute to test at a node. Accuracy—the number of correctly predicted examples—doesn’t work very well, because we’re not trying to predict a particular class; rather, we’re trying to gradually separate the classes until each branch is “pure.” This brings to mind the concept of entropy from information theory. The entropy of a set of objects is a measure of the amount of disorder in it. If a group of 150 people includes 50 Republicans, 50 Democrats, and 50 independents, its political entropy is maximum. On the other hand, if they’re all Republican then the entropy is zero (as far as party affiliation goes). So to learn a good decision tree, we pick at each node the attribute that on average yields the lowest class entropy across all its branches, weighted by how many examples go into each branch.</p>

<p class="noindent chinese">一个突出的问题是如何在一个节点上挑选最佳属性进行测试。准确率 —— 正确预测的例子的数量 —— 不是很好，因为我们不是在试图预测一个特定的类别；相反，我们是在试图逐步分离类别，直到每个分支都是 “纯粹的”。这让人想起信息论中的熵的概念。一组物体的熵是对其中无序程度的一种衡量。如果一个 150 人的团体包括 50 个共和党人、50 个民主党人和 50 个独立人士，其政治熵是最大的。另一方面，如果他们都是共和党人，那么熵值为零（就党派而言）。因此，为了学习一棵好的决策树，我们在每个节点上挑选一个属性，这个属性在其所有分支上平均产生最低的类熵，并以每个分支上有多少例子来加权。</p>

<p class="noindent english">As with rule learning, we don’t want to induce a tree that perfectly predicts the classes of all the training examples, because it would probably overfit. As before, we can use significance tests or a penalty on the size of the tree to prevent this.</p>

<p class="noindent chinese">与规则学习一样，我们不希望诱导出一棵能完美预测所有训练实例类别的树，因为它可能会过度拟合。和以前一样，我们可以使用显著性检验或对树的大小进行惩罚来防止这种情况。</p>

<p class="noindent english">Having a branch for each value of an attribute is fine if the attribute is discrete, but what about numeric attributes? If we had a branch for every value of a continuous variable, the tree would be infinitely wide. A simple solution is to pick a few key thresholds by entropy and use those. For example, is the patient’s temperature above or below 100 degrees Fahrenheit? That, combined with other symptoms, may be all the doctor needs to know about the patient’s temperature to decide if he has an infection.</p>

<p class="noindent chinese">如果属性是离散的，为属性的每个值设置一个分支是可以的，但是数字属性呢？如果我们为一个连续变量的每个值都有一个分支，那么树就会无限宽。一个简单的解决方案是通过熵值挑选几个关键的阈值并使用这些阈值。例如，病人的体温是高于还是低于华氏 100 度？结合其他症状，这可能是医生需要知道的关于病人体温的所有信息，以决定他是否有感染。</p>

<p class="noindent english"><a id="babilu_link-277"></a> Decision trees are used in many different fields. In machine learning, they grew out of work in psychology. Earl Hunt and colleagues used them in the 1960s to model how humans acquire new concepts, and one of Hunt’s graduate students, J. Ross Quinlan, later tried using them for chess. His original goal was to predict the outcome of king-rook versus king-knight endgames from the board positions. From those humble beginnings, decision trees have grown to be, according to surveys, the most widely used machine-learning algorithm. It’s not hard to see why: they’re easy to understand, fast to learn, and usually quite accurate without too much tweaking. Quinlan is the most prominent researcher in the symbolist school. An unflappable, down-to-earth Australian, he made decision trees the gold standard in classification by dint of relentlessly improving them year after year, and writing beautifully clear papers about them.</p>

<p class="noindent chinese">决策树被用于许多不同的领域。在机器学习中，它们是从心理学工作中发展出来的。厄尔·亨特及其同事在 20 世纪 60 年代使用它们来模拟人类如何获得新的概念，亨特的一个研究生，罗斯·昆兰，后来尝试将它们用于国际象棋。他最初的目标是根据棋盘位置预测王·车对王·车残局的结果。根据调查，决策树从最初的不起眼发展到现在成为使用最广泛的机器学习算法。这并不难理解：它们易于理解，学习速度快，而且通常无需太多调整就相当准确。昆兰是象征主义学派中最杰出的研究者。他是一个不慌不忙、脚踏实地的澳大利亚人，通过年复一年地不懈改进决策树，并写出精美清晰的论文，使决策树成为分类的黄金标准。</p>

<p class="noindent english">Whatever you want to predict, there’s a good chance someone has used a decision tree for it. Microsoft’s Kinect uses decision trees to figure out where various parts of your body are from the output of its depth camera; it can then use their motions to control the Xbox game console. In a 2002 head-to-head competition, decision trees correctly predicted three out of every four Supreme Court rulings, while a panel of experts got less than 60 percent correct. Thousands of decision tree users can’t be wrong, you think, and sketch one to predict your friend’s reply when you ask her out:</p>

<p class="noindent chinese">无论你想预测什么，很可能有人已经用决策树来预测了。微软的 Kinect 使用决策树从其深度摄像头的输出中找出你身体的各个部分；然后它可以使用它们的运动来控制 Xbox 游戏机。在 2002 年的一次正面竞争中，决策树正确预测了最高法院每四个裁决中的三个，而一个专家小组的正确率不到 60%。数以千计的决策树用户不可能出错，你想，勾勒出一个预测你的朋友在你约她出去时的回答。</p>

<div>

<div>

<img alt="image" src="images/000032.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-80"></a> According to this tree, tonight she’ll say yes. With a deep breath, you pick up the phone and dial her number.</p>

<p class="noindent chinese">根据这棵树，今晚她会说是。深吸一口气，你拿起电话，拨打她的号码。</p>

<h1 id="babilu_link-407"><b>The symbolists</b></h1>

<h1 id="babilu_link-407"><b>象征主义者</b></h1>

<p class="noindent english">The symbolists’ core belief is that all intelligence can be reduced to manipulating symbols. A mathematician solves equations by moving symbols around and replacing symbols by other symbols according to predefined rules. The same is true of a logician carrying out deductions. According to this hypothesis, intelligence is independent of the substrate; it doesn’t matter if the symbol manipulations are done by writing on a blackboard, switching transistors on and off, firing neurons, or playing with Tinkertoys. If you have a setup with the power of a universal Turing machine, you can do anything. Software can be cleanly separated from hardware, and if your concern is figuring out how machines can learn, you (thankfully) don’t need to worry about the latter beyond buying a PC or cycles on Amazon’s cloud.</p>

<p class="noindent chinese">符号主义者的核心信念是，所有智力都可以归结为对符号的操作。一个数学家通过移动符号和根据预定的规则用其他符号替换符号来解决方程问题。一个逻辑学家进行推理也是如此。根据这一假设，智力与基质无关；无论符号操作是通过在黑板上写字、开关晶体管、发射神经元还是玩 Tinkertoys 来完成，都没有关系。如果你有一个具有通用图灵机能力的设置，你可以做任何事情。软件可以与硬件干净地分开，如果你关心的是弄清楚机器如何学习，你（值得庆幸的是）除了购买一台 PC 或在亚马逊的云上循环之外，不需要担心后者。</p>

<p class="noindent english">Symbolist machine learners share this belief in the power of symbol manipulation with many other computer scientists, psychologists, and philosophers. The psychologist David Marr argued that every information processing system should be studied at three distinct levels: the fundamental properties of the problem it’s solving; the algorithms and representations used to solve it; and how they are physically implemented. For example, addition can be defined by a set of axioms irrespective of how it’s carried out; numbers can be expressed in different ways (e.g., Roman and Arabic) and added using different algorithms; and these can be implemented using an abacus, a pocket calculator, or even, very inefficiently, in your head. Learning is a prime example of a cognitive faculty we can profitably study according to Marr’s levels.</p>

<p class="noindent chinese">符号主义机器学习者与其他许多计算机科学家、心理学家和哲学家一样，对符号操作的力量抱有这种信念。心理学家大卫·马尔认为，每一个信息处理系统都应该在三个不同的层面上进行研究：它所解决的问题的基本属性；用于解决该问题的算法和表征；以及它们如何在物理上实现。例如，加法可以由一组公理来定义，而不管它是如何进行的；数字可以用不同的方式来表达（如罗马和阿拉伯），并使用不同的算法进行加法；这些可以用算盘、袖珍计算器，甚至是非常低效的，在你的头脑中实现。学习是我们根据马尔的水平可以有利地研究认知能力的一个典型例子。</p>

<p class="noindent english">Symbolist machine learning is an offshoot of the knowledge engineering school of AI. In the 1970s, so-called knowledge-based systems scored some impressive successes, and in the 1980s they spread rapidly, but then they died out. The main reason they did was the infamous knowledge acquisition bottleneck: extracting knowledge from experts <a id="babilu_link-282"></a> and encoding it as rules is just too difficult, labor-intensive, and failure-prone to be viable for most problems. Letting the computer automatically learn to, say, diagnose diseases by looking at databases of past patients’ symptoms and the corresponding outcomes turned out to be much easier than endlessly interviewing doctors. Suddenly, the work of pioneers like Ryszard Michalski, Tom Mitchell, and Ross Quinlan had a new relevance, and the field hasn’t stopped growing since. (Another important problem was that knowledge-based systems had trouble dealing with uncertainty, of which more in <a href="#babilu_link-6">Chapter 6</a> .)</p>

<p class="noindent chinese">符号主义机器学习是人工智能的知识工程学派的一个分支。在 20 世纪 70 年代，所谓的基于知识的系统取得了一些令人印象深刻的成功，在 20 世纪 80 年代，它们迅速传播，但随后它们就消亡了。他们这样做的主要原因是臭名昭著的知识获取瓶颈：从专家那里提取知识并将其编码为规则，对于大多数问题来说太难了，劳动密集型，而且容易失败。让计算机自动学习，例如，通过查看过去病人的症状和相应的结果的数据库来诊断疾病，这比无休止地采访医生要容易得多。突然间，像里夏德·米哈尔斯基、汤姆·米切尔和罗斯·昆兰这样的先驱者的工作有了新的意义，此后该领域一直没有停止发展。（另一个重要的问题是，基于知识的系统难以处理不确定性，这一点在<a href="#babilu_link-6">第 6 章有</a>更多介绍）。</p>

<p class="noindent english">Because of its origins and guiding principles, symbolist machine learning is still closer to the rest of AI than the other schools. If computer science were a continent, symbolist learning would share a long border with knowledge engineering. Knowledge is traded in both directions—manually entered knowledge for use in learners, induced knowledge for addition to knowledge bases—but at the end of the day the rationalist-empiricist fault line runs right down that border, and crossing it is not easy.</p>

<p class="noindent chinese">由于其起源和指导原则，符号主义机器学习仍然比其他流派更接近人工智能的其他部分。如果计算机科学是一块大陆，那么符号主义学习将与知识工程有着漫长的边界。知识是双向交易的 —— 手动输入的知识用于学习者，诱导的知识用于添加到知识库中 —— 但在一天结束时，理性主义-empiricist 的断层线就在这条边界上，而且跨越它并不容易。</p>

<p class="noindent english">Symbolism is the shortest path to the Master Algorithm. It doesn’t require us to figure out how evolution or the brain works, and it avoids the mathematical complexities of Bayesianism. Sets of rules and decision trees are easy to understand, so we know what the learner is up to. This makes it easier to figure out what it’s doing right and wrong, fix the latter, and have confidence in the results.</p>

<p class="noindent chinese">符号学是通往主算法的最短路径。它不需要我们弄清楚进化或大脑是如何运作的，而且它避免了贝叶斯主义的数学复杂性。规则集和决策树很容易理解，所以我们知道学习者在做什么。这使得我们更容易弄清它做得对和错，修正后者，并对结果有信心。</p>

<p class="noindent english">Despite the popularity of decision trees, inverse deduction is the better starting point for the Master Algorithm. It has the crucial property that incorporating knowledge into it is easy—and we know Hume’s problem makes that essential. Also, sets of rules are an exponentially more compact way to represent most concepts than decision trees. Converting a decision tree to a set of rules is easy: each path from the root to a leaf becomes a rule, and there’s no blowup. On the other hand, in the worst case converting a set of rules into a decision tree requires converting each rule into a mini-decision tree, and then replacing each leaf of rule 1’s tree with a copy of rule 2’s tree, each leaf of each copy of rule 2 with a copy of rule 3, and so on, causing a massive blowup.</p>

<p class="noindent chinese">尽管决策树很受欢迎，但反推法是主算法的更好的起点。它有一个重要的特性，即把知识纳入其中是很容易的 —— 我们知道休谟的问题使得这一点至关重要。此外，规则集是一种比决策树更紧凑的方式来表示大多数概念。将决策树转换为规则集很容易：从根到叶的每条路径都会成为一条规则，而且不会发生爆炸。另一方面，在最坏的情况下，将一组规则转换为决策树需要将每个规则转换为一个小型决策树，然后用规则 2 的副本替换规则 1 的树的每个叶子，用规则 3 的副本替换规则 2 的每个叶子，以此类推，造成巨大的爆炸。</p>

<p class="noindent english"><a id="babilu_link-206"></a> Inverse deduction is like having a superscientist systematically looking at the evidence, considering possible inductions, collating the strongest, and using those along with other evidence to construct yet further hypotheses—all at the speed of computers. It’s clean and beautiful, at least for the symbolist taste. On the other hand, it has some serious shortcomings. The number of possible inductions is vast, and unless we stay close to our initial knowledge, it’s easy to get lost in space. Inverse deduction is easily confused by noise: how do we figure out what the missing deductive steps are, if the premises or conclusions are themselves wrong? Most seriously, real concepts can seldom be concisely defined by a set of rules. They’re not black and white: there’s a large gray area between, say, spam and nonspam. They require weighing and accumulating weak evidence until a clear picture emerges. Diagnosing an illness involves giving more weight to some symptoms than others, and being OK with incomplete evidence. No one has ever succeeded in learning a set of rules that will recognize a cat by looking at the pixels in an image, and probably no one ever will.</p>

<p class="noindent chinese">逆向演绎就像让一个超级科学家系统地观察证据，考虑可能的归纳，整理最强的，并利用这些证据和其他证据来构建进一步的假设 —— 所有这些都以计算机的速度进行。它是干净而美丽的，至少对象征主义的品味来说是如此。另一方面，它也有一些严重的缺点。可能的归纳法数量庞大，除非我们紧跟我们的初始知识，否则很容易迷失在空间中。逆向演绎很容易被噪音所迷惑：如果前提或结论本身是错误的，我们如何弄清楚缺少的演绎步骤是什么？最严重的是，真正的概念很少能被一套规则简洁地定义。它们不是黑白分明的：比如说，在垃圾邮件和非垃圾邮件之间有一个很大的灰色区域。它们需要权衡和积累薄弱的证据，直到出现一个清晰的画面。诊断一种疾病需要对一些症状给予比其他症状更多的重视，并接受不完整的证据。从来没有人成功地学习了一套规则，通过观察图像中的像素来识别一只猫，可能也没有人会这样做。</p>

<p class="noindent english">Connectionists, in particular, are highly critical of symbolist learning. According to them, concepts you can define with logical rules are only the tip of the iceberg; there’s a lot going on under the surface that formal reasoning just can’t see, in the same way that most of what goes on in our minds is subconscious. You can’t just build a disembodied automated scientist and hope he’ll do something meaningful—you have to first endow him with something like a real brain, connected to real senses, growing up in the world, perhaps even stubbing his toe every now and then. And how do you build such a brain? By reverse engineering the competition. If you want to reverse engineer a car, you look under the hood. If you want to reverse engineer the brain, you look inside the skull.</p>

<p class="noindent chinese">尤其是连接主义者，对符号主义学习持高度批评态度。根据他们的观点，你可以用逻辑规则定义的概念只是冰山一角；在表面之下有很多事情是正式推理无法看到的，就像我们头脑中的大部分事情是潜意识的一样。你不能只是建立一个没有实体的自动科学家，并希望他能做一些有意义的事情 —— 你必须首先赋予他像一个真实的大脑一样的东西，与真实的感官相连，在这个世界上成长，也许甚至偶尔会绊倒他的脚趾。你如何建立这样一个大脑呢？通过逆向工程的竞争。如果你想对一辆汽车进行逆向工程，你要看引擎盖下面。如果你想对大脑进行逆向工程，你就看头骨内部。</p>

</section>

</div>

</div>

<div id="babilu_link-355">

<div>

<section id="babilu_link-338">

<h1><a id="babilu_link-161"></a> <a href="#babilu_link-356">CHAPTER FOUR</a></h1>

<h1><a id="babilu_link-161"></a> <a href="#babilu_link-356">第四章</a></h1>

<h1><a href="#babilu_link-356">How Does Your Brain Learn?</a></h1>

<h1><a href="#babilu_link-356">你的大脑是如何学习的？</a></h1>

<p class="noindent english">Hebb’s rule, as it has come to be known, is the cornerstone of connectionism. Indeed, the field derives its name from the belief that knowledge is stored in the connections between neurons. Donald Hebb, a Canadian psychologist, stated it this way in his 1949 book <i>The Organization of Behavior</i> : “When an axon of cell <i>A</i> is near enough cell <i>B</i> and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that <i>A</i> ’s efficiency, as one of the cells firing <i>B</i> , is increased.” It’s often paraphrased as “Neurons that fire together wire together.”</p>

<p class="noindent chinese">赫伯规则，正如它被人所知的那样，是连接主义的基石。事实上，该领域的名称源于这样一种信念，即知识储存在神经元之间的连接中。加拿大心理学家唐纳德·赫伯在他 1949 年出版的《<i>行为的组织</i>》一书中这样说：“当细胞 <i>A</i> 的轴突靠近足够的细胞 <i>B</i> 并反复或持续地参与发射，一些生长过程或代谢变化就会在一个或两个细胞中发生，从而使 <i>A</i> 作为发射 <i>B</i> 的细胞之一，其效率得到提高。这句话经常被解读为 “一起发射的神经元会一起连接”。</p>

<p class="noindent english">Hebb’s rule was a confluence of ideas from psychology and neuroscience, with a healthy dose of speculation thrown in. Learning by association was a favorite theme of the British empiricists, from Locke and Hume to John Stuart Mill. In his <i>Principles of Psychology</i> , William James enunciates a general principle of association that’s remarkably similar to Hebb’s rule, with neurons replaced by brain processes and firing efficiency by propagation of excitement. Around the same time, the great Spanish neuroscientist Santiago Ramón y Cajal was making the first detailed observations of the brain, staining individual neurons using the recently invented Golgi method and cataloguing what he saw like a <a id="babilu_link-151"></a> botanist classifying new species of trees. By Hebb’s time, neuroscientists had a rough understanding of how neurons work, but he was the first to propose a mechanism by which they could encode associations.</p>

<p class="noindent chinese">赫伯的规则是心理学和神经科学思想的汇合，其中还加入了健康的猜测。联想学习是英国经验主义者最喜欢的主题，从洛克和休谟到约翰·斯图尔特·米尔。在他的《<i>心理学原理</i>》中，威廉·詹姆斯阐述了一个联想的一般原则，它与赫伯规则非常相似，神经元被大脑过程所取代，发射效率被兴奋的传播所取代。大约在同一时间，伟大的西班牙神经科学家圣地亚哥·拉蒙·卡亚尔首次对大脑进行了详细的观察，用最近发明的高尔基方法对单个神经元进行染色，并对他所看到的东西进行编目，就像一个植物学家对新树种进行分类。在赫伯的时代，神经科学家对神经元的工作方式有了粗略的了解，但他是第一个提出神经元可以编码关联的机制的人。</p>

<p class="noindent english">In symbolist learning, there is a one-to-one correspondence between symbols and the concepts they represent. In contrast, connectionist representations are distributed: each concept is represented by many neurons, and each neuron participates in representing many different concepts. Neurons that excite one another form what Hebb called a cell assembly. Concepts and memories are represented in the brain by cell assemblies. Each of these can include neurons from different brain regions and overlap with other assemblies. The cell assembly for “leg” includes the one for “foot,” which includes assemblies for the image of a foot and the sound of the word <i>foot</i> . If you ask a symbolist system where the concept “New York” is represented, it can point to the precise location in memory where it’s stored. In a connectionist system, the answer is “it’s stored a little bit everywhere.”</p>

<p class="noindent chinese">在符号主义学习中，符号和它们所代表的概念之间存在着一对一的对应关系。相反，连接主义的表征是分布式的：每个概念由许多神经元代表，每个神经元都参与代表许多不同的概念。互相激发的神经元形成了赫伯所说的细胞组合。概念和记忆在大脑中由细胞集合体代表。每一个单元都可以包括来自不同脑区的神经元，并与其他单元重叠。“腿” 的细胞组合包括 “脚” 的细胞组合，而 “脚” 的细胞组合包括脚的形象和<i>脚</i>这个词的声音。如果你问一个符号主义系统，“纽约” 这个概念在哪里被代表，它可以指出它在记忆中被储存的准确位置。在连接主义系统中，答案是 “它被储存在各个地方”。</p>

<p class="noindent english">Another difference between symbolist and connectionist learning is that the former is sequential, while the latter is parallel. In inverse deduction, we figure out one step at a time what new rules are needed to arrive at the desired conclusion from the premises. In connectionist models, all neurons learn simultaneously according to Hebb’s rule. This mirrors the different properties of computers and brains. Computers do everything one small step at a time, like adding two numbers or flipping a switch, and as a result they need a lot of steps to accomplish anything useful; but those steps can be very fast, because transistors can switch on and off billions of times per second. In contrast, brains can perform a large number of computations in parallel, with billions of neurons working at the same time; but each of those computations is slow, because neurons can fire at best a thousand times per second.</p>

<p class="noindent chinese">符号主义和连接主义学习之间的另一个区别是，前者是顺序的，而后者是平行的。在逆向演绎中，我们一步步找出需要哪些新规则来从前提中得出所需的结论。在连接主义模型中，所有的神经元都根据赫伯规则同时学习。这反映了计算机和大脑的不同属性。计算机一次只做一小步，比如加两个数字或打开一个开关，因此它们需要很多步骤来完成任何有用的事情；但这些步骤可以非常快，因为晶体管每秒可以开关数十亿次。相比之下，大脑可以并行地进行大量的计算，有数十亿个神经元同时工作；但这些计算中的每一个都很慢，因为神经元每秒最多只能发射一千次。</p>

<p class="noindent english">The number of transistors in a computer is catching up with the number of neurons in a human brain, but the brain wins hands down in the number of connections. In a microprocessor, a typical transistor is directly connected to only a few others, and the planar semiconductor technology used severely limits how much better a computer can do. In <a id="babilu_link-28"></a> contrast, a neuron has thousands of synapses. If you’re walking down the street and come across an acquaintance, it takes you only about a tenth of a second to recognize her. At neuron switching speeds, this is barely enough time for a hundred processing steps, but in those hundred steps your brain manages to scan your entire memory, find the best match, and adapt it to the new context (different clothes, different lighting, and so on). In a brain, each processing step can be very complex and involve a lot of information, consonant with a distributed representation.</p>

<p class="noindent chinese">计算机中的晶体管数量正在追赶人类大脑中的神经元数量，但大脑在连接数量上胜出一筹。在一个微处理器中，一个典型的晶体管只与其他几个晶体管直接相连，所使用的平面半导体技术严重限制了计算机的性能提升。相比之下，一个神经元有成千上万的突触。如果你走在大街上，遇到一个熟人，你只需要大约十分之一秒的时间就能认出她。按照神经元的切换速度，这几乎不够一百个处理步骤的时间，但在这一百个步骤中，你的大脑设法扫描你的整个记忆，找到最佳匹配，并使其适应新的环境（不同的衣服，不同的灯光，等等）。在大脑中，每个处理步骤可能非常复杂，涉及大量的信息，与分布式表征相吻合。</p>

<p class="noindent english">This does not mean that we can’t simulate a brain with a computer; after all, that’s what connectionist algorithms do. Because a computer is a universal Turing machine, it can implement the brain’s computations as well as any others, provided we give it enough time and memory. In particular, the computer can use speed to make up for lack of connectivity, using the same wire a thousand times over to simulate a thousand wires. In fact, these days the main limitation of computers compared to brains is energy consumption: your brain uses only about as much power as a small lightbulb, while Watson’s supply could light up a whole office building.</p>

<p class="noindent chinese">这并不意味着我们不能用计算机模拟大脑；毕竟，这就是连接主义算法的作用。因为计算机是一个通用的图灵机，只要我们给它足够的时间和内存，它就可以像其他任何计算一样实现大脑的计算。特别是，计算机可以用速度来弥补连接性的不足，用同一条线重复一千次来模拟一千个线。事实上，这些天来，与大脑相比，计算机的主要限制是能源消耗：你的大脑使用的能量只相当于一个小灯泡，而 Watson 需要的电量可以照亮整座办公楼。</p>

<p class="noindent english">To simulate a brain, we need more than Hebb’s rule, however; we need to understand how the brain is built. Each neuron is like a tiny tree, with a prodigious number of roots—the dendrites—and a slender, sinuous trunk—the axon. The brain is a forest of billions of these trees, but there’s something unusual about them. Each tree’s branches make connections—synapses—to the roots of thousands of others, forming a massive tangle like nothing you’ve ever seen. Some neurons have short axons and some have exceedingly long ones, reaching clear from one side of the brain to the other. Placed end to end, the axons in your brain would stretch from Earth to the moon.</p>

<p class="noindent chinese">然而，为了模拟大脑，我们需要的不仅仅是赫伯规则；我们需要了解大脑是如何构建的。每个神经元就像一棵小树，有数量惊人的树根 —— 树突，以及细长、蜿蜒的树干 —— 轴突。大脑是由数十亿棵这样的树组成的森林，但它们有一些不寻常之处。每棵树的树枝都与其他数千棵树的树根产生连接 —— 同步连接，形成一个巨大的纠结，这是你从未见过的。有些神经元的轴突很短，有些则非常长，从大脑的一侧延伸到另一侧。将轴突端对端放置，你大脑中的轴突将从地球延伸到月球。</p>

<p class="noindent english">And this jungle crackles with electricity. Sparks run along tree trunks and set off more sparks in neighboring trees. Every now and then, a whole area of the jungle whips itself into a frenzy before settling down again. When you wiggle your toe, a series of electric discharges, called action potentials, runs all the way down your spinal cord and <a id="babilu_link-72"></a> leg until it reaches your toe muscles and tells them to move. Your brain at work is a symphony of these electric sparks. If you could sit inside it and watch what happens as you read this page, the scene you’d see would make even the busiest science-fiction metropolis look laid back by comparison. The end result of this phenomenally complex pattern of neuron firings is your consciousness.</p>

<p class="noindent chinese">这片丛林里充满了电流。火花沿着树干流淌，在邻近的树上掀起更多的火花。每隔一段时间，整个丛林都会陷入狂热，然后再平静下来。当你扭动你的脚趾时，一连串的放电，称为动作电位，一直沿着你的脊髓和腿，直到到达你的脚趾肌肉并告诉它们移动。你的大脑在工作时就是这些电火花的交响乐。如果你能坐在里面，看着你阅读这一页时发生的事情，你会看到的场景甚至会让最繁忙的科幻小说中的大都市看起来很悠闲。这种惊人的复杂的神经元启动模式的最终结果就是你的意识。</p>

<p class="noindent english">In Hebb’s time there was no way to measure synaptic strength or change in it, let alone figure out the molecular biology of synaptic change. Today, we know that synapses do grow (or form anew) when the postsynaptic neuron fires soon after the presynaptic one. Like all cells, neurons have different concentrations of ions inside and outside, creating a voltage across their membrane. When the presynaptic neuron fires, tiny sacs release neurotransmitter molecules into the synaptic cleft. These cause channels in the postsynaptic neuron’s membrane to open, letting in potassium and sodium ions and changing the voltage across the membrane as a result. If enough presynaptic neurons fire close together, the voltage suddenly spikes, and an action potential travels down the postsynaptic neuron’s axon. This also causes the ion channels to become more responsive and new channels to appear, strengthening the synapse. To the best of our knowledge, this is how neurons learn.</p>

<p class="noindent chinese">在赫伯的时代，没有办法测量突触强度或其变化，更不用说弄清突触变化的分子生物学了。今天，我们知道，当突触后的神经元在突触前的神经元爆炸后不久，突触的确会增长（或重新形成）。像所有的细胞一样，神经元内部和外部有不同浓度的离子，在其膜上形成一个电压。当突触前神经元发射时，小囊释放神经递质分子到突触裂隙中。这些分子导致突触后神经元膜上的通道打开，让钾离子和钠离子进入，并因此而改变膜上的电压。如果有足够多的突触前神经元一起开火，电压就会突然飙升，动作电位就会顺着突触后神经元的轴线传播。这也导致离子通道变得更加敏感，新的通道出现，加强了突触。据我们所知，这就是神经元的学习方式。</p>

<p class="noindent english">The next step is to turn it into an algorithm.</p>

<p class="noindent chinese">下一步是将其转化为一种算法。</p>

<h1 id="babilu_link-408"><b>The rise and fall of the perceptron</b></h1>

<h1 id="babilu_link-408"><b>感知器的兴衰</b></h1>

<p class="noindent english">The first formal model of a neuron was proposed by Warren McCulloch and Walter Pitts in 1943. It looked a lot like the logic gates computers are made of. An OR gate switches on when at least one of its inputs is on, and an AND gate when all of them are on. A McCulloch-Pitts neuron switches on when the number of its active inputs passes some threshold. If the threshold is one, the neuron acts as an OR gate; if the threshold is equal to the number of inputs, as an AND gate. In addition, a McCulloch-Pitts neuron can prevent another from switching on, which models both inhibitory synapses and NOT gates. So a network of neurons can do all <a id="babilu_link-310"></a> the operations a computer does. In the early days, computers were often called electronic brains, and this was not just an analogy.</p>

<p class="noindent chinese">第一个正式的神经元模型是由沃伦·麦库洛赫和沃尔特·皮茨在 1943 年提出的。它看起来很像计算机中的逻辑门。一个 OR 门在其至少一个输入被打开时打开，而一个 AND 门在所有输入被打开时打开。一个 McCulloch-Pitts 神经元在其活动输入的数量超过某个阈值时就会开启。如果阈值为 1，该神经元作为一个 OR 门；如果阈值等于输入数，则作为一个 AND 门。此外，一个 McCulloch-Pitts 神经元可以阻止另一个神经元开机，它同时模拟了抑制性突触和非门。因此，一个神经元网络可以做所有计算机的操作。在早期，计算机经常被称为电脑，这不仅仅是一个比喻。</p>

<p class="noindent english">What the McCulloch-Pitts neuron doesn’t do is learn. For that we need to give variable weights to the connections between neurons, resulting in what’s called a perceptron. Perceptrons were invented in the late 1950s by Frank Rosenblatt, a Cornell psychologist. A charismatic speaker and lively character, Rosenblatt did more than anyone else to shape the early days of machine learning. The name <i>perceptron</i> derives from his interest in applying his models to perceptual tasks like speech and character recognition. Rather than implement perceptrons in software, which was very slow in those days, Rosenblatt built his own devices. The weights were implemented by variable resistors like those found in dimmable light switches, and weight learning was carried out by electric motors that turned the knobs on the resistors. (Talk about high tech!)</p>

<p class="noindent chinese">McCulloch-Pitts 神经元不做的是学习。为此，我们需要对神经元之间的连接给予可变权重，从而形成所谓的感知器。感知器是由康奈尔大学的心理学家弗兰克·罗森布拉特在 20 世纪 50 年代末发明的。罗森布拉特是一个富有魅力的演讲者，性格活泼，在塑造机器学习的早期，他比任何人都做得更好。<i>感知器</i>这个名字源于他对将其模型应用于语音和字符识别等感知任务的兴趣。罗森布拉特没有用软件来实现感知器，因为软件在当时非常缓慢，他建立了自己的设备。权重是由可变电阻实现的，就像在可调光开关中发现的那样，权重学习是由电动马达转动电阻上的旋钮来进行的。（说到高科技！）。</p>

<p class="noindent english">In a perceptron, a positive weight represents an excitatory connection, and a negative weight an inhibitory one. The perceptron outputs 1 if the weighted sum of its inputs is above threshold, and 0 if it’s below. By varying the weights and threshold, we can change the function that the perceptron computes. This ignores a lot of the details of how neurons work, of course, but we want to keep things as simple as possible; our goal is to develop a general-purpose learning algorithm, not to build a realistic model of the brain. If some of the details we ignored turn out to be important, we can always add them in later. Despite our simplifying abstractions, however, we can still see how each part of this model corresponds to a part of the neuron:</p>

<p class="noindent chinese">在感知器中，正的权重代表兴奋性连接，而负的权重代表抑制性连接。如果其输入的加权和高于阈值，感知器输出 1，如果低于阈值，则输出 0。通过改变权重和阈值，我们可以改变感知器计算的函数。当然，这忽略了神经元如何工作的很多细节，但是我们希望尽可能地保持简单；我们的目标是开发一个通用的学习算法，而不是建立一个真实的大脑模型。如果我们忽略的一些细节被证明是重要的，我们总是可以在以后把它们加进去。尽管我们进行了简化抽象，然而，我们仍然可以看到这个模型的每一部分是如何与神经元的一部分相对应的。</p>

<div>

<div>

<img alt="image" src="images/000033.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-278"></a> The higher an input’s weight, the stronger the corresponding synapse. The cell body adds up all the weighted inputs, and the axon applies a step function to the result. The axon’s box in the diagram shows the graph of a step function: 0 for low values of the input, abruptly changing to 1 when the input reaches the threshold.</p>

<p class="noindent chinese">一个输入的权重越高，相应的突触就越强。细胞体将所有加权的输入加起来，轴突对结果应用一个阶梯函数。图中轴突的方框显示了一个阶梯函数的图形。输入的低值为 0，当输入达到阈值时突然变为 1。</p>

<p class="noindent english">Suppose a perceptron has two continuous inputs <i>x</i> and <i>y</i> . (In other words, <i>x</i> and <i>y</i> can take on any numeric values, not just 0 and 1.) Then each example can be represented by a point on the plane, and the boundary between positive examples (for which the perceptron outputs 1) and negative ones (output 0) is a straight line:</p>

<p class="noindent chinese">假设一个感知器有两个连续输入 <i>x</i> 和 <i>y</i>。（换句话说，<i>x</i> 和 <i>y</i> 可以是任何数值，而不仅仅是 0 和 1。）那么每个例子都可以用平面上的一个点来表示，正的例子（感知器输出 1）和负的例子（输出 0）之间的界限是一条直线。</p>

<div>

<div>

<img alt="image" src="images/000023.jpg"/>

</div>

</div>

<p class="noindent english">This is because the boundary is the set of points where the weighted sum exactly equals the threshold, and a weighted sum is a linear function. For example, if the weights are 2 for <i>x</i> and 3 for <i>y</i> and the threshold is 6, the boundary is defined by the equation 2 <i>x</i> + 3 <i>y</i> = 6. The point <i>x</i> = 0, <i>y</i> = 2 is on the boundary, and to stay on it we have to take three steps across for every two steps down, so that the gain in <i>x</i> makes up for the loss in <i>y</i> . The resulting points form a straight line.</p>

<p class="noindent chinese">这是因为边界是加权和正好等于阈值的点的集合，而加权和是一个线性函数。例如，如果 <i>x</i> 的权重为 2，<i>y</i> 的权重为 3，阈值为 6，则边界由方程式 2<i>x</i> + 3<i>y</i> = 6 定义。点 <i>x</i> = 0，<i>y</i> = 2 在边界上，为了保持在边界上，我们必须每减少两步，就必须跨过三步，这样，<i>x</i> 的增益就可以弥补 <i>y</i> 的损失。所得的点构成一条直线。</p>

<p class="noindent english">Learning a perceptron’s weights means varying the direction of the straight line until all the positive examples are on one side and all the negative ones on the other. In one dimension, the boundary is a point; in two, it’s a straight line; in three, it’s a plane; and in more than three, it’s a hyperplane. It’s hard to visualize things in hyperspace, but the math works just the same way. In <i>n</i> dimensions, we have <i>n</i> inputs and <a id="babilu_link-266"></a> the perceptron has <i>n</i> weights. To decide whether the perceptron fires or not, we multiply each weight by the corresponding input and compare the sum of all of them with the threshold.</p>

<p class="noindent chinese">学习感知器的权重意味着改变直线的方向，直到所有正面的例子都在一边，所有负面的都在另一边。在一维中，边界是一个点；在二维中，它是一条直线；在三维中，它是一个平面；而在三维以上，它是一个超平面。在超空间中很难将事物形象化，但数学的工作方式是一样的。在 <i>n 个</i>维度中，我们有 <i>n 个</i>输入和感知器有 <i>n 个</i>权重。为了决定感知器是否启动，我们将每个权重乘以相应的输入，并将所有权重的总和与阈值进行比较。</p>

<p class="noindent english">If all inputs have a weight of one and the threshold is half the number of inputs, then the perceptron fires if more than half its inputs fire. In other words, the perceptron is like a tiny parliament where the majority wins. (Or perhaps not so tiny, considering it can have thousands of members.) It’s not altogether democratic, though, because in general not everyone has an equal vote. A neural network is more like a social network, where a few close friends count for more than thousands of Facebook ones. And it’s the friends you trust most that influence you the most. If a friend recommends a movie and you go see it and like it, next time around you’ll probably follow her advice again. On the other hand, if she keeps gushing about movies you didn’t enjoy, you will start to ignore her opinions (and perhaps your friendship even wanes a bit).</p>

<p class="noindent chinese">如果所有输入的权重都是 1，而阈值是输入数量的一半，那么，如果超过一半的输入发射，感知器就会发射。换句话说，感知器就像一个小小的议会，多数人获胜。（也许不是那么小，考虑到它可以有成千上万的成员。）但这并不完全是民主的，因为一般来说，不是每个人都有平等的投票权。神经网络更像一个社交网络，在那里，几个亲密的朋友比成千上万的 Facebook 朋友更有价值。而你最信任的朋友才是对你影响最大的。如果一个朋友推荐了一部电影，你去看了并喜欢上了它，下次你可能会再次听从她的建议。另一方面，如果她一直对你不喜欢的电影津津乐道，你就会开始忽视她的意见（也许你们的友谊甚至有些减弱）。</p>

<p class="noindent english">This is how Rosenblatt’s perceptron algorithm learns weights.</p>

<p class="noindent chinese">这就是 Rosenblatt 的感知器算法学习权重的方法。</p>

<p class="noindent english">Consider the grandmother cell, a favorite thought experiment of cognitive neuroscientists. The grandmother cell is a neuron in your brain that fires whenever you see your grandmother, and only then. Whether or not grandmother cells really exist is an open question, but let’s design one for use in machine learning. A perceptron learns to recognize your grandmother as follows. The inputs to the cell are either the raw pixels in the image or various hardwired features of it, like <i>brown eyes</i> , which takes the value 1 if the image contains a pair of brown eyes and 0 otherwise. In the beginning, all the connections from features to the neuron have small random weights, like the synapses in your brain at birth. Then we show the perceptron a series of images, some of your grandmother and some not. If it fires upon seeing an image of your grandmother, or doesn’t fire upon seeing something else, then no learning needs to happen. (If it ain’t broke, don’t fix it.) But if the perceptron fails to fire when it’s looking at your grandmother, that means the weighted sum of its inputs should have been higher, so we increase the weights of the inputs that are on. (For example, if <a id="babilu_link-248"></a> your grandmother has brown eyes, the weight of that feature goes up.) Conversely, if the perceptron fires when it shouldn’t, we decrease the weights of the active inputs. It’s the errors that drive the learning. Over time, the features that are indicative of your grandmother acquire high weights, and the ones that aren’t get low weights. Once the perceptron always fires upon seeing your grandmother, and only then, the learning is complete.</p>

<p class="noindent chinese">考虑一下祖母细胞，这是认知神经科学家们最喜欢的思想实验。祖母细胞是你大脑中的一个神经元，每当你看到你的祖母时就会启动，而且只在那时。祖母细胞是否真的存在是一个开放的问题，但让我们设计一个用于机器学习的细胞。一个感知器学习如何识别你的祖母，如下所示。细胞的输入要么是图像中的原始像素，要么是图像中的各种硬接线特征，如<i>棕色眼睛</i>，如果图像中包含一对棕色眼睛，则取值为 1，否则为 0。一开始，所有从特征到神经元的连接都有小的随机权重，就像你出生时大脑中的突触。然后我们给感知器看一系列的图像，有些是你祖母的，有些不是。如果它在看到你祖母的图像时启动，或者在看到其他东西时不启动，那么就不需要进行学习。（但如果感知器在看到你祖母时没有启动，这意味着其输入的加权总和应该更高，所以我们要增加开启的输入的权重。（例如，如果你的祖母有棕色的眼睛，这个特征的权重就会上升）。相反，如果感知器在不应该启动的时候启动了，我们就降低活动输入的权重。正是这些错误推动了学习。随着时间的推移，表明你的祖母的特征获得高权重，而不是的特征获得低权重。一旦感知器总是在看到你祖母时启动，而且只有在那时，学习才算完成。</p>

<p class="noindent english">The perceptron generated a lot of excitement. It was simple, yet it could recognize printed letters and speech sounds just by being trained with examples. A colleague of Rosenblatt’s at Cornell proved that, if the positive and negative examples could be separated by a hyperplane, the perceptron would find it. For Rosenblatt and others, a genuine understanding of how the brain learns seemed within reach, and with it a powerful general-purpose learning algorithm.</p>

<p class="noindent chinese">感知器引起了很多人的兴奋。它很简单，但它可以通过实例训练来识别印刷字母和语音。罗森布拉特在康奈尔大学的一位同事证明，如果正反两方面的例子能被一个超平面分开，感知器就能找到它。对于罗森布拉特和其他人来说，对大脑如何学习的真正理解似乎触手可及，随之而来的是一种强大的通用学习算法。</p>

<p class="noindent english">But then the perceptron hit a brick wall. The knowledge engineers were irritated by Rosenblatt’s claims and envious of all the attention and funding neural networks, and perceptrons in particular, were getting. One of them was Marvin Minsky, a former classmate of Rosenblatt’s at the Bronx High School of Science and by then the leader of the AI group at MIT. (Ironically, his PhD had been on neural networks, but he had grown disillusioned with them.) In 1969, Minsky and his colleague Seymour Papert published <i>Perceptrons</i> , a book detailing the shortcomings of the eponymous algorithm, with example after example of simple things it couldn’t learn. The simplest one—and therefore the most damning—was the exclusive-OR function, or XOR for short, which is true if one of its inputs is true but not both. For example, Nike’s two most loyal demographics are supposedly teenage boys and middle-aged women. In other words, you’re likely to buy Nike shoes if you’re young XOR female. Young is good, female is good, but both is not. You’re also an unpromising target for Nike advertising if you’re neither young nor female. The problem with XOR is that there is no straight line capable of separating the positive from the negative examples. This figure shows two failed candidates:</p>

<p class="noindent chinese">但后来感知器遇到了一块砖墙。知识工程师们被罗森布拉特的主张所激怒，并对神经网络，尤其是感知器所获得的所有关注和资金感到嫉妒。其中一位是马文·明斯基，他是罗森布拉特在布朗克斯高中科学学院的前同学，当时是麻省理工学院人工智能小组的负责人。（具有讽刺意味的是，他的博士论文是关于神经网络的，但他已经对这些网络感到失望了）。1969 年，明斯基和他的同事西摩·帕珀特出版了《<i>感知器</i>》（Perceptrons）一书，详细介绍了同名算法的缺点，举了一个又一个它无法学习的简单例子。最简单的一个，因此也是最糟糕的一个，就是排他性 OR 函数，简称 XOR，如果它的一个输入是真的，但不是两个都是真的，它就是真的。例如，耐克公司最忠诚的两个人口群体应该是十几岁的男孩和中年妇女。换句话说，如果你是年轻的 XOR 女性，你就有可能购买耐克鞋。年轻是好事，女性是好事，但两者都不是。如果你既不年轻也不是女性，那么你也是耐克广告的一个不理想的目标。XOR 的问题是，没有一条直线能够把正面和负面的例子分开。这个图显示了两个失败的候选者。</p>

<div>

<div>

<img alt="image" src="images/000024.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-211"></a> Since perceptrons can only learn linear boundaries, they can’t learn XOR. And if they can’t do even that, they’re not a very good model of how the brain learns, or a viable candidate for the Master Algorithm.</p>

<p class="noindent chinese">由于感知器只能学习线性边界，它们不能学习 XOR。如果它们连这个都做不到，那么它们就不是大脑学习的一个很好的模型，也不是主算法的一个可行的候选人。</p>

<p class="noindent english">A perceptron models only a single neuron’s learning, however, and although Minsky and Papert acknowledged that layers of interconnected neurons should be capable of more, they didn’t see a way to learn them. Neither did anyone else. The problem is that there’s no clear way to change the weights of the neurons in the “hidden” layers to reduce the errors made by the ones in the output layer. Every hidden neuron influences the output via multiple paths, and every error has a thousand fathers. Who do you blame? Or, conversely, who gets the credit for correct outputs? This credit-assignment problem shows up whenever we try to learn a complex model and is one of the central problems in machine learning.</p>

<p class="noindent chinese">然而，感知器只模拟了单个神经元的学习，尽管明斯基和帕珀特承认多层互联的神经元应该有更多的能力，但他们没有看到学习它们的方法。其他任何人也没有。问题是，没有明确的方法来改变 “隐藏” 层中神经元的权重，以减少输出层中神经元的错误。每个隐藏的神经元都会通过多条路径影响输出，每个错误都有无数个来源。你会责备谁？或者反过来说，谁能得到正确输出的功劳？每当我们试图学习一个复杂的模型时，这个信用分配问题就会出现，这也是机器学习的核心问题之一。</p>

<p class="noindent english"><i>Perceptrons</i> was mathematically unimpeachable, searing in its clarity, and disastrous in its effects. Machine learning at the time was associated mainly with neural networks, and most researchers (not to mention funders) concluded that the only way to build an intelligent system was to explicitly program it. For the next fifteen years, knowledge engineering would hold center stage, and machine learning seemed to have been consigned to the ash heap of history.</p>

<p class="noindent chinese"><i>感知器</i>在数学上是无可指责的，它的清晰性令人震惊，而它的影响则是灾难性的。当时的机器学习主要与神经网络有关，大多数研究人员（更不用说资助者）得出结论，建立一个智能系统的唯一方法是对其进行明确的编程。在接下来的 15 年里，知识工程将占据中心舞台，而机器学习似乎已经被丢到了历史的灰堆里。</p>

<h1 id="babilu_link-409"><b><a id="babilu_link-153"><b></b></a> Physicist makes brain out of glass</b></h1>

<h1 id="babilu_link-409"><b><a id="babilu_link-153"><b></b></a>物理学家用玻璃制作大脑</b></h1>

<p class="noindent english">If the history of machine learning were a Hollywood movie, the villain would be Marvin Minsky. He’s the evil queen who gives Snow White a poisoned apple, leaving her in suspended animation. (In a 1988 essay, Seymour Papert even compared himself, tongue-in-cheek, to the huntsman the queen sent to kill Snow White in the forest.) And Prince Charming would be a Caltech physicist by the name of John Hopfield. In 1982, Hopfield noticed a striking analogy between the brain and spin glasses, an exotic material much beloved of statistical physicists. This set off a connectionist renaissance that culminated a few years later in the invention of the first algorithms capable of solving the credit-assignment problem, ushering in a new era where machine learning replaced knowledge engineering as the dominant paradigm in AI.</p>

<p class="noindent chinese">如果机器学习的历史是一部好莱坞电影，那么反派将是马文·明斯基。他是邪恶的女王，给白雪公主一个有毒的苹果，让她处于悬浮状态。（在 1988 年的一篇文章中，西摩·帕珀特甚至把自己比作女王派来在森林中杀死白雪公主的猎手，语带嘲讽。而白马王子将是一位名叫约翰·霍普菲尔德的加州理工学院的物理学家。1982 年，霍普菲尔德注意到大脑和自旋眼镜之间有一个惊人的类比，自旋眼镜是统计物理学家非常喜爱的一种奇特材料。这掀起了一场连接主义的复兴，几年后的高潮是发明了第一批能够解决信用分配问题的算法，开创了机器学习取代知识工程成为人工智能的主导范式的新时代。</p>

<p class="noindent english">Spin glasses are not actually glasses, although they have some glass-like properties. Rather, they are magnetic materials. Every electron is a tiny magnet by virtue of its spin, which can point “up” or “down.” In materials like iron, electrons’ spins tend to line up: if an electron with down spin is surrounded by electrons with up spins, it will probably flip to up. When most of the spins in a chunk of iron line up, it turns into a magnet. In ordinary magnets, the strength of interaction between adjacent spins is the same for all pairs, but in a spin glass it can vary; it may even be negative, causing nearby spins to point in opposite directions. The energy of an ordinary magnet is lowest when all its spins align, but in a spin glass, it’s not so simple. Indeed, finding the lowest-energy state of a spin glass is an NP-complete problem, meaning that just about every other difficult optimization problem can be reduced to it. Because of this, a spin glass doesn’t necessarily settle into its overall lowest energy state; much like rainwater may flow downhill into a lake instead of reaching the ocean, a spin glass may get stuck in a local minimum, a state with lower energy than all the states that can be reached from it by flipping a spin, rather than evolve to the global one.</p>

<p class="noindent chinese">自旋玻璃实际上不是玻璃，尽管它们有一些类似玻璃的特性。相反，它们是磁性材料。每个电子都是一个微小的磁铁，因为它的自旋可以指向 “上” 或 “下”。在像铁这样的材料中，电子的自旋倾向于排成一行：如果一个自旋向下的电子被自旋向上的电子所包围，它可能会翻转到向上。当一大块铁中的大部分自旋排成一行时，它就变成了一块磁铁。在普通的磁铁中，相邻的自旋之间的相互作用强度对所有的配对都是一样的，但在自旋玻璃中，它可以变化；它甚至可能是负的，导致附近的自旋指向相反的方向。当一个普通的磁铁的所有自旋对准时，其能量是最低的，但在自旋玻璃中，这并不那么简单。事实上，寻找自旋玻璃的最低能量状态是一个 NP-完全问题，这意味着几乎所有其他困难的优化问题都可以被简化为这个问题。正因为如此，自旋玻璃不一定会进入其整体的最低能量状态；就像雨水可能会顺着山坡流进湖里而不是到达海洋一样，自旋玻璃可能会卡在一个局部最小值，一个比所有通过翻转自旋就能达到的状态能量更低的状态，而不是演化到全局状态。</p>

<p class="noindent english">Hopfield noticed an interesting similarity between spin glasses and neural networks: an electron’s spin responds to the behavior of its <a id="babilu_link-27"></a> neighbors much like a neuron does. In the electron’s case, it flips up if the weighted sum of the neighbors exceeds a threshold and flips (or stays) down otherwise. Inspired by this, he defined a type of neural network that evolves over time in the same way that a spin glass does and postulated that the network’s minimum energy states are its memories. Each such state has a “basin of attraction” of initial states that converge to it, and in this way the network can do pattern recognition: for example, if one of the memories is the pattern of black-and-white pixels formed by the digit nine and the network sees a distorted nine, it will converge to the “ideal” one and thereby recognize it. Suddenly, a vast body of physical theory was applicable to machine learning, and a flood of statistical physicists poured into the field, helping it break out of the local minimum it had been stuck in.</p>

<p class="noindent chinese">霍普菲尔德注意到自旋眼镜和神经网络之间一个有趣的相似之处：一个电子的自旋对其邻居的行为做出反应，就像神经元一样。在电子的情况下，如果邻居的加权和超过一个阈值，它就翻转起来，否则就翻转（或保持）下来。受此启发，他定义了一种神经网络，它以自旋玻璃的方式随时间演变，并推测该网络的最低能量状态是其记忆。每个这样的状态都有一个初始状态的 “吸引盆地”，并以这种方式使网络能够进行模式识别：例如，如果其中一个记忆是由数字 9 形成的黑白像素模式，而网络看到一个扭曲的 9，它将收敛到 “理想” 的 9，从而识别它。突然间，大量的物理理论适用于机器学习，大量的统计物理学家涌入该领域，帮助它走出了它所陷入的局部最小值。</p>

<p class="noindent english">A spin glass is still a very unrealistic model of the brain, though. For one, spin interactions are symmetric, and connections between neurons in the brain are not. Another big issue that Hopfield’s model ignored is that real neurons are statistical: they don’t deterministically turn on and off as a function of their inputs; rather, as the weighted sum of inputs increases, the neuron becomes more likely to fire, but it’s not certain that it will. In 1985, David Ackley, Geoff Hinton, and Terry Sejnowski replaced the deterministic neurons in Hopfield networks with probabilistic ones. A neural network now had a probability distribution over its states, with higher-energy states being exponentially less likely than lower-energy ones. In fact, the probability of finding the network in a particular state was given by the well-known Boltzmann distribution from thermodynamics, so they called their network a Boltzmann machine.</p>

<p class="noindent chinese">不过，自旋玻璃仍然是一个非常不现实的大脑模型。首先，自旋相互作用是对称的，而大脑中神经元之间的连接不是。霍普菲尔德模型忽略的另一个大问题是，真正的神经元是统计学的：它们并不确定地作为其输入的函数而开启和关闭；相反，随着输入的加权和增加，神经元变得更有可能启动，但并不确定它会启动。1985 年，大卫·阿克雷、杰夫·辛顿和特里·塞诺夫斯基用概率性的神经元取代了 Hopfield 网络中的确定性神经元。一个神经网络现在有一个关于其状态的概率分布，高能量的状态比低能量的状态在指数上更不可能。事实上，发现网络处于特定状态的概率是由热力学中著名的玻尔兹曼分布给出的，因此他们把他们的网络称为玻尔兹曼机。</p>

<p class="noindent english">A Boltzmann machine has a mix of sensory and hidden neurons (analogous to, for example, the retina and the brain, respectively). It learns by being alternately awake and asleep, just like humans. While awake, the sensory neurons fire as dictated by the data, and the hidden ones evolve according to the network dynamics and the sensory input. For example, if the network is shown an image of a nine, the neurons corresponding to the black pixels in the image stay on, the others stay <a id="babilu_link-29"></a> off, and the hidden ones fire randomly according to the Boltzmann distribution given those pixel values. During sleep, the machine dreams, leaving both sensory and hidden neurons free to wander. Just before the new day dawns, it compares the statistics of its states during the dream and during yesterday’s activities and changes the connection weights so that they match. If two neurons tend to fire together during the day but less so while asleep, the weight of their connection goes up; if it’s the opposite, they go down. By doing this day after day, the predicted correlations between sensory neurons evolve until they match the real ones. At this point, the Boltzmann machine has learned a good model of the data and effectively solved the credit-assignment problem.</p>

<p class="noindent chinese">玻尔兹曼机有一个混合的感觉和隐藏的神经元（分别类似于，例如，视网膜和大脑）。它通过交替的清醒和睡眠进行学习，就像人类一样。在醒着的时候，感觉神经元按照数据的要求发射，而隐藏的神经元则根据网络的动态和感觉输入的情况进行演变。例如，如果网络被显示了一张九宫格的图像，与图像中的黑色像素相对应的神经元保持开启状态，其他神经元保持而隐藏的神经元则根据给定的像素值的波尔兹曼分布随机启动。在睡眠期间，机器做梦，让感觉神经元和隐藏神经元自由游荡。就在新的一天来临之前，它比较了梦中和昨天活动中的状态统计，并改变了连接权重，使其相匹配。如果两个神经元在白天倾向于一起开火，但在睡觉时却不太喜欢，那么它们的连接权重就会上升；如果情况相反，它们的权重就会下降。通过日复一日地这样做，预测的感觉神经元之间的相关性不断发展，直到它们与真实的相关性相匹配。在这一点上，玻尔兹曼机已经学会了一个良好的数据模型，并有效地解决了信用分配问题。</p>

<p class="noindent english">Geoff Hinton went on to try many variations on Boltzmann machines over the following decades. Hinton, a psychologist turned computer scientist and great-great-grandson of George Boole, the inventor of the logical calculus used in all digital computers, is the world’s leading connectionist. He has tried longer and harder to understand how the brain works than anyone else. He tells of coming home from work one day in a state of great excitement, exclaiming “I did it! I’ve figured out how the brain works!” His daughter replied, “Oh, Dad, not again!” Hinton’s latest passion is deep learning, which we’ll meet later in this chapter. He was also involved in the development of backpropagation, an even better algorithm than Boltzmann machines for solving the credit-assignment problem that we’ll look at next. Boltzmann machines could solve the credit-assignment problem in principle, but in practice learning was very slow and painful, making this approach impractical for most applications. The next breakthrough involved getting rid of another oversimplification that dated all the way back to McCulloch and Pitts.</p>

<p class="noindent chinese">杰夫·辛顿在随后的几十年里继续尝试了许多关于玻尔兹曼机的变化。辛顿是一位从心理学家转为计算机科学家的人，也是所有数字计算机中使用的逻辑微积分的发明者乔治·布尔的曾孙，他是世界领先的连接主义者。他为了解大脑的工作原理所做的努力比任何人都要长久和努力。他说，有一天他下班回家时非常兴奋，感叹道：“我成功了！我已经弄清了大脑的工作原理！” 他的女儿回答说：“哦，爸爸，不要再这样了！” 辛顿最近的热情是深度学习，我们将在本章后面认识它。他还参与了反向传播的开发，这是一种比玻尔兹曼机更好的解决信用分配问题的算法，我们接下来会看一下。玻尔兹曼机原则上可以解决信用分配问题，但在实践中，学习是非常缓慢和痛苦的，这使得这种方法在大多数应用中不切实际。下一个突破涉及到摆脱另一个可以追溯到 McCulloch-Pitts 的过度简化问题。</p>

<h1 id="babilu_link-410"><b>The most important curve in the world</b></h1>

<h1 id="babilu_link-410"><b>世界上最重要的曲线</b></h1>

<p class="noindent english">As far as its neighbors are concerned, a neuron can only be in one of two states: firing or not firing. This misses an important subtlety, however. Action potentials are short lived; the voltage spikes for a small fraction <a id="babilu_link-152"></a> of a second and immediately goes back to its resting state. And a single spike barely registers in the receiving neuron; it takes a train of spikes closely on each other’s heels to wake it up. A typical neuron spikes occasionally in the absence of stimulation, spikes more and more frequently as stimulation builds up, and saturates at the fastest spiking rate it can muster, beyond which increased stimulation has no effect. Rather than a logic gate, a neuron is more like a voltage-to-frequency converter. The curve of frequency as a function of voltage looks like this:</p>

<p class="noindent chinese">就其邻居而言，一个神经元只能处于两种状态中的一种：点火或不点火。然而，这忽略了一个重要的微妙之处。行动电位是短暂的；电压在一秒钟的一小部分然后立即恢复到其静止状态。单个尖峰几乎不会在接收的神经元中被记录下来；需要一连串的尖峰紧跟其后才能唤醒它。一个典型的神经元在没有刺激的情况下偶尔会出现尖峰，随着刺激的增加，尖峰越来越频繁，并在它能达到的最快尖峰速率时达到饱和，超过这个速率，增加刺激就没有效果了。与其说神经元是一个逻辑门，不如说它是一个电压·频率转换器。频率与电压的关系曲线看起来像这样。</p>

<div>

<div>

<img alt="image" src="images/000000.jpg"/>

</div>

</div>

<p class="noindent english">This curve, which looks like an elongated S, is variously known as the logistic, sigmoid, or S curve. Peruse it closely, because it’s the most important curve in the world. At first the output increases slowly with the input, so slowly it seems constant. Then it starts to change faster, then very fast, then slower and slower until it becomes almost constant again. The transfer curve of a transistor, which relates its input and output voltages, is also an S curve. So both computers and the brain are filled with S curves. But it doesn’t end there. The S curve is the shape of phase transitions of all kinds: the probability of an electron flipping its spin as a function of the applied field, the magnetization of iron, the writing of a bit of memory to a hard disk, an ion channel opening in a cell, ice melting, water evaporating, the inflationary expansion of the early universe, punctuated equilibria in evolution, paradigm shifts in science, the spread of new technologies, white flight from multiethnic neighborhoods, rumors, epidemics, revolutions, the fall of empires, and much more. <i>The Tipping Point</i> could equally well (if less appealingly) be <a id="babilu_link-234"></a> entitled <i>The S Curve</i> . An earthquake is a phase transition in the relative position of two adjacent tectonic plates. A bump in the night is just the sound of the microscopic tectonic plates in your house’s walls shifting, so don’t be scared. Joseph Schumpeter said that the economy evolves by cracks and leaps: S curves are the shape of creative destruction. The effect of financial gains and losses on your happiness follows an S curve, so don’t sweat the big stuff. The probability that a random logical formula is satisfiable—the quintessential NP-complete problem—undergoes a phase transition from almost 1 to almost 0 as the formula’s length increases. Statistical physicists spend their lives studying phase transitions.</p>

<p class="noindent chinese">这条看起来像一个拉长的 S 的曲线，被称为 Logistic、sigmoid 或 S 曲线。仔细阅读它，因为它是世界上最重要的曲线。起初，输出随着输入缓慢增加，缓慢到似乎是恒定的。然后它开始快速变化，然后非常快，然后越来越慢，直到它再次变得几乎恒定。晶体管的转移曲线，将其输入和输出电压联系起来，也是一条 S 曲线。所以计算机和大脑都充满了 S 曲线。但这并没有结束。S 曲线是所有种类的相变的形状。电子翻转自旋的概率是外加场的函数，铁的磁化，向硬盘写入一点记忆，细胞中的离子通道打开，冰的融化，水的蒸发，早期宇宙的膨胀，进化中的斑点平衡，科学中的范式转变，新技术的传播，白人逃离多民族社区，谣言，流行病，革命，帝国的衰落，等等。《<i>临界点</i>》同样可以（如果不那么吸引人的话）标题为 《<i>S</i> 曲线》。地震是两个相邻的构造板块的相对位置的阶段性转变。夜晚的颠簸只是你房子墙壁上的微观构造板块移动的声音，所以不要害怕。约瑟夫·熊彼特说，经济的发展是通过裂缝和跳跃。S 曲线是创造性破坏的形状。财务收益和损失对你的幸福感的影响遵循 S 曲线，所以不要为大事情而烦恼。一个随机的逻辑公式是可满足的概率 —— 典型的 NP-完全问题 —— 随着公式长度的增加，经历了从接近 1 到接近 0 的相变。统计物理学家一生都在研究相变。</p>

<p class="noindent english">In Hemingway’s <i>The Sun Also Rises</i> , when Mike Campbell is asked how he went bankrupt, he replies, “Two ways. Gradually and then suddenly.” The same could be said of Lehman Brothers. That’s the essence of an S curve. One of the futurist Paul Saffo’s rules of forecasting is: look for the S curves. When you can’t get the temperature in the shower just right—first it’s too cold, and then it quickly shifts to too hot—blame the S curve. When you make popcorn, watch the S curve’s progress: at first nothing happens, then a few kernels pop, then a bunch more, then the bulk of them in a sudden burst of fireworks, then a few more, and then it’s ready to eat. Every motion of your muscles follows an S curve: slow, then fast, then slow again. Cartoons gained a new naturalness when the animators at Disney figured this out and started copying it. Your eyes move in S curves, fixating on one thing and then another, along with your consciousness. Mood swings are phase transitions. So are birth, adolescence, falling in love, getting married, getting pregnant, getting a job, losing it, moving to a new town, getting promoted, retiring, and dying. The universe is a vast symphony of phase transitions, from the cosmic to the microscopic, from the mundane to the life changing.</p>

<p class="noindent chinese">在海明威的《<i>太阳照常升起</i>》中，当迈克·坎贝尔被问及他是如何破产的，他回答说：“两种方式：渐渐地，然后突然地。” 雷曼兄弟的情况也是如此。这就是 S 曲线的本质。未来学家保罗·萨福的预测规则之一是：寻找 S 曲线。当你不能使淋浴的温度恰到好处时 —— 先是太冷，然后迅速转变为太热 —— 就要归咎于 S 曲线。当你做爆米花时，观察 S 曲线的进展：一开始什么都没发生，然后有几个爆米花，然后又有一堆爆米花，然后大部分爆米花突然爆发，然后又有几个，然后就可以吃了。你的肌肉的每一个动作都遵循一个 S 曲线：慢，然后快，然后又慢。当迪斯尼的动画师发现这一点并开始模仿时，卡通片获得了一种新的自然性。你的眼睛以 S 型曲线移动，固定在一个东西上，然后是另一个，与你的意识一起。情绪波动是阶段性过渡。出生、青春期、恋爱、结婚、怀孕、获得工作、失去工作、搬到新的城市、晋升、退休和死亡也是如此。宇宙是一部巨大的相变交响曲，从宇宙到微观，从平凡到改变生活。</p>

<p class="noindent english">The S curve is not just important as a model in its own right; it’s also the jack-of-all-trades of mathematics. If you zoom in on its midsection, it approximates a straight line. Many phenomena we think of as linear are in fact S curves, because nothing can grow without limit. <a id="babilu_link-99"></a> Because of relativity, and <i>contra</i> Newton, acceleration does not increase linearly with force, but follows an S curve centered at zero. So does electric current as a function of voltage in the resistors found in electronic circuits, or in a light bulb (until the filament melts, which is itself another phase transition). If you zoom out from an S curve, it approximates a step function, with the output suddenly changing from zero to one at the threshold. So depending on the input voltages, the same curve represents the workings of a transistor in both digital computers and analog devices like amplifiers and radio tuners. The early part of an S curve is effectively an exponential, and near the saturation point it approximates exponential decay. When someone talks about exponential growth, ask yourself: How soon will it turn into an S curve? When will the population bomb peter out, Moore’s law lose steam, or the singularity fail to happen? Differentiate an S curve and you get a bell curve: slow, fast, slow becomes low, high, low. Add a succession of staggered upward and downward S curves, and you get something close to a sine wave. In fact, every function can be closely approximated by a sum of S curves: when the function goes up, you add an S curve; when it goes down, you subtract one. Children’s learning is not a steady improvement but an accumulation of S curves. So is technological change. Squint at the New York City skyline and you can see a sum of S curves unfolding across the horizon, each as sharp as a skyscraper’s corner.</p>

<p class="noindent chinese">S 型曲线不仅作为一个模型本身很重要，它也是数学的万能钥匙。如果你把它的中间部分放大，它就接近于一条直线。许多我们认为是线性的现象实际上是 S 型曲线，因为没有任何东西可以无限制地增长。由于相对论，并且<i>与</i>牛顿<i>相反</i>，加速度不会随着力的增加而线性增加，而是遵循以零为中心的 S 型曲线。电流也是如此，它是电子电路中的电阻或灯泡中电压的函数（直到灯丝熔化，这本身就是另一个相变）。如果你把 S 曲线放大，它就近似于一个阶梯函数，输出在阈值处突然从零变为一。因此，根据输入电压的不同，同一条曲线代表了数字计算机和模拟设备（如放大器和无线电调谐器）中晶体管的工作情况。S 曲线的早期部分实际上是一个指数，而在饱和点附近，它近似于指数衰减。当有人谈论指数增长时，问问自己。它多久会变成 S 曲线？什么时候人口炸弹会渐渐消失，摩尔定律会失去动力，或者奇点不会发生？对 S 型曲线进行区分，你会得到一个钟形曲线：慢、快、慢变成低、高、低。加上一连串交错的向上和向下的 S 曲线，你就得到了接近正弦波的东西。事实上，每个函数都可以用 S 曲线的总和来近似：当函数上升时，你增加一条 S 曲线；当它下降时，你减去一条。儿童的学习不是稳定的提高，而是 S 曲线的积累。技术变革也是如此。眯着眼睛看纽约市的天际线，你可以看到地平线上展开的 S 曲线的总和，每一条都像摩天大楼的角一样尖锐。</p>

<p class="noindent english">Most importantly for us, S curves lead to a new solution to the credit-assignment problem. If the universe is a symphony of phase transitions, let’s model it with one. That’s what the brain does: it tunes the system of phase transitions inside to the one outside. So let’s replace the perceptron’s step function with an S curve and see what happens.</p>

<p class="noindent chinese">对我们来说，最重要的是，S 曲线带来了一个解决信用分配问题的新方案。如果宇宙是一曲相变的交响乐，让我们用一个相变来模拟它。这就是大脑的作用：它将内部的相变系统与外部的相变系统进行调整。因此，让我们用 S 曲线取代感知器的阶跃函数，看看会发生什么。</p>

<h1 id="babilu_link-411"><b>Climbing mountains in hyperspace</b></h1>

<h1 id="babilu_link-411"><b>攀登超空间的山峰</b></h1>

<p class="noindent english">In the perceptron algorithm, the error signal is all or none: you got it either right or wrong. That’s not much to go on, particularly if you have a network of many neurons. You may know that the output neuron is <a id="babilu_link-304"></a> wrong (oops, that wasn’t your grandmother), but what about some neuron deep inside the brain? What does it even mean for such a neuron to be right or wrong? If the neurons’ output is continuous instead of binary, the picture changes. For starters, we now know <i>how much</i> the output neuron is wrong by: the difference between it and the desired output. If the neuron should be firing away (“Oh hi, Grandma!”) and is firing a little, that’s better than if it’s not firing at all. More importantly, we can now propagate that error to the hidden neurons: if the output neuron should fire more and neuron A connects to it, then the more A is firing, the more we should strengthen their connection; but if A is inhibited by another neuron B, then B should fire less, and so on. Based on the feedback from all the neurons it’s connected to, each neuron decides how much more or less to fire. Based on that and the activity of <i>its</i> input neurons, it strengthens or weakens its connections to them. I need to fire more, and neuron B is inhibiting me? Lower its weight. And neuron C is firing away, but its connection to me is weak? Strengthen it. My “customer” neurons, downstream in the network, will tell me how well I’m doing in the next round.</p>

<p class="noindent chinese">在感知器算法中，错误信号是全有或全无：你要么对了，要么错了。这没什么可说的，特别是如果你有一个由许多神经元组成的网络。你可能知道输出神经元是错的（哎呀，那不是你奶奶），但是大脑深处的某个神经元呢？这样的神经元到底是对是错又是什么意思呢？如果神经元的输出是连续的，而不是二进制的，情况就会改变。首先，我们现在知道了输出神经元的错误<i>程度</i>，即它与期望输出之间的差异。如果神经元应该发射出去（“哦，你好，奶奶！”），但却发射了一点，这比它根本不发射要好。更重要的是，我们现在可以把这个错误传播给隐藏的神经元：如果输出神经元应该发射更多，而神经元 A 连接到它，那么 A 发射的越多，我们就越应该加强它们的连接；但如果 A 被另一个神经元 B 抑制，那么 B 就应该少发射，以此类推。根据它所连接的所有神经元的反馈，每个神经元都决定要多发射或少发射多少。基于这一点和<i>它的</i>输入神经元的活动，它加强或减弱与它们的连接。我需要发射更多的信号，而神经元 B 正抑制着我？降低它的重量。而神经元 C 正在发射，但它与我的连接很弱？加强它。我的 “客户” 神经元，在网络的下游，将告诉我在下一轮中的表现如何。</p>

<p class="noindent english">Whenever the learner’s “retina” sees a new image, that signal propagates forward through the network until it produces an output. Comparing this output with the desired one yields an error signal, which then propagates back through the layers until it reaches the retina. Based on this returning signal and on the inputs it had received during the forward pass, each neuron adjusts its weights. As the network sees more and more images of your grandmother and other people, the weights gradually converge to values that let it discriminate between the two. Backpropagation, as this algorithm is known, is phenomenally more powerful than the perceptron algorithm. A single neuron could only learn straight lines. Given enough hidden neurons, a multilayer perceptron, as it’s called, can represent arbitrarily convoluted frontiers. This makes backpropagation—or simply backprop—the connectionists’ master algorithm.</p>

<p class="noindent chinese">每当学习者的 “视网膜” 看到一个新的图像，这个信号就会通过网络向前传播，直到产生一个输出。将这一输出与所需输出相比较，产生一个错误信号，然后通过各层传播，直到到达视网膜。根据这个返回的信号和它在前向传递过程中收到的输入，每个神经元调整其权重。随着网络看到越来越多的你的祖母和其他人的图像，权重逐渐收敛到让它能够区分这两者的值。这种算法被称为 “反向传播”（Backpropagation），它比感知器算法强大得多。一个单一的神经元只能学习直线。如果有足够多的隐藏神经元，多层感知器，正如它所说的那样，可以代表任意曲折的边界。这使得反向传播 —— 或者简单地说是反向传播 —— 成为连接主义者的主要算法。</p>

<p class="noindent english"><a id="babilu_link-264"></a> Backprop is an instance of a strategy that is very common in both nature and technology: if you’re in a hurry to get to the top of the mountain, climb the steepest slope you can find. The technical term for this is gradient ascent (if you want to get to the top) or gradient descent (if you’re looking for the valley bottom). Bacteria can find food by swimming up the concentration gradient of, say, glucose molecules, and they can flee from poisons by swimming down their gradient. All sorts of things, from aircraft wings to antenna arrays, can be optimized by gradient descent. Backprop is an efficient way to do it in a multilayer perceptron: keep tweaking the weights so as to lower the error, and stop when all tweaks fail. With backprop, you don’t have to figure out how to tweak each neuron’s weights from scratch, which would be too slow; you can do it layer by layer, tweaking each neuron based on how you tweaked the neurons it connects to. If you had to throw out your entire machine-learning toolkit in an emergency save for one tool, gradient descent is probably the one you’d want to hold on to.</p>

<p class="noindent chinese">“反向传播” 是一个在自然界和技术领域都非常常见的策略的实例：如果你急于到达山顶，就爬上你能找到的最陡峭的斜坡。这方面的技术术语是梯度上升（如果你想到达山顶）或梯度下降（如果你在寻找谷底）。细菌可以通过游向葡萄糖分子的浓度梯度来寻找食物，而它们可以通过游向其梯度来逃离毒物。各种各样的东西，从飞机机翼到天线阵列，都可以通过梯度下降进行优化。逆推法是在多层感知器中进行优化的有效方法：不断调整权重以降低误差，当所有调整都失败时就停止。有了 “反向传播”，你不必从头开始计算如何调整每个神经元的权重，那样会太慢；你可以逐层进行，根据你对它所连接的神经元的调整情况来调整每个神经元。如果你不得不在紧急情况下扔掉你的整个机器学习工具包，除了一个工具之外，梯度下降可能是你想坚持的工具。</p>

<p class="noindent english">So does backprop solve the machine-learning problem? Can we just throw together a big pile of neurons, wait for it to do its magic, and on the way to the bank collect a Nobel Prize for figuring out how the brain works? Alas, life is not that easy. Suppose your network has only one weight, and this is the graph of the error as a function of it:</p>

<p class="noindent chinese">那么，“反向传播” 是否解决了机器学习的问题？我们能不能把一大堆神经元扔在一起，等着它施展魔法，然后在去银行的路上因为弄清了大脑的工作原理而领取诺贝尔奖？唉，生活没有那么简单。假设你的网络只有一个权重，这是误差与权重的函数图。</p>

<div>

<div>

<img alt="image" src="images/000008.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-294"></a> The optimal weight, where the error is lowest, is 2.0. If the network starts out with a weight of 0.75, for example, backprop will get to the optimum in a few steps, like a ball rolling downhill. But if it starts at 5.5, on the other hand, backprop will roll down to 7.0 and remain stuck there. Backprop, with its incremental weight changes, doesn’t know how to find the global error minimum, and local ones can be arbitrarily bad, like mistaking your grandmother for a hat. With one weight, you could try every possible value at increments of 0.01 and find the optimum that way. But with thousands of weights, let alone millions or billions, this is not an option because the number of points on the grid goes up exponentially with the number of weights. The global minimum is hidden somewhere in the unfathomable vastness of hyperspace—and good luck finding it.</p>

<p class="noindent chinese">最佳权重，即误差最低的地方，是 2.0。例如，如果网络一开始的权重是 0，“反向传播” 将在几步内到达最佳状态，就像一个球在下坡时滚动。但是，如果它从 5.5 开始，另一方面，“反向传播” 会滚落到 7.0，并一直停留在那里。由于权重的递增变化，“反向传播” 不知道如何找到全局误差的最小值，而且局部误差可能是任意糟糕的，就像把你的祖母误认为是一顶帽子。有了一个权重，你可以以 0.01 的增量尝试每一个可能的值，并通过这种方式找到最优。但是如果有几千个权重，更不用说几百万或几十亿了，这就不是一个选择，因为网格上的点的数量会随着权重的增加而呈指数级增长。全局最小值隐藏在深不可测的浩瀚的超空间的某个地方 —— 祝你找到它。</p>

<p class="noindent english">Imagine you’ve been kidnapped and left blindfolded somewhere in the Himalayas. Your head is throbbing, and your memory is not too good, either. All you know is you need to get to the top of Mount Everest. What do you do? You take a step forward and nearly slide into a ravine. After catching your breath, you decide to be a bit more systematic. You carefully feel around with your foot until you find the highest point you can and step gingerly to that point. Then you do the same again. Little by little, you get higher and higher. After a while, every step you can take is down, and you stop. That’s gradient ascent. If the Himalayas were just Mount Everest, and Everest was a perfect cone, it would work like a charm. But more likely, when you get to a place where every step is down, you’re still very far from the top. You’re just standing on a foothill somewhere, and you’re stuck. That’s what happens to backprop, except it climbs mountains in hyperspace instead of 3-D. If your network has a single neuron, just climbing to better weights one step at a time will get you to the top. But with a multilayer perceptron, the landscape is very rugged; good luck finding the highest peak.</p>

<p class="noindent chinese">想象一下，你被绑架了，并被蒙在喜马拉雅山的某个地方。你的头在跳动，你的记忆力也不是很好。你只知道你需要到达珠穆朗玛峰的顶端。你是怎么做的？你往前走了一步，差点滑进沟壑。喘了口气后，你决定更系统一点。你小心翼翼地用脚摸索，直到找到你能找到的最高点，然后小心翼翼地走到那个点。然后你再做同样的事。渐渐地，你越走越高。一段时间后，你能迈出的每一步都是向下的，你就会停下来。这就是梯度上升。如果喜马拉雅山只是珠穆朗玛峰，而珠穆朗玛峰是一个完美的圆锥体，那么它就会像一个魅力。但更有可能的是，当你到了一个每一步都是向下的地方，你离山顶还是很远。你只是站在某个山脚下，而你被困住了。这就是 “反向传播” 的情况，只不过它是在超空间而不是三维空间爬山。如果你的网络只有一个神经元，只需一步步爬到更好的权重，就能让你爬到山顶。但是如果是多层感知器，地形就非常崎岖，找到最高的山峰就很幸运了。</p>

<p class="noindent english">This was part of the reason Minsky, Papert, and others couldn’t see how to learn multilayer perceptrons. They could imagine replacing step functions by S curves and doing gradient descent, but then they were faced with the problem of local minima of the error. In those days <a id="babilu_link-295"></a> researchers didn’t trust computer simulations; they demanded mathematical proof that an algorithm would work, and there’s no such proof for backprop. But what we’ve come to realize is that most of the time a local minimum is fine. The error surface often looks like the quills of a porcupine, with many steep peaks and troughs, but it doesn’t really matter if we find the absolute lowest trough; any one will do. Better still, a local minimum may in fact be preferable because it’s less likely to prove to have overfit our data than the global one.</p>

<p class="noindent chinese">这是 Minsky、Papert 和其他人看不到如何学习多层感知器的部分原因。他们可以想象用 S 曲线代替阶梯函数，并进行梯度下降，但他们又面临着误差的局部最小值问题。在那些日子里，研究人员不相信计算机模拟；他们要求用数学证明一种算法会起作用，而 “反向传播” 没有这种证明。但我们已经认识到，大多数时候，局部最小值是好的。误差表面通常看起来像豪猪的羽毛，有许多陡峭的高峰和低谷，但我们是否找到绝对最低的低谷并不重要；任何一个都可以。更妙的是，局部最低点事实上可能是更好的，因为它比全局最低点更不可能证明过度拟合我们的数据。</p>

<p class="noindent english">Hyperspace is a double-edged sword. On the one hand, the higher dimensional the space, the more room it has for highly convoluted surfaces and local optima. On the other hand, to be stuck in a local optimum you have to be stuck in <i>every</i> dimension, so it’s more difficult to get stuck in many dimensions than it is in three. In hyperspace there are mountain passes all over the (hyper) place. So, with a little help from a human sherpa, backprop can often find its way to a perfectly good set of weights. It may be only the mystical valley of Shangri-La, not the sea, but why complain if in hyperspace there are millions of Shangri-Las, each with billions of mountain passes leading to it?</p>

<p class="noindent chinese">超空间是一把双刃剑。一方面，空间的维度越高，它就有越多的空间用于高度复杂的表面和局部最优。另一方面，要卡在局部最优，你必须卡在<i>每个</i>维度上，所以卡在多维空间比卡在三维空间更难。在超空间中，到处都有山路（超）。因此，在人类谢尔巴人的帮助下，反托拉斯往往可以找到一套完美的砝码的方法。它可能只是神秘的香格里拉山谷，而不是大海，但如果在超空间有数以百万计的香格里拉，每一个都有数以亿计的山口通往它，为什么要抱怨？</p>

<p class="noindent english">Beware of attaching too much meaning to the weights backprop finds, however. Remember that there are probably many very different ones that are just as good. Learning in multilayer perceptrons is a chaotic process in the sense that starting in slightly different places can cause you to wind up at very different solutions. The phenomenon is the same whether the slight difference is in the initial weights or the training data and manifests itself in all powerful learners, not just backprop.</p>

<p class="noindent chinese">然而，要注意不要对 “反向传播” 找到的权重附加太多的意义。记住，可能有许多非常不同的权重也同样好。多层感知器的学习是一个混乱的过程，因为从稍微不同的地方开始，就会导致你得到非常不同的解决方案。不管是初始权重还是训练数据上的细微差别，这种现象都是一样的，它体现在所有强大的学习器上，而不仅仅是 “反向传播”。</p>

<p class="noindent english">We <i>could</i> do away with the problem of local optima by taking out the S curves and just letting each neuron output the weighted sum of its inputs. That would make the error surface very smooth, leaving only one minimum—the global one. The problem, though, is that a linear function of linear functions is still just a linear function, so a network of linear neurons is no better than a single neuron. A linear brain, no matter how large, is dumber than a roundworm. S curves are a nice halfway house between the dumbness of linear functions and the hardness of step functions.</p>

<p class="noindent chinese">我们<i>可以</i>去掉 S 曲线，只让每个神经元输出其输入的加权和，从而解决局部优化的问题。这将使误差表面非常平滑，只留下一个最小值 —— 全局最小值。但问题是，线性函数的线性函数仍然只是一个线性函数，所以一个线性神经元网络并不比单个神经元好。一个线性的大脑，不管有多大，都比一只蛔虫要笨。S 曲线是介于线性函数的笨拙和阶梯函数的坚硬之间的一个很好的中间地带。</p>

<h1 id="babilu_link-412"><b><a id="babilu_link-104"><b></b></a> The perceptron’s revenge</b></h1>

<h1 id="babilu_link-412"><b><a id="babilu_link-104"><b></b></a>感知器的报复</b></h1>

<p class="noindent english">Backprop was invented in 1986 by David Rumelhart, a psychologist at the University of California, San Diego, with the help of Geoff Hinton and Ronald Williams. Among other things, they showed that backprop can learn XOR, enabling connectionists to thumb their noses at Minsky and Papert. Recall the Nike example: young men and middle-aged women are the most likely buyers of Nike shoes. We can represent this with a network of three neurons: one that fires when it sees a young male, another that fires when it sees a middle-aged female, and another that fires when either of those does. And with backprop we can learn the appropriate weights, resulting in a successful Nike prospect detector. (So there, Marvin.)</p>

<p class="noindent chinese">“反向传播” 是由加州大学圣地亚哥分校的心理学家大卫·鲁梅尔哈特在杰夫·辛顿和罗纳德·威廉姆斯的帮助下于 1986 年发明的。在其他方面，他们表明 “反向传播” 可以学习 XOR，使连接主义者能够对明斯基和帕珀特竖起大拇指。回顾一下耐克的例子：年轻男子和中年妇女是最有可能购买耐克鞋的人。我们可以用一个由三个神经元组成的网络来表示这一点：一个神经元在看到年轻男性时起作用，另一个神经元在看到中年女性时起作用，还有一个神经元在看到其中任何一个时起作用。通过 “反向传播”，我们可以学习适当的权重，从而形成一个成功的耐克前景检测器。（就是这样，马文。）</p>

<p class="noindent english">In an early demonstration of the power of backprop, Terry Sejnowski and Charles Rosenberg trained a multilayer perceptron to read aloud. Their NETtalk system scanned the text, selected the correct phonemes according to context, and fed them to a speech synthesizer. NETtalk not only generalized accurately to new words, which knowledge-based systems could not, but it learned to speak in a remarkably human-like way. Sejnowski used to mesmerize audiences at research meetings by playing a tape of NETtalk’s progress: babbling at first, then starting to make sense, then speaking smoothly with only the occasional error. (You can find samples on YouTube by typing “sejnowski nettalk.”)</p>

<p class="noindent chinese">在 “反向传播” 力量的早期展示中，特里·塞诺夫斯基和查尔斯·罗森伯格训练了一个多层感知器来朗读。他们的 NETtalk 系统扫描文本，根据上下文选择正确的音素，并将它们输入到语音合成器中。NETtalk 不仅准确地概括了新词，这是基于知识的系统所不能做到的，而且它还学会了以一种非常类似人类的方式说话。塞诺夫斯基曾经在研究会议上通过播放 NETtalk 的进展录像来迷惑听众：一开始胡言乱语，然后开始有意义，然后说得很流畅，只有偶尔的错误。（你可以在 YouTube 上输入 “sejnowski nettalk” 来查找样本）。</p>

<p class="noindent english">Neural networks’ first big success was in predicting the stock market. Because they could detect small nonlinearities in very noisy data, they beat the linear models then prevalent in finance and their use spread. A typical investment fund would train a separate network for each of a large number of stocks, let the networks pick the most promising ones, and then have human analysts decide which of those to invest in. A few funds, however, went all the way and let the learners themselves buy and sell. Exactly how all these fared is a closely guarded secret, but it’s probably not an accident that machine learners keep disappearing into hedge funds at an alarming rate.</p>

<p class="noindent chinese">神经网络的第一个重大成功是在预测股票市场方面。因为它们可以在非常嘈杂的数据中检测出微小的非线性，所以它们击败了当时在金融业盛行的线性模型，其使用范围也随之扩大。一个典型的投资基金将为大量的股票训练一个单独的网络，让网络挑选出最有前途的股票，然后由人类分析师决定投资于哪些股票。然而，有几家基金则一路走来，让学习者自己买入和卖出。所有这些人的确切表现是一个严密的秘密，但机器学习者以惊人的速度消失在对冲基金中，这可能不是一个意外。</p>

<p class="noindent english"><a id="babilu_link-156"></a> Nonlinear models are important far beyond the stock market. Scientists everywhere use linear regression because that’s what they know, but more often than not the phenomena they study are nonlinear, and a multilayer perceptron can model them. Linear models are blind to phase transitions; neural networks soak them up like a sponge.</p>

<p class="noindent chinese">非线性模型的重要性远远超过股票市场。各地的科学家都使用线性回归，因为那是他们所知道的，但更多时候他们研究的现象是非线性的，而多层感知器可以为它们建模。线性模型对相变视而不见；而神经网络却像海绵一样吸收相变。</p>

<p class="noindent english">Another notable early success of neural networks was learning to drive a car. Driverless cars first broke into the public consciousness with the DARPA Grand Challenges in 2004 and 2005, but a over a decade earlier, researchers at Carnegie Mellon had already successfully trained a multilayer perceptron to drive a car by detecting the road in video images and appropriately turning the steering wheel. Carnegie Mellon’s car managed to drive coast to coast across America with very blurry vision (thirty by thirty-two pixels), a brain smaller than a worm’s, and only a few assists from the human copilot. (The project was dubbed “No Hands Across America.”) It may not have been the first truly self-driving car, but it did compare favorably with most teenage drivers.</p>

<p class="noindent chinese">神经网络的另一个值得注意的早期成功是学习驾驶汽车。无人驾驶汽车在 2004 年和 2005 年的 DARPA 大挑战中首次进入公众视野，但在十多年前，卡内基梅隆大学的研究人员已经成功地训练了一个多层感知器，通过检测视频图像中的道路并适当地转动方向盘来驾驶汽车。卡内基梅隆大学的汽车在视觉非常模糊（30×32 像素）、大脑比虫子还小、只有人类副驾驶提供一些帮助的情况下，成功地从海岸线开到了美国。（该项目被称为 “无手穿越美国”。）它可能不是第一辆真正的自动驾驶汽车，但它确实比大多数青少年司机要好。</p>

<p class="noindent english">Backprop’s applications are now too many to count. As its fame has grown, more of its history has come to light. It turns out that, as is often the case in science, backprop was invented more than once. Yann LeCun in France and others hit on it at around the same time as Rumelhart. A paper on backprop was rejected by the leading AI conference in the early 1980s because, according to the reviewers, Minsky and Papert had already proved that perceptrons don’t work. In fact, Rumelhart is credited with inventing backprop by the Columbus test: Columbus was not the first person to discover America, but the last. It turns out that Paul Werbos, a graduate student at Harvard, had proposed a similar algorithm in his PhD thesis in 1974. And in a supreme irony, Arthur Bryson and Yu-Chi Ho, two control theorists, had done the same even earlier: in 1969, the same year that Minsky and Papert published <i>Perceptrons</i> ! Indeed, the history of machine learning itself shows why we need learning algorithms. If algorithms that automatically find related papers in the scientific literature had existed in 1969, they could have potentially helped avoid decades of wasted time and accelerated who knows what discoveries.</p>

<p class="noindent chinese">“反向传播” 的应用现在已经多得数不清了。随着它的名气越来越大，它的更多历史也被发现了。事实证明，正如科学界经常出现的情况，“反向传播” 的发明不止一次。法国的扬·勒库恩和其他人大约在鲁梅尔哈特的同一时间发现了它。一篇关于 “反向传播” 的论文在 20 世纪 80 年代初被领先的人工智能会议拒绝了，因为根据审稿人的说法，明斯基和帕珀特已经证明了感知器不起作用。事实上，鲁梅尔哈特被认为是哥伦布试验发明了 “反向传播”。哥伦布不是第一个发现美洲的人，而是最后一个。事实证明，哈佛大学的研究生保罗·韦伯斯在 1974 年的博士论文中提出了一个类似的算法。而最具讽刺意味的是，阿瑟·布赖森和何裕芝，两位控制理论家，甚至更早的时候就做了同样的事情：1969 年，也就是明斯基和帕珀特发表《<i>感知器</i>》的同一年。事实上，机器学习的历史本身显示了我们为什么需要学习算法。如果能在科学文献中自动找到相关论文的算法在 1969 年就已经存在，那么它们就有可能帮助避免数十年的时间浪费，并加速不知道是什么的发现。</p>

<p class="noindent english"><a id="babilu_link-115"></a> Among the many ironies of the history of the perceptron, perhaps the saddest is that Frank Rosenblatt died in a boating accident in Chesapeake Bay in 1971 and never lived to see the second act of his creation.</p>

<p class="noindent chinese">在感知器历史的众多讽刺中，也许最悲哀的是弗兰克·罗森布拉特于 1971 年在切萨皮克湾的一次划船事故中死亡，并且没有活着看到他的创作的第二幕。</p>

<h1 id="babilu_link-413"><b>A complete model of a cell</b></h1>

<h1 id="babilu_link-413"><b>一个完整的细胞模型</b></h1>

<p class="noindent english">A living cell is a quintessential example of a nonlinear system. The cell performs all of its functions by turning raw materials into end products through a complex web of chemical reactions. We can discover the structure of this network using symbolist methods like inverse deduction, as we saw in the last chapter, but to build a complete model of a cell we need to get quantitative, learning the parameters that couple the expression levels of different genes, relate environmental variables to internal ones, and so on. This is difficult because there is no simple linear relationship between these quantities. Rather, the cell maintains its stability through interlocking feedback loops, leading to very complex behavior. Backpropagation is well suited to this problem because of its ability to efficiently learn nonlinear functions. If we had a complete map of the cell’s metabolic pathways and enough observations of all the relevant variables, backprop could in principle learn a detailed model of the cell, with a multilayer perceptron to predict each variable as a function of its immediate causes.</p>

<p class="noindent chinese">一个活细胞是一个非线性系统的典型例子。细胞通过复杂的化学反应网络将原材料转化为最终产品，从而执行其所有的功能。正如我们在上一章中所看到的，我们可以用反推法等符号学方法发现这个网络的结构，但要建立一个完整的细胞模型，我们需要进行定量分析，学习耦合不同基因表达水平的参数，将环境变量与内部变量联系起来，等等。这很困难，因为这些数量之间没有简单的线性关系。相反，细胞通过互锁的反馈环路维持其稳定性，导致非常复杂的行为。逆向传播很适合这个问题，因为它能够有效地学习非线性函数。如果我们有一个完整的细胞代谢途径图和对所有相关变量的足够观察，“反向传播” 原则上可以学习一个详细的细胞模型，用多层感知器来预测每个变量作为其直接原因的函数。</p>

<p class="noindent english">For the foreseeable future, however, we’ll have only partial knowledge of cells’ metabolic networks and be able to observe only a fraction of the variables we’d like to. Learning useful models despite all this missing information, and despite all the inevitable inconsistencies in the information that is available, calls for Bayesian methods, which we’ll delve into in <a href="#babilu_link-6">Chapter 6</a> . The same goes for making predictions for a particular patient, model in hand: the evidence available is necessarily noisy and incomplete, and Bayesian inference makes the best of it. It helps that, if the goal is to cure cancer, we don’t necessarily need to understand all the details of how tumor cells work, only enough to disable them without harming normal cells. In <a href="#babilu_link-6">Chapter 6</a> , we’ll also see how <a id="babilu_link-100"></a> to orient learning toward the goal while steering clear of the things we don’t know and don’t need to know.</p>

<p class="noindent chinese">然而，在可预见的未来，我们对细胞的代谢网络只有部分了解，而且只能观察到我们想要的一小部分变量。尽管有这些缺失的信息，尽管有所有不可避免的不一致的信息，学习有用的模型需要贝叶斯方法，我们将在<a href="#babilu_link-6">第六章</a>中深入探讨。对某一特定病人的预测也是如此，模型在手：可用的证据必然是嘈杂和不完整的，而贝叶斯推理使其成为最佳选择。如果目标是治愈癌症，我们不一定需要了解肿瘤细胞如何工作的所有细节，只需要在不伤害正常细胞的情况下使其失效即可。在<a href="#babilu_link-6">第六章</a>中，我们还将看到将学习导向目标，同时避开我们不知道和不需要知道的东西。</p>

<p class="noindent english">More immediately, we know we can use inverse deduction to infer the structure of the cell’s networks from data and previous knowledge, but there’s a combinatorial explosion of ways to apply it, and we need a strategy. Since metabolic networks were designed by evolution, perhaps simulating it in our learning algorithms is the way to go. In the next chapter, we’ll see how to do just that.</p>

<p class="noindent chinese">更直接地说，我们知道我们可以使用反推法，从数据和以前的知识中推断出细胞的网络结构，但应用它的方法有一个组合爆炸，我们需要一个策略。既然代谢网络是由进化设计的，也许在我们的学习算法中模拟它是个好办法。在下一章，我们将看到如何做到这一点。</p>

<h1 id="babilu_link-414"><b>Deeper into the brain</b></h1>

<h1 id="babilu_link-414"><b>深入大脑</b></h1>

<p class="noindent english">When backprop first hit the streets, connectionists had visions of quickly learning larger and larger networks until, hardware permitting, they amounted to artificial brains. It didn’t turn out that way. Learning networks with one hidden layer was fine, but after that things soon got very difficult. Networks with a few layers worked only if they were carefully designed for the application (character recognition, say). Beyond that, backprop broke down. As we add layers, the error signal becomes more and more diffuse, like a river branching into smaller and smaller tributaries, until we’re down to individual raindrops that just don’t register. Learning with dozens or hundreds of hidden layers, like the brain, remained a distant dream, and by the mid-1990s, the excitement for multilayer perceptrons had petered out. A hard core of connectionists soldiered on, but by and large the attention of the machine-learning field moved elsewhere. (We’ll survey those lands in <a href="#babilu_link-6">Chapters 6</a> and <a href="#babilu_link-13">7</a> .)</p>

<p class="noindent chinese">当 “反向传播” 第一次出现在大街上时，连接主义者曾幻想着迅速学习越来越大的网络，直到在硬件允许的情况下，它们相当于人工大脑。结果并不是这样的。学习有一个隐藏层的网络还不错，但之后事情就变得非常困难了。只有在为应用（比如说字符识别）精心设计的情况下，几层的网络才能发挥作用。除此之外，“反向传播” 就失效了。随着我们层数的增加，错误信号变得越来越分散，就像一条河流分支成越来越小的支流，直到我们沦落到个别的雨滴，根本无法注册。像大脑一样用几十或几百个隐藏层进行学习，仍然是一个遥远的梦想，到 1990 年代中期，对多层感知器的兴奋已经逐渐消失了。一批核心的连接主义者继续前进，但总的来说，机器学习领域的注意力转移到了其他地方。（我们将在<a href="#babilu_link-6">第 6 章</a>和<a href="#babilu_link-13">第 7 章</a>中对这些地方进行调查）。</p>

<p class="noindent english">Today, however, connectionism is resurgent. We’re learning deeper networks than ever before, and they’re setting new standards in vision, speech recognition, drug discovery, and other areas. The new field of deep learning is on the front page of the <i>New York Times</i> . Look under the hood, and… surprise: it’s the trusty old backprop engine, still humming. What changed? Nothing much, say the critics: just faster computers and bigger data. To which Hinton and others reply: exactly, we were right all along!</p>

<p class="noindent chinese">然而，今天，连接主义正在重新崛起。我们正在学习比以前更深的网络，它们正在视觉、语音识别、药物发现和其他领域设立新的标准。深度学习的新领域出现在《<i>纽约时报</i>》的头版。看看引擎盖下，… 惊喜的是：它是值得信赖的老式反推引擎，仍然在嗡嗡作响。有什么变化？批评者说，没有什么变化：只是更快的计算机和更大的数据。对此，辛顿和其他人回答说：没错，我们一直都是对的</p>

<p class="noindent english"><a id="babilu_link-93"></a> In truth, connectionists have made genuine progress. One of the protagonists of this latest twist in the connectionist roller coaster is an unassuming little device called an autoencoder. An autoencoder is a multilayer perceptron whose output is the same as its input. In goes a picture of your grandmother and out comes—the same picture of your grandmother. At first this seems like a silly idea: What use could such a contraption possibly be? The key is to make the hidden layer much smaller than the input and output layers, so the network can’t just learn to copy the input to the hidden layer and the hidden layer to the output, in which case we may as well throw the whole thing out. But if the hidden layer is small, something interesting happens: the network is forced to encode the input in fewer bits, so it can be represented in the hidden layer, and then decode those bits back to full size. It could, for example, learn to encode a million-pixel image of your grandmother as just the seven-character word <i>grandma</i> , or some such short code invented by itself, and simultaneously learn to decode “grandma” into an image of dear old granny. So an autoencoder is not unlike a file compression tool, with two important advantages: it figures out how to compress things on its own, and like Hopfield networks, it can turn a noisy, distorted image into a nice clean one.</p>

<p class="noindent chinese">事实上，连接主义者已经取得了真正的进展。在连接主义过山车的最新转折中，主角之一是一个不起眼的小装置，叫做自动编码器。自动编码器是一个多层感知器，其输出与输入相同。输入的是你祖母的照片，输出的也是你祖母的照片。起初，这似乎是一个愚蠢的想法。这样的装置能有什么用？关键是要让隐藏层比输入层和输出层小得多，这样网络就不可能学会把输入复制到隐藏层，再把隐藏层复制到输出，这样的话，我们就可以把整个东西都扔掉。但如果隐藏层很小，就会发生一些有趣的事情：网络被迫以较少的比特对输入进行编码，因此它可以在隐藏层中得到体现，然后再将这些比特解码成全尺寸。例如，它可以学会将你祖母的百万像素图像编码为七个字符的 <i>grandma</i>，或一些自己发明的短代码，并同时学会将 “grandma” 解码为亲爱的老奶奶的图像。因此，自动编码器与文件压缩工具没有什么不同，它有两个重要的优点：它能自己找出如何压缩东西，而且像霍普菲尔德网络一样，它能把一个嘈杂的、扭曲的图像变成一个漂亮的干净的图像。</p>

<p class="noindent english">Autoencoders were known in the 1980s, but they were very hard to learn, even though they had a single hidden layer. Figuring out how to pack a lot of information into the same few bits is a hellishly difficult problem (one code for your grandmother, a slightly different one for your grandfather, another one for Jennifer Aniston, etc). The landscape in hyperspace is just too rugged to get to a good peak; the hidden units need to learn what amounts to too many exclusive-ORs of the inputs. So autoencoders didn’t really catch on. The trick that took over a decade to discover was to make the hidden layer larger than the input and output ones. Huh? Actually, that’s only half the trick: the other half is to force all but a few of the hidden units to be off at any given time. This still prevents the hidden layer from just copying the input, and—crucially—it makes learning much easier. If we allow different bits to represent different inputs, the inputs no longer have to compete to set <a id="babilu_link-144"></a> the same bits. Also, the network now has many more parameters, so the hyperspace you’re in has many more dimensions, and you have many more ways to get out of what would otherwise be local maxima. This is called a sparse autoencoder, and it’s a neat trick.</p>

<p class="noindent chinese">自动编码器在 20 世纪 80 年代就已为人所知，但它们非常难学，尽管它们只有一个隐藏层。弄清楚如何将大量的信息打包到同样的几个比特中是一个地狱般的困难问题（一个代码给你的祖母，一个稍微不同的代码给你的祖父，另一个给珍妮弗·安妮斯顿，等等）。超空间的景观实在是太崎岖了，无法达到一个好的峰值；隐藏单元需要学习相当于太多输入的排他性 OR。所以自动编码器并没有真正流行起来。花了十多年时间才发现的诀窍是使隐藏层大于输入和输出层。咦？实际上，这只是技巧的一半：另一半是强迫所有隐藏单元在任何时候都关闭，只有少数隐藏单元是关闭的。这仍然可以防止隐藏层只是复制输入，而且 —— 重要的是 —— 它使学习变得更加容易。如果我们允许不同的比特代表不同的输入，输入就不再需要竞争设置相同的比特。另外，网络现在有更多的参数，所以你所处的超空间有更多的维度，你有更多的方法来摆脱本来是局部最大值的东西。这被称为稀疏自动编码器，它是一个很好的技巧。</p>

<p class="noindent english">We haven’t seen any deep learning yet, though. The next clever idea is to stack sparse autoencoders on top of each other like a club sandwich. The hidden layer of the first autoencoder becomes the input/output layer of the second one, and so on. Because the neurons are nonlinear, each hidden layer learns a more sophisticated representation of the input, building on the previous one. Given a large set of face images, the first autoencoder learns to encode local features like corners and spots, the second uses those to encode facial features like the tip of a nose or the iris of an eye, the third one learns whole noses and eyes, and so on. Finally, the top layer can be a conventional perceptron that learns to recognize your grandmother from the high-level features provided by the layer below it—much easier than using only the crude information provided by a single hidden layer or than trying to backpropagate through all the layers at once. The Google Brain network of <i>New York Times</i> fame is a nine-layer sandwich of autoencoders and other ingredients that learns to recognize cats from YouTube videos. At one billion connections, it was at the time the largest network ever learned. It’s no surprise that Andrew Ng, one of the project’s principals, is also one of the leading proponents of the idea that human intelligence boils down to a single algorithm, and all we need to do is figure it out. Ng, whose affability belies a fierce ambition, believes that stacked sparse autoencoders can take us closer to solving AI than anything that came before.</p>

<p class="noindent chinese">不过，我们还没有看到任何深度学习。下一个聪明的想法是将稀疏自动编码器像俱乐部的三明治一样叠在一起。第一个自动编码器的隐藏层成为第二个自动编码器的输入/输出层，以此类推。由于神经元是非线性的，每个隐藏层都会在前一层的基础上学习更复杂的输入表示。给定一大组人脸图像，第一个自动编码器学习编码局部特征，如角落和斑点，第二个自动编码器使用这些编码面部特征，如鼻尖或眼睛的虹膜，第三个自动编码器学习整个鼻子和眼睛，以此类推。最后，顶层可以是一个传统的感知器，它从下面一层提供的高级特征中学习识别你的祖母 —— 这比只使用单个隐藏层提供的粗略信息或试图一次通过所有层进行反向传播要容易得多。以《<i>纽约时报</i>》闻名的谷歌大脑网络是一个由自动编码器和其他成分组成的九层三明治，它学会了从 YouTube 视频中识别猫咪。它有 10 亿个连接，在当时是有史以来最大的网络学习。该项目的负责人之一安德鲁·吴也是人类智能可归结为单一算法的主要支持者之一，而我们需要做的就是弄清楚这个算法，这一点并不奇怪。吴，他的和蔼可亲掩盖了强烈的野心，他相信堆叠的稀疏自动编码器可以使我们比以前的任何东西更接近解决人工智能。</p>

<p class="noindent english">Stacked autoencoders are not the only kind of deep learner. Another is based on Boltzmann machines, and another—convolutional neural networks—on a model of the visual cortex. Despite their remarkable successes, however, all of these are still a far cry from the brain. The Google network can recognize cat faces seen head on; humans can recognize cats in any pose and even when the face is hard to make out. The Google network is still pretty shallow; only three of its nine layers are autoencoders. A multilayer perceptron is a passable model of <a id="babilu_link-150"></a> the cerebellum, the part of the brain responsible for low-level motor control, but the cortex is another story. It’s missing the backward connections needed to propagate errors, for one, and yet it’s where the real learning wizardry resides. In his book <i>On Intelligence</i> , Jeff Hawkins advocated designing algorithms closely based on the organization of the cortex, but so far none of these algorithms can compete with today’s deep networks.</p>

<p class="noindent chinese">堆叠式自动编码器并不是唯一一种深度学习器。另一种是基于玻尔兹曼机的，还有一种是基于视觉皮层模型的卷积神经网络。尽管它们取得了显著的成功，然而，所有这些与大脑相比仍然相差甚远。谷歌网络可以识别正面看到的猫脸；而人类可以识别任何姿势的猫，甚至是难以辨认的脸。谷歌的网络仍然很浅；它的九层中只有三层是自动编码器。多层感知器是一个合格的模型，小脑是负责低级运动控制的部分，但大脑皮层是另一个故事。它缺少传播错误所需的后向连接，而这正是真正的学习技巧所在。杰夫·霍金斯在他的《<i>论智能</i>》（On Intelligence）一书中，主张密切根据大脑皮层的组织设计算法，但迄今为止，这些算法都无法与当今的深度网络竞争。</p>

<p class="noindent english">This may change as our understanding of the brain improves. Inspired by the human genome project, the new field of connectomics seeks to map every synapse in the brain. The European Union is investing a billion euros to build a soup-to-nuts model of it. America’s BRAIN initiative, with $100 million in funding in 2014 alone, has similar aims. Nevertheless, symbolists are very skeptical of this path to the Master Algorithm. Even if we can image the whole brain at the level of individual synapses, we (ironically) need better machine-learning algorithms to turn those images into wiring diagrams; doing it by hand is out of the question. Worse than that, even if we had a complete map of the brain, we would still be at a loss to figure out what it does. The nervous system of the <i>C. elegans</i> worm consists of only 302 neurons and was completely mapped in 1986, but we still have only a fragmentary understanding of what it does. We need higher-level concepts to make sense of the morass of low-level details, weeding out the ones that are specific to wetware or just quirks of evolution. We don’t build airplanes by reverse engineering feathers, and airplanes don’t flap their wings. Rather, airplane designs are based on the principles of aerodynamics, which all flying objects must obey. We still do not understand those analogous principles of thought.</p>

<p class="noindent chinese">随着我们对大脑认识的提高，这种情况可能会改变。受人类基因组计划的启发，新的连接组学领域寻求绘制大脑中每一个突触。欧盟正在投资 10 亿欧元来建立一个从汤到果的模型。美国的 BRAIN 计划，仅在 2014 年就有 1 亿美元的资金，有类似的目标。然而，象征主义者对这条通往主算法的道路非常怀疑。即使我们能够在单个突触的层面上对整个大脑进行成像，我们（具有讽刺意味的是）需要更好的机器学习算法来将这些图像转化为线路图；手工操作是不可能的。比这更糟糕的是，即使我们有一个完整的大脑地图，我们仍然会对它的作用感到茫然。<i>雅典娜</i>虫的神经系统仅由 302 个神经元组成，并在 1986 年被完全绘制出来，但我们对它的作用仍然只有零星的了解。我们需要更高层次的概念来理解低层次的细节，剔除那些特定于湿器或只是进化的怪癖的细节。我们并不是通过逆向工程来制造飞机，飞机也不会拍打翅膀。相反，飞机的设计是基于空气动力学原理的，所有飞行物体都必须遵守这些原理。我们仍然不了解这些类似的思想原则。</p>

<p class="noindent english">Perhaps connectomics is overkill. Some connectionists have been overheard claiming that backprop is the Master Algorithm and we just need to scale it up. But symbolists pour scorn on this notion. They point to a long list of things that humans can do but neural networks can’t. Take commonsense reasoning. It involves combining pieces of information that may have never been seen together before. Did Mary eat a shoe for lunch? No, because Mary is a person, people only eat edible <a id="babilu_link-193"></a> things, and shoes are not edible. Symbolic systems have no trouble with this—they just chain the relevant rules—but multilayer perceptrons can’t do it; once they’re done learning, they just compute the same fixed function over and over again. Neural networks are not compositional, and compositionality is a big part of human cognition. Another big issue is that humans—and symbolic models like sets of rules and decision trees—can explain their reasoning, while neural networks are big piles of numbers that no one can understand.</p>

<p class="noindent chinese">也许 connectomics 是矫枉过正。一些连接主义者无意中声称，“反向传播” 是主算法，我们只需要扩大它的规模。但符号主义者对这种观念嗤之以鼻。他们指出了一长串人类能做但神经网络做不到的事情。以常识性推理为例。它涉及到将以前可能从未见过的信息片段结合在一起。玛丽在午餐时吃了一只鞋吗？没有，因为玛丽是一个人，人们只吃可食用的而鞋子是不可食用的。符号系统在这方面没有问题 —— 它们只是把相关的规则连在一起 —— 但多层感知器却做不到；一旦它们完成了学习，它们只是重复计算同一个固定的函数。神经网络是没有构成性的，而构成性是人类认知的一个重要部分。另一个大问题是，人类以及规则集和决策树等符号模型可以解释他们的推理，而神经网络是一大堆数字，没有人能够理解。</p>

<p class="noindent english">But if humans have all these abilities that their brains didn’t learn by tweaking synapses, where did they come from? Unless you believe in magic, the answer must be evolution. If you’re a connectionism skeptic and you have the courage of your convictions, it behooves you to figure out how evolution learned everything a baby knows at birth—and the more you think is innate, the taller the order. But if you can figure it out and program a computer to do it, it would be churlish to deny that you’ve invented at least one version of the Master Algorithm.</p>

<p class="noindent chinese">但是，如果人类拥有所有这些能力，而他们的大脑并不是通过调整突触来学习的，那么它们是从哪里来的？除非你相信有魔法，否则答案一定是进化。如果你是一个连接主义的怀疑论者，而且你有勇气相信自己的信念，那么你就应该弄清楚进化论是如何学会一个婴儿出生时的所有知识的 —— 而且你认为是与生俱来的东西越多，顺序就越高。但是，如果你能搞清楚并为计算机编程，否认你至少发明了一个版本的主算法，那就太无耻了。</p>

</section>

</div>

</div>

<div id="babilu_link-329">

<div>

<section id="babilu_link-315">

<h1><a id="babilu_link-209"></a> <a href="#babilu_link-330">CHAPTER FIVE</a></h1>

<h1><a id="babilu_link-209"></a> <a href="#babilu_link-330">第五章</a></h1>

<h1><a href="#babilu_link-330">Evolution: Nature’s Learning Algorithm</a></h1>

<h1><a href="#babilu_link-330">进化：自然界的学习算法</a></h1>

<p class="noindent english">Robotic Park is a massive robot factory surrounded by ten thousand square miles of jungle, urban and otherwise. Ringing that jungle is the tallest, thickest wall ever built, bristling with sentry posts, searchlights, and gun turrets. The wall has two purposes: to keep trespassers out and the park’s inhabitants—millions of robots battling for survival and control of the factory—within. The winning robots get to spawn, their reproduction accomplished by programming the banks of 3-D printers inside. Step-by-step, the robots become smarter, faster—and deadlier. Robotic Park is run by the US Army, and its purpose is to evolve the ultimate soldier.</p>

<p class="noindent chinese">机器人公园是一个巨大的机器人工厂，周围是一万平方英里的丛林，包括城市和其他地方。环绕着这片丛林的是有史以来最高、最厚的墙，上面布满了哨所、探照灯和炮塔。这堵墙有两个目的：把闯入者挡在外面，把公园里的居民 —— 为生存和控制工厂而战斗的数百万机器人挡在里面。胜利的机器人可以产卵，它们的繁殖是通过内部的 3D 打印机编程完成的。一步步地，机器人变得更聪明、更快、更致命。机器人公园由美国陆军管理，其目的是为了进化出终极士兵。</p>

<p class="noindent english">Robotic Park doesn’t exist yet, but it may someday. I suggested it as a thought experiment at a DARPA workshop a few years ago, and one of the military brass present said matter-of-factly, “That’s feasible.” His willingness might seem less startling if you consider that the army already runs a full-blown mockup of an Afghan village in the California desert, complete with villagers, for training its troops, and a few billion dollars would be a small price to pay for the ultimate soldier.</p>

<p class="noindent chinese">机器人公园还不存在，但有一天它可能会存在。几年前，我在 DARPA 的一个研讨会上建议将其作为一个思想实验，在场的一位军方高层人士很认真地说：“这是可行的。” 如果你考虑到军队已经在加利福尼亚的沙漠中运行了一个完整的阿富汗村庄的模拟模型，并配有村民，用于训练其部队，那么他的意愿可能看起来不那么令人吃惊，几十亿美元对于最终的士兵来说是一个很小的代价。</p>

<p class="noindent english">The first steps toward Robotic Park have already been taken. Inside Hod Lipson’s Creative Machines Lab at Cornell University, fantastically <a id="babilu_link-44"></a> shaped robots are learning to crawl and fly, probably even as you read this. One looks like a slithering tower of rubber bricks, another like a helicopter with dragonfly wings, yet another like a shape-shifting Tinkertoy. These robots were not designed by any human engineer but created by evolution, the same process that gave rise to the diversity of life on Earth. Although the robots initially evolve inside a computer simulation, once they look proficient enough to make it in the real world, solid versions are automatically fabricated by 3-D printing. These are not yet ready to take over the world, but they’ve come a long way from the primordial soup of simulated parts they started with.</p>

<p class="noindent chinese">迈向机器人公园的第一步已经迈出。在康奈尔大学 Hod Lipson 的创意机器实验室里，形状奇特的机器人正在学习爬行和飞行，可能就在你读到这篇文章时。一个看起来像一个滑行的橡胶砖塔，另一个像一个有蜻蜓翅膀的直升机，还有一个像一个变形的 Tinkertoy。这些机器人不是由任何人类工程师设计的，而是由进化创造的，与地球上生命的多样性产生的过程相同。尽管这些机器人最初是在计算机模拟中进化的，但一旦它们看起来足够熟练，可以在现实世界中发挥作用，实体版本就会通过 3D 打印自动制造出来。这些机器人还没有准备好接管世界，但它们已经从一开始的模拟零件的原始汤中走了很长一段路。</p>

<p class="noindent english">The algorithm that evolved these robots was invented by Charles Darwin in the nineteenth century. He didn’t think of it as an algorithm at the time, partly because a key subroutine was still missing. Once James Watson and Francis Crick provided it in 1953, the stage was set for the second coming of evolution: <i>in silico</i> instead of <i>in vivo</i> , and a billion times faster. Its prophet was a ruddy-faced, perpetually grinning midwesterner by the name of John Holland.</p>

<p class="noindent chinese">进化这些机器人的算法是由查尔斯·达尔文在 19 世纪发明的。他当时并没有把它当作一种算法，部分原因是仍然缺少一个关键的子程序。一旦詹姆斯·沃森和弗朗西斯·克里克在 1953 年提供了这一程序，进化的第二次到来就有了舞台：<i>在硅中</i>而不是<i>在体内</i>，而且速度快了十亿倍。它的预言家是一个脸色红润、永远咧嘴笑的中西部人，名字叫约翰·霍兰。</p>

<h1 id="babilu_link-415"><b>Darwin’s algorithm</b></h1>

<h1 id="babilu_link-415"><b>达尔文的算法</b></h1>

<p class="noindent english">Like many other early machine-learning researchers, Holland started out working on neural networks, but his interests took a different turn when, while a graduate student at the University of Michigan, he read Ronald Fisher’s classic treatise <i>The Genetical Theory of Natural Selection</i> . In it, Fisher, who was also the founder of modern statistics, formulated the first mathematical theory of evolution. Brilliant as it was, Holland felt that Fisher’s theory left out the essence of evolution. Fisher considered each gene in isolation, but an organism’s fitness is a complex function of all its genes. If genes are independent, the relative frequencies of their variants rapidly converge to the maximum fitness point and remain in equilibrium thereafter. But if genes interact, evolution—the search for maximum fitness—is vastly more complex. With one thousand genes, each with two variants, the genome has 2<sup>1000</sup> possible states, and no planet in the universe is remotely large or ancient enough to <a id="babilu_link-95"></a> have tried them all out. Yet on Earth evolution has managed to come up with some remarkably fit organisms, and Darwin’s theory of natural selection explains how, at least qualitatively. Holland decided to turn it into an algorithm.</p>

<p class="noindent chinese">像许多其他早期的机器学习研究人员一样，霍兰开始研究神经网络，但当他在密歇根大学读研究生时，他的兴趣发生了不同的变化，他读了罗纳德·费希尔的经典论文《<i>自然选择的遗传学理论</i>》。在这本书中，费希尔，同时也是现代统计学的创始人，提出了第一个关于进化的数学理论。尽管很精彩，但霍兰认为费希尔的理论忽略了进化的本质。费雪认为每个基因都是孤立的，但一个生物体的健康状况是其所有基因的一个复杂功能。如果基因是独立的，其变体的相对频率会迅速收敛到最大适应点，并在此后保持平衡状态。但是如果基因相互作用，进化 —— 对最大适配性的寻求 —— 就会复杂得多。有一千个基因，每个基因有两个变体，基因组就有 2<sup>1000</sup> 种可能的状态，而宇宙中没有一个星球大到足以把它们都试过了。然而，在地球上，进化已经成功地提出了一些非常合适的生物体，而达尔文的自然选择理论至少在质量上解释了这一点。荷兰决定把它变成一种算法。</p>

<p class="noindent english">But first he had to graduate. Prudently, he picked a more conservative topic for his dissertation—Boolean circuits with cycles—and in 1959 he earned the world’s first PhD in computer science. His PhD advisor, Arthur Burks, nevertheless encouraged Holland’s interest in evolutionary computation and was instrumental in getting him a faculty job at Michigan and shielding him from senior colleagues who didn’t think that stuff was computer science. Burks himself was so open-minded because he had been a close collaborator of John von Neumann, who had proved the possibility of self-reproducing machines. Indeed, it had fallen to him to complete the work when von Neumann died of cancer in 1957. That von Neumann could prove that such machines are possible was quite remarkable, given the primitive state of genetics and computer science at the time. But his automaton just made exact copies of itself; evolving automata had to wait for Holland.</p>

<p class="noindent chinese">但首先他必须毕业。谨慎起见，他为他的论文选择了一个更保守的题目 —— 有周期的布尔电路，1959 年他获得了世界上第一个计算机科学博士学位。他的博士生导师亚瑟·伯克斯还是鼓励了霍兰对进化计算的兴趣，并帮助他在密歇根得到了一份教职，并使他免受那些不认为那是计算机科学的高级同事的影响。伯克斯本人之所以如此开明，是因为他曾是约翰·冯·诺伊曼的亲密合作者，他曾证明了自我复制机器的可能性。事实上，当冯·诺伊曼于 1957 年死于癌症时，这项工作就由他来完成。鉴于当时遗传学和计算机科学的原始状态，冯·诺伊曼能够证明这种机器是可能的，这一点相当了不起。但是他的自动机只是对自己进行了精确的复制；不断进化的自动机必须等待荷兰。</p>

<p class="noindent english">The key input to a genetic algorithm, as Holland’s creation came to be known, is a fitness function. Given a candidate program and some purpose it is meant to fill, the fitness function assigns the program a numeric score reflecting how well it fits the purpose. In natural selection, it’s questionable whether fitness can be interpreted this way: while the fitness of a wing for flight makes intuitive sense, evolution as a whole has no known purpose. Nevertheless, in machine learning having something like a fitness function is a no-brainer. If we need a program that can diagnose a patient, one that correctly diagnoses 60 percent of the patients in our database is better than one that only gets it right 55 percent of the time, and thus a possible fitness function is the fraction of correctly diagnosed cases.</p>

<p class="noindent chinese">遗传算法（霍兰的创造被称为遗传算法）的关键输入是一个适应函数。给出一个候选程序和它所要达到的一些目的，适应函数给该程序分配一个数字分数，反映它与目的的吻合程度。在自然选择中，是否可以用这种方式来解释适配性是值得怀疑的：虽然飞行的翅膀的适配性有直观的意义，但作为一个整体，进化没有已知的目的。然而，在机器学习中，拥有类似适应函数的东西是不费吹灰之力的。如果我们需要一个能够诊断病人的程序，一个能够正确诊断我们数据库中 60% 的病人的程序要比一个只能正确诊断 55% 的程序要好，因此一个可能的适应函数就是正确诊断的案例的比例。</p>

<p class="noindent english">In this regard, genetic algorithms are a lot like selective breeding. Darwin opened <i>The Origin of Species</i> with a discussion of it, as a stepping-stone to the more difficult concept of natural selection. All the domesticated plants and animals we take for granted today are the result <a id="babilu_link-212"></a> of selecting and mating, generation after generation, the organisms that best served our purposes: the corn with the largest corncobs, the sweetest fruit trees, the shaggiest sheep, the hardiest horses. Genetic algorithms do the same, except they breed programs instead of living creatures, and a generation is a few seconds of computer time instead of a creature’s lifetime.</p>

<p class="noindent chinese">在这方面，遗传算法很像选择性育种。达尔文在《<i>物种起源</i>》中开篇就讨论了这个问题，作为通往更困难的自然选择概念的垫脚石。我们今天认为理所当然的所有被驯化的植物和动物都是一代又一代地选择和交配最符合我们目的的生物体：有最大玉米棒的玉米，最甜的果树，最蓬松的羊，最坚韧的马。遗传算法也是如此，只是它们培育的是程序而不是生物，而且一代是几秒钟的计算机时间而不是生物的一生。</p>

<p class="noindent english">The fitness function encapsulates the human’s role in the process. But the more subtle part is nature’s. Starting with a population of not-very-fit individuals—possibly completely random ones—the genetic algorithm has to come up with variations that can then be selected according to fitness. How does nature do that? Darwin didn’t know. This is where the genetic part of the algorithm comes in. In the same way that DNA encodes an organism as a sequence of base pairs, we can encode a program as a string of bits. Instead of 0 and 1, the DNA alphabet has four characters—the four bases adenine, thymine, cytosine, and guanine—but that’s a superficial difference. Variations, whether in DNA sequences or bit strings, can be generated in several ways. The simplest approach is point mutation, flipping a random bit in the string or changing a single base in a stretch of DNA. But for Holland, the real power of genetic algorithms lay in something more complicated: sex.</p>

<p class="noindent chinese">适应功能概括了人类在这个过程中的作用。但更微妙的部分是大自然的作用。遗传算法从一群不太合适的个体 —— 可能是完全随机的个体 —— 开始，必须想出一些变化，然后根据适合度进行选择。自然界是如何做到这一点的？达尔文也不知道。这就是算法的遗传部分的作用。与 DNA 将生物体编码为碱基对序列的方式一样，我们可以将程序编码为一串比特。DNA 字母表没有 0 和 1，而是有四个字符 —— 四个碱基腺嘌呤、胸腺嘧啶、胞嘧啶和鸟嘌呤，但这只是表面上的区别。无论是 DNA 序列还是比特串中的变异，都可以通过几种方式产生。最简单的方法是点突变，翻转字符串中的一个随机位或改变一段 DNA 中的一个碱基。但对霍兰来说，遗传算法的真正力量在于更复杂的东西：性。</p>

<p class="noindent english">Stripped down to its bare essentials (no giggles, please), sexual reproduction consists of swapping material between chromosomes from the mother and father, a process called crossing over. This produces two new chromosomes, one of which consists of the mother’s chromosome up to the crossover point and the father’s thereafter, and the other one is the opposite:</p>

<p class="noindent chinese">从本质上讲（请不要傻笑），有性生殖包括来自母亲和父亲的染色体之间的物质交换，这一过程称为交叉。这就产生了两条新的染色体，其中一条由母亲的染色体到交叉点和父亲的染色体组成，而另一条则相反。</p>

<div>

<div>

<img alt="image" src="images/000034.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-251"></a> A genetic algorithm works by mimicking this process. In each generation, it mates the fittest individuals, producing two offspring from each pair of parents by crossing over their bit strings at a random point. After applying point mutations to the new strings, it lets them loose in its virtual world. Each one returns with a fitness score, and the process repeats. Each generation is fitter than the previous one, and the process terminates when the desired fitness is reached or time runs out.</p>

<p class="noindent chinese">遗传算法通过模仿这一过程来工作。在每一代中，它与最合适的个体交配，通过在随机点上交叉它们的位串，从每对父母中产生两个后代。在对新的字符串进行点突变后，它让它们在其虚拟世界中自由活动。每个后代都有一个适配分数，这个过程不断重复。每一代都比上一代更适合，当达到所需的适合度或时间耗尽时，该过程就会终止。</p>

<p class="noindent english">For example, suppose we want to evolve a rule for filtering spam. If ten thousand different words appear in the training data, each candidate rule can be represented by a string of twenty thousand bits, two for each word. The first bit corresponding to the word <i>free</i> is one if e-mails containing <i>free</i> are allowed to match the rule, and zero if they’re not. The second bit is the opposite: one if e-mails <i>not</i> containing <i>free</i> are allowed to match, and zero if they’re not. So if both bits are one, e-mails are allowed to match the rule regardless of whether they contain <i>free</i> , and the rule effectively has no condition on that word. On the other hand, if both bits are zero, no e-mails match the rule, since one or the other bit always fails, and all e-mails get through the filter (yikes). Overall, an e-mail matches a rule only if its entire pattern of present and absent words is allowed by the rule. A rule’s fitness is, say, the percentage of e-mails it classifies correctly. Starting from a population of random strings, each representing a rule with random conditions, the genetic algorithm can now evolve better and better rules by repeatedly crossing over and mutating the fittest strings in each generation. For example, if the current population includes the rules <i>If the e-mail contains the word</i> free <i>then it’s spam</i> and <i>If the e-mail contains the word</i> easy <i>then it’s spam,</i> crossing them over will yield the probably fitter rule <i>If the e-mail contains</i> free <i>and</i> easy <i>then it’s spam,</i> provided the crossover point does not fall between the two bits corresponding to one of those words. It will also yield the rule <i>All e-mail is spam,</i> which results from dropping both conditions, but that rule is unlikely to have much progeny in the next generation.</p>

<p class="noindent chinese">例如，假设我们想演化出一条过滤垃圾邮件的规则。如果训练数据中出现了一万个不同的词，那么每个候选规则可以用一串两万个比特来表示，每个词有两个。如果含有 <i>free</i> 的电子邮件被允许与规则相匹配，那么对应于 <i>free</i> 这个词的第一个比特就是 1，如果不允许，就是 0。第二位则相反：如果<i>不含</i> <i>free</i> 的电子邮件被允许匹配，则为 1，如果不匹配则为 0。因此，如果两个位都是 1，那么无论电子邮件是否包含 <i>free</i>，都允许与规则相匹配，而规则实际上对这个词没有条件。另一方面，如果两个比特都是零，就没有电子邮件符合规则，因为其中一个比特总是失败，所有的电子邮件都能通过过滤器（哎呀）。总的来说，只有当一封电子邮件的整个存在和不存在的词的模式被规则所允许时，它才符合规则。一个规则的适用性是，比如说，它正确分类的电子邮件的百分比。遗传算法从一个随机字符串的种群开始，每个字符串代表一个具有随机条件的规则，现在可以通过在每一代中反复交叉和变异最适合的字符串来进化出更好的规则。例如，如果当前的群体包括这样的规则：<i>如果电子邮件中含有</i> free 这个 <i>词</i>，<i>那么它就是垃圾邮件</i>；<i>如果电子邮件中含有</i> easy 这个词，<i>那么它就是垃圾邮件；</i>如果交叉点不在这两个词对应的两个位之间，那么交叉点将产生可能更适合的规则：<i>如果电子邮件中含有</i> free <i>和</i> easy，<i>那么它就是垃圾邮件</i>。它也会产生<i>所有的电子邮件都是垃圾邮件</i>的规则，这是放弃这两个条件的结果，但是这个规则不太可能在下一代有很多后代。</p>

<p class="noindent english"><a id="babilu_link-279"></a> Since our goal is to produce the best spam filter we can, as opposed to faithfully simulating real natural selection, we can cheat liberally by modifying the algorithm to fit our needs. One way in which genetic algorithms routinely cheat is by allowing immortality. (Too bad we can’t do that in real life.) That way, a highly fit individual doesn’t simply compete to reproduce within its own generation, but also with its children, and then its grandchildren, great-grandchildren, and so on, as long as it remains one of the fittest individuals in the population. In contrast, in the real world the best a highly fit individual can do is pass on half its genes to many children, each of which will probably be less fit because of the genes it inherited from its other parent. Immortality avoids this backsliding and with any luck, lets the algorithm reach the desired fitness sooner. Of course, since the fittest humans in history as measured by number of descendants are the likes of Genghis Khan—ancestor to one in two hundred men alive today—perhaps it’s not so bad that in real life immortality is <i>verboten</i> .</p>

<p class="noindent chinese">由于我们的目标是生产最好的垃圾邮件过滤器，而不是忠实地模拟真实的自然选择，所以我们可以通过修改算法来自由地作弊，以适应我们的需要。遗传算法经常作弊的一种方式是允许永生。（太糟糕了，我们不能在现实生活中这样做。）这样一来，一个高度适合的个体不只是在自己的一代中竞争繁殖，而且还与它的孩子，然后是它的孙子，曾孙，等等，只要它仍然是人群中最适合的个体之一。相反，在现实世界中，一个高度适合的个体所能做的最好的事情就是把它的一半基因传给许多孩子，而每一个孩子都可能因为从另一个父母那里继承的基因而变得不太适合。永生避免了这种倒退，如果运气好的话，可以让算法更快达到理想的适配度。当然，由于历史上以后代数量衡量的最合适的人类是成吉思汗这样的人 —— 他是今天每两百个人中就有一个人的祖先 —— 也许在现实生活中<i>不朽</i>并不是那么糟糕。</p>

<p class="noindent english">If we want to evolve a whole set of spam-filtering rules, not just one, we can represent a candidate set of <i>n</i> rules by a string of <i>n</i> × 20,000 bits (20,000 for each rule, assuming ten thousand different words in the data, as before). Rules containing 00 for some word effectively disappear from the rule set, since they don’t match any e-mails, as we saw before. If an e-mail matches any rule in the set, it’s classified as spam; otherwise it’s legit. We can still let fitness be the percentage of correctly classified e-mails, but to combat overfitting, we’ll probably want to subtract from it a penalty proportional to the total number of active conditions in the rule set.</p>

<p class="noindent chinese">如果我们想演化出一整套垃圾邮件过滤规则，而不仅仅是一个，我们可以用一个 <i>n</i> × 20,000 位的字符串来表示一个 <i>n 个</i>规则的候选集（每个规则 20,000 位，假设数据中有一万个不同的词，如前所述）。含有 00 的某个词的规则实际上从规则集中消失了，因为它们不匹配任何电子邮件，正如我们之前看到的那样。如果一封邮件与规则集中的任何规则相匹配，它就被归类为垃圾邮件；否则就是合法的。我们仍然可以让 fitness 成为正确分类的电子邮件的百分比，但是为了防止过度拟合，我们可能想从它那里减去一个与规则集中有效条件总数成比例的惩罚。</p>

<p class="noindent english">We can get even fancier by allowing rules for intermediate concepts to evolve, and then chaining these rules at performance time. For example, we could evolve the rules <i>If the e-mail contains the word</i> loan <i>then it’s a scam</i> and <i>If the e-mail is a scam then it’s spam</i> . Since a rule’s consequent is no longer always <i>spam</i> , this requires introducing additional bits in rule strings to represent their consequents. Of course, the computer doesn’t literally use the word <i>scam</i> ; it just comes up with some arbitrary bit string to represent the concept, but that’s good enough for our <a id="babilu_link-157"></a> purposes. Sets of rules like this, which Holland called classifier systems, are one of the workhorses of the machine-learning tribe he founded: the evolutionaries. Like multilayer perceptrons, classifier systems face the credit-assignment problem—what is the fitness of rules for intermediate concepts?—and Holland devised the so-called bucket brigade algorithm to solve it. Nevertheless, classifier systems are much less widely used than multilayer perceptrons.</p>

<p class="noindent chinese">我们可以通过允许中间概念的规则演化，然后在表现时间将这些规则连锁起来，从而变得更加复杂。例如，我们可以演化出这样的规则<i>：如果电子邮件包含</i>贷款<i>这个词</i>，<i>那么它就是一个骗局</i>；<i>如果电子邮件是骗局，那么它就是垃圾邮件</i>。由于规则的结果不再总是<i>垃圾邮件</i>，这就需要在规则字符串中引入额外的比特来表示其结果。当然，计算机并没有从字面上使用<i>诈骗</i>这个词；它只是想出了一些任意的位串来表示这个概念，但这对于我们。像这样的规则集，霍兰称之为分类器系统，是他创立的机器学习部落的主力军之一：进化者。像多层感知器一样，分类器系统也面临着信用分配问题 —— 什么是中间概念的规则的适用性？尽管如此，分类器系统的应用远不如多层感知器广泛。</p>

<p class="noindent english">Compared to the simple model in Fisher’s book, genetic algorithms are quite a leap forward. Darwin lamented his lack of mathematical ability, but if he had lived a century later he probably would have yearned for programming prowess instead. Indeed, capturing natural selection by a set of equations is extremely difficult, but expressing it as an algorithm is another matter, and can shed light on many otherwise vexing questions. Why do species appear suddenly in the fossil record? Where’s the evidence that they evolved gradually from earlier species? In 1972, Niles Eldredge and Stephen Jay Gould proposed that evolution consists of a series of “punctuated equilibria,” alternating long periods of stasis with short bursts of rapid change, like the Cambrian explosion. This sparked a heated debate, with critics of the theory nicknaming it “evolution by jerks” and Eldredge and Gould retorting that gradualism is “evolution by creeps.” Experience with genetic algorithms lends support to the jerks. If you run a genetic algorithm for one hundred thousand generations and observe the population at one-thousand-generation intervals, the graph of fitness against time will probably look like an uneven staircase, with sudden improvements followed by flat periods that tend to become longer over time. It’s also not hard to see why. Once the algorithm reaches a local maximum of fitness—a peak in the fitness landscape—it will stay there for a long time until a lucky mutation or crossover lands an individual on the slope to a higher peak, at which point that individual will multiply and climb up the slope with each passing generation. And the higher the current peak, the longer before that happens. Of course, natural evolution is more complicated than this: for one, the environment may change, either physically or because other organisms have themselves evolved, and an organism that was on <a id="babilu_link-103"></a> a fitness peak may suddenly find itself under pressure to evolve again. So, while helpful, current genetic algorithms are far from the end of the story.</p>

<p class="noindent chinese">与费希尔书中的简单模型相比，遗传算法是一个相当大的飞跃。达尔文感叹自己缺乏数学能力，但如果他生活在一个世纪之后，他可能会渴望拥有编程的能力。的确，用一组方程来捕捉自然选择是非常困难的，但用算法来表达则是另一回事，它可以揭示许多原本令人困惑的问题。为什么物种会突然出现在化石记录中？哪里有证据表明它们是从早期物种逐渐进化而来的？1972 年，奈尔斯·埃尔德里奇和斯蒂芬·杰伊·古尔德提出，进化是由一系列的 “点状平衡” 组成的，长时间的停滞与短时间的快速变化交替进行，就像寒武纪的爆炸。这引发了一场激烈的辩论，该理论的批评者将其称为 “笨蛋的进化”，而埃尔德里奇和古尔德则反驳说，渐进主义是 “小人的进化”。遗传算法的经验为 “混蛋” 提供了支持。如果你运行一个遗传算法十万代，并在一千代的时间间隔内观察种群，适应性与时间的关系图可能看起来像一个不平坦的楼梯，突然的改善之后是平坦期，随着时间的推移，平坦期往往会变得更长。这也不难看出原因。一旦算法达到一个局部的最大适配度 —— 适配度景观中的一个峰值 —— 它将在那里停留很长时间，直到一个幸运的突变或杂交使一个个体在斜坡上达到一个更高的峰值，在这一点上，这个个体将随着每一代人的成长而繁殖并爬上斜坡。而当前的峰值越高，发生这种情况的时间就越长。当然，自然进化比这更复杂：其一，环境可能会发生变化，要么是物理变化，要么是因为其他生物体本身已经进化，而一个正在适应高峰的生物体可能突然发现自己面临再次进化的压力。因此，尽管有帮助，但目前的遗传算法远不是故事的终点。</p>

<h1 id="babilu_link-416"><b>The exploration-exploitation dilemma</b></h1>

<h1 id="babilu_link-416"><b>勘探·开发的两难境地</b></h1>

<p class="noindent english">Notice how much genetic algorithms differ from multilayer perceptrons. Backprop entertains a single hypothesis at any given time, and the hypothesis changes gradually until it settles into a local optimum. Genetic algorithms consider an entire population of hypotheses at each step, and these can make big jumps from one generation to the next, thanks to crossover. Backprop proceeds deterministically after setting the initial weights to small random values. Genetic algorithms, in contrast, are full of random choices: which hypotheses to keep alive and cross over (with fitter hypotheses being more likely candidates), where to cross two strings, which bits to mutate. Backprop learns weights for a predefined network architecture; denser networks are more flexible but also harder to learn. Genetic algorithms make no a priori assumptions about the structures they will learn, other than their general form.</p>

<p class="noindent chinese">请注意遗传算法与多层感知器有多大区别。“反向传播” 在任何时候都只接受一个假设，而且这个假设会逐渐改变，直到它进入一个局部最优。遗传算法在每一步都考虑整个假说群体，由于交叉作用，这些假说可以从一代跳到另一代。在将初始权重设置为小的随机值之后，“反向传播” 是确定地进行的。相比之下，遗传算法充满了随机选择：哪些假设要保持活力和交叉（更适合的假设更有可能成为候选人），在哪里交叉两个字符串，哪些位要变异。“反向传播” 为预定的网络结构学习权重；密集的网络更灵活，但也更难学习。遗传算法对它们要学习的结构没有任何先验的假设，除了它们的一般形式之外。</p>

<p class="noindent english">Because of all this, genetic algorithms are much less likely than backprop to get stuck in a local optimum and in principle better able to come up with something truly new. But they are also much more difficult to analyze. How do we know a genetic algorithm will get somewhere meaningful instead of randomly walking around like the proverbial drunkard? The key is to think in terms of building blocks. Every subset of a string’s bits potentially encodes a useful building block, and when we cross over two strings, those building blocks come together into a larger one, which in turn becomes grist for the mill. Holland likes to use police sketches to illustrate the power of building blocks. In the days before computers, a police artist could quickly put together a portrait of a suspect from eyewitness interviews by selecting a mouth from a set of paper strips depicting typical mouth shapes and doing the same for the eyes, nose, chin, and so on. With only ten building blocks and <a id="babilu_link-111"></a> ten options for each, this system would allow for ten billion different faces, more than there are people on Earth.</p>

<p class="noindent chinese">正因为如此，遗传算法比 “反向传播” 更不可能陷入局部最优状态，而且原则上更有能力想出真正的新东西。但它们也更难分析。我们怎么知道一个遗传算法会得到有意义的东西，而不是像传说中的醉汉一样随意走动？关键是要从构建模块的角度来思考。一个字符串的每一个比特子集都可能编码一个有用的积木，当我们把两个字符串交叉在一起时，这些积木就会汇聚成一个更大的积木，而这又会成为磨坊的磨料。荷兰喜欢用警察的草图来说明积木的力量。在没有电脑的年代，警察艺术家可以从目击者的采访中快速拼凑出嫌疑人的肖像，方法是从一组描绘典型嘴形的纸条中选择一张嘴，然后对眼睛、鼻子、下巴等做同样的处理。只有 10 个构件和每个构件的 10 个选项，这个系统将允许 100 亿张不同的脸，比地球上的人还多。</p>

<p class="noindent english">In machine learning, as elsewhere in computer science, there’s nothing better than getting such a combinatorial explosion to work for you instead of against you. What’s clever about genetic algorithms is that each string implicitly contains an exponential number of building blocks, known as schemas, and so the search is a lot more efficient than it seems. This is because every subset of the string’s bits is a schema, representing some potentially fit combination of properties, and a string has an exponential number of subsets. We can represent a schema by replacing the bits in the string that aren’t part of it with *. For example, the string 110 contains the schemas ***, **0, *1*, 1**, *10, 11*, 1*0, and 110. We get a different schema for every different choice of bits to include; since we have two choices for each bit (include/don’t include), we have 2<sup><i>n</i></sup> schemas. Conversely, a particular schema may be represented in many different strings in a population, and is implicitly evaluated every time they are. Suppose that a hypothesis’s probability of surviving into the next generation is proportional to its fitness. Holland showed that, in this case, the fitter a schema’s representatives in one generation are compared to the average, the more of them we can expect to see in the next generation. So, while the genetic algorithm explicitly manipulates strings, it implicitly searches the much larger space of schemas. Over time, fitter schemas come to dominate the population, and so unlike the drunkard, the genetic algorithm finds its way home.</p>

<p class="noindent chinese">在机器学习中，就像计算机科学的其他领域一样，没有什么比让这样的组合爆炸为你工作而不是反对你更好了。遗传算法的聪明之处在于，每个字符串都隐含着指数级数量的构件，被称为模式，因此搜索的效率比它看起来要高很多。这是因为字符串的每一个比特子集都是一个模式，代表一些潜在的合适的属性组合，而一个字符串有指数级的子集。我们可以通过用*替换字符串中不属于它的位来表示一个模式。例如，字符串 110 包含模式 ***、**0、*1*、1**、*10、11*、1*0 和 110。我们为每一个不同的位的选择得到一个不同的模式；因为我们对每个位有两个选择（包括/不包括），所以我们有 2<sup><i>n</i></sup>模式。相反，一个特定的模式可能在人群中以许多不同的字符串表示，并且每次都会被隐含地评估。假设一个假说存活到下一代的概率与它的适配性成正比。霍兰表明，在这种情况下，一个模式在一代中的代表与平均水平相比越适合，我们就可以期望在下一代中看到越多的代表。因此，虽然遗传算法明确地操纵了字符串，但它隐含地搜索了大得多的模式空间。随着时间的推移，更适合的模式会在群体中占据主导地位，因此与醉汉不同，遗传算法会找到自己的归宿。</p>

<p class="noindent english">One of the most important problems in machine learning—and life—is the exploration-exploitation dilemma. If you’ve found something that works, should you just keep doing it? Or is it better to try new things, knowing it could be a waste of time but also might lead to a better solution? Would you rather be a cowboy or a farmer? Start a company or run an existing one? Go steady or play the field? A midlife crisis is the yearning to explore after many years spent exploiting. On an impulse, you fly to Vegas, ready to gamble away your life’s savings on the chance of becoming a millionaire. You enter the first casino and <a id="babilu_link-187"></a> face a row of slot machines. The one to play is the one that gives you the best payoff on average, but you don’t know which that is. You have to try each one enough times to figure it out. But if you do this for too long, you waste your money on losing machines. Conversely, if you jump the gun and pick a machine that looked good by chance on the first few turns but is in fact not the best one, you waste your money playing it for the rest of the night. That’s the exploration-exploitation dilemma. Each time you play, you have to choose between repeating the best move you’ve found so far, which gives you the best payoff, or trying other moves, which gather information that may lead to even better payoffs. With two slot machines, Holland showed that the optimal strategy is to flip a biased coin each time, where the coin becomes exponentially more biased as you go along. (Don’t sue me if it doesn’t work for you, though. Remember the house always wins in the end.) The better a slot machine looks, the more you should play it, but never completely give up on the other one, in case it turns out to be the best one after all.</p>

<p class="noindent chinese">机器学习和生活中最重要的问题之一是探索·开发的困境。如果你已经找到了有效的东西，你应该继续做下去吗？还是尝试新的东西，知道这可能是浪费时间，但也可能导致一个更好的解决方案？你愿意做一个牛仔还是一个农民？创办公司还是经营现有公司？稳扎稳打还是打野战？中年危机是在多年的探索之后，对探索的渴望。一时冲动，你飞到拉斯维加斯，准备把一生的积蓄都赌掉，争取成为一个百万富翁。你进入第一家赌场，面对一排老虎机。要玩的是平均回报率最高的那台，但你不知道那是哪台。你必须每台机器都试够了才知道。但如果你这样做的时间太长，你就会把钱浪费在亏损的机器上。相反，如果你跳枪，选了一台在前几轮偶然看起来不错的机器，但实际上不是最好的机器，你就会浪费你的钱在其余的时间里玩它。这就是探索·开发的两难问题。每次玩的时候，你必须选择重复你目前发现的最好的招数，这给你带来最好的回报，还是尝试其他的招数，这收集的信息可能导致更好的回报。霍兰用两台老虎机表明，最佳策略是每次抛出一枚有偏向性的硬币，随着你的前进，硬币的偏向性会呈指数增长。（如果它对你不起作用，请不要起诉我。记住，最后房子总是赢的）。一台老虎机看起来越好，你就越应该玩它，但永远不要完全放弃另一台老虎机，以防它终究是最好的老虎机。</p>

<p class="noindent english">A genetic algorithm is like the ringleader of a group of gamblers, playing slot machines in every casino in town at the same time. Two schemas compete with each other if they include the same bits and differ in at least one of them, like *10 and *11, and <i>n</i> competing schemas are like <i>n</i> slot machines. Every set of competing schemas is a casino, and the genetic algorithm simultaneously figures out the winning machine in every casino, following the optimal strategy of playing the better-seeming machines with exponentially increasing frequency. Pretty smart.</p>

<p class="noindent chinese">遗传算法就像一群赌徒的头目，同时在城里的每个赌场玩老虎机。如果两个模式包括相同的比特，并且至少在其中一个比特上有差异，就会相互竞争，比如 *10 和 *11，而 <i>n 个</i>竞争的模式就像 <i>n 个</i>老虎机。每一组竞争的模式都是一个赌场，遗传算法同时找出每个赌场的赢家机器，遵循最佳策略，以指数级增加的频率玩看起来更好的机器。相当聪明。</p>

<p class="noindent english">In <i>The Hitchhiker’s Guide to the Galaxy</i> , an alien race builds a massive supercomputer to answer the ultimate question, and after a long time the computer spits out “42.” But the computer also points out that the aliens don’t know what the question is, so they build an even bigger computer to figure that out. This computer—otherwise known as planet Earth—is unfortunately destroyed to make way for a space freeway minutes before finishing its multimillion-year computation. We can only guess at the question now, but perhaps it was: Which slot machine should you play?</p>

<p class="noindent chinese">在《<i>银河系搭车指南</i>》中，一个外星种族建造了一台巨大的超级计算机来回答终极问题，经过很长一段时间，计算机吐出了 “42”。但计算机也指出，外星人不知道问题是什么，所以他们建造了一台更大的计算机来解决这个问题。这台电脑 —— 也就是地球 —— 在完成其数百万年的计算前几分钟不幸被毁，为一条太空高速公路让路。我们现在只能猜测这个问题，但也许它是。你应该玩哪台老虎机？</p>

<h1 id="babilu_link-417"><b><a id="babilu_link-148"><b></b></a> Survival of the fittest programs</b></h1>

<h1 id="babilu_link-417"><b><a id="babilu_link-148"><b></b></a>适者生存项目</b></h1>

<p class="noindent english">For the first few decades, the genetic algorithms community consisted mainly of John Holland, his students, and their students. Circa 1983, the biggest problem genetic algorithms had been able to solve was learning to control gas pipeline systems. But then, at around the same time neural networks were making their comeback, interest in evolutionary computation took off. The first international conference on genetic algorithms was held in Pittsburgh in 1985, and a Cambrian explosion of genetic algorithm variants was under way. Some of these tried to model evolution more closely—the basic genetic algorithm was only a very crude approximation, after all—and others radiated in very different directions, crossing over evolutionary ideas with computer science concepts that would have bemused Darwin.</p>

<p class="noindent chinese">在最初的几十年里，遗传算法界主要由约翰·霍兰和他的学生以及他们的学生组成。1983 年左右，遗传算法所能解决的最大问题是学习控制天然气管道系统。但后来，大约在神经网络卷土重来的同时，人们对进化计算的兴趣也开始上升。第一届遗传算法国际会议于 1985 年在匹兹堡举行，遗传算法变体的寒武纪爆炸正在进行中。其中一些试图更紧密地模拟进化 —— 毕竟基本的遗传算法只是一个非常粗略的近似 —— 而另一些则向非常不同的方向辐射，将进化思想与计算机科学概念交叉，这将使达尔文感到困惑。</p>

<p class="noindent english">One of Holland’s more remarkable students was John Koza. In 1987, while flying back to California from a conference in Italy, he had a lightbulb moment. Instead of evolving comparatively simple things like <i>If</i> … <i>then</i> … rules and gas pipeline controllers, why not evolve full-blown computer programs? And if that’s the goal, why stick with bit strings as the representation? A program is really a tree of subroutine calls, so better to directly cross over those subtrees than to shoehorn them into bit strings and run the risk of destroying perfectly good subroutines when you cross them over at a random point.</p>

<p class="noindent chinese">约翰·科扎是霍兰更杰出的学生之一。1987 年，当他从意大利的一个会议上飞回加利福尼亚时，他有了一个闪光点。与其进化相对简单的东西，如 <i>If</i>… <i>then</i>… 规则和天然气管道控制器，为什么不进化出完整的计算机程序？如果这是个目标，为什么要坚持用比特串来表示？一个程序实际上是一棵由子程序调用组成的树，所以最好是直接跨越这些子树，而不是把它们塞进比特串，当你随意跨越它们时，有可能会破坏完美的子程序。</p>

<p class="noindent english">For example, suppose you want to evolve a program to compute the duration of a planet’s year, <i>T,</i> from its average distance to the sun, <i>D</i> . According to Kepler’s third law, <i>T</i> is the square root of <i>D</i> cubed, times a constant <i>C</i> that depends on the units you use for time and distance. A genetic algorithm should be able to discover this by looking at Tycho Brahe’s data on planetary motions like Kepler did. In Koza’s approach, <i>D</i> and <i>C</i> are the leaves of a program tree, and the operations that combine them, like multiplication and taking the square root, are the internal nodes. The following program tree correctly computes <i>T</i> :</p>

<p class="noindent chinese">例如，假设你想开发一个程序来计算一个行星一年的时间，<i>T</i>，从它到太阳的平均距离，<i>D</i>。根据开普勒第三定律，<i>T</i> 是 <i>D</i> 的平方根乘以一个常数 <i>C</i>，这个常数取决于你使用的时间和距离单位。遗传算法应该能够像开普勒那样通过查看第谷·布拉赫的行星运动数据来发现这一点。在 Koza 的方法中，<i>D</i> 和 <i>C</i> 是程序树的叶子，而结合它们的操作，如乘法和取平方根，是内部节点。下面的程序树正确地计算了 <i>T</i>。</p>

<div>

<div>

<img alt="image" src="images/000017.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-252"></a> In genetic programming, as Koza called his method, we cross over two program trees by randomly swapping two of their subtrees. For example, crossing over these two trees at the highlighted nodes yields the correct program for computing <i>T</i> as one of the children:</p>

<p class="noindent chinese">在遗传编程中，正如 Koza 所称的那样，我们通过随机交换两棵程序树中的两棵子树来进行交叉。例如，在突出显示的节点上交叉这两棵树，可以得到计算 <i>T</i> 的正确程序，作为其中的一个子程序。</p>

<div>

<div>

<img alt="image" src="images/000018.jpg"/>

</div>

</div>

<p class="noindent english">We can measure a program’s fitness (or lack thereof) by the distance between its output and the correct one on the training data. For example, if the program says an Earth year is three hundred days, that would subtract sixty-five points from its fitness. Starting with a population of random program trees, genetic programming uses crossover, mutation, and survival to gradually evolve better programs until it’s satisfied.</p>

<p class="noindent chinese">我们可以通过它的输出与训练数据上的正确输出之间的距离来衡量一个程序的适用性（或缺乏适用性）。例如，如果程序说一个地球年是三百天，这将从它的适配性中减去六十五分。从一个随机的程序树群开始，遗传编程使用交叉、变异和生存来逐渐进化出更好的程序，直到它满意为止。</p>

<p class="noindent english">Of course, computing the length of a planet’s year is a very simple problem, involving only multiplication and square roots. In general, program trees can include the full range of programming constructs, such as <i>If</i> … <i>then</i> … statements, loops, and recursion. A more <a id="babilu_link-240"></a> illustrative example of what genetic programming can do is figuring out the sequence of actions a robot needs to perform to achieve some goal. Suppose I ask my officebot to bring me a stapler from the closet down the hall. The robot has a large set of behaviors available to it, such as moving down a hallway, opening a door, picking up an object, and so on. Each of these can in turn be composed of various sub-behaviors: move the robot’s hand toward the object, or grasp it at various possible points, for example. Each behavior may be executed or not depending on the results of previous behaviors, may need to be repeated some number of times, and so on. The challenge is to assemble the right structure of behaviors and sub-behaviors, together with the parameters for each, such as how far to move the hand. Starting with the robot’s “atomic” behaviors and their allowed combinations, genetic programming can assemble a complex behavior that accomplishes the desired goal. A number of researchers have evolved strategies for robot soccer players in this way.</p>

<p class="noindent chinese">当然，计算一个星球的年限是一个非常简单的问题，只涉及乘法和平方根。一般来说，程序树可以包括全部的编程结构，如 <i>If</i>… <i>then</i>… 语句、循环和递归。遗传编程可以做的一个更多的说明性的例子是找出一个机器人需要执行的行动序列来实现一些目标。假设我要求我的办公机器人从走廊尽头的壁橱里给我拿一个订书机。该机器人有一大套可用的行为，如在走廊上移动、开门、拿起一个物体等等。这些行为中的每一个又可以由各种子行为组成：例如，将机器人的手向物体移动，或者在各种可能的点上抓住它。每个行为的执行与否都取决于之前行为的结果，可能需要重复一定的次数，等等。挑战在于如何将正确的行为和子行为结构，以及每个行为的参数，如手要移动多远，结合起来。从机器人的 “原子” 行为及其允许的组合开始，遗传编程可以组装出一个复杂的行为，以实现预期的目标。一些研究人员已经用这种方式为机器人足球运动员演化出了策略。</p>

<p class="noindent english">One consequence of crossing over program trees instead of bit strings is that the resulting programs can have any size, making the learning more flexible. The overall tendency is for bloat, however, with larger and larger trees growing as evolution goes on longer (also known as “survival of the fattest”). Evolutionaries can take comfort from the fact that human-written programs are no different (Microsoft Windows: forty-five million lines of code and counting), and that human-made code doesn’t allow a solution as simple as adding a complexity penalty to the fitness function.</p>

<p class="noindent chinese">跨越程序树而不是比特串的一个后果是，产生的程序可以有任何大小，使学习更加灵活。然而，总的趋势是膨胀，随着进化时间的延长，树越来越大（也被称为 “最肥者的生存”）。进化论者可以从以下事实中得到安慰：人类编写的程序也不例外（微软视窗：四千五百万行代码，而且还在不断增加），而且人类编写的代码不允许像在适应函数中加入复杂性惩罚这样简单的解决方案。</p>

<p class="noindent english">Genetic programming’s first success, in 1995, was in designing electronic circuits. Starting with a pile of electronic components such as transistors, resistors, and capacitors, Koza’s system reinvented a previously patented design for a low-pass filter, a circuit that can be used for things like enhancing the bass on a dance-music track. Since then he’s made a sport of reinventing patented devices, turning them out by the dozen. The next milestone came in 2005, when the US Patent and Trademark Office awarded a patent to a genetically designed factory optimization system. If the Turing test had been to fool a patent examiner <a id="babilu_link-158"></a> instead of a conversationalist, then January 25, 2005, would have been a date for the history books.</p>

<p class="noindent chinese">1995 年，遗传编程的第一次成功是在设计电子电路方面。从一堆电子元件（如晶体管、电阻器和电容器）开始，科扎的系统重新发明了一个以前获得专利的低通滤波器设计，该电路可用于增强舞曲的低音。从那时起，他就把重新发明专利设备作为一项运动，一打一打地推出。下一个里程碑出现在 2005 年，当时美国专利和商标局授予一个基因设计的工厂优化系统专利。如果图灵测试是为了骗过专利审查员而不是骗过对话者，那么 2005 年 1 月 25 日将是一个载入史册的日子。</p>

<p class="noindent english">Koza’s confidence stands out even in a field not known for its shrinking violets. He sees genetic programming as an invention machine, a silicon Edison for the twenty-first century. He and other evolutionaries believe it can learn any program, making it their entry in the Master Algorithm sweepstakes. In 2004, they instituted the annual Humie Awards to recognize “human-competitive” genetic creations; thirty-nine have been awarded to date.</p>

<p class="noindent chinese">即使在一个不以萎缩的紫罗兰著称的领域，科扎的自信也很突出。他认为遗传编程是一种发明机器，是二十一世纪的硅基爱迪生。他和其他进化论者认为它可以学习任何程序，使它成为他们在算法大师竞赛中的参赛项目。2004 年，他们设立了年度胡米奖，以表彰 “人类竞争” 的基因创造；迄今已有 39 项获奖。</p>

<h1 id="babilu_link-418"><b>What is sex for?</b></h1>

<h1 id="babilu_link-418"><b>性是为了什么？</b></h1>

<p class="noindent english">Despite their successes, and the insights they’ve provided on issues like gradualism versus punctuated equilibria, genetic algorithms have left one great mystery unsolved: the role of sex in evolution. Evolutionaries set great store by crossover, but members of the other tribes think it’s not worth the trouble. None of Holland’s theoretical results show that crossover actually helps; mutation suffices to exponentially increase the frequency of the fittest schemas in the population over time. And the “building blocks” intuition is appealing but quickly runs into trouble, even when genetic programming is used. As larger blocks evolve, crossover also becomes increasingly likely to break them up. Also, once a highly fit individual appears, its descendants tend to quickly take over the population, crowding out potentially better schemas that were trapped in overall less fit individuals. This effectively reduces the search to variations of the fitness champ. Researchers have come up with a number of schemes for preserving diversity in the population, but the results so far are inconclusive. Engineers certainly use building blocks extensively, but combining them involves, well, a lot of engineering; it’s not just a matter of throwing them together any old way, and it’s not clear crossover can do the trick.</p>

<p class="noindent chinese">尽管他们取得了成功，并且在渐进主义与点状平衡等问题上提供了深刻的见解，但遗传算法却留下了一个巨大的谜团：性在进化中的作用。进化论者对交叉作用非常重视，但其他部落的成员认为这不值得麻烦。霍兰的理论结果没有一个显示出交叉实际上是有帮助的；随着时间的推移，突变足以成倍地增加人口中最合适的模式的频率。而且，“积木式” 的直觉很吸引人，但很快就遇到了麻烦，即使是在使用遗传编程的时候。随着更大的积木的进化，交叉也变得越来越有可能将其分解。另外，一旦出现一个高度适合的个体，它的后代往往会迅速占领人口，排挤掉那些被困在总体上不太适合的个体中的潜在的更好的图式。这就有效地减少了对适应冠军的搜索。研究人员已经想出了许多方案来保护种群的多样性，但到目前为止，结果还没有定论。工程师们当然广泛地使用积木，但将它们结合起来涉及到，嗯，很多工程；这不仅仅是将它们随便扔在一起的问题，而且还不清楚交叉可以做到这一点。</p>

<p class="noindent english">Eliminating sex would leave evolutionaries with only mutation to power their engine. If the size of the population is substantially larger than the number of genes, chances are that every point mutation is <a id="babilu_link-131"></a> represented in it, and the search becomes a type of hill climbing: try all possible one-step variations, pick the best one, and repeat. (Or pick several of the best variations, in which case it’s called beam search.) Symbolists, in particular, use this all the time to learn sets of rules, although they don’t think of it as a form of evolution. To avoid getting trapped in local maxima, hill climbing can be enhanced with randomness (make a downhill move with some probability) and random restarts (after a while, jump to a random state and continue from there). Doing this is enough to find good solutions to problems; whether the benefit of adding crossover to it justifies the extra computational cost remains an open question.</p>

<p class="noindent chinese">消除性别将使进化者只有突变来驱动他们的引擎。如果种群的规模远远大于基因的数量，那么每一个点突变都有可能在其中得到体现搜索就变成了一种爬坡：尝试所有可能的单步变化，挑选最好的一个，然后重复。（或者挑选几个最好的变化，在这种情况下，它被称为波束搜索）。特别是符号学家，一直在使用这种方法来学习规则集，尽管他们不认为这是一种进化形式。为了避免陷入局部最大值，可以用随机性（以某种概率进行下坡）和随机重启（一段时间后，跳到一个随机状态并从那里继续）来加强爬坡。这样做足以找到问题的良好解决方案；在其中加入交叉的好处是否能证明额外的计算成本是合理的，这仍然是一个开放的问题。</p>

<p class="noindent english">No one is sure why sex is pervasive in nature, either. Several theories have been proposed, but none is widely accepted. The leader of the pack is the Red Queen hypothesis, popularized by Matt Ridley in the eponymous book. As the Red Queen said to Alice in <i>Through the Looking Glass</i> , “It takes all the running you can do, to keep in the same place.” In this view, organisms are in a perpetual arms race with parasites, and sex helps keep the population varied, so that no single germ can infect all of it. If this is the answer, then sex is irrelevant to machine learning, at least until learned programs have to vie with computer viruses for processor time and memory. (Intriguingly, Danny Hillis claims that deliberately introducing coevolving parasites into a genetic algorithm can help it escape local maxima by gradually ratcheting up the difficulty, but no one has followed up on this yet.) Christos Papadimitriou and colleagues have shown that sex optimizes not fitness but what they call mixability: a gene’s ability to do well on average when combined with other genes. This can be useful when the fitness function is either not known or not constant, as in natural selection, but in machine learning and optimization, hill climbing tends to do better.</p>

<p class="noindent chinese">也没有人确定为什么性在自然界中是普遍存在的。已经提出了几种理论，但没有一种被广泛接受。其中的佼佼者是红皇后假说，由马特·雷德利在同名书中推广。正如红皇后在《<i>穿越时空的玻璃</i>》中对爱丽丝说的那样，“你要用尽所有的奔跑，才能保持在同一个地方”。在这种观点中，生物体处于与寄生虫的永久军备竞赛中，而性有助于保持人口的多样性，以便没有一个病菌可以感染所有的人口。如果这就是答案，那么性别就与机器学习无关，至少在学习的程序必须与计算机病毒争夺处理器时间和内存之前是如此。（有趣的是，丹尼·希利斯声称，故意在遗传算法中引入共同进化的寄生虫，可以通过逐渐提高难度来帮助它摆脱局部最大值，但目前还没有人跟进这个问题）。克里斯托斯·帕帕迪米特里欧及其同事已经证明，性别优化的不是适应，而是他们所谓的混合性：一个基因与其他基因结合时平均表现良好的能力。当适应功能不为人知或不恒定时，这可能是有用的，如在自然选择中，但在机器学习和优化中，爬坡往往会做得更好。</p>

<p class="noindent english">The problems for genetic programming do not end there. Indeed, even its successes might not be as genetic as evolutionaries would like. Take circuit design, which was genetic programming’s emblematic success. As a rule, even relatively simple designs require an enormous amount of search, and it’s not clear how much the results owe to brute <a id="babilu_link-145"></a> force rather than genetic smarts. To address the growing chorus of critics, Koza included in his 1992 book <i>Genetic Programming</i> experiments showing that genetic programming beat randomly generating candidates on Boolean circuit synthesis problems, but the margin of victory was small. Then, at the 1995 International Conference on Machine Learning (ICML) in Lake Tahoe, California, Kevin Lang published a paper showing that hill climbing beat genetic programming on the same problems, often by a large margin. Koza and other evolutionaries had repeatedly tried to publish papers in ICML, a leading venue in the field, but to their increasing frustration they kept being rejected due to insufficient empirical validation. Already frustrated with his papers being rejected, seeing Lang’s paper made Koza blow his top. On short order, he produced a twenty-three-page paper in two-column ICML format refuting Lang’s conclusions and accusing the ICML reviewers of scientific misconduct. He then placed a copy on every seat in the conference auditorium. Depending on your point of view, either Lang’s paper or Koza’s response was the last straw; regardless, the Tahoe incident marked the final divorce between the evolutionaries and the rest of the machine-learning community, with the evolutionaries moving out of the house. Genetic programmers started their own conference, which merged with the genetic algorithms conference to form GECCO, the Genetic and Evolutionary Computing Conference. For its part, the machine-learning mainstream largely forgot them. A sad <i>dénouement</i> , but not the first time in history that sex is to blame for a breakup.</p>

<p class="noindent chinese">遗传编程的问题并没有就此结束。事实上，即使它的成功也不像进化论者所希望的那样具有遗传性。以电路设计为例，它是遗传编程的标志性成功。通常情况下，即使是相对简单的设计也需要大量的搜索，而且不清楚结果在多大程度上归功于粗暴的而不是遗传的智慧。为了应对越来越多的批评者，Koza 在他 1992 年出版的《<i>遗传编程</i>》一书中加入了一些实验，表明遗传编程在布尔电路合成问题上击败了随机生成的候选者，但胜利的幅度很小。然后，在 1995 年加利福尼亚塔霍湖的国际机器学习会议（ICML）上，凯文·兰发表了一篇论文，表明在同样的问题上，爬坡法击败了遗传编程，而且往往是以很大的优势。科扎和其他进化论者曾多次试图在 ICML 这个该领域的领先场所发表论文，但令他们越来越沮丧的是，这些论文由于没有充分的经验验证而一直被拒绝。科扎对自己的论文被拒已经很沮丧了，看到兰的论文后，科扎大为光火。在很短的时间内，他用 ICML 的两栏格式写了一篇 23 页的论文，反驳了兰的结论，并指责 ICML 的评审员科学行为不端。然后他在会议礼堂的每个座位上都放了一份副本。根据你的观点，无论是兰的论文还是 Koza 的回应都是最后一根稻草；无论如何，Tahoe 事件标志着进化论者和机器学习社区其他成员之间的最终离婚，进化论者搬出了房子。遗传程序员开始了他们自己的会议，与遗传算法会议合并为 GECCO，即遗传和进化计算会议。就其本身而言，机器学习的主流在很大程度上忘记了他们。一个悲伤的<i>结局</i>，但不是历史上第一次将分手归咎于性的问题。</p>

<p class="noindent english">Sex may not have succeeded in machine learning, but as a consolation, it has played a prominent role in the evolution of technology in other ways. Pornography was the unacknowledged “killer app” of the World Wide Web, not to mention the printing press, photography, and video before it. The vibrator was the first handheld electrical device, predating the cell phone by a century. Scooters took off in postwar Europe, particularly Italy, because they let young couples get away from their families. Facilitating dating was surely one of the “killer apps” of fire when <i>Homo erectus</i> discovered it a million years ago; and equally surely, a key driver of increasing realism in humanlike robots will be the <a id="babilu_link-205"></a> sexbot industry. Sex just seems to be the end, rather than the means, of technological evolution.</p>

<p class="noindent chinese">性可能没有在机器学习方面取得成功，但作为一种安慰，它在其他方面的技术演变中发挥了突出作用。色情制品是万维网不为人知的 “杀手级应用”，更不用说之前的印刷术、摄影和视频。振动器是第一个手持式电气设备，比手机早了一个世纪。滑板车在战后的欧洲起飞，特别是意大利，因为它们让年轻夫妇远离他们的家庭。当<i>直立人</i>在一百万年前发现火时，促进约会肯定是火的 “杀手级应用” 之一；同样肯定的是，像人一样的机器人越来越逼真的一个关键驱动力将是性机器人行业。性似乎只是技术进化的目的，而不是手段。</p>

<h1 id="babilu_link-419"><b>Nurturing nature</b></h1>

<h1 id="babilu_link-419"><b>孕育自然</b></h1>

<p class="noindent english">Evolutionaries and connectionists have something important in common: they both design learning algorithms inspired by nature. But then they part ways. Evolutionaries focus on learning structure; to them, fine-tuning an evolved structure by optimizing parameters is of secondary importance. In contrast, connectionists prefer to take a simple, hand-coded structure with lots of connections and let weight learning do all the work. This is machine learning’s version of the nature versus nurture controversy, and there are good arguments on both sides.</p>

<p class="noindent chinese">进化论者和连接论者有一些重要的共同点：他们都在设计受自然启发的学习算法。但后来他们分道扬镳。进化论者专注于学习结构；对他们来说，通过优化参数对进化的结构进行微调是次要的。相比之下，连接主义者更喜欢采用一个简单的、具有大量连接的手工编码结构，并让加权学习完成所有工作。这是机器学习的天性与教养之争的版本，双方都有很好的论据。</p>

<p class="noindent english">On the one hand, evolution has produced many amazing things, none more amazing than you. With or without crossover, evolving structure is an essential part of the Master Algorithm. The brain can learn anything, but it can’t evolve a brain. If we thoroughly understood its architecture, we could just implement it in hardware, but we’re very far from that; getting an assist from computer-simulated evolution is a no-brainer. What’s more, we also want to evolve the brains of robots, systems with arbitrary sensors, and super-AIs. There’s no reason to stick with the design of the human brain if there are better ones for those tasks. On the other hand, evolution is excruciatingly slow. The entire life of an organism yields only one piece of information about its genome: its fitness, reflected in the organism’s number of offspring. That’s a colossal waste of information, which neural learning avoids by acquiring the information at the point of use (so to speak). As connectionists like Geoff Hinton like to point out, there’s no advantage to carrying around in the genome information that we can readily acquire from the senses. When a newborn opens his eyes, the visual world comes flooding in; the brain just has to organize it. What does need to be specified in the genome, however, is the architecture of the machine that does the organizing.</p>

<p class="noindent chinese">一方面，进化产生了许多神奇的东西，没有比你更神奇的了。不管有没有交叉，进化结构是主算法的一个重要部分。大脑可以学习任何东西，但它不能进化出一个大脑。如果我们彻底了解它的结构，我们可以直接在硬件中实现它，但我们离这一点非常遥远；从计算机模拟的进化中获得帮助是不需要考虑的。更重要的是，我们还想进化机器人的大脑，具有任意传感器的系统，以及超级人工智能。如果有更好的设计来完成这些任务，那么就没有理由坚持使用人脑的设计。另一方面，进化是非常缓慢的。一个生物体的整个生命只产生关于其基因组的一个信息：它的健康状况，反映在该生物体的后代数量上。这是一种巨大的信息浪费，而神经学习通过在使用点上获取信息（可以这么说）来避免这种浪费。正如杰夫·辛顿等连接主义者喜欢指出的那样，在基因组中携带我们可以轻易从感官获得的信息并没有什么好处。当一个新生儿睁开眼睛时，视觉世界就会涌入；大脑只需要组织它。然而，需要在基因组中指定的是进行组织工作的机器的结构。</p>

<p class="noindent english">As in the nature versus nurture debate, neither side has the whole answer; the key is figuring out how to combine the two. The Master <a id="babilu_link-107"></a> Algorithm is neither genetic programming nor backprop, but it has to include the key elements of both: structure learning and weight learning. In the conventional view, nature does its part first—evolving a brain—and then nurture takes it from there, filling the brain with information. We can easily reproduce this in learning algorithms. First, learn the structure of the network, using (for example) hill climbing to decide which neurons connect to which: try adding each possible new connection to the network, keep the one that most improves performance, and repeat. Then learn the connection weights using backprop, and your brand-new brain is ready to use.</p>

<p class="noindent chinese">正如在自然与培养的辩论中，双方都没有全部的答案；关键是要弄清楚如何将两者结合起来。主算法既不是遗传编程，也不是反推法，但它必须包括两者的关键因素：结构学习和权重学习。在传统的观点中，自然界首先做它的部分 —— 进化出一个大脑 —— 然后培养者从那里开始，用信息填充大脑。我们可以很容易地在学习算法中重现这一点。首先，学习网络的结构，使用（例如）爬坡法来决定哪些神经元连接到哪些神经元：尝试向网络添加每个可能的新连接，保留最能提高性能的连接，然后重复。然后用 “反向传播” 学习连接权重，你的全新大脑就可以使用了。</p>

<p class="noindent english">But now there’s an important subtlety, in both natural and artificial evolution. We need to learn weights for every candidate structure along the way, not just the final one, in order to see how well it does in the struggle for life (in the natural case) or on the training data (in the artificial case). The structure we want to select at each step is the one that does best after learning weights, not before. So in reality, nature does not come before nurture; rather, they alternate, with each round of “nurture” learning setting the stage for the next round of “nature” learning and vice versa. Nature evolves for the nurture it gets. The evolutionary growth of the cortex’s associative areas builds on neural learning in the sensory areas, without which it would be useless. Goslings follow their mother around (evolved behavior) but that requires recognizing her (learned ability). If you’re the first thing they see when they hatch, they’ll follow you instead, as Konrad Lorenz memorably showed. The newborn brain already encodes features of the environment but not explicitly; rather, evolution optimized it to extract those features from the expected input. Likewise, in an algorithm that iteratively learns both structure and weights, each new structure is implicitly a function of the weights learned in previous rounds.</p>

<p class="noindent chinese">但现在有一个重要的微妙之处，在自然和人工进化中都是如此。我们需要为沿途的每一个候选结构学习权重，而不仅仅是最终的一个，以便看看它在生命斗争中（在自然情况下）或在训练数据上（在人工情况下）表现如何。我们想在每一步选择的结构是在学习权重后做得最好的结构，而不是在学习前。因此，在现实中，“自然” 并不先于 “培养”；相反，它们交替进行，每一轮 “培养” 的学习都为下一轮 “自然” 的学习创造条件，反之亦然。自然为它得到的培养而进化。大脑皮层联想区的进化增长是建立在感觉区的神经学习之上的，没有它就没有用。雏鸟跟在母亲身边（进化的行为），但这需要认识母亲（学习的能力）。如果你是它们孵化时看到的第一个东西，它们反而会跟着你，正如康拉德·洛伦兹令人难忘的表现。新生的大脑已经对环境的特征进行了编码，但不是明确的；相反，进化对它进行了优化，以便从预期输入中提取这些特征。同样，在一个迭代学习结构和权重的算法中，每个新结构都隐含着前几轮学习的权重的功能。</p>

<p class="noindent english">Of all the possible genomes, very few correspond to viable organisms. The typical fitness landscape thus consists of vast flatlands with occasional sharp peaks, making evolution very hard. If you start out blindfolded in Kansas, you have no idea which way the Rockies lie, and you’ll wander around for a long time before you bump into their <a id="babilu_link-108"></a> foothills and start climbing. But if you combine evolution with neural learning, something interesting happens. If you’re on flat ground, but not too far from the foothills, neural learning can get you there, and the closer you are to the foothills, the more likely it will. It’s like being able to scan the horizon: it won’t help you in Wichita, but in Denver you’ll see the Rockies in the distance and head that way. Denver now looks a lot fitter than it did when you were blindfolded. The net effect is to widen the fitness peaks, making it possible for you to find your way to them from previously very tough places, like point A in this graph:</p>

<p class="noindent chinese">在所有可能的基因组中，只有极少数对应于可生存的生物体。因此，典型的适应景观由广阔的平地和偶尔的尖峰组成，使得进化非常困难。如果你在堪萨斯州蒙着眼睛开始，你不知道落基山脉在哪个方向，你会徘徊很久，然后才会撞到它们的山麓并开始攀登。但是如果你把进化和神经学习结合起来，就会发生一些有趣的事情。如果你在平地上，但离山麓不太远，神经学习可以把你带到那里，而且你越接近山麓，就越有可能。这就像能够扫描地平线一样：在威奇托它不会帮助你，但在丹佛你会看到远处的落基山脉，并向那边走去。丹佛现在看起来比你被蒙住眼睛时要合适得多。净效果是扩大了适应高峰，使你有可能从以前非常艰难的地方找到通往这些地方的路，比如这个图中的 A 点。</p>

<div>

<div>

<img alt="image" src="images/000009.jpg"/>

</div>

</div>

<p class="noindent english">In biology, this is called the Baldwin effect, after J. M. Baldwin, who proposed it in 1896. In Baldwinian evolution, behaviors that are first learned later become genetically hardwired. If dog-like mammals can learn to swim, they have a better chance to evolve into seals—as they did—than if they drown. Thus individual learning can influence evolution without recourse to Lamarckism. Geoff Hinton and Steven Nowlan demonstrated the Baldwin effect in machine learning by using genetic algorithms to evolve neural network structure and observing that fitness increased over time only when individual learning was allowed.</p>

<p class="noindent chinese">在生物学中，这被称为鲍德温效应，是以 1896 年提出的鲍德温命名的。在鲍德温式的进化中，首先学会的行为后来成为基因上的硬性规定。如果狗类哺乳动物能学会游泳，它们就有更好的机会进化成海豹 —— 就像它们那样 —— 而不是淹死。因此，个体学习可以影响进化，而无需求助于拉马克主义。杰夫·辛顿和史蒂芬·诺兰通过使用遗传算法来进化神经网络结构，并观察到只有在允许个体学习的情况下，适应才会随着时间的推移而增加，从而证明了机器学习中的鲍德温效应。</p>

<h1 id="babilu_link-420"><b>He who learns fastest wins</b></h1>

<h1 id="babilu_link-420"><b>学得最快的人获胜</b></h1>

<p class="noindent english">Evolution searches for good structures, and neural learning fills them in: this combination is the easiest of the steps we’ll take toward the Master Algorithm. This may come as a surprise to anyone familiar with the never-ending twists and turns of the nature versus nurture controversy, <a id="babilu_link-109"></a> 2,500 years old and still going strong. Seeing life through the eyes of a computer clarifies a lot of things, however. “Nature” for a computer is the program it runs, and “nurture” is the data it gets. The question of which one is more important is clearly absurd; there’s no output without both program and data, and it’s not like the output is, say, 60 percent caused by the program and 40 percent by the data. That’s the kind of linear thinking that a familiarity with machine learning immunizes you against.</p>

<p class="noindent chinese">进化寻找良好的结构，而神经学习则填补这些结构：这种组合是我们走向主算法的最简单步骤。这对于任何熟悉自然与养育之争永无休止的曲折的人来说，可能会感到惊讶，已有 2500 年的历史，而且还在继续。然而，通过计算机的眼睛来看待生活，可以澄清很多事情。对计算机来说，“自然” 是它运行的程序，而 “教养” 是它获得的数据。哪一个更重要的问题显然是荒谬的；没有程序和数据就没有输出，而且输出并不是说 60% 由程序造成，40% 由数据造成。这就是那种熟悉机器学习的人能够免疫的线性思维。</p>

<p class="noindent english">On the other hand, you may be wondering why we’re not done at this point. Surely if we’ve combined nature’s two master algorithms, evolution and the brain, that’s all we could ask for. Unfortunately, what we have so far is only a very crude cartoon of how nature learns, good enough for a lot of applications but still a pale shadow of the real thing. For example, the development of the embryo is a crucial part of life, but there’s no analog of it in machine learning: the “organism” is a very straightforward function of the genome, and we may be missing something important there. But another reason is that we wouldn’t be satisfied even if we had completely figured out how nature learns. For one thing, it’s too slow. Evolution takes billions of years to learn, and the brain takes a lifetime. Culture is better: I can distill a lifetime of learning into a book, and you can read it in a few hours. But learning algorithms should be able to learn in minutes or seconds. He who learns fastest wins, whether it’s the Baldwin effect speeding up evolution, verbal communication speeding up human learning, or computers discovering patterns at the speed of light. Machine learning is the latest chapter in the arms race of life on Earth, and swifter hardware is only half the equation. The other half is smarter software.</p>

<p class="noindent chinese">另一方面，你可能想知道为什么我们在这一点上没有完成。当然，如果我们结合了自然界的两个主要算法，即进化和大脑，这就是我们能要求的一切。不幸的是，到目前为止，我们所拥有的只是自然界如何学习的一个非常粗略的卡通，对于很多应用来说已经足够好了，但仍然是真实事物的一个苍白的阴影。例如，胚胎的发育是生命的一个关键部分，但在机器学习中没有类似的东西：“有机体” 是基因组的一个非常直接的功能，我们可能在那里错过了一些重要的东西。但另一个原因是，即使我们完全弄清了自然界的学习方式，我们也不会感到满意。首先，它太慢了。进化需要数十亿年的时间来学习，而大脑需要一生的时间。文化是更好的。我可以把一生的学习提炼成一本书，而你可以在几个小时内读完它。但学习算法应该能在几分钟或几秒钟内学会。谁学得最快谁就赢了，无论是鲍德温效应加速了进化，口头交流加速了人类的学习，还是计算机以光速发现模式。机器学习是地球上生命军备竞赛的最新章节，而更快的硬件只是其中的一半。另一半则是更智能的软件。</p>

<p class="noindent english">Most of all, the goal of machine learning is to find the best possible learning algorithm, by any means available, and evolution and the brain are unlikely to provide it. The products of evolution have many obvious faults. For example, the mammalian optic nerve attaches to the front of the retina instead of the back, causing an unnecessary—and egregious—blind spot right next to the fovea, the area of sharpest vision. <a id="babilu_link-129"></a> The molecular biology of living cells is such a mess that molecular biologists often quip that only people who don’t know any of it could believe in intelligent design. The architecture of the brain may well have similar faults—the brain has many constraints that computers don’t, like very limited short-term memory—and there’s no reason to stay within them. Moreover, we know of many situations where humans seem to consistently do the wrong thing, as Daniel Kahneman illustrates at length in his book <i>Thinking, Fast and Slow</i> .</p>

<p class="noindent chinese">最重要的是，机器学习的目标是通过任何手段找到最好的学习算法，而进化和大脑不太可能提供这种算法。进化的产物有许多明显的缺点。例如，哺乳动物的视神经附着在视网膜的前面而不是后面，导致紧挨着眼窝的地方出现不必要的、令人震惊的盲点，而眼窝是视觉最敏锐的区域。活细胞的分子生物学是如此混乱，以至于分子生物学家经常调侃说，只有不了解这些的人才会相信有智能设计。大脑的结构很可能有类似的缺陷 —— 大脑有许多计算机没有的限制，比如非常有限的短期记忆 —— 没有理由停留在这些限制中。此外，我们知道在许多情况下，人类似乎总是做错事，正如丹尼尔·卡尼曼在他的《<i>思考，快与慢</i>》一书中详细说明的那样。</p>

<p class="noindent english">In contrast to the connectionists and evolutionaries, symbolists and Bayesians do not believe in emulating nature. Rather, they want to figure out from first principles what learners should do—and that includes us humans. If we want to learn to diagnose cancer, for example, it’s not enough to say “this is how nature learns; let’s do the same.” There’s too much at stake. Errors cost lives. Doctors should diagnose in the most foolproof way they can, with methods similar to those mathematicians use to prove theorems, or as close to that as they can manage, given that it’s seldom possible to be that rigorous. They need to weigh the evidence to minimize the chances of a wrong diagnosis; or more precisely, so that the costlier an error is, the less likely they are to make it. (For example, failing to find a tumor that’s really there is potentially much worse than inferring one that isn’t.) They need to make <i>optimal</i> decisions, not just decisions that seem good.</p>

<p class="noindent chinese">与连接主义者和进化论者相反，符号主义者和贝叶斯主义者不相信模仿自然。相反，他们想从第一原则中找出学习者应该做什么 —— 这包括我们人类。例如，如果我们想学习诊断癌症，仅仅说 “这就是自然界的学习方式；让我们也这么做” 是不够的。这关系到太多的事情。错误会造成生命损失。医生应该以最万无一失的方式进行诊断，采用类似于数学家用来证明定理的方法，或者尽可能地接近这种方法，因为很少有可能做到如此严格。他们需要权衡证据，以尽量减少错误诊断的机会；或者更准确地说，这样，错误的代价越大，他们就越不可能犯错误。（例如，未能发现一个真正存在的肿瘤，可能比推断出一个不存在的肿瘤要糟糕得多）。他们需要做出<i>最佳</i>决定，而不仅仅是看起来不错的决定。</p>

<p class="noindent english">This is an instance of a tension that runs throughout much of science and philosophy: the split between descriptive and normative theories, between “this is how it is” and “this is how it should be.” Symbolists and Bayesians like to point out, however, that figuring out how we should learn can also help us to understand how we do learn because the two are presumably not entirely unrelated—far from it. In particular, behaviors that are important for survival and have had a long time to evolve should not be far from optimal. We’re not very good at answering written questions about probabilities, but we are very good at instantly choosing hand and arm movements to hit a target. Many psychologists have used symbolist or Bayesian models to explain aspects of <a id="babilu_link-421"></a> human behavior. Symbolists dominated the first few decades of cognitive psychology. In the 1980s and 1990s, connectionists held sway, but now Bayesians are on the rise.</p>

<p class="noindent chinese">这是一个贯穿大部分科学和哲学的紧张关系的例子：描述性理论和规范性理论之间的分裂，“事情就是这样” 和 “事情应该这样” 之间的分裂。然而，符号主义者和贝叶斯主义者喜欢指出，弄清楚我们应该如何学习也可以帮助我们理解我们如何学习，因为这两者大概不是完全无关的 —— 远非如此。特别是，对生存很重要并且经过长时间进化的行为不应该远离最佳状态。我们不善于回答关于概率的书面问题，但我们非常善于即时选择手和手臂的动作来击中一个目标。许多心理学家使用符号主义或贝叶斯模型来解释人类行为的各个方面。符号主义者在认知心理学的前几十年中占主导地位。在 20 世纪 80 年代和 90 年代，连接主义者占据了主导地位，但现在贝叶斯主义者正在崛起。</p>

<p class="noindent english">For the hardest problems—the ones we really want to solve but haven’t been able to, like curing cancer—pure nature-inspired approaches are probably too uninformed to succeed, even given massive amounts of data. We can in principle learn a complete model of a cell’s metabolic networks by a combination of structure search, with or without crossover, and parameter learning via backpropagation, but there are too many bad local optima to get stuck in. We need to reason with larger chunks, assembling and reassembling them as needed and using inverse deduction to fill in the gaps. And we need our learning to be guided by the goal of optimally diagnosing cancer and finding the best drugs to cure it.</p>

<p class="noindent chinese">对于最难的问题 —— 那些我们真正想解决但一直无法解决的问题，比如治愈癌症 —— 纯自然启发的方法可能太不靠谱了，即使有大量的数据也无法成功。原则上，我们可以通过结构搜索的组合来学习一个完整的细胞代谢网络模型，无论是否有交叉，以及通过反向传播的参数学习，但有太多糟糕的局部优化会陷入其中。我们需要对更大的块进行推理，根据需要进行组装和重新组装，并使用反推法来填补空白。我们需要我们的学习以优化诊断癌症和寻找最佳药物来治疗癌症的目标为指导。</p>

<p class="noindent english">Optimal learning is the Bayesians’ central goal, and they are in no doubt that they’ve figured out how to reach it. This way, please…</p>

<p class="noindent chinese">最佳学习是贝叶斯派的中心目标，他们毫不怀疑自己已经找到了达到这一目标的方法。这样一来，请… </p>

</section>

</div>

</div>

<div id="babilu_link-322">

<div>

<section id="babilu_link-6">

<h1><a id="babilu_link-122"></a> <a href="#babilu_link-323">CHAPTER SIX</a></h1>

<h1><a id="babilu_link-122"></a> <a href="#babilu_link-323">第六章</a></h1>

<h1><a href="#babilu_link-323">In the Church of the Reverend Bayes</a></h1>

<h1><a href="#babilu_link-323">在贝叶斯牧师的教堂里</a></h1>

<p class="noindent english">The dark hulk of the cathedral rises from the night. Light pours from its stained-glass windows, projecting intricate equations onto the streets and buildings beyond. As you approach, you can hear chanting inside. It seems to be Latin, or perhaps math, but the Babel fish in your ear translates it into English: “Turn the crank! Turn the crank!” Just as you enter, the chant dissolves into an “Aaaah!” of satisfaction, and a murmur of “The posterior! The posterior!” You peek through the crowd. A massive stone tablet towers above the altar with a formula engraved on it in ten-foot letters:</p>

<p class="noindent chinese">大教堂的黑暗躯体从夜色中升起。光线从它的彩色玻璃窗中倾泻而出，将复杂的方程式投射到外面的街道和建筑上。当你走近时，你可以听到里面的诵读声。这似乎是拉丁语，也可能是数学，但你耳朵里的巴别鱼把它翻译成了英语。“转动曲柄！转动曲柄！” 就在你进去的时候，诵经声消失了，变成了满意的 “啊！” 声，以及 “后部！后部！” 的杂音。后面的！"你透过人群偷看。一块巨大的石碑耸立在祭坛之上，上面用十英尺长的文字刻着一个公式。</p>

<div>

<p class="noindent english"><i>P</i> (<i>A|B</i>) = <i>P</i> (<i>A</i>) <i>P(B|A) / P(B)</i></p>

<p class="noindent chinese"><i>p</i><i>（a|b</i>）=<i>p</i><i>（a</i>）<i>p(b|a)/ p(b</i>)</p>

</div>

<p class="noindent english">As you stare uncomprehendingly at it, your Google Glass helpfully flashes: “Bayes’ theorem.” Now the crowd starts to chant “More data! More data!” A stream of sacrificial victims is being inexorably pushed toward the altar. Suddenly, you realize that you’re in the middle of it—too late. As the crank looms over you, you scream, “No! I don’t want to be a data point! Let me gooooo!”</p>

<p class="noindent chinese">当你不理解地盯着它时，你的谷歌眼镜就会帮助你闪现。“贝叶斯定理”。现在人群开始高呼 “更多数据！更多数据！” 一批批牺牲者正被不可阻挡地推向祭坛。突然间，你意识到你正处于其中 —— 太晚了。当曲柄笼罩着你的时候，你尖叫起来，“不！我不想成为一个数据点！让我走！让我走吧！”</p>

<p class="noindent english"><a id="babilu_link-113"></a> You wake up in a cold sweat. Lying on your lap is a book entitled <i>The Master Algorithm</i> . Shaking off the nightmare, you resume reading where you had left off.</p>

<p class="noindent chinese">你醒来时出了一身冷汗。躺在你腿上的是一本名为<i>“主算法”</i>的书。摆脱恶梦，你继续阅读你离开的地方。</p>

<h1 id="babilu_link-422"><b>The theorem that runs the world</b></h1>

<h1 id="babilu_link-422"><b>掌管世界的定理</b></h1>

<p class="noindent english">The path to optimal learning begins with a formula that many people have heard of: Bayes’ theorem. But here we’ll see it in a whole new light and realize that it’s vastly more powerful than you’d guess from its everyday uses. At heart, Bayes’ theorem is just a simple rule for updating your degree of belief in a hypothesis when you receive new evidence: if the evidence is consistent with the hypothesis, the probability of the hypothesis goes up; if not, it goes down. For example, if you test positive for AIDS, your probability of having it goes up. Things get more interesting when you have many pieces of evidence, such as the results of multiple tests. To combine them all without suffering a combinatorial explosion, we need to make simplifying assumptions. Things get even more interesting when we consider many hypotheses at once, such as all the different possible diagnoses for a patient. Computing the probability of each disease from the patient’s symptoms in a reasonable amount of time can take a lot of smarts. Once we know how to do all these things, we’ll be ready to learn the Bayesian way. For Bayesians, learning is “just” another application of Bayes’ theorem, with whole models as the hypotheses and the data as the evidence: as you see more data, some models become more likely and some less, until ideally one model stands out as the clear winner. Bayesians have invented fiendishly clever kinds of models. So let’s get started.</p>

<p class="noindent chinese">通往最佳学习的道路始于一个很多人都听说过的公式。贝叶斯定理。但在这里，我们将从一个全新的角度来看待它，并认识到它比你从日常使用中猜测的要强大得多。从本质上讲，贝叶斯定理只是一个简单的规则，当你收到新的证据时，更新你对一个假设的相信程度：如果证据与假设一致，假设的概率就会上升；如果不一致，它就会下降。例如，如果你的艾滋病检测结果呈阳性，你患艾滋病的概率就会上升。当你有许多证据时，事情就变得更有趣了，比如多项测试的结果。为了在不出现组合爆炸的情况下将它们全部结合起来，我们需要做出简化的假设。当我们同时考虑许多假设时，事情就变得更加有趣了，比如一个病人的所有不同的可能诊断。在合理的时间内从病人的症状中计算出每种疾病的概率，这需要大量的聪明才智。一旦我们知道如何做所有这些事情，我们就会准备好以贝叶斯的方式进行学习。对于贝叶斯学家来说，学习 “只是” 贝叶斯定理的另一种应用，整个模型作为假设，数据作为证据：随着你看到更多的数据，一些模型变得更有可能，一些则更不可能，直到理想的情况下，一个模型作为明显的赢家脱颖而出。贝叶斯学家已经发明了非常聪明的模型种类。因此，让我们开始吧。</p>

<p class="noindent english">Thomas Bayes was an eighteenth-century English clergyman who, without realizing it, became the center of a new religion. You may well ask how that could happen, until you notice that it happened to Jesus, too: Christianity as we know it was invented by Saint Paul, while Jesus saw himself as the pinnacle of the Jewish faith. Similarly, Bayesianism as we know it was invented by Pierre-Simon de Laplace, a Frenchman who was born five decades after Bayes. Bayes was the preacher who first <a id="babilu_link-169"></a> described a new way to think about chance, but it was Laplace who codified those insights into the theorem that bears Bayes’s name.</p>

<p class="noindent chinese">托马斯·贝叶斯是十八世纪的英国牧师，他在不知不觉中成为一种新宗教的中心。你很可能会问这怎么可能发生，直到你注意到它也发生在耶稣身上。我们所知道的基督教是由圣保罗发明的，而耶稣则将自己视为犹太教信仰的顶峰。同样，我们所知道的贝叶斯主义是由皮埃尔·西蒙·德·拉普拉斯发明的，他是一个比贝叶斯晚出生五十年的法国人。贝叶斯是第一个描述思考机会的新方法的传教士但将这些见解编入以贝叶斯名字命名的定理的是拉普拉斯。</p>

<p class="noindent english">One of the greatest mathematicians of all time, Laplace is perhaps best known for his dream of Newtonian determinism:</p>

<p class="noindent chinese">作为有史以来最伟大的数学家之一，拉普拉斯也许因其对牛顿决定论的梦想而最为著名。</p>

<div>

<p class="noindent english"><i>An intelligence that, at a given instant, could comprehend all the forces by which nature is animated and the respective situation of the beings that make it up, if moreover it were vast enough to submit these data to analysis, would encompass in the same formula the movements of the greatest bodies of the universe and those of the lightest atoms. For such an intelligence nothing would be uncertain, and the future, like the past, would be open to its eyes.</i></p>

<p class="noindent chinese"><i>如果一个智力在某一瞬间能够理解自然界的所有力量，以及构成自然界的生物的各自情况，而且它足够庞大，能够将这些数据提交分析，那么它将在同一个公式中包含宇宙中最大的物体和最轻的原子的运动。对于这样的智能来说，没有什么是不确定的，未来，就像过去一样，将在它的眼里是开放的。</i></p>

</div>

<p class="noindent english">This is ironic, since Laplace was also the father of probability theory, which he believed was just common sense reduced to calculation. At the heart of his explorations in probability was a preoccupation with Hume’s question. For example, how do we know the sun will rise tomorrow? It has done so every day until today, but that’s no guarantee it will continue. Laplace’s answer had two parts. The first is what we now call the principle of indifference, or principle of insufficient reason. We wake up one day—at the beginning of time, let’s say, which for Laplace was five thousand years or so ago—and after a beautiful afternoon, we see the sun go down. Will it come back? We’ve never seen the sun rise, and there is no particular reason to believe it will or won’t. Therefore we should consider the two scenarios equally likely and say that the sun will rise again with a probability of one-half. But, Laplace went on, if the past is any guide to the future, every day that the sun rises should increase our confidence that it will continue to do so. After five thousand years, the probability that the sun will rise yet again tomorrow should be very close to one, but not quite there, since we can never be completely certain. From this thought experiment, Laplace derived his so-called rule of succession, which estimates the probability that the sun will rise again after having risen <i>n</i> times as (<i>n</i> + 1) / (<i>n</i> + 2). When <i>n</i> = 0, <a id="babilu_link-309"></a> this is just ½; and as <i>n</i> increases, so does the probability, approaching 1 when <i>n</i> approaches infinity.</p>

<p class="noindent chinese">这很有讽刺意味，因为拉普拉斯也是概率论之父，他认为概率论只是被简化为计算的常识。他在概率论方面的探索的核心是对休谟问题的关注。例如，我们怎么知道太阳明天会升起？直到今天，它每天都在这样做，但这并不能保证它将继续下去。拉普拉斯的答案有两个部分。第一部分是我们现在所说的冷漠原则，或称不充分理由原则。我们有一天醒来 —— 比方说，在时间之初，对拉普拉斯来说，那是五千多年前，在一个美丽的下午之后，我们看到太阳下山了。它还会回来吗？我们从未见过太阳升起，也没有特别的理由相信它会或不会。因此，我们应该认为这两种情况的可能性相同，并说太阳将以二分之一的概率再次升起。但是，拉普拉斯继续说，如果过去对未来有任何指导意义的话，太阳每升起一天，都应该增加我们对它将继续升起的信心。五千年后，太阳明天再次升起的概率应该非常接近 1，但还没有完全达到，因为我们永远无法完全确定。从这个思想实验中，拉普拉斯得出了他所谓的继承法则，估计太阳在升起 <i>n</i> 次后再次升起的概率为 (<i>n</i> + 1) / (<i>n</i> + 2)。当 <i>n</i> = 0 时，这只是 1/2；随着 <i>n</i> 的增加，概率也在增加，当 <i>n</i> 接近无穷大时，就会接近 1。</p>

<p class="noindent english">This rule arises from a more general principle. Suppose you awake in the middle of the night on a strange planet. Even though all you can see is the starry sky, you have reason to believe that the sun will rise at some point, since most planets revolve around themselves and their sun. So your estimate of the corresponding probability should be greater than one-half (two-thirds, say). We call this the <i>prior probability</i> that the sun will rise, since it’s prior to seeing any evidence. It’s not based on counting the number of times the sun has risen on this planet in the past, because you weren’t there to see it; rather, it reflects your a priori beliefs about what will happen, based on your general knowledge of the universe. But now the stars start to fade, so your confidence that the sun does rise on this planet goes up, based on your experience on Earth. Your confidence is now a <i>posterior probability</i> , since it’s after seeing some evidence. The sky begins to lighten, and the posterior probability takes another leap. Finally, a sliver of the sun’s bright disk appears above the horizon and perhaps catches “the Sultan’s turret in a noose of light,” as in the opening verse of the <i>Rubaiyat</i> . Unless you’re hallucinating, it is now certain that the sun will rise.</p>

<p class="noindent chinese">这一规则产生于一个更普遍的原则。假设你在一个陌生星球的半夜醒来。尽管你能看到的只是星空，但你有理由相信太阳会在某个时刻升起，因为大多数行星都是围绕自己和太阳旋转的。所以你对相应概率的估计应该大于二分之一（例如三分之二）。我们把这称为太阳将升起的<i>先验概率</i>，因为它是在看到任何证据之前。它不是基于计算过去太阳在这个星球上升起的次数，因为你不在那里看到它；相反，它反映了你对将会发生什么的先验信念，基于你对宇宙的一般知识。但是现在星星开始褪色，所以根据你在地球上的经验，你对太阳确实在这个星球上升起的信心上升了。你的信心现在是一个<i>后验概率</i>，因为它是在看到一些证据之后。天空开始变亮，后验概率又有了一个飞跃。最后，太阳光盘的一角出现在地平线上，也许会像《<i>鲁拜亚特</i>》开篇所写的那样，“苏丹的炮台被套上了光环”。除非你产生了幻觉，否则现在可以肯定的是，太阳将会升起。</p>

<p class="noindent english">The crucial question is exactly how the posterior probability should evolve as you see more evidence. The answer is Bayes’ theorem. We can think of it in terms of cause and effect. Sunrise causes the stars to fade and the sky to lighten, but the latter is stronger evidence of daybreak, since the stars could fade in the middle of the night due to, say, fog rolling in. So the probability of sunrise should increase more after seeing the sky lighten than after seeing the stars fade. In mathematical notation, we say that <i>P(sunrise | lightening-sky)</i> , the conditional probability of sunrise given that the sky is lightening, is greater than <i>P(sunrise | fading-stars)</i> , its conditional probability given that the stars are fading. According to Bayes’ theorem, the more likely the effect is given the cause, the more likely the cause is given the effect: if <i>P(lightening-sky | sunrise)</i> is higher than <i>P(fading-stars | sunrise)</i> , perhaps because some planets <a id="babilu_link-271"></a> are far enough from their sun that the stars still shine after sunrise, then <i>P(sunrise | lightening sky)</i> is also higher than <i>P(sunrise | fading-stars)</i> .</p>

<p class="noindent chinese">关键问题是，当你看到更多的证据时，后验概率究竟应该如何演变。答案是贝叶斯定理。我们可以从因果关系的角度来考虑它。日出会使星星变淡，天空变亮，但后者是天亮的更有力的证据，因为星星可能会在半夜由于比如说雾气滚滚而变淡。因此，看到天空变亮后，日出的概率应该比看到星星变暗后增加。在数学符号中，我们说<i>p(日出 | 变量的天空)</i>，即在天空变亮的情况下日出的条件概率，大于 <i>p(日出 | 消逝的星星)</i>，即在星星变暗的情况下日出的条件概率。根据贝叶斯定理，在给定原因的情况下，结果的可能性越大，原因的可能性也越大：如果<i>p(变亮的天空 | 日出)</i>高于<i>p(消逝的星星 | 日出)</i>，也许是因为一些行星离太阳足够远，所以星星在日出后仍然闪耀，那么<i>p(日出 | 变亮的天空)</i>也高于<i>p(日出 | 消逝的星星)</i>。</p>

<p class="noindent english">This is not the whole story, however. If we observe an effect that would happen even without the cause, then surely that’s not much evidence of the cause being present. Bayes’ theorem incorporates this by saying that <i>P(cause | effect)</i> goes down with <i>P(effect),</i> the prior probability of the effect (i.e., its probability in the absence of any knowledge of the causes). Finally, other things being equal, the more likely a cause is a priori, the more likely it should be a posteriori. Putting all of these together, Bayes’ theorem says that</p>

<p class="noindent chinese">然而，这并不是故事的全部。如果我们观察到一个即使没有原因也会发生的效果，那么这肯定不是原因存在的证据。贝叶斯定理包含了这一点，它说<i>p(原因 | 效果)</i>随着<i>p(效果)</i>的下降而下降，<i>即</i> 效果的先验概率（即在不了解原因的情况下的概率）。最后，在其他条件相同的情况下，一个原因在先验时的可能性越大，它在后验时的可能性就越大。把所有这些放在一起，贝叶斯定理说</p>

<div>

<p class="noindent english"><i>P(cause | effect) = P(cause) × P(effect | cause) / P(effect).</i></p>

<p class="noindent chinese"><i>p(因 | 果)= p(因)× p(果 | 因)/ p(果)。</i></p>

</div>

<p class="noindent english">Replace <i>cause</i> by <i>A</i> and <i>effect</i> by <i>B</i> and omit the multiplication sign for brevity, and you get the ten-foot formula in the cathedral.</p>

<p class="noindent chinese">用 <i>A</i> 代替 <i>因</i>，用 <i>B</i> 代替 <i>果</i>，为了简洁起见，省略乘号，你就得到了大教堂里的十英尺公式。</p>

<p class="noindent english">That’s just a statement of the theorem, not a proof, of course. But the proof is surprisingly simple. We can illustrate it with an example from medical diagnosis, one of the “killer apps” of Bayesian inference. Suppose you’re a doctor, and you’ve diagnosed a hundred patients in the last month. Fourteen of them had the flu, twenty had a fever, and eleven had both. The conditional probability of fever given flu is therefore eleven out of fourteen, or 11/14. Conditioning reduces the size of the universe that we’re considering, in this case from all patients to only patients with the flu. In the universe of all patients, the probability of fever is 20/100; in the universe of flu-stricken patients, it’s 11/14. The probability that a patient has the flu <i>and</i> a fever is the fraction of patients that have the flu times the fraction of <i>those</i> that have a fever: <i>P(flu, fever) = P(flu) × P(fever | flu)</i> = 14/100 × 11/14 = 11/100. But we could equally well have done this the other way around: <i>P(flu, fever) = P(fever) × P(flu | fever)</i> . Therefore, since they’re both equal <i>to P(flu,fever), P(fever) × P(flu | fever) = P(flu) × P(fever | flu)</i> . Divide both sides by <i>P(fever)</i> , and you get <i>P(flu | fever) = P(flu) × P(fever | flu) / P(fever)</i> . That’s it! That’s Bayes’ theorem, with flu as the cause and fever as the effect.</p>

<p class="noindent chinese">当然，这只是定理的陈述，不是证明。但证明却出奇地简单。我们可以用医学诊断的一个例子来说明，医学诊断是贝叶斯推理的 “杀手级应用” 之一。假设你是一名医生，你在上个月诊断了 100 个病人。其中 14 人患了流感，20 人发烧，11 人两者都有。因此，发烧的条件概率是 14 人中的 11 人，或 11/14。条件反射减少了我们所考虑的宇宙的大小，在这种情况下，从所有的病人到只有感冒的病人。在所有病人的宇宙中，发烧的概率是 20/100；在流感病人的宇宙中，它是 11/14。一个病人患流感<i>和</i>发烧的概率是患流感病人的比例乘以发烧病人的比例。<i>p(流感 | 发烧)= p(流感)× p(发烧 | 流感)</i> = 14/100 × 11/14 = 11/100。但我们同样可以反过来做。<i>p(流感 | 发烧)=p(发烧)×p(流感 | 发烧)</i>。因此，既然它们都等于<i>p(流感 | 发烧)，那么 p(发烧)×p(流感 | 发烧)=p(流感)×p(发烧 | 流感)</i>。两边都除以<i>p(发烧)</i>，就可以得到<i>p(流感 | 发烧)=p(流感)× p(发烧 | 流感)/ p(发烧)</i>。就是这样！这就是贝叶斯定理，流感是因，发烧是果。</p>

<p class="noindent english"><a id="babilu_link-299"></a> Humans, it turns out, are not very good at Bayesian inference, at least when verbal reasoning is involved. The problem is that we tend to neglect the cause’s prior probability. If you test positive for HIV, and the test only gives 1 percent false positives, should you panic? At first sight, it seems like your chances of having AIDS are now 99 percent. Yikes! But let’s keep a cool head and apply Bayes’ theorem step-by-step: <i>P(HIV | positive) = P(HIV) × P(positive | HIV) / P(positive)</i> . <i>P(HIV)</i> is the prevalence of HIV in the general population, which is about 0.3 percent in the United States. <i>P(positive)</i> is the probability that the test comes out positive whether or not you have AIDS; let’s say that’s 1 percent. So <i>P(HIV | positive)</i> = 0.003 × 0.99 / 0.01 = 0.297. That’s very different from 0.99! The reason is that HIV is rare in the general population. The test coming out positive increases your chances of having AIDS by two orders of magnitude, but they’re still less than half. If you test positive for HIV, the right thing to do is to stay calm and take another, more definitive test. Chances are you’ll be fine.</p>

<p class="noindent chinese">事实证明，人类并不擅长贝叶斯推理，至少在涉及言语推理的时候是这样。问题在于，我们倾向于忽视原因的先验概率。如果你的艾滋病毒检测结果呈阳性，而测试只给出 1% 的假阳性，你应该恐慌吗？乍一看，你现在患艾滋病的概率似乎是 99%。呀！但是，让我们保持冷静的头脑，一步步地应用贝叶斯定理。<i>p(HIV | 阳性)= p(HIV)× p(阳性 | HIV)/ p(阳性)</i>。<i>p(HIV)</i>是 HIV 在普通人群中的流行率，在美国大约是 0.3%。<i>p(阳性)</i>是指无论你是否有艾滋病，测试结果都是阳性的概率；我们假设是 1%。所以<i>p(HIV | 阳性)</i>= 0.003 × 0.99 / 0.01 = 0.297。这与 0.99 有很大的不同！原因是 HIV 在人们的生活中很少见。原因是 HIV 在一般人群中是罕见的。测试结果呈阳性，使你患艾滋病的几率增加了两个数量级，但仍然不到一半。如果你的 HIV 测试呈阳性，正确的做法是保持冷静，再做一次更明确的测试。估计你会没事的。</p>

<p class="noindent english">Bayes’ theorem is useful because what we usually know is the probability of the effects given the causes, but what we want to know is the probability of the causes given the effects. For example, we know what percentage of flu patients have a fever, but what we really want to know is how likely a patient with a fever is to have the flu. Bayes’ theorem lets us go from one to the other. Its significance extends far beyond that, however. For Bayesians, this innocent-looking formula is the <i>F = ma</i> of machine learning, the foundation from which a vast number of results and applications flow. And whatever the Master Algorithm is, it must be “just” a computational implementation of Bayes’ theorem. I put <i>just</i> in quotes because implementing Bayes’ theorem on a computer turns out to be fiendishly hard for all but the simplest problems, for reasons that we’re about to see.</p>

<p class="noindent chinese">贝叶斯定理很有用，因为我们通常知道的是给定原因的效果的概率，但我们想知道的是给定原因的效果的概率。例如，我们知道流感患者中发烧的比例是多少，但我们真正想知道的是发烧的患者患流感的可能性是多少。贝叶斯定理让我们从一个到另一个。然而，它的意义远远超出了这个范围。对于贝叶斯学者来说，这个看起来天真的公式是机器学习的 <i>F = ma</i>，是大量成果和应用的基础。不管主算法是什么，它一定 “只不过” 是贝叶斯定理的一个计算实现。我之所以用在 “只不过” 上用引号，是因为在计算机上实现贝叶斯定理，除了最简单的问题之外，其他问题都是非常困难的，原因我们即将要看到。</p>

<p class="noindent english">Bayes’ theorem as a foundation for statistics and machine learning is bedeviled not just by computational difficulty but also by extreme controversy. You might be forgiven for wondering why: Isn’t it a straightforward consequence of the notion of conditional probability, as we saw in the flu example? Indeed, no one has a problem with the formula itself. The controversy is in how Bayesians obtain the probabilities that go into <a id="babilu_link-128"></a> it and what those probabilities mean. For most statisticians, the only legitimate way to estimate probabilities is by counting how often the corresponding events occur. For example, the probability of fever is 0.2 because twenty out of one hundred observed patients had it. This is the “frequentist” interpretation of probability, and the dominant school of thought in statistics takes its name from it. But notice that in the sunrise example, and in Laplace’s principle of indifference, we did something different: we pulled a probability out of thin air. What exactly justifies assuming a priori that the probability the sun will rise is one-half, or two-thirds, or whatever? Bayesians’ answer is that a probability is not a frequency but a subjective degree of belief. Therefore it’s up to you what you make it, and all that Bayesian inference lets you do is update your prior beliefs with new evidence to obtain your posterior beliefs (also known as “turning the Bayesian crank”). Bayesians’ devotion to this idea is near religious, enough to withstand two hundred years of attacks and counting. And with the appearance on the stage of computers powerful enough to do Bayesian inference, and the massive data sets to go with it, they’re beginning to gain the upper hand.</p>

<p class="noindent chinese">贝叶斯定理作为统计学和机器学习的基础，不仅被计算上的困难所困扰，也被极端的争论所困扰。你可以理解为为什么。正如我们在流感的例子中看到的那样，它不是条件概率概念的一个直接结果吗？事实上，没有人对这个公式本身有异议。争议在于贝叶斯学家如何获得进入的概率以及这些概率意味着什么。对于大多数统计学家来说，估计概率的唯一合法方式是通过计算相应事件的发生频率。例如，发烧的概率是 0.2，因为在一百个观察到的病人中，有二十人发烧。这是对概率的 “频繁主义” 解释，统计学的主流学派也是以此命名的。但是请注意，在日出的例子中，以及在拉普拉斯的冷漠原则中，我们做了一些不同的事情：我们凭空拉出一个概率。到底是什么证明了先验地假设太阳升起的概率是二分之一，或三分之二，或其他什么？贝叶斯主义者的回答是，概率不是一个频率，而是一个主观的信念程度。因此，这取决于你怎么做，而贝叶斯推理让你做的就是用新的证据更新你的先验信念，以获得你的后验信念（也被称为 “转动贝叶斯的曲柄”）。贝叶斯主义者对这一思想的虔诚近乎宗教，足以经受住两百年的攻击和计数。而随着足以进行贝叶斯推断的计算机的出现，以及随之而来的大量数据集的出现，他们开始占据了上风。</p>

<h1 id="babilu_link-423"><b>All models are wrong, but some are useful</b></h1>

<h1 id="babilu_link-423"><b>所有的模型都是错的，但有些是有用的</b></h1>

<p class="noindent english">In reality, a doctor doesn’t diagnose the flu just based on whether you have a fever; she takes a whole bunch of symptoms into account, including whether you have a cough, a sore throat, a runny nose, a headache, chills, and so on. So what we really need to compute is <i>P(flu | fever, cough, sore throat, runny nose, headache, chills,</i> …). By Bayes’ theorem, we know that this is proportional to <i>P(fever, cough, sore throat, runny nose, headache, chills,…| flu)</i> . But now we run into a problem. How are we supposed to estimate this probability? If each symptom is a Boolean variable (you either have it or you don’t) and the doctor takes <i>n</i> symptoms into account, a patient could have 2<sup><i>n</i></sup> possible combinations of symptoms. If we have, say, twenty symptoms and a database of ten thousand patients, we’ve only seen a small fraction of the roughly one million possible combinations. Worse still, to accurately estimate the <a id="babilu_link-87"></a> probability of a particular combination, we need at least tens of observations of it, meaning the database would need to include tens of millions of patients. Add another ten symptoms, and we’d need more patients than there are people on Earth. With a hundred symptoms, even if we were somehow able to magically get the data, there wouldn’t be enough space on all the hard disks in the world to store all the probabilities. And if a patient walks in with a combination of symptoms we haven’t seen before, we won’t know how to diagnose him. We’re face-to-face with our old foe: the combinatorial explosion.</p>

<p class="noindent chinese">在现实中，医生并不只是根据你是否发烧来诊断流感；她会考虑到一大堆症状，包括你是否有咳嗽、喉咙痛、流鼻涕、头痛、发冷等等。所以我们真正需要计算的是<i>p(流感 | 发烧、咳嗽、喉咙痛、流鼻涕、头痛、发冷… )</i>。根据贝叶斯定理，我们知道这与<i>p(发烧、咳嗽、喉咙痛、流鼻涕、头痛、发冷… | 流感)</i>成正比。但现在我们遇到了一个问题。我们应该如何估计这个概率呢？如果每个症状都是一个布尔变量（你要么有，要么没有），而医生要考虑到<i>n 个</i>症状，那么一个病人可能有 2 个<sup><i>n</i></sup>症状的可能组合。如果我们有，比如说，20 个症状和一个有一万个病人的数据库，我们只看到了大约一百万种可能组合中的一小部分。更糟糕的是，为了准确估计某个特定组合的概率，我们至少需要对其进行数十次观察，这意味着数据库需要包括数千万的病人。再加上十个症状，我们需要的病人就会比地球上的人还多。有了一百个症状，即使我们能够以某种方式神奇地获得数据，世界上所有的硬盘也没有足够的空间来存储所有的概率。如果一个病人带着我们以前没有见过的症状组合进来，我们将不知道如何诊断他。我们正与我们的老对手面对面：组合爆炸。</p>

<p class="noindent english">Therefore we do what we always have to do in life: compromise. We make simplifying assumptions that whittle the number of probabilities we have to estimate down to something manageable. A very simple and popular assumption is that all the effects are independent given the cause. This means that, for example, having a fever doesn’t change how likely you are to also have a cough, if we already know you have the flu. Mathematically, this is saying that <i>P(fever, cough | flu)</i> is just <i>P(fever | flu) × P(cough | flu)</i> . Lo and behold: each of these is easy to estimate from a small number of observations. In fact, we did it for fever in the previous section, and it would be no different for cough or any other symptom. The number of observations we need no longer goes up exponentially with the number of symptoms; in fact, it doesn’t go up at all.</p>

<p class="noindent chinese">因此，我们做了我们在生活中一直要做的事情：妥协。我们做一些简化的假设，把我们必须估计的概率数量减少到可控的范围内。一个非常简单和流行的假设是，所有的影响都是独立于原因的。这意味着，例如，如果我们已经知道你得了流感，那么发烧并不会改变你咳嗽的可能性。在数学上，这就是说，<i>p(发烧、咳嗽 | 流感)</i>只是<i>p(发烧 | 流感)× p(咳嗽 | 流感)</i>。看吧：每一个都很容易从少量的观察中估计出来。事实上，我们在上一节中对发烧做了估计，对咳嗽或任何其他症状也是如此。我们需要的观察值的数量不再随着症状数量的增加而呈指数增长；事实上，它根本就没有增加。</p>

<p class="noindent english">Notice that we’re only saying that fever and cough are independent given that you have the flu, not overall. Clearly, if we don’t know whether you have the flu, fever and cough are highly correlated, since you’re much more likely to have a cough if you already have a fever. <i>P(fever, cough)</i> is <i>not</i> equal to <i>P(fever) × P(cough)</i> . All we’re saying is that, if we know you have the flu, knowing whether you have a fever gives us no <i>additional</i> information about whether you have a cough. Likewise, if you don’t know the sun is about to rise and you see the stars fade, your expectation that the sky will lighten increases; but if you already know that sunrise is imminent, seeing the stars fade makes no difference.</p>

<p class="noindent chinese">请注意，我们只是说，在你患有流感的情况下，发烧和咳嗽是独立的，而不是整体。显然，如果我们不知道你是否得了流感，那么发烧和咳嗽是高度相关的，因为如果你已经发烧了，那么你更有可能出现咳嗽。<i>p(发烧，咳嗽)</i><i>不</i>等于<i>p(发烧)× p(咳嗽)</i>。我们要说的是，如果我们知道你得了流感，知道你是否发烧并不能给我们提供关于你是否咳嗽的<i>额外</i>信息。同样，如果你不知道太阳即将升起，而你看到星星变淡，你对天空变亮的期望就会增加；但如果你已经知道日出即将到来，看到星星变淡并没有什么不同。</p>

<p class="noindent english">Notice also that it’s only thanks to Bayes’ theorem that we were able to pull off this trick. If we wanted to directly estimate <i>P(flu | fever, cough, etc.)</i> , without first turning it into <i>P(fever, cough, etc. | flu)</i> using the <a id="babilu_link-147"></a> theorem, we’d still need an exponential number of probabilities, one for each combination of symptoms and flu/not flu.</p>

<p class="noindent chinese">还要注意的是，多亏了贝叶斯定理，我们才能玩出这个把戏。如果我们想直接估计<i>p(流感 | 发烧、咳嗽等)</i>，而不先用定理将其转化为<i>p(发烧、咳嗽等 | 流感)</i>，我们仍然需要一个指数级的概率，每个症状和流感/非流感的组合都要有一个。</p>

<p class="noindent english">A learner that uses Bayes’ theorem and assumes the effects are independent given the cause is called a Naïve Bayes classifier. That’s because, well, that’s such a naïve assumption. In reality, having a fever makes having a cough more likely, even if you already know you have the flu, because (for example) it makes you more likely to have a bad flu. But machine learning is the art of making false assumptions and getting away with it. As the statistician George Box famously put it: “All models are wrong, but some are useful.” An oversimplified model that you have enough data to estimate is better than a perfect one that you don’t. It’s astonishing how simultaneously very wrong and very useful some models can be. The economist Milton Friedman even argued in a highly influential essay that the best theories are the most oversimplified, provided their predictions are accurate, because they explain the most with the least. That seems to me like a bridge too far, but it illustrates that, counter to Einstein’s dictum, science often progresses by making things as simple as possible, and then some.</p>

<p class="noindent chinese">一个使用贝叶斯定理并假定在给定原因的情况下效果是独立的学习者被称为天真贝叶斯分类器。这是因为，嗯，这是一个非常天真的假设。在现实中，发烧使咳嗽的可能性更大，即使你已经知道你有流感，因为（例如）它使你更有可能患严重流感。但是，机器学习是一种做出错误假设并逃脱的艺术。正如统计学家乔治·博克斯的一句名言。“所有的模型都是错的，但有些是有用的”。一个你有足够的数据来估计的过度简化的模型，比一个你没有的完美的模型要好。令人惊讶的是，一些模型可以同时非常错误和非常有用。经济学家米尔顿·弗里德曼甚至在一篇非常有影响力的文章中认为，只要预测准确，最好的理论就是最简化的，因为它们能用最少的东西解释最多的东西。在我看来，这似乎是一座太远的桥，但它说明，与爱因斯坦的箴言相反，科学的进步往往是通过使事情尽可能简单，然后是再简单一些。</p>

<p class="noindent english">No one is sure who invented the Naïve Bayes algorithm. It was mentioned without attribution in a 1973 pattern recognition textbook, but it only took off in the 1990s, when researchers noticed that, surprisingly, it was often more accurate than much more sophisticated learners. I was a graduate student at the time, and when I belatedly decided to include Naïve Bayes in my experiments, I was shocked to find it did better than all the other algorithms I was comparing, save one—luckily, the algorithm I was developing for my thesis, or I might not be here now.</p>

<p class="noindent chinese">没有人确定是谁发明了天真贝叶斯算法。它在 1973 年的一本模式识别教科书中被提及，但它在 20 世纪 90 年代才开始兴起，当时研究人员注意到，令人惊讶的是，它往往比更复杂的学习者更加准确。当时我是一名研究生，当我迟迟难以决定将天真贝叶斯纳入我的实验时，我震惊地发现它比我所比较的所有其他算法都做得更好，除了一个幸运的是，我正在为我的论文开发的算法，否则我现在可能不会在这里。</p>

<p class="noindent english">Naïve Bayes is now very widely used. For example, it forms the basis of many spam filters. It all began when David Heckerman, a prominent Bayesian researcher who is also a medical doctor, had the idea of treating spam as a disease whose symptoms are the words in the e-mail: <i>Viagra</i> is a symptom, and so is <i>free</i> , but your best friend’s first name probably signals a legit e-mail. We can then use Naïve Bayes to classify e-mails into spam and nonspam, provided spammers generate e-mails by picking words at random. That’s a ridiculous assumption, of course: it would only be true <a id="babilu_link-202"></a> if sentences had no syntax and no content. But that summer Mehran Sahami, then a Stanford graduate student, tried it out during an internship at Microsoft Research, and it worked great. When Bill Gates asked Heckerman how this could be, he pointed out that to identify spam you don’t need to understand the details of the message; it’s enough to get the gist of it by seeing which words it contains.</p>

<p class="noindent chinese">天真贝叶斯现在被非常广泛地使用。例如，它构成了许多垃圾邮件过滤器的基础。这一切都始于大卫·赫克曼，一位著名的贝叶斯研究者，同时也是一位医生，他的想法是将垃圾邮件视为一种疾病，其症状是电子邮件中的文字。<i>伟哥</i>是一种症状，<i>免费</i>也是一种症状，但你最好的朋友的名字可能是合法电子邮件的信号。然后，我们可以使用天真贝叶斯将电子邮件分类为垃圾邮件和非垃圾邮件，前提是垃圾邮件发送者通过随机选取单词来生成电子邮件。当然，这是一个可笑的假设：只有在句子没有句法和内容的情况下，它才会是真的。但那年夏天，当时还是斯坦福大学研究生的梅兰·萨哈米在微软研究院实习期间尝试了这一方法，效果很好。当比尔·盖茨问赫克曼这是怎么回事时，他指出，要识别垃圾邮件，你不需要了解信息的细节；只要看到它包含哪些词，就足以了解它的大意。</p>

<p class="noindent english">A basic search engine also uses an algorithm quite similar to Naïve Bayes to decide which web pages to return in answer to your query. The main difference is that, instead of spam/not-spam, it’s trying to predict relevant/not-relevant. The list of prediction problems Naïve Bayes has been applied to is practically endless. Peter Norvig, director of research at Google, told me at one point that it was the most widely used learner there, and Google uses machine learning in every nook and cranny of what it does. It’s not hard to see why Naïve Bayes would be popular among Googlers. Surprising accuracy aside, it scales great; learning a Naïve Bayes classifier is just a matter of counting how many times each attribute co-occurs with each class and takes barely longer than reading the data from disk.</p>

<p class="noindent chinese">一个基本的搜索引擎也使用一种与天真贝叶斯相当类似的算法来决定哪些网页要返回作为对你查询的回答。主要区别在于，它不是预测垃圾邮件/非垃圾邮件，而是预测相关/不相关。天真贝叶斯被应用于预测问题的清单实际上是无穷无尽的。谷歌研究总监彼得·诺维格曾告诉我，它是那里使用最广泛的学习者，谷歌在其工作的每个角落都使用机器学习。这就不难理解为什么天真贝叶斯会受到谷歌员工的欢迎。除了令人惊讶的准确性，它的扩展性也很好；学习天真贝叶斯分类器只是计算每个属性与每个类别共同出现的次数，所需时间几乎不超过从磁盘上读取数据。</p>

<p class="noindent english">You could even use Naïve Bayes, tongue-in-cheek, on a much larger scale than Google’s: to model the whole universe. Indeed, if you believe in an omnipotent God, then you can model the universe as a vast Naïve Bayes distribution where everything that happens is independent given God’s will. The catch, of course, is that we can’t read God’s mind, but in <a href="#babilu_link-5">Chapter 8</a> we’ll investigate how to learn Naïve Bayes models even when we don’t know the classes of the examples.</p>

<p class="noindent chinese">你甚至可以在一个比谷歌的规模更大的地方使用天真贝叶斯，口无遮拦：为整个宇宙建模。事实上，如果你相信有一个无所不能的上帝，那么你就可以把宇宙建模为一个巨大的天真贝叶斯分布，其中发生的一切都独立于上帝的意志。当然，问题是我们无法读懂上帝的思想，但在<a href="#babilu_link-5">第八章</a>中，我们将研究如何学习天真贝叶斯模型，即使我们不知道例子的类别。</p>

<p class="noindent english">It might not seem so at first, but Naïve Bayes is closely related to the perceptron algorithm. The perceptron adds weights and Naïve Bayes multiplies probabilities, but if you take a logarithm, the latter reduces to the former. Both can be seen as generalizations of simple <i>If</i> … <i>then</i> … rules, where each antecedent can count more or less toward the conclusion instead of being “all or none.” This is just one example of the deeper connections among learners that hint at a Master Algorithm. You may not consciously know Bayes’ theorem (well, now you do), but in a way every one of the ten billion neurons in your brain is a tiny instance of it.</p>

<p class="noindent chinese">乍看起来不是这样，但天真贝叶斯与感知器算法密切相关。感知器加权，天真贝叶斯乘以概率，但如果你取一个对数，后者就会减少到前者。两者都可以被看作是简单的 <i>“如果</i>… <i>那么</i>… ” 规则的概括，其中每个前因都可以或多或少地计入结论，而不是 “全部或没有”。这只是暗示主算法的学习者之间更深层次联系的一个例子。你可能不会有意识地知道贝叶斯定理（好吧，现在你知道了），但在某种程度上，你大脑中一百亿个神经元中的每一个都是它的一个小实例。</p>

<p class="noindent english"><a id="babilu_link-126"></a> Naïve Bayes is a good conceptual model of a learner to use when reading the press: it captures the pairwise correlation between each input and the output, which is often all that’s needed to understand references to learning algorithms in news stories. But machine learning is not just pairwise correlations, of course, any more than the brain is just one neuron. The real action begins when we look for more complex patterns.</p>

<p class="noindent chinese">天真贝叶斯是一个很好的学习者概念模型，可以在阅读新闻时使用：它抓住了每个输入和输出之间的成对相关性，这往往是理解新闻故事中提到的学习算法所需要的全部。但是，机器学习不仅仅是成对的相关性，当然，就像大脑只是一个神经元一样。当我们寻找更复杂的模式时，真正的行动才开始。</p>

<h1 id="babilu_link-424"><b>From <b><i>Eugene Onegin</i></b> to Siri</b></h1>

<h1 id="babilu_link-424"><b>从 <b><i>尤金·奥涅金</i></b>致 Siri</b></h1>

<p class="noindent english">In 1913, on the eve of World War I, the Russian mathematician Andrei Markov published a paper applying probability to, of all things, poetry. In it, he modeled a classic of Russian literature, Pushkin’s <i>Eugene Onegin,</i> using what we now call a Markov chain. Rather than assume that each letter was generated at random independently of the rest, he introduced a bare minimum of sequential structure: he let the probability of each letter depend on the letter immediately preceding it. He showed that, for example, vowels and consonants tend to alternate, so if you see a consonant, the next letter (ignoring punctuation and white space) is much more likely to be a vowel than it would be if letters were independent. This may not seem like much, but in the days before computers, it required spending hours manually counting characters, and Markov’s idea was quite new. If <i>Vowel</i> <sub>i</sub> is a Boolean variable that’s true if the <i>i</i> th letter of <i>Eugene Onegin</i> is a vowel and false if it’s a consonant, we can represent Markov’s model with a chain-like graph like this, with an arrow between two nodes indicating a direct dependency between the corresponding variables:</p>

<p class="noindent chinese">1913 年，在第一次世界大战的前夕，俄罗斯数学家安德烈·马尔科夫发表了一篇论文，将概率应用于所有事物，即诗歌。在论文中，他用我们现在所说的马尔科夫链对俄罗斯文学的经典作品普希金的《<i>欧仁·奥涅金</i>》进行了建模。他没有假设每个字母都是独立于其他字母随机产生的，而是引入了一个最低限度的顺序结构：他让每个字母的概率取决于紧随其后的字母。他表明，例如，元音和辅音倾向于交替出现，因此，如果你看到一个辅音，下一个字母（忽略标点符号和空白处）比字母独立时更可能是元音。这可能看起来不多，但在计算机之前的时代，这需要花几个小时手动计算字符，而马尔科夫的想法是相当新的。如果<i>元音</i> <sub>i</sub>是一个布尔变量，如果<i>尤金·奥涅金</i>的第 <i>i</i> 个字母是元音就为真，如果是辅音就为假，我们可以用这样一个链状图表示马尔科夫的模型，两个节点之间的箭头表示相应变量之间的直接依赖关系。</p>

<div>

<div>

<img alt="image" src="images/000005.jpg"/>

</div>

</div>

<p class="noindent english">Markov assumed (wrongly but usefully) that the probabilities are the same at every position in the text. Thus we need to estimate only three probabilities: <i>P(Vowel<sub><i>1</i></sub></i> = <i>True)</i> , <i>P(Vowel<sub><i>i+1</i></sub></i> = <i>True | Vowel<sub><i>i</i></sub></i> = <i>True)</i> , and <i>P(Vowel<sub><i>i+1</i></sub></i> = <i>True | Vowel<sub><i>i</i></sub></i> = <i>False)</i> . (Since probabilities sum to one, from these we can immediately obtain <i>P(Vowel<sub><i>1</i></sub></i> = <i>False)</i> , etc.) As with <a id="babilu_link-123"></a> Naïve Bayes, we can have as many variables as we want without the number of probabilities we need to estimate going through the roof, but now the variables actually depend on each other.</p>

<p class="noindent chinese">马尔科夫假设（错误但有用），在文本的每个位置上，概率是相同的。因此，我们只需要估计三个概率。<i>P(Vowel<sub><i>1</i></sub></i> = <i>True)</i>，<i>P(Vowel<sub><i>i+1</i></sub></i> = <i>True | Vowel<sub><i>i</i></sub></i> = <i>True)</i>, 和<i>P(Vowel<sub><i>i+1</i></sub></i> = <i>True | Vowel<sub><i>i</i></sub></i> = <i>False)</i>。(由于概率之和为 1，从这些概率中我们可以立即得到<i>P(Vowel<sub><i>1</i></sub></i>=<i>False)</i>，等等。）与天真贝叶斯一样，我们可以有任意多的变量，我们需要估计的概率数量也不会超过屋顶，但现在这些变量实际上相互依赖。</p>

<p class="noindent english">If we measure not just the probability of vowels versus consonants, but the probability of each letter in the alphabet following each other, we can have fun generating new texts with the same statistics as <i>Onegin</i> : choose the first letter, then choose the second based on the first, and so on. The result is complete gibberish, of course, but if we let each letter depend on several previous letters instead of just one, it starts to sound more like the ramblings of a drunkard, locally coherent even if globally meaningless. Still not enough to pass the Turing test, but models like this are a key component of machine-translation systems, like Google Translate, which lets you see the whole web in English (or almost), regardless of the language the pages were originally written in.</p>

<p class="noindent chinese">如果我们不仅测量元音与辅音的概率，而且测量字母表中每个字母相互跟随的概率，我们就可以用与<i>奥涅金</i>相同的统计方法来生成新的文本：选择第一个字母，然后在第一个字母的基础上选择第二个字母，如此循环。当然，其结果是完全的胡言乱语，但如果我们让每个字母都取决于之前的几个字母，而不是只有一个，那么它开始听起来更像是一个醉汉的胡言乱语，即使全局无意义，也是局部一致的。这仍然不足以通过图灵测试，但像这样的模型是机器翻译系统的一个关键组成部分，比如谷歌翻译，它让你看到整个网络的英语（或几乎是），而不管网页最初是用什么语言写的。</p>

<p class="noindent english">PageRank, the algorithm that gave rise to Google, is itself a Markov chain. Larry Page’s idea was that web pages with many incoming links are probably more important than pages with few, and links from important pages should themselves count for more. This sets up an infinite regress, but we can handle it with a Markov chain. Imagine a web surfer going from page to page by randomly following links: the states of this Markov chain are web pages instead of characters, making it a vastly larger problem, but the math is the same. A page’s score is then the fraction of the time the surfer spends on it, or equivalently, his probability of landing on the page after wandering around for a long time.</p>

<p class="noindent chinese">PageRank，这个催生了谷歌的算法，本身就是一个马尔可夫链。拉里·佩奇的想法是，有很多传入链接的网页可能比传入链接少的网页更重要，而来自重要网页的链接本身应该算得更多。这设置了一个无限的回归，但我们可以用马尔可夫链来处理它。想象一下，一个网络冲浪者通过随机跟踪链接从一个页面到另一个页面：这个马尔可夫链的状态是网页而不是字符，这使得它成为一个大得多的问题，但数学原理是一样的。一个页面的得分是冲浪者在上面花费的时间，或者说，他在徘徊了很长时间后登陆到该页面的概率。</p>

<p class="noindent english">Markov chains turn up everywhere and are one of the most intensively studied topics in mathematics, but they’re still a very limited kind of probabilistic model. We can go one step further with a model like this:</p>

<p class="noindent chinese">马尔科夫链随处可见，是数学中研究最深入的课题之一，但它们仍然是一种非常有限的概率模型。我们可以用这样的模型更进一步。</p>

<div>

<div>

<img alt="image" src="images/000027.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-124"></a> The states form a Markov chain, as before, but we don’t get to see them; we have to infer them from the observations. This is called a hidden Markov model, or HMM for short. (Slightly misleading, because it’s the states that are hidden, not the model.) HMMs are at the heart of speech-recognition systems like Siri. In speech recognition, the hidden states are written words, the observations are the sounds spoken to Siri, and the goal is to infer the words from the sounds. The model has two components: the probability of the next word given the current one, as in a Markov chain, and the probability of hearing various sounds given the word being pronounced. (How exactly to do the inference is a fascinating problem that we’ll turn to after the next section.)</p>

<p class="noindent chinese">状态形成一个马尔可夫链，就像以前一样，但我们看不到它们；我们必须从观察中推断出它们。这被称为隐藏马尔可夫模型，简称 HMM（hidden Markov model，略有误导，因为隐藏的是状态，而不是模型）。HMM 是 Siri 等语音识别系统的核心。在语音识别中，隐藏状态是书面文字，观察值是对 Siri 说的声音，目标是从声音中推断出文字。该模型有两个组成部分：在马尔可夫链中，鉴于当前的单词，下一个单词的概率，以及鉴于正在发音的单词，听到各种声音的概率。（具体如何进行推理是一个迷人的问题，我们将在下一节后讨论）。</p>

<p class="noindent english">Siri aside, you use an HMM every time you talk on your cell phone. That’s because your words get sent over the air as a stream of bits, and the bits get corrupted in transit. The HMM then figures out the intended bits (hidden state) from the ones received (observations), which it should be able to do as long as not too many bits got mangled.</p>

<p class="noindent chinese">撇开 Siri 不谈，你每次用手机通话时都会用到 HMM。这是因为你的话是以比特流的形式在空中发送的，而这些比特在传输过程中会被破坏。然后，HMM 从收到的比特（观测值）中找出预定的比特（隐藏状态），只要没有太多的比特被破坏，它就应该能够做到这一点。</p>

<p class="noindent english">HMMs are also a favorite tool of computational biologists. A protein is a sequence of amino acids, and DNA is a sequence of bases. If we want to predict, for example, how a protein will fold into a 3-D shape, we can treat the amino acids as the observations and the type of fold at each point as the hidden state. Similarly, we can use an HMM to identify the sites in DNA where gene transcription is initiated and many other properties.</p>

<p class="noindent chinese">HMMs 也是计算生物学家最喜欢的工具。蛋白质是一个氨基酸的序列，而 DNA 是一个碱基的序列。例如，如果我们想预测一个蛋白质将如何折叠成一个三维形状，我们可以把氨基酸当作观察值，把每个点的折叠类型当作隐藏状态。同样，我们可以用 HMM 来识别 DNA 中基因转录开始的位点和其他许多属性。</p>

<p class="noindent english">If the states and observations are continuous variables instead of discrete ones, the HMM becomes what’s known as a Kalman filter. Economists use Kalman filters to remove noise from time series of quantities like GDP, inflation, and unemployment. The “true” GDP values are the hidden states; at each time step, the true value should be similar to the observed one, but also to the previous true value, since the economy seldom makes abrupt jumps. The Kalman filter trades off these two, yielding a smoother curve that still accords with the observations. When a missile cruises to its target, it’s a Kalman filter that keeps it on track. Without it, there would have been no man on the moon.</p>

<p class="noindent chinese">如果状态和观测值是连续变量而不是离散变量，那么 HMM 就变成了所谓的卡尔曼过滤器。经济学家使用卡尔曼滤波器来消除 GDP、通货膨胀和失业等数量的时间序列中的噪音。GDP 的 “真实” 值是隐藏的状态；在每个时间步骤中，真实值应该与观察到的值相似，但也与之前的真实值相似，因为经济很少会出现突然的跳跃。卡尔曼滤波器在这两者之间进行权衡，产生一个更平滑的曲线，但仍与观测值相一致。当一枚导弹巡航到它的目标时，是卡尔曼滤波器使它保持在轨道上。没有它，就不会有人类登上月球。</p>

<h1 id="babilu_link-425"><b><a id="babilu_link-117"><b></b></a> Everything is connected, but not directly</b></h1>

<h1 id="babilu_link-425"><b><a id="babilu_link-117"><b></b></a>一切都有联系，但不直接</b></h1>

<p class="noindent english">HMMs are good for modeling sequences of all kinds, but they’re still a far cry from the flexibility of the symbolists’ <i>If</i> … <i>then</i> … rules, where anything can appear as an antecedent, and a rule’s consequent can in turn be an antecedent in any downstream rule. If we allow such an arbitrary structure in practice, however, the number of probabilities we need to learn blows up. For a long time no one knew how to square this circle, and researchers resorted to ad-hoc schemes, like attaching confidence estimates to rules and somehow combining them. If A implies B with confidence 0.8 and B implies C with confidence 0.7, then perhaps A implies C with confidence 0.8 × 0.7.</p>

<p class="noindent chinese">HMMs 适合于对各种序列进行建模，但它们与符号学家的 “<i>如果</i>… <i>那么</i>… ” 规则的灵活性相差甚远，在这种规则中，任何东西都可以作为前因出现，而一个规则的后果又可以成为任何下游规则的前因。然而，如果我们在实践中允许这样一种任意的结构，我们需要学习的概率数量就会爆炸。很长时间以来，没有人知道如何解决这个问题，研究人员求助于临时方案，比如将置信度估计附加到规则上，并以某种方式将它们结合起来。如果 A 暗示 B，置信度为 0.8，B 暗示 C，置信度为 0.7，那么也许 A 暗示 C，置信度为 0.8 × 0.7。</p>

<p class="noindent english">The problem with these schemes is that they can go badly awry. From the two perfectly reasonable rules <i>If the sprinkler is on, then the grass is wet</i> and <i>If the grass is wet, then it rained</i> , I can infer the nonsensical rule <i>If the sprinkler is on, then it rained</i> . A more insidious problem is that with confidence-rated rules we’re prone to double-counting evidence. Suppose you read in the <i>New York Times</i> that aliens have landed. Maybe it’s a prank, even though it’s not April 1. But now you see the same headline in the <i>Wall Street Journal</i> , <i>USA Today,</i> and the <i>Washington Post</i> . You start to panic, like the listeners to Orson Welles’s infamous <i>War of the Worlds</i> radio broadcast who didn’t realize it was a dramatization. If, however, you check the fine print and notice that all four newspapers got the story from the Associated Press, you go back to suspecting it’s a prank, this time by an AP reporter. Rule systems have no way of dealing with this, and neither does Naïve Bayes. If it uses features like <i>Reported in the</i> New York Times as predictors that a news story is true, all it can do is add <i>Reported by AP,</i> which only makes things worse.</p>

<p class="noindent chinese">这些计划的问题是，它们可能会出现严重的错误。从两条完全合理的规则中<i>，如果洒水车开着，那么草是湿的</i>，<i>如果草是湿的，那么下雨了</i>，我可以推断出毫无意义的规则，<i>如果洒水车开着，那么下雨了</i>。一个更隐蔽的问题是，有了信心评级的规则，我们很容易重复计算证据。假设你在《<i>纽约时报</i>》上读到外星人登陆的消息。也许这是个恶作剧，尽管现在不是 4 月 1 日。但现在你在《<i>华尔街日报</i>》、《<i>今日美国</i>》和《<i>华盛顿邮报</i>》上看到同样的头条新闻。你开始惊慌失措，就像奥森·威尔斯臭名昭著的《<i>世界大战</i>》广播的听众一样，他们没有意识到这是一场戏剧化。然而，如果你检查一下细小的字体，发现这四家报纸的报道都来自美联社，你又会怀疑这是个恶作剧，这次是美联社的记者。规则系统没有办法处理这个问题，天真贝叶斯也是如此。如果它使用像《纽约时报》<i>的报道</i>这样的特征来预测一个新闻故事的真实性，它所能做的就是增加<i>美联社的报道</i>，这只会让事情变得更糟。</p>

<p class="noindent english">The breakthrough came in the early 1980s, when Judea Pearl, a professor of computer science at the University of California, Los Angeles, invented a new representation: Bayesian networks. Pearl is one of the most distinguished computer scientists in the world, his methods having swept through machine learning, AI, and many other fields. He won the Turing Award, the Nobel Prize of computer science, in 2012.</p>

<p class="noindent chinese">突破出现在 20 世纪 80 年代初，当时加利福尼亚大学洛杉矶分校的计算机科学教授朱迪亚·珀尔发明了一种新的表示方法。贝叶斯网络。佩尔是世界上最杰出的计算机科学家之一，他的方法已经席卷了机器学习、人工智能和许多其他领域。他在 2012 年获得了图灵奖，即计算机科学的诺贝尔奖。</p>

<p class="noindent english"><a id="babilu_link-160"></a> Pearl realized that it’s OK to have a complex network of dependencies among random variables, provided each variable depends directly on only a few others. We can represent these dependencies with a graph like the ones we saw for Markov chains and HMMs, except now the graph can have any structure (as long as the arrows don’t form closed loops). One of Pearl’s favorite examples is burglar alarms. The alarm at your house should go off if a burglar attempts to break in, but it could also be triggered by an earthquake. (In Los Angeles, where Pearl lives, earthquakes are almost as frequent as burglaries.) If you’re working late one night and your neighbor Bob calls to say he just heard your alarm go off, but your neighbor Claire doesn’t, should you call the police? Here’s the graph of dependencies:</p>

<p class="noindent chinese">珀尔意识到，在随机变量之间有一个复杂的依赖网络是可以的，只要每个变量只直接依赖于其他几个变量。我们可以用一个图来表示这些依赖关系，就像我们看到的马尔科夫链和 HMMs 那样，只不过现在这个图可以有任何结构（只要箭头不形成闭环）。珀尔最喜欢的一个例子是防盗报警器。如果有小偷试图闯入，你家里的警报器就会响起，但它也可能被地震触发。（在珀尔居住的洛杉矶，地震几乎和入室盗窃一样频繁）。如果一天晚上你工作到很晚，你的邻居鲍勃打电话说他刚听到你的警报器响了，但你的邻居克莱尔却没有，你应该报警吗？这是依赖关系的图表。</p>

<div>

<div>

<img alt="image" src="images/000019.jpg"/>

</div>

</div>

<p class="noindent english">If there’s an arrow from one node to another in the graph, we say that the first node is a parent of the second. So <i>Alarm</i> ’s parents are <i>Burglary</i> and <i>Earthquake</i> , and <i>Alarm</i> is the sole parent of <i>Bob calls</i> and <i>Claire calls</i> . A Bayesian network is a graph of dependencies like this, together with a table for each variable, giving its probability for each combination of values of its parents. For <i>Burglary</i> and <i>Earthquake</i> we only need one probability each, since they have no parents. For <i>Alarm</i> we need four: the probability that it goes off even if there’s no burglary or earthquake, the probability that it goes off if there’s a burglary and no earthquake, and so on. For <i>Bob calls</i> we need two probabilities (given alarm and given no alarm), and similarly for Claire.</p>

<p class="noindent chinese">如果图中有一个箭头从一个节点到另一个节点，我们就说第一个节点是第二个节点的父节点。所以 <i>警报器</i> 的父节点是 <i>入室盗窃</i> 和 <i>地震</i>，而 <i>警报器</i> 是 <i>Bob 的呼叫</i> 和 <i>Claire 的呼叫</i> 的唯一父节点。贝叶斯网络是一个这样的依赖关系图，同时还有一个每个变量的表格，给出它在其父辈的每个数值组合中的概率。对于<i>入室盗窃</i>和<i>地震</i>，我们只需要一个概率，因为它们没有父节点。对于<i>报警器</i>，我们需要四个：即使没有入室盗窃或地震，它也会响的概率，如果有入室盗窃和没有地震，它也会响的概率，以此类推。对于<i>鲍勃的电话</i>，我们需要两个概率（给定警报和给定无警报），对于克莱尔也是如此。</p>

<p class="noindent english">Here’s the crucial point: Bob calling depends on <i>Burglary</i> and <i>Earthquake</i> , but only through <i>Alarm</i> . Bob’s call is <i>conditionally independent</i> of <i>Burglary</i> and <i>Earthquake</i> given <i>Alarm</i> , and so is Claire’s. If the alarm <a id="babilu_link-142"></a> doesn’t go off, your neighbors sleep soundly, and the burglar proceeds undisturbed. Also, Bob and Claire are independent given <i>Alarm</i> . Without this independence structure, you’d need to learn 2<sup>5</sup> = 32 probabilities, one for each possible state of the five variables. (Or 31, if you’re a stickler for details, since the last one can be left implicit.) With the conditional independencies, all you need is 1 + 1 + 4 + 2 + 2 = 10, a savings of 68 percent. And that’s just in this tiny example; with hundreds or thousands of variables, the savings would be very close to 100 percent.</p>

<p class="noindent chinese">这里是关键的一点：Bob 的呼叫取决于 <i>入室盗窃</i>和<i>地震</i>，但只是通过<i>警报器</i>。鉴于<i>警报器</i>，Bob 的呼叫<i>有条件地</i>独立于<i>入室盗窃</i>和<i>地震</i>，而 Claire 的呼叫也是如此。如果报警器未响，那么你的邻居就会睡得很香，而窃贼就不会受到干扰。另外，鲍勃和克莱尔是独立的，给定<i>警报器</i>。如果没有这种独立性结构，你就需要学习 2<sup>5</sup> = 32 个概率，五个变量的每个可能状态都有一个。（或者 31，如果你是一个坚持细节的人，因为最后一个可以隐含。）有了条件独立性，你只需要 1 + 1 + 4 + 2 + 2 = 10，节省了 68%。这只是在这个小例子中；如果有成百上千的变量，节省的费用就会非常接近 100% 了。</p>

<p class="noindent english">The first law of ecology, according to biologist Barry Commoner, is that everything is connected to everything else. That may be true, but it would also make the world impossible to understand, if not for the saving grace of conditional independence: everything is connected, but only indirectly. In order to affect me, something that happens a mile away must first affect something in my neighborhood, even if only through the propagation of light. As one wag put it, space is the reason everything doesn’t happen to you. Put another way, the structure of space is an instance of conditional independence.</p>

<p class="noindent chinese">生物学家巴里·康伯尔认为，生态学的第一条定律是一切事物都与其他事物相联系。这可能是真的，但它也会使世界变得无法理解，如果不是因为有条件的独立性的拯救之恩：一切都有联系，但只是间接的。为了影响我，一英里外发生的事情必须首先影响我附近的东西，即使只是通过光的传播。正如一位学者所说，空间是一切不发生在你身上的原因。换句话说，空间的结构是一个条件独立的例子。</p>

<p class="noindent english">In the burglary example, the full table of thirty-two probabilities is never represented explicitly, but it’s implicit in the collection of smaller tables and graph structure. To obtain <i>P(Burglary, Earthquake, Alarm, Bob calls, Claire calls)</i> , all I have to do is multiply <i>P(Burglary)</i> , <i>P(Earthquake)</i> , <i>P(Alarm | Burglary, Earthquake)</i> , <i>P(Bob calls | Alarm)</i> , and <i>P(Claire calls | Alarm)</i> . It’s the same in any Bayesian network: to obtain the probability of a complete state, just multiply the probabilities from the corresponding lines in the individual variables’ tables. So, provided the conditional independencies hold, no information is lost by switching to the more compact representation. And in this way we can easily compute the probabilities of extremely unusual states, including states that were never observed before. Bayesian networks give the lie to the common misconception that machine learning can’t predict very rare events, or “black swans,” as Nassim Taleb calls them.</p>

<p class="noindent chinese">在入室盗窃的例子中，三十二个概率的完整表格从未明确表示，但它隐含在小表格和图结构的集合中。为了得到<i>p(入室盗窃，地震，报警，鲍勃打电话，克莱尔打电话)</i>，我所要做的就是把<i>p(入室盗窃)</i>，<i>p(地震)</i>，<i>p(报警 | 入室盗窃，地震)</i>，<i>p(鲍勃打电话 | 报警)</i>，以及<i>p(克莱尔打电话 | 报警)</i>相乘。这在任何贝叶斯网络中都是一样的：要获得一个完整状态的概率，只需将各个变量表中相应行的概率相乘。因此，只要条件独立性成立，切换到更紧凑的表示法就不会丢失任何信息。通过这种方式，我们可以很容易地计算出极其不寻常的状态的概率，包括以前从未观察到的状态。贝叶斯网络掩盖了一种常见的误解，即机器学习不能预测非常罕见的事件，或纳西姆·塔勒布所说的 “黑天鹅”。</p>

<p class="noindent english">In retrospect, we can see that Naïve Bayes, Markov chains, and HMMs are all special cases of Bayesian networks. The structure of Naïve Bayes is:</p>

<p class="noindent chinese">回过头来，我们可以看到，天真贝叶斯、马尔科夫链和 HMMs 都是贝叶斯网络的特例。天真贝叶斯的结构是：</p>

<div>

<div>

<img alt="image" src="images/000025.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-35"></a> Markov chains encode the assumption that the future is conditionally independent of the past given the present. HMMs assume in addition that each observation depends only on the corresponding state. Bayesian networks are for Bayesians what logic is for symbolists: a lingua franca that allows us to elegantly encode a dizzying variety of situations and devise algorithms that work uniformly in all of them.</p>

<p class="noindent chinese">马尔科夫链编码的假设是，在给定现在的情况下，未来有条件地独立于过去。此外，HMMs 还假设每个观察值只取决于相应的状态。贝叶斯网络对于贝叶斯学家来说，就像逻辑学对于符号学家一样：一种语言，使我们能够优雅地编码各种令人眼花缭乱的情况，并设计出在所有情况下统一工作的算法。</p>

<p class="noindent english">We can think of a Bayesian network as a “generative model,” a recipe for probabilistically generating a state of the world: first decide independently whether there’s a burglary and/or an earthquake, then based on that decide whether the alarm goes off, and then based on that whether Bob and Claire call. A Bayesian network tells a story: A happened, and it led to B; at the same time, C also happened, and B and C together caused D. To compute the probability of a particular story, we just multiply the probabilities of all of its different strands.</p>

<p class="noindent chinese">我们可以把贝叶斯网络看作是一个 “生成模型”，一个概率地生成世界状态的配方：首先独立决定是否有入室盗窃和/或地震，然后根据这个决定警报是否响起，再根据这个决定鲍勃和克莱尔是否打电话。一个贝叶斯网络讲述了一个故事。A 发生了，它导致了 B；同时，C 也发生了，B 和 C 一起导致了 D。为了计算一个特定故事的概率，我们只需将其所有不同线索的概率相乘。</p>

<p class="noindent english">One of the most exciting applications of Bayesian networks is modeling how genes regulate each other in living cells. Billions of dollars have been spent trying to discover pairwise correlations between individual genes and specific diseases, but the yield has been disappointingly low. In retrospect, this is not so surprising: a cell’s behavior is the result of complex interactions among genes and the environment, and a single gene has limited predictive power. But with Bayesian networks, we can uncover these interactions, provided we have the requisite data, and with the spread of DNA microarrays, we increasingly do.</p>

<p class="noindent chinese">贝叶斯网络最令人兴奋的应用之一是模拟基因如何在活细胞中相互调节。为了发现单个基因和特定疾病之间的成对相关性，已经花费了数十亿美元，但收益却低得令人失望。现在回想起来，这并不令人惊讶：细胞的行为是基因和环境之间复杂互动的结果，单一基因的预测能力有限。但是通过贝叶斯网络，我们可以发现这些相互作用，只要我们有必要的数据，而且随着 DNA 微阵列的普及，我们越来越多地做到了。</p>

<p class="noindent english">After pioneering the application of machine learning to spam filtering, David Heckerman turned to using Bayesian networks in the fight against AIDS. The AIDS virus is a tough adversary because it mutates rapidly, making it difficult for any one vaccine or drug to pin it down <a id="babilu_link-34"></a> for long. Heckerman noticed that this is the same cat-and-mouse game that spam filters play with spam and decided to apply a lesson he had learned there: attack the weakest link. In the case of spam, weak links include the URLs you have to use to take payment from the customer. In the case of HIV, they’re small regions of the virus protein that can’t change without hurting the virus. If he could train the immune system to recognize these regions and attack the cells displaying them, he just might have an AIDS vaccine. Heckerman and coworkers used a Bayesian network to help identify the vulnerable regions and developed a vaccine delivery mechanism that could teach the immune system to attack just those regions. The delivery mechanism worked in mice, and clinical trials are now in preparation.</p>

<p class="noindent chinese">在率先将机器学习应用于垃圾邮件过滤之后，大卫·赫克曼转而将贝叶斯网络用于抗击艾滋病。艾滋病病毒是一个难缠的对手，因为它变异迅速，使得任何一种疫苗或药物都很难长期锁定它。赫克曼注意到，这与垃圾邮件过滤器与垃圾邮件玩的猫捉老鼠的游戏一样，并决定应用他在那里学到的一个教训：攻击最薄弱的环节。在垃圾邮件的情况下，薄弱环节包括你必须用来从客户那里获得付款的 URL。在艾滋病毒的案例中，它们是病毒蛋白的小区域，在不伤害病毒的情况下不能改变。如果他能训练免疫系统识别这些区域并攻击显示这些区域的细胞，他就可能有一种艾滋病疫苗。赫克曼和他的同事使用贝叶斯网络来帮助识别脆弱的区域，并开发了一种疫苗投放机制，可以教导免疫系统只攻击这些区域。这种传递机制在小鼠身上发挥了作用，目前正在准备临床试验。</p>

<p class="noindent english">It often happens that, even after we take all conditional independences into account, some nodes in a Bayesian network still have too many parents. Some networks are so dense with arrows that when we print them, the page turns solid black. (The physicist Mark Newman calls them “ridiculograms.”) A doctor needs to simultaneously diagnose all the possible diseases a patient could have, not just one, and every disease is a parent of many different symptoms. A fever could be caused by any number of conditions besides the flu, but it’s hopeless to try to predict its probability given every possible combination of conditions. All is not lost. Instead of a table specifying the node’s conditional probability for every state of its parents, we can learn a simpler distribution. The most popular choice is a probabilistic version of the logical OR operation: any cause alone can provoke a fever, but each cause has a certain probability of failing to do so, even if it’s usually sufficient. Heckerman and others have learned Bayesian networks that diagnose hundreds of infectious diseases in this way. Google uses a giant Bayesian network of this type in its AdSense system for automatically choosing ads to place on web pages. The network relates a million content variables to each other and to twelve million words and phrases via over three hundred million arrows, all learned from a hundred billion text snippets and search queries.</p>

<p class="noindent chinese">经常发生的情况是，即使我们考虑了所有的条件独立性，贝叶斯网络中的一些节点仍然有太多的父节点。有些网络中的箭头是如此密集，以至于当我们打印它们时，页面变成了纯黑色。（物理学家马克·纽曼称它们为 “可笑的图表”。）医生需要同时诊断病人可能得的所有疾病，而不仅仅是一种，而且每一种疾病都是许多不同症状的父代。除了流感之外，发烧可能是由任何数量的疾病引起的，但是在每一种可能的条件组合下，试图预测其概率是没有希望的。一切都没有失去。我们可以学习一个更简单的分布，而不是用一个表格来指定节点对其父母的每个状态的条件概率。最流行的选择是逻辑 OR 操作的概率版本：任何单独的原因都可以激起发烧，但每个原因都有一定的概率无法做到，即使它通常是足够的。赫克曼和其他人已经学会了贝叶斯网络，以这种方式诊断了数百种传染病。谷歌在其 AdSense 系统中使用了这种类型的巨型贝叶斯网络，用于自动选择在网页上投放广告。该网络将一百万个内容变量相互联系起来，并通过三亿多个箭头与一千二百万个单词和短语联系起来，所有这些都是从一千亿个文本片段和搜索查询中学习的。</p>

<p class="noindent english">On a lighter note, Microsoft’s Xbox Live uses a Bayesian network to rate players and match players of similar skill. The outcome of a game is <a id="babilu_link-119"></a> a probabilistic function of the opponents’ skill levels, and using Bayes’ theorem we can infer a player’s skill from the outcomes of his games.</p>

<p class="noindent chinese">说句题外话，微软的 Xbox Live 使用贝叶斯网络对玩家进行评级，并匹配技能相似的玩家。一场比赛的结果是是对手技能水平的概率函数，利用贝叶斯定理，我们可以从一个玩家的比赛结果中推断出他的技能。</p>

<h1 id="babilu_link-426"><b>The inference problem</b></h1>

<h1 id="babilu_link-426"><b>推理问题</b></h1>

<p class="noindent english">There’s a big snag in all of this, unfortunately. Just because a Bayesian network lets us compactly represent a probability distribution doesn’t mean we can also reason efficiently with it. Suppose you want to compute <i>P(Burglary | Bob called, Claire didn’t)</i> . By Bayes’ theorem, you know this is just <i>P(Burglary) P(Bob called, Claire didn’t | Burglary) / P(Bob called, Claire didn’t)</i> , or equivalently, <i>P(Burglary, Bob called, Claire didn’t) / P(Bob called, Claire didn’t)</i> . If you had the full table with the probabilities of all states, you could obtain both of these probabilities by adding up the corresponding lines in the table. For example, <i>P(Bob called, Claire didn’t)</i> is the sum of the probabilities of all the lines where Bob calls and Claire doesn’t. But the Bayesian network doesn’t give you the full table. You could always construct it from the individual tables, but that takes exponential time and space. What we really want is to compute <i>P(Burglary | Bob called, Claire didn’t)</i> without building the full table. That, in a nutshell, is the problem of inference in Bayesian networks.</p>

<p class="noindent chinese">不幸的是，这一切都有一个很大的障碍。贝叶斯网络让我们紧凑地表示一个概率分布，并不意味着我们也能有效地用它进行推理。假设你想计算<i>p(入室盗窃 | Bob 呼叫, Claire 没有)</i>。根据贝叶斯定理，你知道这只是<i>p(入室盗窃)p(Bob 呼叫, Claire 没有 | 入室盗窃)/ p(Bob 呼叫, Claire 没有)</i>，或者等价地，<i>p(入室盗窃, Bob 呼叫, Claire 没有)/ p(Bob 呼叫, Claire 没有)</i>。如果你有一张包含所有状态概率的完整表格，你可以通过将表格中的相应行相加来获得这两个概率。例如，<i>p(Bob 呼叫, Claire 没有)</i>是 Bob 打电话而 Claire 没有打电话的所有行的概率之和。但是贝叶斯网络并没有给你完整的表格。你总是可以从单个表格中构建它，但这需要指数级的时间和空间。我们真正想要的是计算<i>p(入室盗窃 | Bob 呼叫, Claire 没有)</i>而不需要建立完整的表格。简而言之，这就是贝叶斯网络中的推理问题。</p>

<p class="noindent english">In many cases we can do this and avoid the exponential blowup. Suppose you’re leading a platoon in single file through enemy territory in the dead of night, and you want to make sure that all your soldiers are still with you. You could stop and count them yourself, but that wastes too much time. A cleverer solution is to just ask the first soldier behind you: “How many soldiers are behind you?” Each soldier asks the next the same question, until the last one says “None.” The next-to-last soldier can now say “One,” and so on all the way back to the first soldier, with each soldier adding one to the number of soldiers behind him. Now you know how many soldiers are still with you, and you didn’t even have to stop.</p>

<p class="noindent chinese">在许多情况下，我们可以这样做，避免指数爆炸。假设你正带领一个排在夜深人静的时候以单行线穿过敌人的领土，你想确保所有的士兵都还在你身边。你可以停下来自己数数，但那会浪费太多时间。一个更聪明的办法是直接问你身后的第一个士兵。“你后面有多少个士兵？” 每个士兵都向下一个士兵问同样的问题，直到最后一个士兵说 “没有”。接下来的最后一个士兵现在可以说 “一个”，以此类推，一直问到第一个士兵，每个士兵在他身后的士兵数量上加一个。现在你知道还有多少士兵和你在一起了，而且你甚至都不用停下来。</p>

<p class="noindent english">Siri uses the same idea to compute the probability that you just said, “Call the police” from the sounds it picked up from the microphone. Think of “Call the police” as a platoon of words marching across the <a id="babilu_link-427"></a> page in single file. <i>Police</i> wants to know its probability, but for that it needs to know the probability of <i>the</i> ; and <i>the</i> in turn needs to know the probability of <i>call</i> . So <i>call</i> computes its probability and passes it on to <i>the</i> , which does the same and passes the result to <i>police</i> . Now <i>police</i> knows its probability, duly influenced by every word in the sentence, but we never had to construct the full table of eight possibilities (the first word is <i>call</i> or isn’t, the second is <i>the</i> or isn’t, and the third is <i>police</i> or isn’t). In reality, Siri considers all words that could appear in each position, not just whether the first word is <i>call</i> or not and so on, but the algorithm is the same. Perhaps Siri thinks, based on the sounds, that the first word was either <i>call</i> or <i>tell</i> , the second was <i>the</i> or <i>her</i> , and the third was <i>police</i> or <i>please</i> . Individually, perhaps the most likely words are <i>call</i> , <i>the</i> , and <i>please</i> . But that forms the nonsensical sentence “Call the please,” so taking the other words into account, Siri concludes that the sentence is really “Call the police.” It makes the call, and with luck the police get to your house in time to catch the burglar.</p>

<p class="noindent chinese">Siri 利用同样的思路，从麦克风发出的声音中计算出你刚才说 “Call the police” 的概率。把 “Call the police” 想象成一个排的单词，排成一列，穿过一个文件的某页。<i>Police</i> 想知道它的概率，但为此它需要知道;<i>the</i> 的概率，而 <i>the</i> 又需要知道 <i>call</i> 的概率。因此，<i>call</i> 计算其概率并将其传递给 <i>the</i>，后者也做同样的计算并将结果传递给<i>police</i>。现在，<i>police</i>知道了自己的概率，它受到了句子中每个词的影响，但我们从来都不需要构建八个可能性的完整表格（第一个词是 <i>call</i> 或者不是，第二个是 <i>the</i> 或者不是，第三个是 <i>police</i> 或者不是）。实际上，Siri 考虑的是每个位置可能出现的所有单词，而不仅仅是第一个单词是否是 <i>call</i>，以此类推，但算法是一样的。也许 Siri 根据声音认为，第一个词是 <i>call</i> 或 <i>tell</i>，第二个是 <i>the</i> 或 <i>her</i>，第三个是 <i>police</i> 或 <i>please</i>。单独来看，也许最可能的词是 <i>call</i>, <i>the</i>, 和 <i>please</i>。但这构成了一个毫无意义的句子：“Call the please”，所以考虑到其他的词，Siri 得出结论，这个句子实际上是 “Call the police”。它打了这个电话，如果运气好的话，警察会及时赶到你的房子，抓住小偷。</p>

<p class="noindent english">The same idea still works if the graph is a tree instead of a chain. If instead of a platoon you’re in command of a whole army, you can ask each of your company commanders how many soldiers are behind him and add up their answers. Each company commander in turn asks each of his platoon commanders, and so on. But if the graph forms loops, you’re in trouble. If there’s a liaison officer who’s a member of two platoons, he gets counted twice; in fact, everyone behind him gets counted twice. This is what happens in the “aliens have landed” scenario, if you want to compute, say, the probability of panic:</p>

<p class="noindent chinese">如果图是一棵树而不是一条链，同样的想法仍然有效。如果你指挥的不是一个排，而是整个军队，你可以问你的每个连长他后面有多少士兵，然后把他们的答案加起来。每个连长反过来问他的每个排长，以此类推。但如果图形形成循环，你就有麻烦了。如果有一个联络官是两个排的成员，他就会被计算两次；事实上，他身后的所有人都会被计算两次。这就是在 “外星人登陆” 情况下发生的事情，如果你想计算，比如，恐慌的概率。</p>

<div>

<div>

<img alt="image" src="images/000015.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-244"></a> One solution is to combine <i>The</i> Times <i>reports it</i> and <i>The</i> Journal <i>reports it</i> into a single megavariable with four values: <i>YesYes</i> if they both do, <i>YesNo</i> if the <i>Times</i> reports a landing and the <i>Journal</i> doesn’t, and so on. This turns the graph into a chain of three variables, and all is well. However, every time you add a news source, the number of values of the megavariable doubles. If instead of two news sources you have fifty, the megavariable has 2<sup>50</sup> values. So this method can only get you so far, and no other known method does any better.</p>

<p class="noindent chinese">一个解决方案是将 《<i>泰晤士报</i>》的<i>报道</i>和《<i>日报</i>》的<i>报道</i>合并为一个有四个值的巨型变量。如果他们都这样做了，就是 <i>YesYes</i>；如果《<i>泰晤士报</i>》报道了着陆，而《<i>日报</i>》没有，就是<i> YesNo</i>，以此类推。这就把图表变成了三个变量的链条，一切都很好。然而，每当你增加一个新闻来源，巨型变量的数值数量就会增加一倍。如果你没有两个新闻来源，而是有 50 个，那么巨变数就有 2<sup>50</sup> 个值。因此，这种方法只能让你走到这一步，而且没有其他已知的方法做得更好。</p>

<p class="noindent english">The problem is worse than it seems, because Bayesian networks in effect have “invisible” arrows to go along with the visible ones. <i>Burglary</i> and <i>Earthquake</i> are a priori independent, but the alarm going off entangles them: the alarm makes you suspect a burglary, but if now you hear on the radio that there’s been an earthquake, you assume that’s what caused the alarm. The earthquake has <i>explained away</i> the alarm, making a burglary less likely, and the two are therefore dependent. In a Bayesian network, all parents of the same variable are interdependent in this way, and this in turn introduces further dependencies, making the resulting graph often much denser than the original one.</p>

<p class="noindent chinese">这个问题比它看起来更糟糕，因为贝叶斯网络实际上有 “不可见” 的箭头，与可见的箭头一起。<i>入室盗窃</i>和<i>地震</i>在先验上是独立的，但是报警器的响起使它们纠缠在一起：报警器使你怀疑是入室盗窃，但是如果你现在从广播中听到发生了地震，你就会认为那是引起报警的原因。地震<i>解释了</i>警报，使入室盗窃的可能性降低，因此这两者是相互依存的。在贝叶斯网络中，同一变量的所有父代都以这种方式相互依赖，而这反过来又引入了进一步的依赖关系，使得所产生的图往往比原来的图要密集得多。</p>

<p class="noindent english">The crucial question for inference is whether you can make the filled-in graph “look like a tree” without the trunk getting too thick. If the megavariable in the trunk has too many possible values, the tree grows out of control until it covers the whole planet, like the baobabs in <i>The Little Prince</i> . In the tree of life, each species is a branch, but inside each branch is a graph, with each creature having two parents, four grandparents, some number of offspring, and so on. The “thickness” of a branch is the size of the species’ population. When the branches are too thick, our only choice is to resort to approximate inference.</p>

<p class="noindent chinese">推理的关键问题是，你是否能使填入的图形 “看起来像一棵树”，而不使树干变得太粗。如果树干中的巨型变量有太多的可能值，那么这棵树就会失去控制，直到它覆盖整个地球，就像《<i>小王子</i>》中的猴面包树一样。在生命之树中，每个物种都是一个分支，但每个分支里面都是一个图形，每个生物都有两个父母，四个祖父母，一些数量的后代，等等。树枝的 “厚度” 就是该物种的人口规模。当分支太粗时，我们唯一的选择就是诉诸于近似推理。</p>

<p class="noindent english">One solution, left as an exercise by Pearl in his book on Bayesian networks, is to pretend the graph has no loops and just keep propagating probabilities back and forth until they converge. This is known as loopy belief propagation, both because it works on graphs with loops and because it’s a crazy idea. Surprisingly, it turns out to work quite well in many cases. For instance, it’s a state-of-the art method for wireless communication, with the random variables being the bits in the <a id="babilu_link-285"></a> message, encoded in a clever way. But loopy belief propagation can also converge to the wrong answers or oscillate forever. Another solution, which originated in physics but was imported into machine learning and greatly extended by Michael Jordan and others, is to approximate an intractable distribution with a tractable one and optimize the latter’s parameters to make it as close as possible to the former.</p>

<p class="noindent chinese">珀尔在他的《贝叶斯网络》一书中留下了一个解决方案，那就是假装图没有循环，只是不断地来回传播概率，直到它们收敛。这被称为 “循环信念传播”，既是因为它在有循环的图上起作用，也因为它是一个疯狂的想法。令人惊讶的是，它在许多情况下都能很好地工作。例如，它是一种最先进的无线通信方法，随机变量是信息中的比特，以一种巧妙的方式编码。但是，循环的信念传播也会收敛到错误的答案或永远振荡。另一个解决方案，起源于物理学，但被引入机器学习，并由迈克尔·乔丹和其他人大大扩展，就是用一个可操作的分布来近似一个难以处理的分布，并优化后者的参数，使其尽可能地接近前者。</p>

<p class="noindent english">The most popular option, however, is to drown our sorrows in alcohol, get punch drunk, and stumble around all night. The technical term for this is <i>Markov chain Monte Carlo</i> , or MCMC for short. The “Monte Carlo” part is because the method involves chance, like a visit to the eponymous casino, and the “Markov chain” part is because it involves taking a sequence of steps, each of which depends only on the previous one. The idea in MCMC is to do a random walk, like the proverbial drunkard, jumping from state to state of the network in such a way that, in the long run, the number of times each state is visited is proportional to its probability. We can then estimate the probability of a burglary, say, as the fraction of times we visited a state where there was a burglary. A “well-behaved” Markov chain converges to a stable distribution, so after a while it always gives approximately the same answers. For example, when you shuffle a deck of cards, after a while all card orders are equally likely, no matter the initial order; so you know that if there are <i>n</i> possible orders, the probability of each one is 1/<i>n</i> . The trick in MCMC is to design a Markov chain that converges to the distribution of our Bayesian network. One easy option is to repeatedly cycle through the variables, sampling each one according to its conditional probability given the state of its neighbors. People often talk about MCMC as a kind of simulation, but it’s not: the Markov chain does not simulate any real process; rather, we concocted it to efficiently generate samples from a Bayesian network, which is itself not a sequential model.</p>

<p class="noindent chinese">然而，最受欢迎的选择是借酒浇愁，喝得酩酊大醉，整晚跌跌撞撞。这方面的技术术语是<i>马尔科夫链蒙特卡洛</i>（Markov chain Monte Carlo），或简称 MCMC。“蒙特卡洛” 的部分是因为该方法涉及到机会，就像去同名的赌场一样，而 “马尔科夫链” 的部分是因为它涉及到采取一系列的步骤，每一个步骤只取决于前一个步骤。MCMC 的想法是做一个随机行走，就像传说中的醉汉一样，在网络的各个状态之间跳跃，从长远来看，每个状态被访问的次数与它的概率成正比。然后，我们可以估计发生入室盗窃的概率，比如说，我们访问一个发生过入室盗窃的状态的次数。一个 “行为良好” 的马尔科夫链会收敛到一个稳定的分布，所以一段时间后它总是给出大致相同的答案。例如，当你洗一副牌的时候，过了一段时间，所有的牌序都是同样的可能性，不管初始的牌序是什么；所以你知道，如果有 <i>n</i> 个可能的牌序，每个牌序的概率是 1/<i>n</i>。MCMC 的诀窍是设计一个马尔科夫链，使之收敛于我们的贝叶斯网络的分布。一个简单的选择是反复循环变量，根据其邻居的状态，根据其条件概率对每个变量进行采样。人们经常把 MCMC 说成是一种模拟，但其实不然：马尔可夫链并不模拟任何真实的过程；相反，我们炮制它是为了有效地从贝叶斯网络中生成样本，而贝叶斯网络本身并不是一个顺序模型。</p>

<p class="noindent english">The origins of MCMC go all the way back to the Manhattan Project, when physicists needed to estimate the probability that neutrons would collide with atoms and set off a chain reaction. But in more recent decades, it has sparked such a revolution that it’s often considered one of the most important algorithms of all time. MCMC is good not just for <a id="babilu_link-227"></a> computing probabilities but for integrating any function. Without it, scientists were limited to functions they could integrate analytically, or to well-behaved, low-dimensional integrals they could approximate as a series of trapezoids. With MCMC, they’re free to build complex models, knowing the computer will do the heavy lifting. Bayesians, for one, probably have MCMC to thank for the rising popularity of their methods more than anything else.</p>

<p class="noindent chinese">MCMC 的起源可以追溯到曼哈顿计划，当时物理学家需要估计中子与原子碰撞并引发连锁反应的概率。但在最近的几十年里，它引发了一场革命，以至于它经常被认为是有史以来最重要的算法之一。MCMC 不仅适用于计算概率，而且适用于整合任何函数。如果没有 MCMC，科学家们只能对他们可以分析的函数进行积分，或者对他们可以近似为一系列梯形的乖巧的低维积分进行积分。有了 MCMC，他们就可以自由地建立复杂的模型，因为他们知道计算机会做这些繁重的工作。就贝叶斯学家而言，他们的方法之所以越来越受欢迎，可能更要感谢 MCMC。</p>

<p class="noindent english">On the downside, MCMC is often excruciatingly slow to converge, or fools you by looking like it’s converged when it hasn’t. Real probability distributions are usually very peaked, with vast wastelands of minuscule probability punctuated by sudden Everests. The Markov chain then converges to the nearest peak and stays there, leading to very biased probability estimates. It’s as if the drunkard followed the scent of alcohol to the nearest tavern and stayed there all night, instead of wandering all around the city like we wanted him to. On the other hand, if instead of using a Markov chain we just generated independent samples, like simpler Monte Carlo methods do, we’d have no scent to follow and probably wouldn’t even find that first tavern; it would be like throwing darts at a map of the city, hoping they land smack dab on the pubs.</p>

<p class="noindent chinese">在缺点方面，MCMC 的收敛速度往往慢得令人发指，或者在没有收敛的情况下以看起来已经收敛的方式欺骗你。真实的概率分布通常都是非常尖锐的，有巨大的微不足道的概率的荒地，被突然出现的珠穆朗玛峰点缀着。马尔科夫链然后收敛到最近的峰值并停留在那里，导致非常有偏见的概率估计。这就好比醉汉循着酒香来到最近的酒馆，并在那里呆了一晚上，而不是像我们希望的那样在城市里到处游荡。另一方面，如果我们不使用马尔科夫链，而只是生成独立样本，就像更简单的蒙特卡洛方法那样，我们就没有气味可循，甚至可能找不到第一个酒馆；这就像在城市地图上投掷飞镖，希望它们落在酒馆上一样。</p>

<p class="noindent english">Inference in Bayesian networks is not limited to computing probabilities. It also includes finding the most probable explanation for the evidence, such as the disease that best explains the symptoms or the words that best explain the sounds Siri heard. This is not the same as just picking the most probable word at each step, because words that are individually likely given their sounds may be unlikely to occur together, as in the “Call the please” example. However, similar kinds of algorithms also work for this task (and they are, in fact, what most speech recognizers use). Most importantly, inference includes making the best decisions, guided not just by the probabilities of different outcomes but also by the corresponding costs (or utilities, to use the technical term). The cost of ignoring an e-mail from your boss asking you to do something by tomorrow is much greater than the cost of seeing a piece of spam, so often it’s better to let an e-mail through even if it does seem fairly likely to be spam.</p>

<p class="noindent chinese">贝叶斯网络中的推理并不限于计算概率。它还包括为证据找到最可能的解释，如最能解释症状的疾病或最能解释 Siri 听到的声音的词语。这与在每个步骤中只挑选最可能的单词不同，因为考虑到声音而单独可能的单词可能不太可能一起出现，如 “请打电话” 的例子。然而，类似的算法也适用于这项任务（事实上，它们是大多数语音识别器所使用的）。最重要的是，推理包括做出最好的决定，不仅由不同结果的概率指导，还由相应的成本（或效用，用技术术语来说）指导。忽视你的老板要求你在明天之前做某件事的成本要比看到一封垃圾邮件的成本大得多，所以通常情况下，让一封邮件通过更好，即使它看起来相当可能是垃圾邮件。</p>

<p class="noindent english"><a id="babilu_link-114"></a> Driverless cars and other robots are a prime example of probabilistic inference in action. As the car drives around, it simultaneously builds up a map of the territory and figures out its location on it with increasing certainty. According to a recent study, London taxi drivers grow a larger posterior hippocampus, a brain region involved in memory and map making, as they learn the layout of the city. Perhaps they use similar probabilistic inference algorithms, with the notable difference that in the case of humans, drinking doesn’t seem to help.</p>

<p class="noindent chinese">无人驾驶汽车和其他机器人是概率推理发挥作用的一个典型例子。当汽车四处行驶时，它同时建立了一张领土地图，并以越来越大的确定性在上面算出自己的位置。根据最近的一项研究，伦敦出租车司机在学习城市布局的过程中，后海马体增大，这是一个参与记忆和地图制作的大脑区域。也许他们使用类似的概率推理算法，值得注意的是，在人类的情况下，喝酒似乎没有帮助。</p>

<h1 id="babilu_link-428"><b>Learning the Bayesian way</b></h1>

<h1 id="babilu_link-428"><b>以贝叶斯的方式学习</b></h1>

<p class="noindent english">Now that we know how to (more or less) solve the inference problem, we’re ready to learn Bayesian networks from data, because for Bayesians learning is just another kind of probabilistic inference. All you have to do is apply Bayes’ theorem with the hypotheses as the possible causes and the data as the observed effect:</p>

<p class="noindent chinese">现在我们知道了如何（或多或少）解决推理问题，我们已经准备好从数据中学习贝叶斯网络，因为对贝叶斯学家来说，学习只是另一种概率推理。你所要做的就是应用贝叶斯定理，将假设作为可能的原因，将数据作为观察到的效果。</p>

<div>

<p class="noindent english"><i>P(hypothesis | data) = P(hypothesis) × P(data | hypothesis) / P(data)</i></p>

<p class="noindent chinese"><i>p(假设 | 数据)= p(假设)× p(数据 | 假设)/ p(数据)</i></p>

</div>

<p class="noindent english">The hypothesis can be as complex as a whole Bayesian network, or as simple as the probability that a coin will come up heads. In the latter case, the data is just the outcome of a series of coin flips. If, say, we obtain seventy heads in a hundred flips, a frequentist would estimate the probability of heads as 0.7. This is justified by the so-called maximum likelihood principle: of all the possible probabilities of heads, 0.7 is the one under which seeing seventy heads in a hundred flips is most likely. The likelihood of a hypothesis is <i>P(data | hypothesis)</i> , and the principle says we should pick the hypothesis that maximizes it. Bayesians do something more subtle, though. They point out that we never know for sure which hypothesis is the true one, and so we shouldn’t just pick one hypothesis, like a value of 0.7 for the probability of heads; rather, we should compute the posterior probability of every possible hypothesis and entertain all of them when making predictions. The sum of the <a id="babilu_link-188"></a> probabilities of all the hypotheses must be one, so if one becomes more likely, the others become less. For a Bayesian, in fact, there is no such thing as the truth; you have a prior distribution over hypotheses, after seeing the data it becomes the posterior distribution, as given by Bayes’ theorem, and that’s all.</p>

<p class="noindent chinese">假设可以像整个贝叶斯网络一样复杂，也可以像一枚硬币出现头像的概率一样简单。在后一种情况下，数据只是一系列投掷硬币的结果。比如说，我们在一百次掷硬币中得到了七十个人头，那么频繁论者会估计人头的概率为 0.7。这是由所谓的最大可能性原则证明的：在所有可能的人头概率中，0.7 是最有可能在一百次投掷中看到七十个人头的概率。假设的可能性是 <i>p(数据 | 假设)</i>，而该原则说我们应该选择使其最大化的假设。不过，贝叶斯主义者做了一些更微妙的事情。他们指出，我们永远不知道哪一个假设是真实的，因此我们不应该只选择一个假设，比如头的概率值为 0.7；相反，我们应该计算每个可能的假设的后验概率，并在进行预测时考虑所有的假设。所有假设的概率之和必须是 1，所以如果一个假设变得更有可能，其他假设就会变得更少。事实上，对于贝叶斯主义者来说，不存在所谓的真理；你有一个关于假设的先验分布，在看到数据后，它就变成了贝叶斯定理所给出的后验分布，仅此而已。</p>

<p class="noindent english">This is a radical departure from the way science is usually done. It’s like saying, “Actually, neither Copernicus nor Ptolemy was right; let’s just predict the planets’ future trajectories assuming Earth goes round the sun and vice versa and average the results.”</p>

<p class="noindent chinese">这与科学通常的工作方式有很大的不同。这就像说：“实际上，哥白尼和托勒密都不对；让我们假设地球绕着太阳转，反之亦然，预测行星的未来轨迹，并对结果进行平均。”</p>

<p class="noindent english">Of course, it’s a weighted average, the weight of a hypothesis being its posterior probability, so a hypothesis that explains the data better will count for more. Still, as the joke goes, being Bayesian means never having to say you’re certain.</p>

<p class="noindent chinese">当然，这是一个加权平均数，一个假设的权重是它的后验概率，所以一个能更好地解释数据的假设将被计入更多。不过，正如笑话所言，作为贝叶斯主义者意味着永远不必说你是确定的。</p>

<p class="noindent english">Needless to say, carrying around a multitude of hypotheses instead of just one is a huge pain. In the case of learning a Bayesian network, we’re supposed to make predictions by averaging over all possible Bayesian networks, including all possible graph structures and all possible parameter values for each structure. In some cases, we can compute the average over parameters in closed form, but with varying structures we’re out of luck. We have to resort to, for example, doing MCMC over the space of networks, jumping from one possible network to another as the Markov chain progresses. Combine all this complexity and computational cost with Bayesians’ controversial notion that there’s really no such thing as objective reality, and it’s not hard to see why frequentism has dominated science for the last century.</p>

<p class="noindent chinese">不用说，带着众多的假设而不是只有一个假设是一个巨大的痛苦。在学习贝叶斯网络的情况下，我们应该通过对所有可能的贝叶斯网络进行平均化来进行预测，包括所有可能的图结构和每个结构的所有可能的参数值。在某些情况下，我们可以以封闭的形式计算参数的平均值，但是对于不同的结构，我们就不太走运了。我们不得不求助于，例如，在网络空间上做 MCMC，随着马尔科夫链的进展从一个可能的网络跳到另一个。将所有这些复杂性和计算成本与贝叶斯派有争议的概念相结合，即真的没有客观现实这样的东西，这就不难理解为什么频繁主义在上个世纪主导了科学。</p>

<p class="noindent english">There’s a saving grace, however, and some major reasons to prefer the Bayesian way. The saving grace is that, most of the time, almost all hypotheses wind up with a tiny posterior probability, and we can safely ignore them. In fact, just considering the single most probable hypothesis is usually a very good approximation. Suppose our prior distribution for the coin flip problem is that all probabilities of heads are equally likely. The effect of seeing the outcomes of successive flips is to concentrate the distribution more and more on the hypotheses that best agree <a id="babilu_link-301"></a> with the data. For example, if <i>h</i> ranges over the possible probabilities of heads and a coin comes out heads 70 percent of the time, we’ll see something like this:</p>

<p class="noindent chinese">然而，有一个拯救之恩，以及一些更喜欢贝叶斯方式的主要原因。拯救之恩是，在大多数情况下，几乎所有的假说都有一个很小的后验概率，我们可以安全地忽略它们。事实上，仅仅考虑单一的最有可能的假设通常是一个非常好的近似值。假设我们对掷硬币问题的先验分布是，所有头的概率都是一样的。看到连续翻转的结果的效果是使分布越来越集中在与数据最一致的假设上。例如，如果 <i>h</i> 涵盖了可能的头像概率，而硬币有 70% 的时间是头像，我们会看到这样的结果。</p>

<div>

<div>

<img alt="image" src="images/000029.jpg"/>

</div>

</div>

<p class="noindent english">The posterior after each flip becomes the prior for the next flip, and flip by flip, we become increasingly certain that <i>h</i> = 0.7. If we just take the single most probable hypothesis (<i>h</i> = 0.7 in this case), the Bayesian approach becomes quite similar to the frequentist one, but with one crucial difference: Bayesians take the prior <i>P(hypothesis)</i> into account, not just the likelihood <i>P(data | hypothesis)</i> . (The data prior <i>P(data)</i> can be ignored because it’s the same for all hypotheses and therefore doesn’t affect the choice of winner.) If we’re willing to assume that all hypotheses are equally likely a priori, the Bayesian approach now reduces to the maximum likelihood principle. So Bayesians can say to frequentists: “See, what you do is a special case of what we do, but at least we make our assumptions explicit.” And if the hypotheses are not equally likely a priori, maximum likelihood’s implicit assumption that they are leads to the wrong answers.</p>

<p class="noindent chinese">每次翻转后的后验成为下一次翻转的先验，逐次翻转后，我们越来越确定 <i>h</i> = 0.7。如果我们只取一个最可能的假设 <i>（</i> 本例中 <i>h</i> = 0.7），贝叶斯方法就变得与频繁主义方法很相似，但有一个关键的区别。贝叶斯主义者考虑到了先验<i>p(假设)</i>，而不仅仅是可能性<i>p(数据 | 假设)</i>。（数据先验<i>p(数据)</i>可以被忽略，因为它对所有的假设都是一样的，因此不会影响赢家的选择。）如果我们愿意假设所有假设的先验可能性相同，那么贝叶斯方法现在就简化为最大可能性原则。因此，贝叶斯主义者可以对常数学家说。“你看，你所做的是我们所做的的一个特例，但至少我们明确了我们的假设。” 如果假设的先验可能性不一样，最大似然法的隐含假设就会导致错误的答案。</p>

<p class="noindent english">This might seem like a theoretical discussion, but it has tremendous practical consequences. If we’ve seen only one coin flip and it came out heads, maximum likelihood says that the probability of heads must be one. This could be wildly inaccurate and leaves us woefully unprepared for the coin coming up tails. Once we’ve seen a lot of flips, the estimate becomes more reliable, but in many problems, we never see enough flips, no matter how big the data. Suppose the word <i>supercalifragilisticexpialidocious</i> never appears in a spam e-mail in our training data <a id="babilu_link-270"></a> and appears once in an e-mail talking about <i>Mary Poppins</i> . A Naïve Bayes spam filter with maximum likelihood probability estimates will then decide that an e-mail containing it cannot be spam, regardless of whether every other word in the e-mail screams “Spam! Spam!” In contrast, a Bayesian would give the word a low but nonzero probability of appearing in spam, allowing the other words to override it.</p>

<p class="noindent chinese">这似乎是一个理论上的讨论，但它有巨大的实际后果。如果我们只见过一次抛出的硬币，而且是正面，那么最大可能性说正面的概率一定是 1。这可能是非常不准确的，并且让我们对硬币的结果是反面毫无准备。一旦我们看到大量的翻转，估计就会变得更加可靠，但在许多问题中，我们永远不会看到足够的翻转，无论数据有多大。假设在我们的训练数据中，<i>supercalifragilisticexpialidocious 这个</i>词从未出现在垃圾邮件中而在谈论<i>玛丽·波平斯</i>的邮件中出现了一次。那么，一个具有最大可能性估计的天真贝叶斯垃圾邮件过滤器将决定包含该词的电子邮件不可能是垃圾邮件，无论该电子邮件中的其他每个词是否都在尖叫 “垃圾邮件！垃圾邮件！” 相反，贝叶斯会给这个词一个低但非零的概率出现在垃圾邮件中，让其他的词来覆盖它。</p>

<p class="noindent english">The problem only gets worse if we try to learn the structure of a Bayesian network as well as its parameters. We can do this by hill climbing, starting with an empty network (no arrows), adding the arrow that most increases likelihood, and so on until no arrow causes an improvement. Unfortunately, this quickly leads to massive overfitting, with a network that assigns zero probability to all states not appearing in the data. Bayesians can do something much more interesting. They can use the prior distribution to encode experts’ knowledge about the problem—their answer to Hume’s question. For example, we can design an initial Bayesian network for medical diagnosis by interviewing doctors, asking them which symptoms they think depend on which diseases, and adding the corresponding arrows. This is the “prior network,” and the prior distribution can penalize alternative networks by the number of arrows that they add or remove from it. But doctors are fallible, so we’ll let the data override them: if the increase in likelihood from adding an arrow outweighs the penalty, we do it.</p>

<p class="noindent chinese">如果我们试图学习贝叶斯网络的结构以及它的参数，问题只会变得更糟。我们可以通过爬坡来做到这一点，从一个空的网络开始（没有箭头），添加最能增加可能性的箭头，以此类推，直到没有箭头能引起改进。不幸的是，这很快就会导致大规模的过拟合，一个网络对所有没有出现在数据中的状态赋予了零概率。贝叶斯人可以做一些更有趣的事情。他们可以使用先验分布来编码专家对问题的知识 —— 他们对休谟问题的答案。例如，我们可以通过采访医生，询问他们认为哪些症状取决于哪些疾病，并添加相应的箭头，来设计一个用于医疗诊断的初始贝叶斯网络。这就是 “先验网络”，而先验分布可以通过增加或删除箭头的数量来惩罚替代网络。但医生是易变的，所以我们会让数据凌驾于他们之上：如果增加一个箭头带来的可能性的增加超过了惩罚，我们就这样做。</p>

<p class="noindent english">Of course, frequentists are aware of this issue, and their answer is to, for example, multiply the likelihood by a factor that penalizes more complex networks. But at this point frequentism and Bayesianism have become indistinguishable, and whether you call the scoring function “penalized likelihood” or “posterior probability” is really just a matter of taste.</p>

<p class="noindent chinese">当然，频繁论者也意识到了这个问题，他们的答案是，比如说，将可能性乘以一个惩罚更复杂网络的系数。但是在这一点上，频繁主义和贝叶斯主义已经无法区分了，你把评分函数称为 “惩罚性似然” 还是 “后验概率”，实际上只是一个品味问题。</p>

<p class="noindent english">Despite the convergence of frequentist and Bayesian thinking on some issues, there remains the philosophical difference about the meaning of probability. Viewing it as subjective makes many scientists queasy, but it also enables many otherwise-forbidden uses. If you’re a frequentist, you can only estimate probabilities of events that can occur more than once. So a question like “What is the probability that Hillary <a id="babilu_link-116"></a> Clinton will beat Jeb Bush in the next presidential election?” is unanswerable, because there’s never been an election pitting them against each other. But for a Bayesian, a probability is a subjective degree of belief, so he’s free to make an educated guess, and the inference calculus keeps all his guesses consistent.</p>

<p class="noindent chinese">尽管频繁主义和贝叶斯思想在某些问题上趋于一致，但关于概率的意义仍然存在着哲学上的分歧。把它看作是主观的使许多科学家感到不安，但它也使许多其他被禁止的用途成为可能。如果你是一个频繁论者，你只能估计那些可能发生不止一次的事件的概率。因此，像 “希拉里克林顿在下届总统选举中击败杰布·布什的概率是多少？” 这样的问题是无法回答的，因为从来没有发生过让他们互相竞争的选举。但是对于贝叶斯人来说，概率是一种主观的信仰程度，所以他可以自由地进行有根据的猜测，推理计算使他所有的猜测保持一致。</p>

<p class="noindent english">The Bayesian method is not just applicable to learning Bayesian networks and their special cases. (Conversely, despite their name, Bayesian networks aren’t necessarily Bayesian: frequentists can learn them, too, as we just saw.) We can put a prior distribution on any class of hypotheses—sets of rules, neural networks, programs—and then update it with the hypotheses’ likelihood given the data. Bayesians’ view is that it’s up to you what representation you choose, but then you have to learn it using Bayes’ theorem. In the 1990s, they mounted a spectacular takeover of the Conference on Neural Information Processing Systems (NIPS for short), the main venue for connectionist research. The ringleaders (so to speak) were David MacKay, Radford Neal, and Michael Jordan. MacKay, a Brit who was a student of John Hopfield’s at Caltech and later became chief scientific advisor to the UK’s Department of Energy, showed how to learn multilayer perceptrons the Bayesian way. Neal introduced the connectionists to MCMC, and Jordan introduced them to variational inference. Finally, they pointed out that in the limit you could “integrate out” the neurons in a multilayer perceptron, leaving a type of Bayesian model that made no reference to them. Before long, the word <i>neural</i> in the title of a paper submitted to NIPS became a good predictor of rejection. Some researchers joked that the conference should change its name to BIPS, for Bayesian Information Processing Systems.</p>

<p class="noindent chinese">贝叶斯方法不仅仅适用于学习贝叶斯网络和它们的特殊情况。（相反，尽管贝叶斯网络的名称是贝叶斯的，但它不一定是贝叶斯的：正如我们刚刚看到的，常数学家也可以学习它们）。我们可以在任何一类假设 —— 规则集、神经网络、程序上设置一个先验分布，然后根据数据更新这些假设的可能性。贝叶斯主义者的观点是，选择什么表示法由你自己决定，但随后你必须用贝叶斯定理来学习它。在 20 世纪 90 年代，他们对神经信息处理系统会议（简称 NIPS）进行了惊人的接管，这是连接主义研究的主要场所。他们的头目（可以这么说）是大卫·麦凯、拉德福德·尼尔和迈克尔·乔丹。麦凯是一个英国人，他是约翰·霍普菲尔德在加州理工学院的学生，后来成为英国能源部的首席科学顾问，他展示了如何以贝叶斯的方式学习多层感知器。尼尔向连接主义者介绍了 MCMC，而乔丹向他们介绍了变异推理。最后，他们指出，在极限情况下，你可以把多层感知器中的神经元 “整合掉”，留下一种不提及它们的贝叶斯模型。不久之后，提交给 NIPS 的论文标题中的 “<i>神经</i>” 一词成了被拒绝的一个很好的预测因素。一些研究人员开玩笑说，该会议应该改名为 BIPS，即贝叶斯信息处理系统。</p>

<h1 id="babilu_link-429"><b>Markov weighs the evidence</b></h1>

<h1 id="babilu_link-429"><b>马尔科夫衡量证据</b></h1>

<p class="noindent english">But something funny happened on the way to world domination. Researchers using Bayesian models kept noticing that you got better results by tweaking the probabilities in illegal ways. For example, raising <i>P(words)</i> to some power in speech recognizers improved accuracy, but then it wasn’t Bayes’ theorem any more. What was going on? The <a id="babilu_link-265"></a> culprit, it turns out, was the false independence assumptions that generative models make. The simplified graph structure makes the models learnable and is worth keeping, but then we’re better off just learning the best parameters we can for the task at hand, irrespective of whether they’re probabilities. The real strength of, say, Naïve Bayes is that it provides a small, informative set of features from which to predict the class and a fast, robust way to learn the corresponding parameters. In a spam filter, each feature is the occurrence of a particular word in spam, and the corresponding parameter is how often it occurs; and similarly for nonspam. Viewed in this way, Naïve Bayes can be optimal, in the sense of making the best predictions possible, even in many cases where its independence assumptions are wildly violated. When I realized this and published a paper about it in 1996, people’s suspicion of Naïve Bayes melted away, helping it to take off. But it was also a step on the way to a different kind of model, which in the last two decades has increasingly replaced Bayesian networks in machine learning: Markov networks.</p>

<p class="noindent chinese">但在称霸世界的道路上发生了一些有趣的事情。使用贝叶斯模型的研究人员不断注意到，通过以非法方式调整概率，你会得到更好的结果。例如，在语音识别器中把<i>p(words)</i>提高到一定的功率，可以提高准确率，但这时就不再是贝叶斯定理了。到底发生了什么？事实证明，罪魁祸首是生成模型的错误独立性假设。简化的图结构使模型可以学习，值得保留，但是我们最好只学习手头任务的最佳参数，不管它们是否是概率。比方说，天真贝叶斯的真正优势在于它提供了一个小的、信息量大的特征集来预测类别，以及一个快速、稳健的方法来学习相应的参数。在一个垃圾邮件过滤器中，每个特征都是垃圾邮件中某个特定词的出现，相应的参数是它出现的频率；对于非垃圾邮件也是如此。从这个角度看，天真贝叶斯可以是最佳的，在可能的情况下做出最好的预测，甚至在许多情况下它的独立性假设被严重违反。当我意识到这一点并在 1996 年发表了一篇关于它的论文时，人们对天真贝叶斯的怀疑消失了，帮助它起飞了。但这也是迈向另一种模型的一步，在过去的二十年里，这种模型在机器学习中越来越多地取代贝叶斯网络：马尔科夫网络。</p>

<p class="noindent english">A Markov network is a set of features and corresponding weights, which together define a probability distribution. A feature can be as simple as <i>This is a ballad</i> or as elaborate as <i>This is a ballad by a hip-hop artist, with a saxophone riff and a descending chord progression</i> . Pandora uses a large set of features, which it calls the Music Genome Project, to select songs to play for you. Suppose we plug them into a Markov network. If you like ballads, the weight of the corresponding feature goes up, and you’re more likely to hear ballads when you turn on Pandora. If you also like songs by hip-hop artists, that feature’s weight also goes up. The songs you’re most likely to hear are now ones that have both features, namely ballads by hip-hop artists. If you don’t like ballads or hip-hop artists per se, but only enjoy them in combination, the more elaborate feature <i>Ballad by a hip-hop artist</i> is what you need. Pandora’s features are handcrafted, but in Markov networks we can also learn features using hill climbing, similar to rule induction. Either way, gradient descent is a good way to learn the weights.</p>

<p class="noindent chinese">马尔科夫网络是一组特征和相应的权重，它们共同定义了一个概率分布。一个特征可以是简单的 “<i>这是一首民谣</i>”，也可以是复杂的 “这是一首<i>由嘻哈艺术家创作的民谣，有萨克斯风和下降的和弦进行</i>”。潘多拉使用一套庞大的功能，它称之为音乐基因组计划，来选择歌曲为你播放。假设我们把它们插入一个马尔可夫网络。如果你喜欢民谣，相应特征的权重就会上升，当你打开潘多拉时，你就更可能听到民谣。如果你也喜欢嘻哈歌手的歌曲，该特征的权重也会上升。现在你最有可能听到的歌曲是那些同时具有这两个特征的歌曲，即嘻哈歌手的民谣歌曲。如果你不喜欢民谣或嘻哈歌手本身，而只是喜欢它们的组合，那么更精细的<i>嘻哈歌手的民谣</i>功能就是你需要的。潘多拉的特征是手工制作的，但在马尔可夫网络中，我们也可以用爬坡法学习特征，类似于规则归纳法。无论哪种方式，梯度下降都是学习权重的好方法。</p>

<p class="noindent english">Like Bayesian networks, Markov networks can be represented by graphs, but they have undirected arcs instead of arrows. Two variables <a id="babilu_link-64"></a> are connected, meaning they depend directly on each other, if they appear together in some feature, like <i>Ballad</i> and <i>By a hip-hop artist</i> in <i>Ballad by a hip-hop artist</i> .</p>

<p class="noindent chinese">与贝叶斯网络一样，马尔科夫网络也可以用图来表示，但它们有不定向的弧，而不是箭头。两个变量如果它们在某些特征中一起出现，那么它们是相连的，也就是说，它们直接依赖于对方，比如 <i>Ballad</i> 和<i>By a hip-hop artist</i> 在 <i>Ballad by a hip-hop artist</i> 之中。</p>

<p class="noindent english">Markov networks are a staple in many areas, such as computer vision. For instance, a driverless car needs to segment each image it sees into road, sky, and countryside. One option is to label each pixel as one of the three according to its color, but this is not nearly good enough. Images are very noisy and variable, and the car will hallucinate rocks strewn all over the roadway and patches of road in the sky. We know, however, that nearby pixels in an image are usually part of the same object, and we can introduce a corresponding set of features: for each pair of neighboring pixels, the feature is true if they belong to the same object, and false otherwise. Now images with large, contiguous blocks of road and sky are much more likely than images without, and the car goes straight instead of continually swerving left and right to avoid imaginary rocks.</p>

<p class="noindent chinese">马尔科夫网络是许多领域的主力军，例如计算机视觉。例如，一辆无人驾驶汽车需要将它看到的每张图片分割成道路、天空和乡村。一种选择是根据每个像素的颜色将其标记为三者之一，但这还远远不够好。图像是非常嘈杂和多变的，汽车会幻化出散落在道路上的石头和天空中的道路斑块。然而，我们知道，图像中邻近的像素通常是同一物体的一部分，我们可以引入一组相应的特征：对于每一对邻近的像素，如果它们属于同一物体，该特征为真，否则为假。现在，有大块的、连续的道路和天空的图像比没有的图像要多得多，汽车直行，而不是不断地左右摇摆以避免想象中的岩石。</p>

<p class="noindent english">Markov networks can be trained to maximize either the likelihood of the whole data or the conditional likelihood of what we want to predict given what we know. For Siri, the likelihood of the whole data is <i>P(words, sounds)</i> , and the conditional likelihood we’re interested in is <i>P(words | sounds)</i> . By optimizing the latter, we can ignore <i>P(sounds)</i> , which is only a distraction from our goal. And since we ignore it, it can be arbitrarily complex. This is much better than HMMs’ unrealistic assumption that sounds depend solely on the corresponding words, without any influence from the surroundings. In fact, if all Siri cares about is figuring out which words you just spoke, perhaps it doesn’t even need to worry about probabilities; it just needs to make sure the correct words score higher than incorrect ones when it tots up the weights of their features—ideally a lot higher, just to be safe.</p>

<p class="noindent chinese">马尔科夫网络可以被训练成使整个数据的可能性最大化，或者使我们想要预测的东西的条件可能性最大化，因为我们知道什么。对于 Siri 来说，整个数据的可能性是<i>p(单词，声音)</i>，而我们感兴趣的条件可能性是 <i>p(单词 | 声音)</i>。通过优化后者，我们可以忽略<i>p(sounds)</i>，它只是分散了我们的目标。而且，由于我们忽略了它，它可以是任意复杂的。这比 HMMs 的不切实际的假设要好得多，即声音只取决于相应的单词，而没有来自周围环境的任何影响。事实上，如果 Siri 关心的只是弄清楚你刚才说了哪些话，也许它甚至不需要担心概率问题；它只需要确保正确的词在其特征权重上的得分高于不正确的词 —— 为了安全起见，最好能高得多。</p>

<p class="noindent english">Analogizers took this line of reasoning to its logical conclusion, as we’ll see in the next chapter. In the first decade of the new millennium, they in turn took over NIPS. Now the connectionists dominate once more, under the banner of deep learning. Some say that research goes in cycles, but it’s more like a spiral, with loops winding around the <a id="babilu_link-125"></a> direction of progress. In machine learning, the spiral converges to the Master Algorithm.</p>

<p class="noindent chinese">正如我们将在下一章看到的那样，模拟者将这一推理推向了合乎逻辑的结论。在新千年的第一个十年，他们反过来接管了 NIPS。现在，在深度学习的旗帜下，连接主义者再次占据了主导地位。有人说，研究是有周期的，但它更像是一个螺旋，环环相扣，朝着的方向发展。在机器学习中，螺旋式的发展会收敛到主算法。</p>

<h1 id="babilu_link-430"><b>Logic and probability: The star-crossed couple</b></h1>

<h1 id="babilu_link-430"><b>逻辑和概率：错综复杂的夫妇</b></h1>

<p class="noindent english">You’d think that Bayesians and symbolists would get along great, given that they both believe in a first-principles approach to learning, rather than a nature-inspired one. Far from it. Symbolists don’t like probabilities and tell jokes like “How many Bayesians does it take to change a lightbulb? They’re not sure. Come to think of it, they’re not sure the lightbulb is burned out.” More seriously, symbolists point to the high price we pay for probability. Inference suddenly becomes a lot more expensive, all those numbers are hard to understand, we have to deal with priors, and hordes of zombie hypotheses chase us around forever. The ability to compose pieces of knowledge on the fly, so dear to symbolists, is gone. Worst of all, we don’t know how to put probability distributions on many of the things we need to learn. A Bayesian network is a distribution over a vector of variables, but what about distributions over networks, databases, knowledge bases, languages, plans, and computer programs, to name a few? All of these are easily handled in logic, and an algorithm that can’t learn them is clearly not the Master Algorithm.</p>

<p class="noindent chinese">你会认为贝叶斯主义者和符号主义者会相处得很好，因为他们都相信学习的第一原理方法，而不是自然启发的方法。远非如此。符号主义者不喜欢概率，并讲笑话说：“换一个灯泡需要多少个贝叶斯人？他们不确定。仔细想想，他们也不确定灯泡是否烧坏了。” 更严重的是，符号主义者指出了我们为概率付出的高昂代价。推理突然变得昂贵起来，所有这些数字都很难理解，我们必须处理先验，成群的僵尸假设永远追着我们跑。象征主义者所珍视的随心所欲地组成知识片断的能力已经消失了。最糟糕的是，我们不知道如何把概率分布放在我们需要学习的许多东西上。贝叶斯网络是一个变量向量上的分布，但网络、数据库、知识库、语言、计划和计算机程序上的分布呢，仅举几例？所有这些都很容易用逻辑来处理，一个不能学习它们的算法显然不是主算法。</p>

<p class="noindent english">Bayesians, in turn, point to the brittleness of logic. If I have a rule like <i>Birds fly</i> , a world with even one flightless bird is impossible. If I try to patch things by adding exceptions, such as <i>Birds fly, unless they’re penguins</i> , I’ll never be done. (What about ostriches? Birds in cages? Dead birds? Birds with broken wings? Soaked wings?) A doctor diagnoses you with cancer, and you decide to get a second opinion. If the second doctor disagrees, you’re stuck. You can’t weigh the two opinions; you just have to believe them both. And then a catastrophe happens: pigs fly, perpetual motion is possible, and Earth doesn’t exist—because in logic everything can be inferred from a contradiction. Furthermore, if knowledge is learned from data, I can never be sure it’s true. Why do symbolists pretend otherwise? Surely Hume would frown on such insouciance.</p>

<p class="noindent chinese">而贝叶斯主义者则指出了逻辑的脆性。如果我有一条规则，比如<i>鸟儿会飞</i>，那么世界上即使有一只不会飞的鸟也是不可能的。如果我试图通过添加例外情况来修补事情，比如<i>鸟儿会飞，除非它们是企鹅</i>，我永远不会完成。（那鸵鸟呢？笼子里的鸟？死鸟？翅膀折断的鸟？翅膀湿透的鸟？）一个医生诊断你患了癌症，你决定征求第二个意见。如果第二个医生不同意，你就被困住了。你不能权衡这两种意见，你只能相信他们两个。然后一场灾难发生了：猪飞了，永动机是可能的，地球也不存在 —— 因为在逻辑上，一切都可以从矛盾中推断出来。此外，如果知识是从数据中学习的，我永远无法确定它是真的。为什么象征主义者要装作不知道呢？当然，休谟会对这种不自觉的行为皱眉。</p>

<p class="noindent english"><a id="babilu_link-86"></a> Bayesians and symbolists agree that prior assumptions are inevitable, but they differ in the kinds of prior knowledge they allow. For Bayesians, knowledge goes in the prior distribution over the structure and parameters of the model. In principle, the parameter prior could be anything we please, but ironically, Bayesians tend to choose uninformative priors (like assigning the same probability to all hypotheses) because they’re easier to compute with. In any case, humans are not very good at estimating probabilities. For structure, Bayesian networks provide an intuitive way to incorporate knowledge: draw an arrow from A to B if you think that A directly causes B. But symbolists are much more flexible: you can provide as prior knowledge to your learner anything you can encode in logic, and practically anything can be encoded in logic—provided it’s black and white.</p>

<p class="noindent chinese">贝叶斯主义者和符号主义者都认为先验假设是不可避免的，但他们在允许的先验知识的种类上有所不同。对于贝叶斯主义者来说，知识是在模型的结构和参数的先验分布中。原则上，参数先验可以是我们所希望的任何东西，但具有讽刺意味的是，贝叶斯主义者倾向于选择无信息的先验（比如给所有假设分配相同的概率），因为它们更容易计算。在任何情况下，人类都不太擅长估计概率。对于结构来说，贝叶斯网络提供了一种纳入知识的直观方式：如果你认为 A 直接导致 B，就从 A 到 B 画一个箭头。但符号主义者要灵活得多：你可以向学习者提供任何可以用逻辑编码的先验知识，实际上任何东西都可以用逻辑编码 —— 只要它是黑白的。</p>

<p class="noindent english">Clearly, we need both logic and probability. Curing cancer is a good example. A Bayesian network can model a single aspect of how cells function, like gene regulation or protein folding, but only logic can put all the pieces together into a coherent picture. On the other hand, logic can’t deal with incomplete or noisy information, which is pervasive in experimental biology, but Bayesian networks can handle it with aplomb.</p>

<p class="noindent chinese">显然，我们同时需要逻辑和概率。治愈癌症就是一个很好的例子。贝叶斯网络可以模拟细胞功能的单一方面，如基因调控或蛋白质折叠，但只有逻辑可以把所有的碎片拼成一个连贯的画面。另一方面，逻辑不能处理不完整或嘈杂的信息，这在实验生物学中是普遍存在的，但贝叶斯网络可以很好地处理它。</p>

<p class="noindent english">Bayesian learning works on a single table of data, where each column represents a variable (for example, the expression level of one gene) and each row represents an instance (for example, a single microarray experiment, with each gene’s observed expression level). It’s OK if the table has “holes” and measurement errors because we can use probabilistic inference to fill in the holes and average over the errors. But if we have more than one table, Bayesian learning is stuck. It doesn’t know how to, for example, combine gene expression data with data about which DNA segments get translated into proteins, and how in turn the three-dimensional shapes of those proteins cause them to lock on to different parts of the DNA molecule, affecting the expression of other genes. In logic, we can easily write rules relating all of these aspects, and learn them from the relevant combinations of tables—but only provided the tables have no holes or errors.</p>

<p class="noindent chinese">贝叶斯学习在一个单一的数据表格上工作，其中每一列代表一个变量（例如，一个基因的表达水平），每一行代表一个实例（例如，一个单一的微阵列实验，每个基因的观察表达水平）。如果表格有 “洞” 和测量误差也没关系，因为我们可以用概率推理来填补这些洞，并对误差进行平均。但是如果我们有一个以上的表，贝叶斯学习就被卡住了。例如，它不知道如何将基因表达数据与关于哪些 DNA 片段被翻译成蛋白质的数据结合起来，以及这些蛋白质的三维形状如何反过来导致它们锁定在 DNA 分子的不同部分，影响其他基因的表达。在逻辑学中，我们可以很容易地写出与所有这些方面相关的规则，并从相关的表格组合中学习它们 —— 但前提是这些表格没有漏洞或错误。</p>

<p class="noindent english"><a id="babilu_link-71"></a> Combining connectionism and evolutionism was fairly easy: just evolve the network structure and learn the parameters by backpropagation. But unifying logic and probability is a much harder problem. Attempts to do it go all the way back to Leibniz, who was a pioneer of both. Some of the best philosophers and mathematicians of the nineteenth and twentieth centuries, like George Boole and Rudolf Carnap, worked hard on it but ultimately didn’t get very far. More recently, computer scientists and AI researchers have joined the fray. But as the millennium turned around, the best we had were partial successes, like adding some logical constructs to Bayesian networks. Most experts believed that unifying logic and probability was impossible. The prospects for a Master Algorithm did not look good, particularly since the existing evolutionary and connectionist algorithms couldn’t deal with incomplete information or multiple data sets, either.</p>

<p class="noindent chinese">结合连接主义和进化主义是相当容易的：只要进化网络结构并通过反向传播学习参数即可。但是将逻辑和概率统一起来是一个更难的问题。做到这一点的尝试可以一直追溯到莱布尼茨，他是两者的先驱者。十九世纪和二十世纪的一些最好的哲学家和数学家，如乔治·布尔和鲁道夫·卡尔纳普，在这方面做了很多努力，但最终都没有取得很大进展。最近，计算机科学家和人工智能研究人员也加入了战团。但在千禧年之际，我们所拥有的最好成绩是部分成功，比如在贝叶斯网络中加入一些逻辑结构。大多数专家认为，将逻辑和概率统一起来是不可能的。主算法的前景并不乐观，特别是由于现有的进化和连接主义算法也无法处理不完整的信息或多个数据集。</p>

<p class="noindent english">Luckily, we have since cracked the problem, and the Master Algorithm now looks that much closer. We’ll see how we did it in <a href="#babilu_link-12">Chapter 9</a> and take it from there. But first we need to gather a very important, still-missing piece of the puzzle: how to learn from very little data. That might seem unnecessary in these days of data deluge, but the truth is that we often find ourselves with reams of data about some parts of the problem we want to solve and almost none about others. This is where one of the most important ideas in machine learning comes in: analogy. All of the tribes we’ve met so far have one thing in common: they learn an explicit model of the phenomenon under consideration, whether it’s a set of rules, a multilayer perceptron, a genetic program, or a Bayesian network. When they don’t have enough data to do that, they’re stumped. But analogizers can learn from as little as one example because they never form a model. Let’s see what they do instead.</p>

<p class="noindent chinese">幸运的是，我们后来已经破解了这个问题，主算法现在看起来更接近了。我们将在<a href="#babilu_link-12">第九章</a>中看到我们是如何做到的，并从那里开始。但首先，我们需要收集一个非常重要的、仍然缺少的拼图：如何从很少的数据中学习。在这个数据泛滥的时代，这似乎是不必要的，但事实是，我们经常发现自己拥有关于我们想要解决的问题的某些部分的大量数据，而关于其他部分几乎没有。这就是机器学习中最重要的想法之一：类比。到目前为止，我们遇到的所有部落都有一个共同点：他们学习所考虑的现象的明确模型，无论是一套规则、一个多层感知器、一个遗传程序，还是一个贝叶斯网络。当他们没有足够的数据来做到这一点时，他们就会陷入困境。但是，模拟者可以从很少的一个例子中学习，因为他们从不形成一个模型。让我们看看他们是怎么做的。</p>

</section>

</div>

</div>

<div id="babilu_link-0">

<div>

<section id="babilu_link-13">

<h1><a id="babilu_link-18"></a> <a href="#babilu_link-1">CHAPTER SEVEN</a></h1>

<h1><a id="babilu_link-18"></a> <a href="#babilu_link-1">第七章</a></h1>

<h1><a href="#babilu_link-1">You Are What You Resemble</a></h1>

<h1><a href="#babilu_link-1">你是你所喜欢的人</a></h1>

<p class="noindent english">Frank Abagnale Jr. is one of the most notorious con men in history. Abagnale, portrayed by Leonardo DiCaprio in Spielberg’s movie <i>Catch Me If You Can</i> , forged millions of dollars’ worth of checks, impersonated an attorney and a college instructor, and traveled the world as a fake Pan Am pilot—all before his twenty-first birthday. But perhaps his most jaw-dropping exploit was to successfully pose as a doctor for nearly a year in late-1960s Atlanta. Practicing medicine supposedly requires many years in med school, a license, a residency, and whatnot, but Abagnale managed to bypass all these niceties and never got called on it.</p>

<p class="noindent chinese">小弗兰克·阿巴戈内尔是历史上最臭名昭著的骗子之一。阿巴戈内尔在斯皮尔伯格的电影《<i>抓住我</i>》中由莱昂纳多·迪卡普里奥扮演，他伪造了价值数百万美元的支票，冒充律师和大学教师，并作为一名假冒的泛美航空公司飞行员周游世界 —— 所有这些都发生在他 21 岁生日之前。但他最令人瞠目结舌的伎俩也许是在 1960 年代末的亚特兰大成功地冒充了近一年的医生。据称，行医需要在医学院学习多年，获得执照、住院医师资格等等，但阿巴戈内尔设法绕过所有这些礼节，而且从未被要求这样做。</p>

<p class="noindent english">Imagine for a moment trying to pull off such a stunt. You sneak into an absent doctor’s office, and before long a patient comes in and tells you all his symptoms. Now you have to diagnose him, except you know nothing about medicine. All you have is a cabinet full of patient files: their symptoms, diagnoses, treatments undergone, and so on. What do you do? The easiest way out is to look in the files for the patient whose symptoms most closely resemble your current one’s and make the same diagnosis. If your bedside manner is as convincing as Abagnale’s, that might just do the trick. The same idea applies well beyond medicine. If you’re a young president faced with a world crisis, as Kennedy was <a id="babilu_link-68"></a> when a US spy plane revealed Soviet nuclear missiles being deployed in Cuba, chances are there’s no script ready to follow. Instead, you look for historical analogs of the current situation and try to learn from them. The Joint Chiefs of Staff urged an attack on Cuba, but Kennedy, having just read <i>The Guns of August</i> , a best-selling account of the outbreak of World War I, was keenly aware of how easily that could escalate into all-out war. So he opted for a naval blockade instead, perhaps saving the world from nuclear war.</p>

<p class="noindent chinese">想象一下，试图完成这样一个特技。你潜入一个不在的医生的办公室，不久，一个病人进来，告诉你他的所有症状。现在你必须对他进行诊断，只是你对医学一无所知。你所拥有的只是一个装满病人档案的柜子：他们的症状、诊断、接受过的治疗等等。你该怎么做？最简单的办法是在档案中寻找症状与你现在的病人最相似的病人，并做出同样的诊断。如果你的床边态度像阿巴戈内尔一样令人信服，这可能就能解决问题。同样的想法也适用于医学之外。如果你是一位面临世界危机的年轻总统，就像肯尼迪在一架美国间谍飞机发现苏联在古巴部署核导弹时很可能没有现成的剧本可循。相反，你要寻找当前局势的历史类比，并试图从中学习。参谋长联席会议敦促对古巴进行攻击，但肯尼迪刚刚读过《<i>八月的枪声</i>》，这是一本关于第一次世界大战爆发的畅销书，他敏锐地意识到这很容易升级为全面战争。因此，他选择了海上封锁，也许能使世界避免核战争。</p>

<p class="noindent english">Analogy was the spark that ignited many of history’s greatest scientific advances. The theory of natural selection was born when Darwin, on reading Malthus’s <i>Essay on Population</i> , was struck by the parallels between the struggle for survival in the economy and in nature. Bohr’s model of the atom arose from seeing it as a miniature solar system, with electrons as the planets and the nucleus as the sun. Kekulé discovered the ring shape of the benzene molecule after daydreaming of a snake eating its own tail.</p>

<p class="noindent chinese">类比是点燃许多历史上最伟大的科学进步的火花。达尔文在阅读马尔萨斯的《<i>人口论</i>》时，被经济中的生存斗争与自然界中的生存斗争之间的相似之处所震撼，自然选择理论由此诞生。玻尔的原子模型产生于将其视为一个微型的太阳系，电子为行星，原子核为太阳。凯库莱在做了关于蛇吃自己尾巴的白日梦之后发现了苯分子的环形。</p>

<p class="noindent english">Analogical reasoning has a distinguished intellectual pedigree. Aristotle expressed it in his law of similarity: if two things are similar, the thought of one will tend to trigger the thought of the other. Empiricists like Locke and Hume followed suit. Truth, said Nietzche, is a mobile army of metaphors. Kant was also a fan. William James believed that “this sense of sameness is the very keel and backbone of our thinking.” Some contemporary psychologists even argue that human cognition in its entirety is a fabric of analogies. We rely on it to find our way around a new town and to understand expressions like “see the light” and “stand tall.” Teenagers who insert “like” into every sentence they say would probably, like, agree that analogy is important, dude.</p>

<p class="noindent chinese">类比推理有一个杰出的智力血统。亚里士多德在他的相似性法则中表达了这一点：如果两件事物相似，对一件事物的思考将倾向于引发对另一件事物的思考。像洛克和休谟这样的经验主义者紧随其后。尼采说，真理是一支流动的隐喻部队。康德也是一个粉丝。威廉·詹姆斯认为，“这种同一性的感觉是我们思维的龙骨和骨干”。一些当代心理学家甚至认为，人类的认知在整体上是一种类比的结构。我们依靠它在一个新的城市中找到我们的路，并理解像 “看到光明” 和 “站在高处” 这样的表达。那些在他们说的每一句话中都插入 “喜欢” 的青少年可能会同意，类比是很重要的，伙计。</p>

<p class="noindent english">Given all this, it’s not surprising that analogy plays a prominent role in machine learning. It got off to a slow start, though, and was initially overshadowed by neural networks. Its first algorithmic incarnation appeared in an obscure technical report written in 1951 by two Berkeley statisticians, Evelyn Fix and Joe Hodges, and was not published in a mainstream journal until decades later. But in the meantime, other papers on Fix and Hodges’s algorithm started to appear and then to <a id="babilu_link-59"></a> multiply until it was one of the most researched in all of computer science. The nearest-neighbor algorithm, as it’s called, is the first stop on our tour of analogy-based learning. The second is support vector machines, an idea that took machine learning by storm around the turn of the millennium and was only recently overshadowed by deep learning. The third and last is full-blown analogical reasoning, which has been a staple of psychology and AI for several decades, and a background theme in machine learning for nearly as long.</p>

<p class="noindent chinese">鉴于这一切，类比在机器学习中发挥着突出的作用，这并不令人惊讶。不过，它的起步很慢，最初被神经网络掩盖了。它的第一个算法化身出现在 1951 年由两位伯克利统计学家伊夫林·菲克斯和乔·霍奇斯撰写的一份不知名的技术报告中，直到几十年后才在主流期刊上发表。但在此期间，关于菲克斯和霍奇斯算法的其他论文开始出现，然后直到它成为所有计算机科学中研究最多的算法之一。最近的邻居算法，正如它被称为的那样，是我们基于类比的学习之旅的第一站。第二站是支持向量机，这个想法在千禧年前后掀起了机器学习的风暴，直到最近才被深度学习所掩盖。第三个也是最后一个是全面的类比推理，几十年来一直是心理学和人工智能的主要内容，也是机器学习的一个背景主题，几乎同样长。</p>

<p class="noindent english">The analogizers are the least cohesive of the five tribes. Unlike the others, which have a strong identity and common ideals, the analogizers are more of a loose collection of researchers, united only by their reliance on similarity judgments as the basis for learning. Some, like the support vector machine folks, might even object to being brought under such an umbrella. But it’s raining deep models outside, and I think they would benefit greatly from making common cause. Similarity is one of the central ideas in machine learning, and the analogizers in all their guises are its keepers. Perhaps in a future decade, machine learning will be dominated by deep analogy, combining in one algorithm the efficiency of nearest-neighbor, the mathematical sophistication of support vector machines, and the power and flexibility of analogical reasoning. (There, I just gave away one of my secret research projects.)</p>

<p class="noindent chinese">类比者是五个部落中最没有凝聚力的。不像其他部落有强烈的认同感和共同的理想，类比者更像是一个松散的研究者集合体，仅仅因为他们依赖相似性判断作为学习的基础而团结起来。有些人，比如支持向量机的人，甚至可能反对被带到这样的伞下。但是外面正在下着深层模型的雨，我认为他们会从共同的事业中受益匪浅。相似性是机器学习的核心思想之一，而所有的模拟者都是它的守护者。也许在未来的十年里，机器学习将由深度类比主导，在一个算法中结合最近邻的效率，支持向量机的数学复杂性，以及类比推理的力量和灵活性。（在这里，我刚刚泄露了我的一个秘密研究项目）。</p>

<h1 id="babilu_link-431"><b>Match me if you can</b></h1>

<h1 id="babilu_link-431"><b>如果你可以的话，请与我匹配</b></h1>

<p class="noindent english">Nearest-neighbor is the simplest and fastest learning algorithm ever invented. In fact, you could even say it’s the fastest algorithm of any kind that could ever be invented. It consists of doing exactly nothing, and therefore takes zero time to run. Can’t beat that. If you want to learn to recognize faces and have a vast database of images labeled face/not face, just let it sit there. Don’t worry, be happy. Without knowing it, those images already implicitly form a model of what a face is. Suppose you’re Facebook and you want to automatically identify faces in photos people upload as a prelude to tagging them with their friends’ names. It’s nice to not have to do anything, given that Facebook users upload upward of three hundred <a id="babilu_link-288"></a> million photos per day. Applying any of the learners we’ve seen so far to them, with the possible exception of Naïve Bayes, would take a truckload of computers. And Naïve Bayes is not smart enough to recognize faces.</p>

<p class="noindent chinese">最近的邻居是有史以来最简单和最快的学习算法。事实上，你甚至可以说它是有史以来最快的任何一种算法。它完全不做任何事情，因此运行所需时间为零。这一点是无可厚非的。如果你想学习识别人脸，并拥有一个标有人脸/非人脸的庞大的图像数据库，就让它坐在那里吧。别担心，开心点。在不知不觉中，这些图像已经隐含地形成了一个关于人脸的模型。假设你是 Facebook，你想在人们上传的照片中自动识别人脸，作为用朋友的名字来标记他们的前奏。考虑到 Facebook 用户每天上传的照片多达 3 亿张，不需要做任何事情是很好的。除了天真贝叶斯之外，将我们迄今为止看到的任何学习者应用于这些照片，都需要一卡车的计算机。而且，天真贝叶斯还没有聪明到可以识别人脸。</p>

<p class="noindent english">Of course, there’s a price to pay, and the price comes at test time. Jane User has just uploaded a new picture. Is it a face? Nearest-neighbor’s answer is: find the picture most similar to it in Facebook’s entire database of labeled photos—its “nearest neighbor”—and if that picture contains a face, so does this one. Simple enough, but now you have to scan through potentially billions of photos in (ideally) a fraction of a second. Like a lazy student who doesn’t bother to study for the test, nearest-neighbor is caught unprepared and has to scramble. But unlike real life, where your mother taught you to never leave until tomorrow what you can do today, in machine learning procrastination can really pay off. In fact, the entire genre of learning that nearest-neighbor is part of is sometimes called “lazy learning,” and in this context there’s nothing pejorative about the term.</p>

<p class="noindent chinese">当然，要付出代价，而代价是在测试时间。简用户刚刚上传了一张新照片。它是一张脸吗？最近的邻居的答案是：在 Facebook 的整个标签照片数据库中找到与它最相似的照片 —— 它的 “最近的邻居” —— 如果那张照片包含一张脸，那么这张照片也是。这很简单，但现在你必须在（理想情况下）几分之一秒的时间内扫描潜在的数十亿张照片。就像一个懒惰的学生懒得为考试复习一样，最近的邻居在毫无准备的情况下被抓住了，不得不争分夺秒。但与现实生活不同的是，在现实生活中，你的母亲教你永远不要把今天能做的事留到明天，而在机器学习中，拖延真的可以得到回报。事实上，最近邻居学习的整个流派有时被称为 “懒惰学习”，在这种情况下，这个词没有任何贬义。</p>

<p class="noindent english">The reason lazy learners are a lot smarter than they seem is that their models, although implicit, can in fact be extremely sophisticated. Consider the extreme case where we have only one example of each class. For instance, we’d like to guess where the border between two countries is, but all we know is their capitals’ locations. Most learners would be stumped, but nearest-neighbor happily guesses that the border is a straight line lying halfway between the two cities:</p>

<p class="noindent chinese">懒人学习者比他们看起来要聪明得多的原因是，他们的模型虽然是隐性的，但事实上可以是非常复杂的。考虑到一个极端的情况，即我们只有每个类别的一个例子。例如，我们想猜测两个国家的边界在哪里，但我们所知道的只是它们的首都的位置。大多数学习者都会被难住，但最近的邻居却能愉快地猜出边界是一条位于两个城市之间的直线。</p>

<div>

<div>

<img alt="image" src="images/000021.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-229"></a> The points on the line are at the same distance from the two capitals; points to the left of the line are closer to Positiville, so nearest-neighbor assumes they’re part of Posistan and vice versa. Of course, it would be a lucky day if that was the exact border, but as an approximation it’s probably a lot better than nothing. It’s when we know a lot of towns on both sides of the border, though, that things get really interesting:</p>

<p class="noindent chinese">线上的点与两国首都的距离相同；线的左边的点更接近波西米亚，所以最近邻假设它们是波西斯坦的一部分，反之亦然。当然，如果这就是确切的边界，那将是一个幸运的日子，但作为一个近似值，它可能比没有好得多。不过，当我们知道边界两边有很多城镇的时候，事情就变得非常有趣了。</p>

<div>

<div>

<img alt="image" src="images/000012.jpg"/>

</div>

</div>

<p class="noindent english">Nearest-neighbor is able to implicitly form a very intricate border, even though all it’s doing is remembering where the towns are and assigning points to countries accordingly! We can think of the “metro area” of a town as all the points that are closer to it than to any other town; the boundaries between metro areas are shown as dashed lines in the diagram. Now Posistan is just the union of the metro areas of all its cities, as is Negaland. In contrast, a decision tree (for example) would only be able to form borders running alternately north–south and east–west, probably a much worse approximation to the real border. Thus, even though decision tree learners are “eager,” trying hard at learning time to figure out where the border lies, “lazy” nearest-neighbor actually wins out.</p>

<p class="noindent chinese">最近的邻居能够隐含地形成一个非常复杂的边界，尽管它所做的只是记住城镇的位置，并相应地将点分配给国家我们可以把一个城镇的 “都市区” 看作是所有离它比其他城镇更近的点；都市区之间的边界在图中显示为虚线。现在，Posistan 只是其所有城市的都会区的联合，Negaland 也是如此。相比之下，决策树（例如）只能形成南北向和东西向交替的边界，可能对真实边界的近似度更差。因此，尽管决策树学习者是 “急切的”，在学习时努力找出边界的位置，但 “懒惰的” 最近的邻居实际上胜出。</p>

<p class="noindent english">The reason lazy learning wins is that forming a global model, such as a decision tree, is much harder than just figuring out where specific <a id="babilu_link-173"></a> query points lie, one at a time. Imagine trying to define what a face is with a decision tree. You could say it has two eyes, a nose, and a mouth, but what is an eye and how do you find it in an image? What if the person’s eyes are closed? Reliably defining a face all the way down to individual pixels is extremely difficult, particularly given all the different expressions, poses, contexts, and lighting conditions a face could appear in. Instead, nearest-neighbor takes a shortcut: if the image in its database most similar to the one Jane just uploaded is of a face, then so is Jane’s. For this to work, the database needs to contain an image that’s similar enough to the new one—for example, a face with similar pose, lighting, and so on—so the bigger the database, the better. For a simple two-dimensional problem like guessing the border between two countries, a tiny database suffices. For a very hard problem like identifying faces, where the color of each pixel is a dimension of variation, we need a huge database. But these days we have them. Learning from them may be too costly for an eager learner, which explicitly draws the border between faces and nonfaces. For nearest-neighbor, however, the border is implicit in the locations of the data points and the distance measure, and the only cost is at query time.</p>

<p class="noindent chinese">懒惰学习获胜的原因是，形成一个全局模型，如决策树，比仅仅找出具体的查询点的位置要难得多，一次一个。想象一下，试图用决策树来定义一张脸是什么。你可以说它有两只眼睛、一个鼻子和一张嘴，但眼睛是什么，你如何在图像中找到它？如果这个人的眼睛是闭着的呢？可靠地将一张脸定义为单个像素是非常困难的，特别是考虑到一张脸可能出现的所有不同表情、姿势、背景和照明条件。取而代之的是，最近的邻居采取了一个捷径：如果其数据库中与 Jane 刚刚上传的图片最相似的是一张脸，那么 Jane 的也是。要做到这一点，数据库需要包含一张与新图片足够相似的图片，例如，一张具有类似姿势、光线的脸，等等，所以数据库越大越好。对于一个简单的二维问题，如猜测两个国家的边界，一个很小的数据库就足够了。对于像识别人脸这样一个非常困难的问题，每个像素的颜色是一个变化的维度，我们需要一个巨大的数据库。但是这些天我们有了它们。对于一个急于求成的学习者来说，从他们身上学习的成本可能太高了，它明确地画出了脸和非脸之间的边界。然而，对于最近的邻居，边界是隐含在数据点的位置和距离测量中的，唯一的成本是在查询时。</p>

<p class="noindent english">The same idea of forming a local model rather than a global one applies beyond classification. Scientists routinely use linear regression to predict continuous variables, but most phenomena are not linear. Luckily, they’re locally linear because smooth curves are locally well approximated by straight lines. So if instead of trying to fit a straight line to all the data, you just fit it to the points near the query point, you now have a very powerful nonlinear regression algorithm. Laziness pays. If Kennedy had needed a complete theory of international relations to decide what to do about the Soviet missiles in Cuba, he would have been in trouble. Instead, he saw an analogy between that crisis and the outbreak of World War I, and that analogy guided him to the right decisions.</p>

<p class="noindent chinese">形成局部模型而不是全局模型的想法同样适用于分类之外。科学家们经常使用线性回归来预测连续变量，但大多数现象都不是线性的。幸运的是，它们是局部线性的，因为平滑的曲线在局部被直线很好地近似。因此，如果你不试图将一条直线拟合到所有的数据上，而只是将它拟合到查询点附近的点上，你现在就有了一个非常强大的非线性回归算法。懒惰是有代价的。如果肯尼迪需要一个完整的国际关系理论来决定如何处理古巴的苏联导弹，他就会遇到麻烦。相反，他看到了这场危机和第一次世界大战爆发之间的类比，这种类比引导他做出了正确的决定。</p>

<p class="noindent english">Nearest-neighbor can save lives, as Steven Johnson recounted in <i>The Ghost Map</i> . In 1854, London was struck by a cholera outbreak, which killed as many as one in eight people in parts of the city. The then-prevailing theory that cholera was caused by “bad air” did nothing to <a id="babilu_link-179"></a> prevent its spread. But John Snow, a physician who was skeptical of the theory, had a better idea. He marked on a map of London the locations of all the known cases of cholera and divided the map into the regions closest to each public water pump. Eureka: nearly all deaths were in the “metro area” of one particular pump, located on Broad Street in the Soho district. Inferring that the water in that well was contaminated, Snow convinced the locals to disable the pump, and the epidemic died out. This episode gave birth to the science of epidemiology, but it’s also the first success of the nearest-neighbor algorithm—almost a century before its official invention.</p>

<p class="noindent chinese">最近的邻居可以拯救生命，正如史蒂芬·约翰逊在《<i>幽灵地图</i>》中叙述的那样。1854 年，伦敦爆发了霍乱，该市部分地区每 8 人中就有 1 人死亡。当时流行的霍乱是由 “坏空气” 引起的理论对阻止其传播没有任何作用。但对这一理论持怀疑态度的医生约翰·斯诺有一个更好的想法。他在伦敦的地图上标出了所有已知霍乱病例的位置，并将地图划分为最接近每个公共水泵的区域。尤里卡：几乎所有的死亡病例都在一个特定水泵的 “地铁区域”，该水泵位于苏荷区的布罗德街。斯诺推断那口井里的水被污染了，于是说服当地人停用水泵，疫情也就消失了。这一事件催生了流行病学，但它也是最近邻居算法的第一次成功 —— 几乎比其正式发明早了一个世纪。</p>

<p class="noindent english">With nearest-neighbor, each data point is its own little classifier, predicting the class for all the query examples it wins. Nearest-neighbor is like an army of ants, in which each soldier by itself does little, but together they can move mountains. If an ant’s load is too heavy, it can share it with its neighbors. In the same spirit, in the <i>k</i> -nearest-neighbor algorithm, a test example is classified by finding its <i>k</i> nearest neighbors and letting them vote. If the nearest image to the new upload is a face but the next two nearest ones aren’t, three-nearest-neighbor decides that the new upload is not a face after all. Nearest-neighbor is prone to overfitting: if we have the wrong class for a data point, it spreads to its entire metro area. <i>K</i> -nearest-neighbor is more robust because it only goes wrong if a majority of the <i>k</i> nearest neighbors is noisy. The price, of course, is that its vision is blurrier: fine details of the frontier get washed away by the voting. When <i>k</i> goes up, variance decreases, but bias increases.</p>

<p class="noindent chinese">通过最近邻，每个数据点都是它自己的小分类器，预测它赢得的所有查询实例的类别。最近的邻居就像一支蚂蚁军队，其中每个士兵本身的作用不大，但他们在一起可以移动山脉。如果一只蚂蚁的负荷太重，它可以和它的邻居们分担。本着同样的精神，在 <i>k</i>-nearest-neighbor 算法中，通过找到一个测试实例的 <i>k</i> 个最近的邻居并让他们投票来进行分类。如果离新上传的图片最近的是一张脸，但接下来的两个最近的不是，那么三个最近的邻居决定新上传的图片毕竟不是一张脸。最近的邻居很容易过度拟合：如果我们对一个数据点有错误的分类，它就会蔓延到它的整个城市区域。i>k</i>-nearest-neighbor 比较稳健，因为只有当 <i>k</i> 个近邻中的大多数是噪声时，它才会出错。当然，代价是它的视野更模糊：边界的细微细节被投票冲走。当 <i>k</i> 增加时，方差减少，但偏差增加。</p>

<p class="noindent english">Using the <i>k</i> nearest neighbors instead of one is not the end of the story. Intuitively, the examples closest to the test example should count for more. This leads us to the weighted <i>k</i> -nearest-neighbor algorithm. In 1994, a team of researchers from the University of Minnesota and MIT built a recommendation system based on what they called “a deceptively simple idea”: people who agreed in the past are likely to agree again in the future. That notion led directly to the collaborative filtering systems that all self-respecting e-commerce sites have. Suppose that, like Netflix, you’ve gathered a database of movie ratings, with each user <a id="babilu_link-56"></a> giving a rating of one to five stars to the movies he or she has seen. You want to decide whether your user Ken will like <i>Gravity</i> , so you find the users whose past ratings correlate most highly with his. If they all gave <i>Gravity</i> high ratings, then probably so will Ken, and you can recommend it to him. If they disagree on <i>Gravity</i> , however, you need a fallback point, which in this case is ranking users by how highly they correlate with Ken. So if Lee’s correlation with Ken is higher than Meg’s, his ratings should count for correspondingly more. Ken’s predicted rating is then the weighted average of his neighbors’, with each neighbor’s weight being his coefficient of correlation with Ken.</p>

<p class="noindent chinese">使用 <i>k</i> 个最近的邻居而不是一个，并不是故事的结束。直观地说，与测试实例最接近的实例应该算得更多。这让我们想到了加权的 <i>k</i>-nearest-neighbor 算法。1994 年，来自明尼苏达大学和麻省理工学院的一个研究小组建立了一个推荐系统，该系统基于他们所谓的 “一个看似简单的想法”：过去同意的人很可能在未来再次同意。这一概念直接导致了所有有自尊心的电子商务网站所拥有的协作过滤系统。假设像 Netflix 一样，你已经收集了一个电影评分数据库，每个用户对他或她看过的电影给予一到五颗星的评分。你想决定你的用户 Ken 是否会喜欢《<i>地心引力</i>》，所以你找到那些过去的评分与他的评分关联度最高的用户。如果他们都给了 《<i>地心引力</i>》很高的评价，那么可能 Ken 也会喜欢，你可以把它推荐给他。如果他们对《<i>地心引力</i>》意见不一致，你就需要一个后备点，在这种情况下，就是根据用户与 Ken 的相关程度进行排名。因此，如果 Lee 与 Ken 的相关性比 Meg 高，他的评分就会相应地多一些。那么，Ken 的预测评分就是他的邻居的加权平均值，每个邻居的权重就是他与 Ken 的相关系数。</p>

<p class="noindent english">There’s an interesting twist, though. Suppose Lee and Ken have very similar tastes, but Lee is grumpier than Ken. Whenever Ken gives a movie five stars, Lee gives three; when Ken gives three, Lee gives one, and so on. We’d like to use Lee’s ratings to predict Ken’s, but if we just do it directly, we’ll always be off by two stars. Instead, what we need to do is predict how much Ken’s ratings will be above or below his average, based on how much Lee’s are. And now, since Ken is always two stars above his average when Lee is two stars above his, and so on, our predictions will be spot on.</p>

<p class="noindent chinese">不过，有一个有趣的转折。假设 Lee 和 Ken 的口味非常相似，但 Lee 比 Ken 更暴躁。每当 Ken 给一部电影打五星， Lee 就打三星；当 Ken 打三星， Lee 就打一星，以此类推。我们想用 Lee 的评分来预测 Ken 的评分，但如果我们直接这样做，我们总是会差两颗星。相反，我们需要做的是根据 Lee 的评分，预测 Ken 的评分将高于或低于他的平均水平多少。现在，因为当 Lee 比他的平均水平高两颗星时，Ken 总是比他的平均水平高两颗星，以此类推，我们的预测将是准确的。</p>

<p class="noindent english">You don’t need explicit ratings to do collaborative filtering, by the way. If Ken ordered a movie on Netflix, that means he expects to like it. So the “ratings” can just be ordered/not ordered, and two users are similar if they’ve ordered a lot of the same movies. Even just clicking on something implicitly shows interest in it. Nearest-neighbor works with all of the above. These days all kinds of algorithms are used to recommend items to users, but weighted <i>k</i> -nearest-neighbor was the first widely used one, and it’s still hard to beat.</p>

<p class="noindent chinese">顺便说一句，你不需要明确的评级来做协同过滤。如果 Ken 在 Netflix 上订购了一部电影，那就意味着他希望能喜欢这部电影。因此，“评级” 可以只是订购/不订购，如果两个用户订购了很多相同的电影，他们就是相似的。即使只是点击某个东西，也隐含着对它的兴趣。最近的邻居适用于上述所有情况。这些天来，各种算法都被用来向用户推荐项目，但加权 <i>k</i>-nearest-neighbor 是第一个被广泛使用的算法，而且它仍然很难被打败。</p>

<p class="noindent english">Recommender systems, as they’re also called, are big business: a third of Amazon’s business comes from its recommendations, as does three-quarters of Netflix’s. It’s a far cry from the early days of nearest-neighbor, when it was considered impractical because of its memory requirements. Back then, computer memories were made of small iron rings, one per bit, and storing even a few thousand examples was taxing. How times have changed. Nevertheless, it’s not necessarily smart to <a id="babilu_link-210"></a> remember all the examples you’ve seen and then have to search through them, particularly since most are probably irrelevant. If you look back at the map of Posistan and Negaland, you may notice that if Positiville disappeared, nothing would change. The metro areas of nearby cities would expand into the land formerly occupied by Positiville, but since they’re all Posistan cities, the border with Negaland would stay the same. The only cities that really matter are the ones across the border from a city in the other country; all others we can omit. So a simple way to make nearest-neighbor more efficient is to delete all the examples that are correctly classified by their neighbors. This and other tricks enable nearest-neighbor methods to be used in some surprising areas, like controlling robot arms in real time. But needless to say, they’re still not the first choice for things like high-frequency trading, where computers buy and sell stocks in fractions of a second. In a race between a neural network, which can be applied to an example with only a fixed number of additions, multiplications, and sigmoids and an algorithm that needs to search a large database for the example’s nearest neighbors, the neural network is sure to win.</p>

<p class="noindent chinese">推荐系统，也被称为 “大生意”：亚马逊三分之一的业务来自其推荐，Netflix 四分之三的业务也是如此。这与早期的 “最近邻” 系统相差甚远，当时由于其内存要求，它被认为是不切实际的。那时，计算机内存是由小铁环制成的，每一个比特一个，即使是存储几千个例子也很费力。时代变了。尽管如此，记住所有你见过的例子，然后再去搜索它们，这不一定是明智之举，尤其是大多数可能是不相关的。如果你回过头来看波西斯坦和尼加兰的地图，你可能会注意到，如果波西特维尔消失了，什么也不会改变。附近城市的市区会扩展到以前被波西提尔占据的土地上，但由于它们都是波西斯坦的城市，与尼加兰的边界将保持不变。真正重要的城市是那些与另一个国家的城市隔着边界的城市；其他城市我们可以省略。因此，使最近的邻居更有效率的一个简单方法是，删除所有被邻居正确分类的例子。这种方法和其他技巧使最近邻方法能够用于一些令人惊讶的领域，如实时控制机器人手臂。但不用说，它们仍然不是高频交易等事情的首选，在高频交易中，计算机在几分之一秒内买入和卖出股票。在神经网络和需要在大型数据库中搜索例子的最近邻居的算法之间的竞赛中，神经网络肯定会获胜，前者只需用固定数量的加法、乘法和 sigmoids 就可以应用于一个例子。</p>

<p class="noindent english">Another reason researchers were initially skeptical of nearest-neighbor was that it wasn’t clear if it could learn the true borders between concepts. But in 1967 Tom Cover and Peter Hart proved that, given enough data, nearest-neighbor is at worst only twice as error-prone as the best imaginable classifier. If, say, at least 1 percent of test examples will inevitably be misclassified because of noise in the data, then nearest-neighbor is guaranteed to get at most 2 percent wrong. This was a momentous revelation. Up until then, all known classifiers assumed that the frontier had a very specific form, typically a straight line. This was a double-edged sword: on the one hand, it made proofs of correctness possible, as in the case of the perceptron, but it also meant that the classifier was strictly limited in what it could learn. Nearest-neighbor was the first algorithm in history that could take advantage of unlimited amounts of data to learn arbitrarily complex concepts. No human being could hope to trace the frontiers it forms in hyperspace from millions of examples, but because of Cover and Hart’s proof, we know that they’re <a id="babilu_link-66"></a> probably not far off the mark. According to Ray Kurzweil, the Singularity begins when we can no longer understand what computers do. By that standard, it’s not entirely fanciful to say that it’s already under way—it began all the way back in 1951, when Fix and Hodges invented nearest-neighbor, the little algorithm that could.</p>

<p class="noindent chinese">研究人员最初对最近的邻居持怀疑态度的另一个原因是，不清楚它是否能学到概念之间的真正边界。但在 1967 年，汤姆·盖尔和彼得·哈特证明，在给定足够数据的情况下，最接近邻居的错误率最差也不过是可以想象的最佳分类器的两倍。比如说，由于数据中的噪声，至少有 1% 的测试实例将不可避免地被错误分类，那么近邻保证最多只会有 2% 的错误。这是一个重大的启示。直到那时，所有已知的分类器都假定边界有一个非常具体的形式，通常是一条直线。这是一把双刃剑：一方面，它使正确性的证明成为可能，如感知器的情况，但它也意味着分类器在它能学到的东西方面受到严格限制。最近邻接是历史上第一个可以利用无限量的数据来学习任意复杂概念的算法。没有人能够希望从数以百万计的例子中追踪它在超空间中形成的边界，但由于盖尔和哈特的证明，我们知道他们可能离目标不远。根据雷·库兹韦尔的说法，当我们不再能理解计算机的工作时，奇点就开始了。按照这个标准，说它已经在进行中并不完全是幻想 —— 它开始于 1951 年，当菲克斯和霍奇斯发明了最近的邻居，这个小算法可以。</p>

<h1 id="babilu_link-432"><b>The curse of dimensionality</b></h1>

<h1 id="babilu_link-432"><b>维度的诅咒</b></h1>

<p class="noindent english">There’s a serpent in this Eden, of course. It’s called the curse of dimensionality, and while it affects all learners to a greater or lesser degree, it’s particularly bad for nearest-neighbor. In low dimensions (like two or three), nearest-neighbor usually works quite well. But as the number of dimensions goes up, things fall apart pretty quickly. It’s not uncommon today to have thousands or even millions of attributes to learn from. For an e-commerce site trying to learn your preferences, every click you make is an attribute. So is every word on a web page, and every pixel on an image. But even with just tens or hundreds of attributes, chances are nearest-neighbor is already in trouble. The first problem is that most attributes are irrelevant: you may know a million factoids about Ken, but chances are only a few of them have anything to say about (for example) his risk of getting lung cancer. And while knowing whether he smokes is crucial for making that particular prediction, it’s probably not much help in deciding whether he’ll enjoy seeing <i>Gravity</i> . Symbolist methods, for one, are fairly good at disposing of irrelevant attributes. If an attribute has no information about the class, it’s just never included in the decision tree or rule set. But nearest-neighbor is hopelessly confused by irrelevant attributes because they all contribute to the similarity between examples. With enough irrelevant attributes, accidental similarity in the irrelevant dimensions swamps out meaningful similarity in the important ones, and nearest-neighbor becomes no better than random guessing.</p>

<p class="noindent chinese">当然，这个伊甸园里有一条毒蛇。它被称为维度的诅咒，虽然它或多或少地影响了所有的学习者，但它对最近相邻关系尤其不利。在低维度（如 2 或 3）中，最近相邻关系通常工作得很好。但是随着维数的增加，事情很快就会崩溃。今天，有数千甚至数百万个属性需要学习的情况并不少见。对于一个试图了解你喜好的电子商务网站来说，你的每一次点击都是一个属性。网页上的每一个字，以及图片上的每一个像素也是如此。但即使只有几十个或几百个属性，最近的邻居也有可能已经陷入困境。第一个问题是，大多数属性都是不相关的：你可能知道一百万个关于 Ken 的事实，但其中只有少数属性与（例如）他患肺癌的风险有关。虽然知道他是否吸烟对作出这一特定的预测至关重要，但对决定他是否喜欢看《<i>地心引力</i>》可能没有什么帮助。象征主义的方法，在处理不相关的属性方面相当出色。如果一个属性没有关于这个类别的信息，它就不会被纳入决策树或规则集。但是，最近的邻居会被不相关的属性所迷惑，因为它们都对例子之间的相似性有所贡献。有了足够多的不相关属性，不相关维度上的偶然相似性就会淹没重要维度上的有意义的相似性，而最近的邻居就变得比随机猜测好不了多少。</p>

<p class="noindent english">A bigger problem is that, surprisingly, having more attributes can be harmful even when they’re all relevant. You’d think that more information is always better—isn’t that the motto of our age? But as the number <a id="babilu_link-254"></a> of dimensions goes up, the number of training examples you need to locate the concept’s frontiers goes up exponentially. With twenty Boolean attributes, there are roughly a million different possible examples. With twenty-one, there are two million, and a corresponding number of ways the frontier could wind between them. Every extra attribute makes the learning problem twice as hard, and that’s just with Boolean attributes. If the attribute is highly informative, the benefit of adding it may exceed the cost. But if you have only weakly informative attributes, like the words in an e-mail or the pixels in an image, you’re probably in trouble, even though collectively they may have enough information to predict what you want.</p>

<p class="noindent chinese">更大的问题是，令人惊讶的是，拥有更多的属性可能是有害的，即使它们都是相关的。你会认为更多的信息总是更好的 —— 这不正是我们这个时代的座右铭吗？但是，随着维度的数量你需要用来定位概念边界的训练实例的数量也会成倍增加。如果有 20 个布尔属性，大约有一百万个不同的可能例子。如果是 21 个，就有 200 万个，边界也有相应数量的方式可以在它们之间缠绕。每一个额外的属性都会使学习问题变得加倍困难，而这仅仅是布尔属性的情况。如果属性的信息量很大，增加它的好处可能超过成本。但是，如果你只有信息量较小的属性，比如电子邮件中的单词或图像中的像素，你可能就有麻烦了，尽管它们总体上可能有足够的信息来预测你想要的东西。</p>

<p class="noindent english">It gets even worse. Nearest-neighbor is based on finding similar objects, and in high dimensions, the notion of similarity itself breaks down. Hyperspace is like the Twilight Zone. The intuitions we have from living in three dimensions no longer apply, and weird and weirder things start to happen. Consider an orange: a tasty ball of pulp surrounded by a thin shell of skin. Let’s say 90 percent of the radius of an orange is occupied by pulp, and the remaining 10 percent by skin. That means 73 percent of the volume of the orange is pulp (0.9<sup>3</sup>). Now consider a hyperorange: still with 90 percent of the radius occupied by pulp, but in a hundred dimensions, say. The pulp has shrunk to only about three thousandths of a percent of the hyperorange’s volume (0.9<sup>100</sup>). The hyperorange is all skin, and you’ll never be done peeling it!</p>

<p class="noindent chinese">情况甚至更糟。最近的邻居是基于寻找相似的物体，而在高维度上，相似性的概念本身就会被打破。超空间就像暮光之城。我们生活在三维空间中的直觉不再适用，奇怪和更奇怪的事情开始发生。考虑一个橙子：一个美味的果肉球，周围有一层薄薄的皮壳。比方说，橙子半径的 90% 被果肉占据，剩下的 10% 被皮肤占据。这意味着橙子体积的 73% 是果肉（0.9<sup>3</sup>）。现在考虑一个超级橘子：仍然有 90% 的半径被果肉占据，但在一百个维度上，比如说。果肉已经缩减到只占超橙体积的千分之三（0.9<sup>100</sup>）。超橙是所有的皮肤，而你永远不会完成剥皮的工作！</p>

<p class="noindent english">Another disturbing example is what happens with our good old friend, the normal distribution, aka a bell curve. What a normal distribution says is that data is essentially located at a point (the mean of the distribution), but with some fuzz around it (given by the standard deviation). Right? Not in hyperspace. With a high-dimensional normal distribution, you’re more likely to get a sample far from the mean than close to it. A bell curve in hyperspace looks more like a doughnut than a bell. And when nearest-neighbor walks into this topsy-turvy world, it gets hopelessly confused. All examples look equally alike, and at the same time they’re too far from each other to make useful predictions. If you sprinkle examples uniformly at random inside a high-dimensional hypercube, most are closer to a face of the cube than to their nearest <a id="babilu_link-90"></a> neighbor. In medieval maps, uncharted areas were marked with dragons, sea serpents, and other fantastical creatures, or just with the phrase <i>here be dragons</i> . In hyperspace, the dragons are everywhere, including at your front door. Try to walk to your next-door neighbor’s house, and you’ll never get there; you’ll be forever lost in strange lands, wondering where all the familiar things went.</p>

<p class="noindent chinese">另一个令人不安的例子是我们的好朋友 —— 正态分布（又称钟形曲线）的情况。正态分布说的是，数据基本上位于一个点（分布的平均值），但周围有一些模糊的地方（由标准差给出）。对吗？在超空间则不然。在高维正态分布中，你更有可能得到一个远离平均值的样本，而不是接近它的样本。超空间中的钟形曲线看起来更像一个甜甜圈，而不是一个钟形。而当最近的邻居走进这个颠覆性的世界时，它就会无可奈何地感到困惑。所有的例子看起来都一样，同时它们之间的距离又太远，无法做出有用的预测。如果你在一个高维超立方体中随机均匀地撒下例子，大多数例子都比它们最近的邻居更靠近立方体的一个面。在中世纪的地图上，未知的区域被标记为龙、海蛇和其他幻想中的生物，或者只是用 “<i>这里是龙</i>” 这句话。在超空间，龙无处不在，包括在你的前门。试着走到你隔壁邻居的房子，你永远也到不了那里；你将永远迷失在陌生的土地上，不知道所有熟悉的东西去了哪里。</p>

<p class="noindent english">Decision trees are not immune to the curse of dimensionality either. Let’s say the concept you’re trying to learn is a sphere: points inside it are positive, and points outside it are negative. A decision tree can approximate a sphere by the smallest cube it fits inside. Not perfect, but not too bad either: only the corners of the cube get misclassified. But in high dimensions, almost the entire volume of the hypercube lies outside the hypersphere. For every example you correctly classify as positive, you incorrectly classify many negative ones as positive, causing your accuracy to plummet.</p>

<p class="noindent chinese">决策树也不能免于维度的诅咒。假设你要学习的概念是一个球体：球体内部的点是正的，而球体外部的点是负的。决策树可以通过它适合的最小的立方体来接近球体。这并不完美，但也不算太差：只有立方体的角落会被错误分类。但在高维度上，超立方体的整个体积几乎都在超球体之外。对于每一个你正确归类为正面的例子，你都会错误地将许多负面的例子归类为正面，导致你的准确率直线下降。</p>

<p class="noindent english">In fact, no learner is immune to the curse of dimensionality. It’s the second worst problem in machine learning, after overfitting. The term <i>curse of dimensionality</i> was coined by Richard Bellman, a control theorist, in the fifties. He observed that control algorithms that worked fine in three dimensions became hopelessly inefficient in higher-dimensional spaces, such as when you want to control every joint in a robot arm or every knob in a chemical plant. But in machine learning the problem is more than just computational cost—it’s that learning itself becomes harder and harder as the dimensionality goes up.</p>

<p class="noindent chinese">事实上，没有一个学习者能免于维度的诅咒。它是机器学习中第二糟糕的问题，仅次于过度拟合。<i>维度诅咒</i>一词是由控制理论家理查德·贝尔曼在 50 年代提出的。他观察到，在三维空间中运行良好的控制算法在高维空间中变得无可救药地低效，例如当你想控制机器人手臂的每个关节或化工厂的每个旋钮时。但在机器学习中，问题不仅仅是计算成本，而是随着维度的增加，学习本身变得越来越难。</p>

<p class="noindent english">All is not lost, however. The first thing we can do is get rid of the irrelevant dimensions. Decision trees do this automatically by computing the information gain of each attribute and using only the most informative ones. For nearest-neighbor, we can accomplish something similar by first discarding all attributes whose information gain is below some threshold and then measuring similarity only in the reduced space. This is quick and good enough for some applications, but unfortunately it precludes learning many concepts, like exclusive-OR: if an attribute only says something about the class when combined with <a id="babilu_link-91"></a> others, but not on its own, it will be discarded. A more expensive but smarter option is to “wrap” the attribute selection around the learner itself, with a hill-climbing search that keeps deleting attributes as long as that doesn’t hurt nearest-neighbor’s accuracy on held-out data. Newton did a lot of attribute selection when he decided that all that matters for predicting an object’s trajectory is its mass—not its color, smell, age, or myriad other properties. In fact, the most important thing about an equation is all the quantities that don’t appear in it: once we know what the essentials are, figuring out how they depend on each other is often the easier part.</p>

<p class="noindent chinese">然而，一切并没有失去。我们可以做的第一件事是摆脱不相关的维度。决策树通过计算每个属性的信息增益并只使用信息量最大的属性来自动做到这一点。对于最近的邻居，我们可以通过首先抛弃所有信息增益低于某个阈值的属性来完成类似的工作，然后只在缩小的空间内测量相似性。这对某些应用来说是很快速的，也是很好的，但不幸的是，它排除了对许多概念的学习，比如排他性 OR：如果一个属性只有在与而不是单独的时候才能说明一些问题，它就会被抛弃。一个更昂贵但更聪明的选择是将属性选择 “包裹” 在学习者本身，通过爬坡搜索不断删除属性，只要这样做不影响最近邻居对所保留数据的准确性。当牛顿决定预测一个物体的运动轨迹时，他做了大量的属性选择，而不是它的颜色、气味、年龄或无数其他属性。事实上，一个方程最重要的是所有没有出现在方程中的数量：一旦我们知道基本要素是什么，弄清楚它们如何相互依赖往往是更容易的部分。</p>

<p class="noindent english">To handle weakly relevant attributes, one option is to learn attribute weights. Instead of letting the similarity along all dimensions count equally, we “shrink” the less-relevant ones. Suppose the training examples are points in a room, and the height dimension is not that important for our purposes. Discarding it would project all examples onto the floor. Downweighting it is more like giving the room a lower ceiling. The height of a point still counts when computing its distance to other points, but less than its horizontal position. And like many other things in machine learning, we can learn attribute weights by gradient descent.</p>

<p class="noindent chinese">为了处理弱相关的属性，一种选择是学习属性的权重。我们不是让所有维度的相似性都算在一起，而是 “缩小” 那些不太相关的属性。假设训练实例是一个房间里的点，高度维度对我们的目的并不重要。抛弃它就会把所有的例子都投射到地板上。降低它的权重更像是给房间一个较低的天花板。在计算一个点与其他点的距离时，该点的高度仍然算数，但比它的水平位置要少。像机器学习中的许多其他事情一样，我们可以通过梯度下降来学习属性权重。</p>

<p class="noindent english">It may happen that the room has a high ceiling, but the data points are all near the floor, like a thin layer of dust settling on the carpet. In that case, we’re in luck: the problem looks three dimensional, but in effect it’s closer to two dimensional. We don’t have to shrink height because nature has already shrunk it for us. This “blessing of nonuniformity,” whereby data is not spread uniformly in (hyper) space, is often what saves the day. The examples may have a thousand attributes, but in reality they all “live” in a much lower-dimensional space. That’s why nearest-neighbor can be good for handwritten digit recognition, for example: each pixel is a dimension, so there are many, but only a tiny fraction of all possible images are digits, and they all live together in a cozy little corner of hyperspace. The shape of the lower-dimensional space the data lives in may be quite capricious, however. For example, if a room has furniture in it, the dust doesn’t just settle on the floor; <a id="babilu_link-69"></a> it settles on the tabletops, chair seats, bed covers, and whatnot. If we can figure out the approximate shape of the blanket of dust covering the room, then all we need is each point’s coordinates on it. As we’ll see in the next chapter, there’s a whole subfield of machine learning dedicated to, so to speak, discovering blanket shapes by groping around in the darkness of hyperspace.</p>

<p class="noindent chinese">可能发生的情况是，房间的天花板很高，但数据点都在地板附近，就像地毯上沉淀的一层薄薄的灰尘。在这种情况下，我们很幸运：问题看起来是三维的，但实际上它更接近于二维的。我们不必缩减高度，因为大自然已经为我们缩减了。这种 “非均匀性的祝福”，即数据在（超）空间中的分布不均匀，往往是拯救世界的原因。例子可能有一千个属性，但实际上它们都 “生活” 在一个更低维的空间里。这就是为什么最近的邻居可以很好地识别手写数字，例如：每个像素是一个维度，所以有很多，但所有可能的图像中只有很小的一部分是数字，它们都生活在超空间的一个舒适的小角落里。然而，数据所处的低维空间的形状可能是相当任性的。例如，如果一个房间里有家具，灰尘就不会只沉淀在地板上；它会沉淀在桌面、椅座、床罩和其他地方。如果我们能算出覆盖房间的灰尘毯子的大致形状，那么我们所需要的就是每个点在上面的坐标。正如我们在下一章中所看到的，机器学习有一个完整的子领域，可以说是通过在超空间的黑暗中摸索发现毯子的形状。</p>

<h1 id="babilu_link-433"><b>Snakes on a plane</b></h1>

<h1 id="babilu_link-433"><b>飞机上的蛇</b></h1>

<p class="noindent english">Up until the mid-1990s, the most widely used analogical learner was nearest-neigbhor, but it was overshadowed by its more glamorous cousins from the other tribes. But then a new similarity-based algorithm burst onto the scene, sweeping all before it. In fact, you could say it was another “peace dividend” from the end of the Cold War. Support vector machines, or SVMs for short, were the brainchild of Vladimir Vapnik, a Soviet frequentist. Vapnik spent most of his career at the Institute of Control Sciences in Moscow, but in 1990, as the Soviet Union unraveled, he emigrated to the United States, where he joined the legendary Bell Labs. While in Russia, Vapnik had been mostly content to do theoretical, pencil-and-paper work, but the atmosphere at Bell Labs was different. Researchers were looking for practical results, and Vapnik finally decided to turn his ideas into an algorithm. Within a few years, he and his colleagues at Bell Labs had developed SVMs, and before long they were everywhere, setting new accuracy records left and right.</p>

<p class="noindent chinese">直到 20 世纪 90 年代中期，最广泛使用的类比学习器是最近的相似性，但它被其他部落更有魅力的表亲所掩盖。但后来一种新的基于相似性的算法突然出现在舞台上，横扫了所有的人。事实上，你可以说这是冷战结束后的另一个 “和平红利”。支持向量机，或简称 SVM，是苏联经常性学者弗拉基米尔·瓦普尼克的创意。瓦普尼克在莫斯科的控制科学研究所度过了他职业生涯的大部分时间，但在 1990 年，随着苏联的解体，他移民到了美国，在那里他加入了传奇的贝尔实验室。在俄罗斯时，瓦普尼克主要满足于做理论性的、笔头上的工作，但贝尔实验室的气氛不同。研究人员正在寻找实际的结果，瓦普尼克最终决定把他的想法变成一种算法。在几年内，他和他在贝尔实验室的同事们开发出了 SVM，不久之后，它们就随处可见，不断创造新的准确性记录。</p>

<p class="noindent english">Superficially, an SVM looks a lot like weighted <i>k</i> -nearest-neighbor: the frontier between the positive and negative classes is defined by a set of examples and their weights, together with a similarity measure. A test example belongs to the positive class if, on average, it looks more like the positive examples than the negative ones. The average is weighted, and the SVM remembers only the key examples required to pin down the frontier. If you look back at the Posistan/Negaland example, once we throw away all the towns that aren’t on the border, all that’s left is this map:</p>

<p class="noindent chinese">表面上看，SVM 很像加权的 <i>k</i>-nearest-neighbor：正类和负类之间的边界由一组例子和它们的权重以及相似性度量来定义。如果平均而言，一个测试例子与正面的例子相比更像负面的例子，那么它就属于正面类。平均数是经过加权的，而且 SVM 只记住了确定边界所需的关键例子。如果你回顾一下 Posistan/Negaland 的例子，一旦我们抛开所有不在边界上的城镇，剩下的就是这张地图。</p>

<div>

<div>

<img alt="image" src="images/000006.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-255"></a> These examples are called support vectors because they’re the vectors that “hold up” the frontier: remove one, and a section of the frontier slides to a different place. You may also notice that the frontier is a jagged line, with sudden corners that depend on the exact location of the examples. Real concepts tend to have smoother borders, which means nearest-neighbor’s approximation is probably not ideal. But with SVMs, we can learn smooth frontiers, more like this:</p>

<p class="noindent chinese">这些例子被称为支持向量，因为它们是 “支撑” 边界的向量：去掉一个，边界的一个部分就会滑向另一个地方。你可能还注意到，边界是一条锯齿状的线，有突然出现的角，这取决于例子的确切位置。真正的概念往往有更平滑的边界，这意味着最近邻居的近似值可能并不理想。但是用 SVM，我们可以学习平滑的边界，更像这样。</p>

<div>

<div>

<img alt="image" src="images/000035.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-286"></a> To learn an SVM, we need to choose the support vectors and their weights. The similarity measure, which in SVM-land is called the kernel, is usually chosen a priori. One of Vapnik’s key insights was that not all borders that separate the positive training examples from the negative ones are created equal. Suppose Posistan and Negaland are at war, and they’re separated by a no-man’s-land with minefields on either side. Your mission is to survey the no-man’s-land, walking from one end of it to the other without stepping on any mines. Luckily, you have a map of where the mines are buried. Obviously, you don’t just take any old path: you give the mines the widest possible berth. That’s what SVMs do, with the examples as mines and the learned border as the chosen path. The closest the border ever comes to an example is its margin of safety, and the SVM chooses the support vectors and weights that yield the maximum possible margin. For example, the solid straight-line border in this figure is better than the dotted one:</p>

<p class="noindent chinese">为了学习一个 SVM，我们需要选择支持向量和它们的权重。相似性测量，在 SVM 领域被称为核，通常是先验选择的。瓦普尼克的一个重要见解是，并非所有将正面训练例子与负面例子分开的边界都是平等的。假设 Posistan 和 Negaland 在打仗，它们被两边的雷区隔开，是一个无人区。你的任务是勘察无人区，从一端走到另一端而不踩到任何地雷。幸运的是，你有一张地图，上面有地雷的埋藏位置。显然，你不能只走任何一条老路：你要尽可能给地雷留出最宽的空间。这就是 SVM 所做的，把例子作为地雷，把学到的边界作为选择的路径。边界离一个例子最近的地方就是它的安全边际，SVM 选择支持向量和权重以产生最大的边际。例如，该图中的实心直线边界比虚线边界要好。</p>

<div>

<div>

<img alt="image" src="images/000026.jpg"/>

</div>

</div>

<p class="noindent english">The dotted border separates the positive and negative examples just fine, but it comes dangerously close to stepping on the landmines at A and B. These examples are support vectors: delete one of them, and the maximum-margin border moves to a different place. In general, the border can be curved, of course, making the margin harder to visualize, but we can think of the border as a snake slithering down the no-man’s-land, <a id="babilu_link-207"></a> and the margin is how fat the snake can be. If a very fat snake can slither all the way down without blowing itself to smithereens, then the SVM can separate the positive and negative examples very well, and Vapnik showed that in this case we can be confident that the SVM didn’t overfit. Intuitively, compared to a thin snake, there are fewer ways a fat snake can slither down while avoiding the landmines; and likewise, compared to a low-margin SVM, a high-margin one has fewer chances of overfitting by drawing an overly intricate border.</p>

<p class="noindent chinese">虚线边界将正反两方面的例子分开得很好，但它危险地接近于踩到了 A 和 B 的地雷。这些例子是支持向量：删除其中一个，最大边际边界就会移到不同的地方。当然，在一般情况下，边界可以是弯曲的，这使得边际更难直观化，但我们可以把边界想象成一条在无人区滑行的蛇，而边际就是蛇可以有多肥。如果一条很肥的蛇能一路下滑而不把自己炸成碎片，那么 SVM 就能很好地分离正反两方面的例子，而且瓦普尼克表明在这种情况下我们可以确信 SVM 没有过度拟合。直观地说，与瘦蛇相比，胖蛇在避开地雷的同时能滑下去的方法更少；同样，与低边际的 SVM 相比，高边际的 SVM 因画出过于复杂的边界而过拟合的机会更少。</p>

<p class="noindent english">The second part of the story is how the SVM finds the fattest snake that fits between the positive and negative landmines. At first sight, it might seem like learning a weight for each training example by gradient descent would do the trick. All we have to do is find the weights that maximize the margin, and any examples that end up with zero weight can be discarded. Unfortunately, this would just make the weights grow without limit, because mathematically, the larger the weights, the larger the margin. If you’re one foot from a landmine and you double the size of everything including yourself, you are now two feet from the landmine, but that doesn’t make you any less likely to step on it. Instead, we have to maximize the margin under the constraint that the weights can only increase up to some fixed value. Or, equivalently, we can minimize the weights under the constraint that all examples have a given margin, which could be one—the precise value is arbitrary. This is what SVMs usually do.</p>

<p class="noindent chinese">故事的第二部分是 SVM 如何找到适合正反两方面地雷的最肥的蛇。乍一看，似乎通过梯度下降为每个训练实例学习一个权重就可以了。我们所要做的就是找到使边际最大化的权重，任何最终权重为零的例子都可以被丢弃。不幸的是，这只会使权重无限制地增长，因为从数学上讲，权重越大，边际就越大。如果你离地雷只有一英尺，而你把包括你自己在内的所有东西的大小增加了一倍，你现在离地雷有两英尺，但这并没有使你踩到地雷的可能性降低。相反，我们必须在权重只能增加到某个固定值的约束条件下，最大限度地提高保证金。或者说，我们可以在所有例子都有一个给定的余量（可能是 1）的约束条件下，使权重最小化，而精确的值是任意的。这就是 SVM 通常所做的。</p>

<p class="noindent english">Constrained optimization is the problem of maximizing or minimizing a function subject to constraints. The universe maximizes entropy subject to keeping energy constant. Problems of this type are widespread in business and technology. For example, we may want to maximize the number of widgets a factory produces, subject to the number of machine tools available, the widgets’ specs, and so on. With SVMs, constrained optimization became crucial for machine learning as well. Unconstrained optimization is getting to the top of the mountain, and that’s what gradient descent (or, in this case, ascent) does. Constrained optimization is going as high as you can while staying on the road. If the road goes up to the very top, the constrained and unconstrained <a id="babilu_link-434"></a> problems have the same solution. More often, though, the road zigzags up the mountain and then back down without ever reaching the top. You know you’ve reached the highest point on the road when you can’t go any higher without driving off the road; in other words, when the path to the top is at right angles to the road. If the road and the path to the top form an oblique angle, you can always get higher by driving farther along the road, even if that doesn’t get you higher as quickly as aiming straight for the top of the mountain. So the way to solve a constrained optimization problem is to follow not the gradient but the part of it that’s parallel to the constraint surface—in this case the road—and stop when that part is zero.</p>

<p class="noindent chinese">约束优化是指在约束条件下使一个函数最大化或最小化的问题。宇宙在保持能量不变的情况下使熵最大化。这种类型的问题广泛存在于商业和技术中。例如，我们可能想使工厂生产的小部件数量最大化，但要考虑到可用的机床数量、小部件的规格等等。有了 SVM，有约束的优化对机器学习也变得至关重要。无约束的优化是到达山顶，这就是梯度下降（或者，在这种情况下，上升）的作用。有约束的优化是在保持道路畅通的情况下尽可能地往上走。如果道路上升到最顶端，那么约束和非约束问题的解决方案是一样的。但更多的时候，道路是以之字形上山，然后再下山，永远也到不了山顶。当你不能再往上走而不离开道路时，你就知道你已经到达了道路的最高点；换句话说，当通往山顶的道路与道路成直角时。如果道路和通往山顶的道路形成一个斜角，你总是可以通过沿着道路开得更远来获得更高的位置，即使这样做不能像直接瞄准山顶那样快速获得更高位置。因此，解决限制性优化问题的方法不是沿着坡度，而是沿着与限制性表面平行的部分，在这种情况下是道路，当这部分为零时就停止。</p>

<p class="noindent english">In general, we have to deal with many constraints at once (one per example, in the case of SVMs). Suppose you wanted to get as close as possible to the North Pole but couldn’t leave your room. Each of the room’s four walls is a constraint, and the solution is to follow the compass until you bump into the corner where the northeast and northwest walls meet. We say that these two walls are the active constraints because they’re what prevents you from reaching the optimum, namely the North Pole. If your room has a wall facing exactly north, that’s the sole active constraint, and the solution is a point in the middle of it. And if you’re Santa and your room is already over the North Pole, all constraints are inactive, and you can just sit there pondering the optimal toy distribution problem instead. (Traveling salesmen have it easy compared to Santa.) In an SVM, the active constraints are the support vectors since their margin is already the smallest it’s allowed to be; moving the frontier would violate one or more constraints. All other examples are irrelevant, and their weight is zero.</p>

<p class="noindent chinese">一般来说，我们必须同时处理许多约束条件（在 SVM 的情况下，每个例子一个）。假设你想尽可能地接近北极，但不能离开你的房间。房间的四面墙中的每一面都是一个约束条件，解决方案是沿着罗盘走，直到你撞到东北墙和西北墙交汇的角落。我们说这两面墙是主动约束，因为它们是阻止你到达最佳状态，即北极的原因。如果你的房间有一堵墙正好朝北，那就是唯一的主动约束，而解决方案就是墙中间的一个点。如果你是圣诞老人，而你的房间已经在北极上空，那么所有的约束都是不活跃的，你就可以坐在那里思考最佳的玩具分配问题。（与圣诞老人相比，旅行推销员很容易。）在 SVM 中，主动约束是支持向量，因为它们的边际已经是允许的最小值了；移动前沿将违反一个或多个约束。所有其他的例子都是不相关的，其权重为零。</p>

<p class="noindent english">In reality, we usually let SVMs violate some constraints, meaning classify some examples incorrectly or by less than the margin, because otherwise they would overfit. If there’s a noisy negative example somewhere in the middle of the positive region, we don’t want the frontier to wind around inside the positive region just to get that example right. But the SVM pays a penalty for each example it gets wrong, which encourages it to keep those to a minimum. SVMs are like the sandworms <a id="babilu_link-231"></a> in <i>Dune</i> : big, tough, and able to survive a few explosions from slithering over landmines but not too many.</p>

<p class="noindent chinese">在现实中，我们通常会让 SVM 违反一些约束条件，也就是说，对一些例子进行错误的分类，或者以低于边际的方式进行分类，因为否则他们会过度拟合。如果在正向区域中间的某个地方有一个嘈杂的负向例子，我们不希望边界在正向区域内绕来绕去，只是为了让这个例子正确。但是，SVM 为每一个错误的例子付出了代价，这促使它把这些错误降到最低。SVM 就像《<i>沙丘</i>》中的沙虫：高大、坚韧，能够在滑过地雷时经受一些爆炸，但不会太多。</p>

<p class="noindent english">Looking around for applications, Vapnik and his coworkers soon alighted on handwritten digit recognition, which their connectionist colleagues at Bell Labs were the world experts on. To everyone’s surprise, SVMs did as well out of the box as multilayer perceptrons that had been carefully crafted for digit recognition over the years. This set the stage for a long-running, wide-ranging competition between the two. SVMs can be seen as a generalization of the perceptron, because a hyperplane boundary between classes is what you get when you use a particular similarity measure (the dot product between vectors). But SVMs have a major advantage compared to multilayer perceptrons: the weights have a single optimum instead of many local ones and so learning them reliably is much easier. Despite this, SVMs are no less expressive than multilayer perceptrons; the support vectors effectively act as a hidden layer and their weighted average as the output layer. For example, an SVM can easily represent the exclusive-OR function by having one support vector for each of the four possible configurations. But the connectionists didn’t give up without a fight. In 1995, Larry Jackel, the head of Vapnik’s department at Bell Labs, bet him a fancy dinner that by 2000 neural networks would be as well understood as SVMs. He lost. But in return, Vapnik bet that by 2005 no one would use neural networks any more, and he also lost. (The only one to get a free dinner was Yann LeCun, their witness.) Moreover, with the advent of deep learning, connectionists have regained the upper hand. Provided you can learn them, networks with many layers can express many functions more compactly than SVMs, which always have just one layer, and this can make all the difference.</p>

<p class="noindent chinese">在四处寻找应用时，瓦普尼克和他的同事们很快就想到了手写数字识别，他们在贝尔实验室的连接主义同事是这方面的世界专家。令所有人惊讶的是，SVM 的表现与多年来为数字识别精心设计的多层感知器一样好。这为两者之间的长期、广泛的竞争奠定了基础。SVMs 可以被看作是感知器的概括，因为当你使用特定的相似度测量（向量之间的点积）时，你会得到类之间的超平面边界。但是，与多层感知器相比，SVMs 有一个主要的优势：权重有一个单一的最佳值，而不是许多局部的最佳值，因此可靠地学习它们要容易得多。尽管如此，SVM 的表现力并不亚于多层感知器；支持向量有效地充当了隐藏层，其加权平均值则是输出层。例如，SVM 可以通过为四种可能的配置各设置一个支持向量来轻松表示排他性-OR 函数。但是，连接主义者并没有不战而降。1995 年，瓦普尼克在贝尔实验室的部门主管拉里·杰克尔和他打赌，说到 2000 年，神经网络会像 SVM 一样被人们所理解。他输了。但作为回报，瓦普尼克打赌到 2005 年将没有人再使用神经网络了，他也输了。（唯一得到免费晚餐的是扬·勒库恩，他是他们的见证人。）此外，随着深度学习的出现，连接主义者已经重新占据了上风。只要你能学会它们，具有许多层的网络可以比 SVM 更紧凑地表达许多函数，而 SVM 总是只有一层，这可以使所有的差异。</p>

<p class="noindent english">Another notable early success of SVMs was in text classification, which proved a major boon because the web was then just taking off. At the time, Naïve Bayes was the state-of-the-art text classifier, but when every word in the language is a dimension, even it can start to overfit. All it takes is a word that, by chance, occurs in, say, all sports pages in the training data and no others, and Naïve Bayes starts to hallucinate that <a id="babilu_link-213"></a> every page containing that word is a sports page. But, thanks to margin maximization, SVMs can resist overfitting even in very high dimensions.</p>

<p class="noindent chinese">SVM 的另一个显著的早期成功是在文本分类方面，这被证明是一个重大的福音，因为当时网络刚刚起飞。当时，天真贝叶斯是最先进的文本分类器，但当语言中的每个词都是一个维度时，即使它也会开始过度适应。只要有一个词偶然出现在训练数据中的所有体育网页中，而没有其他的，天真贝叶斯就会开始产生幻觉，认为每个包含这个词的网页都是体育网页。但是，由于边际最大化，SVM 即使在很高的维度上也能抵制过度拟合。</p>

<p class="noindent english">Generally, the fewer support vectors an SVM selects, the better it generalizes. Any training example that is not a support vector would be correctly classified if it showed up as a test example instead because the frontier between positive and negative examples would still be in the same place. So the expected error rate of an SVM is at most the fraction of examples that are support vectors. As the number of dimensions goes up, this fraction tends to go up as well, so SVMs are not immune to the curse of dimensionality. But they’re more resistant to it than most.</p>

<p class="noindent chinese">一般来说，SVM 选择的支持向量越少，它的概括性就越好。任何不是支持向量的训练实例，如果作为测试实例出现，都会被正确分类，因为正面和反面实例之间的边界仍然在同一个地方。因此，SVM 的预期错误率最多就是支持向量的例子的一部分。随着维数的增加，这一比例也趋于上升，所以 SVM 对维数的诅咒没有免疫力。但它们比大多数人更能抵抗它。</p>

<p class="noindent english">Practical successes aside, SVMs also turned a lot of machine-learning conventional wisdom on its head. For example, they gave the lie to the notion, sometimes misidentified with Occam’s razor, that simpler models are more accurate. On the contrary, an SVM can have an infinite number of parameters and still not overfit, provided it has a large enough margin.</p>

<p class="noindent chinese">除了实际的成功，SVM 还颠覆了很多机器学习的传统智慧。例如，它们推翻了有时被误认为是奥卡姆剃刀的概念，即更简单的模型更准确。相反，一个 SVM 可以有无限多的参数，只要它有足够大的余量，仍然不会过度拟合。</p>

<p class="noindent english">The single most surprising property of SVMs, however, is that no matter how curvy the frontiers they form, those frontiers are always just straight lines (or hyperplanes, in general). The reason that’s not a contradiction is that the straight lines are in a different space. Suppose the examples live on the (<i>x,y</i>) plane, and the boundary between the positive and negative regions is the parabola <i>y</i> = <i>x</i> <sup>2</sup> . There’s no way to represent it with a straight line, but if we add a third coordinate <i>z</i> , meaning the data now lives in (<i>x,y,z</i>) space, and we set each example’s <i>z</i> coordinate to the square of its <i>x</i> coordinate, the frontier is now just the diagonal plane defined by <i>y</i> = <i>z</i> . In effect, the data points rise up into the third dimension, some rise more than others by just the right amount, and presto—in this new dimension the positive and negative examples can be separated by a plane. It turns out that we can view what SVMs do with kernels, support vectors, and weights as mapping the data to a higher-dimensional space and finding a maximum-margin hyperplane in that space. For some kernels, the derived space has infinite dimensions, but SVMs are completely unfazed by that. Hyperspace may be the Twilight Zone, but SVMs have figured out how to navigate it.</p>

<p class="noindent chinese">然而，SVM 最令人惊讶的一个特性是，无论它们形成的边界多么弯曲，这些边界始终只是直线（或超平面，一般来说）。这不是矛盾的原因是，直线是在一个不同的空间。假设这些例子生活在<i>（x,y）</i>平面上，而正负区域之间的边界是抛物线 <i>y</i> = <i>x</i><sup>2</sup>。没有办法用直线来表示，但是如果我们增加第三个坐标 <i>z</i>，意味着数据现在生活在<i>（x,y,z）</i>空间，并且我们将每个例子的 <i>z</i> 坐标设置为其 <i>x</i> 坐标的平方，那么边界现在只是由 <i>y</i> = <i>z</i> 定义的对角线平面。实际上，数据点上升到了第三维，有些点比其他点上升得恰到好处，在这个新的维度上，正反两方面的例子可以被一个平面分开。事实证明，我们可以把 SVM 对核、支持向量和权重的处理看作是将数据映射到一个高维空间，并在该空间中找到一个最大边际的超平面。对于某些核，派生空间有无限的维度，但 SVM 完全不受影响。超空间可能是暮光之城，但 SVM 已经想出了如何驾驭它。</p>

<h1 id="babilu_link-435"><b><a id="babilu_link-60"><b></b></a> Climbing the ladder</b></h1>

<h1 id="babilu_link-435"><b><a id="babilu_link-60"><b></b></a>攀登的阶梯</b></h1>

<p class="noindent english">Two things are similar if they agree with one another in some respects. If they agree in some respects, they will probably also agree in others. This is the essence of analogy. It also points to the two main subproblems in analogical reasoning: figuring out how similar two things are and deciding what else to infer from their similarities. So far we’ve explored the “low power” end of analogy, with algorithms like nearest-neighbor and SVMs, where the answers to both these questions are very simple. They’re the most widely used, but a chapter on analogical learning would not be complete without at least a whirlwind tour of the more powerful parts of the spectrum.</p>

<p class="noindent chinese">如果两件事物在某些方面相互一致，那么它们就是相似的。如果它们在某些方面一致，它们很可能在其他方面也会一致。这就是类比的本质。它还指出了类比推理中的两个主要子问题：弄清两件事物的相似程度，以及决定从它们的相似性中推断出什么。到目前为止，我们已经探索了类比的 “低功率” 端，如最近的邻居和 SVM 的算法，其中这两个问题的答案都非常简单。它们是使用最广泛的，但是如果不对光谱中更强大的部分进行旋风式的参观，关于类比学习的章节就不完整。</p>

<p class="noindent english">The most important question in any analogical learner is how to measure similarity. It could be as simple as Euclidean distance between data points, or as complex as a whole program with multiple levels of subroutines whose final output is a similarity value. Either way, the similarity function controls how the learner generalizes from known examples to new ones. It’s where we insert our knowledge of the problem domain into the learner, making it the analogizers’ answer to Hume’s question. We can apply analogical learning to all kinds of objects, not just vectors of attributes, provided we have a way of measuring the similarity between them. For example, we can measure the similarity between two molecules by the number of identical substructures they contain. Methane and methanol are similar because they have three carbon-hydrogen bonds in common and differ only in the replacement of a hydrogen atom by a hydroxyl group:</p>

<p class="noindent chinese">任何类比学习者中最重要的问题是如何测量相似性。它可以是简单的数据点之间的欧几里得距离，也可以是复杂的具有多级子程序的整个程序，其最终输出是一个相似度值。无论哪种方式，相似性函数都控制着学习者如何从已知的例子归纳到新的例子。这就是我们在学习器中插入我们对问题领域的知识，使其成为类比者对休谟问题的回答。我们可以将类比学习应用于所有类型的对象，而不仅仅是属性的向量，只要我们有办法测量它们之间的相似性。例如，我们可以通过两个分子所包含的相同子结构的数量来衡量它们之间的相似性。甲烷和甲醇是相似的，因为它们有三个共同的碳氢键，区别只在于一个羟基取代了一个氢原子。</p>

<div>

<div>

<img alt="image" src="images/000016.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-47"></a> However, that doesn’t mean their chemical behavior is similar. Methane is a gas, while methanol is an alcohol. The second part of analogical reasoning is figuring out what we can infer about the new object based on similar ones we’ve found. This can be very simple or very complex. In nearest-neighbor or SVMs, it just consists of predicting the new object’s class based on the classes of the nearest neighbors or support vectors. But in case-based reasoning, another type of analogical learning, the output can be a complex structure formed by composing parts of the retrieved objects. Suppose your HP printer is spewing out gibberish, and you call up their help desk. Chances are they’ve seen your problem many times before, so a good strategy is to find those records and piece together a potential solution for your problem from them. This is not just a matter of finding complaints with many similar attributes to yours: for example, whether you’re using your printer with Windows or Mac OS X may cause very different settings of the system and the printer to become relevant. And once you’ve found the most relevant cases, the sequence of steps needed to solve your problem may be a combination of steps from different cases, with some further tweaks specific to yours.</p>

<p class="noindent chinese">然而，这并不意味着它们的化学行为是相似的。甲烷是一种气体，而甲醇是一种酒精。类比推理的第二部分是根据我们已经发现的类似物体，弄清楚我们可以推断出新物体的哪些方面。这可以是非常简单的，也可以是非常复杂的。在最近的邻居或 SVM 中，它只是包括根据最近的邻居或支持向量的类别来预测新对象的类别。但在基于案例的推理中，也就是另一种类型的类比学习中，输出可以是由检索到的对象的一部分组成的复杂结构。假设你的惠普打印机在喷出胡言乱语，而你打电话给他们的服务台。他们有可能以前就见过你的问题，所以一个好的策略是找到这些记录，并从这些记录中拼凑出一个潜在的解决问题的方法。这不仅仅是找到与你有许多相似属性的投诉的问题：例如，你在 Windows 或 Mac OS X 上使用打印机，可能会导致系统和打印机的设置有很大不同。一旦你找到了最相关的案例，解决你的问题所需的步骤顺序可能是不同案例的步骤的组合，再加上一些针对你的具体调整。</p>

<p class="noindent english">Help desks are currently the most popular application of case-based reasoning. Most still employ a human intermediary, but IPsoft’s Eliza talks directly to the customer. Eliza, who comes complete with a 3-D interactive video persona, has solved over twenty million customer problems to date, mostly for blue-chip US companies. “Greetings from Robotistan, outsourcing’s cheapest new destination,” is how an outsourcing blog recently put it. And, just as outsourcing keeps climbing the skills ladder, so does analogical learning. The first robo-lawyers that argue for a particular verdict based on precedents have already been built. One such system correctly predicted the outcomes of over 90 percent of the trade secret cases it examined. Perhaps in a future cybercourt, in session somewhere on Amazon’s cloud, a robo-lawyer will beat the speeding ticket that RoboCop issued to your driverless car, all while you go to the beach, and Leibniz’s dream of reducing all argument to calculation will finally have come true.</p>

<p class="noindent chinese">服务台是目前最流行的基于案例的推理的应用。大多数服务台仍然采用人类中介，但 IPsoft 的 Eliza 直接与客户交谈。Eliza 带有一个三维互动视频角色，迄今已解决了两千多万个客户问题，主要是为美国的蓝筹公司服务。一个外包博客最近这样说：“来自机器人斯坦的问候，这是外包最便宜的新目的地。” 而且，正如外包不断攀升的技能阶梯一样，模拟学习也是如此。第一批根据先例论证特定判决的机器人律师已经建成。一个这样的系统正确预测了它所审查的 90% 以上的商业秘密案件的结果。也许在未来的网络法庭上，在亚马逊云端的某个地方开庭，机器人律师将击败机器人警察向你的无人驾驶汽车开出的超速罚单，而你却去了海滩，莱布尼茨将所有争论简化为计算的梦想终于实现了。</p>

<p class="noindent english"><a id="babilu_link-45"></a> Arguably even higher up in the skills ladder is music composition. David Cope, an emeritus professor of music at the University of California, Santa Cruz, designed an algorithm that creates new music in the style of famous composers by selecting and recombining short passages from their work. At a conference I attended some years ago, he played three “Mozart” pieces: one by the real Mozart, one by a human composer imitating Mozart, and one by his system. He then asked the audience to vote for the authentic Amadeus. Wolfgang won, but the computer beat the human imitator. This being an AI conference, the audience was delighted. Audiences at other events were less happy. One listener angrily accused Cope of ruining music for him. If Cope is right, creativity—the ultimate unfathomable—boils down to analogy and recombination. Judge for yourself by googling “david cope mp3.”</p>

<p class="noindent chinese">可以说，技能阶梯中更高的是音乐创作。加州大学圣克鲁兹分校的名誉音乐教授大卫·科普设计了一种算法，通过选择和重新组合著名作曲家作品中的简短段落，创造出具有他们风格的新音乐。在我几年前参加的一次会议上，他演奏了三首 “莫扎特” 作品：一首是真正的莫扎特，一首是模仿莫扎特的人类作曲家，还有一首是他的系统。然后他让观众投票选出真正的莫扎特。沃尔夫冈赢了，但计算机击败了人类模仿者。作为一个人工智能会议，观众们都很高兴。其他活动的听众则不太高兴。一位听众愤怒地指责科普毁了他的音乐。如果科普是对的，创造力 —— 最终的深不可测 —— 就会归结为类比和重新组合。你可以通过谷歌搜索 “David cope mp3” 来判断。</p>

<p class="noindent english">Analogizers’ neatest trick, however, is learning across problem domains. Humans do it all the time: an executive can move from, say, a media company to a consumer-products one without starting from scratch because many of the same management skills still apply. Wall Street hires lots of physicists because physical and financial problems, although superficially very different, often have a similar mathematical structure. Yet all the learners we’ve seen so far would fall flat if we, say, trained them to predict Brownian motion and then asked them to predict the stock market. Stock prices and the velocities of particles suspended in a fluid are just different variables, so the learner wouldn’t even know where to start. But analogizers can do this using structure mapping, an algorithm invented by Dedre Gentner, a psychologist at Northwestern University. Structure mapping takes two descriptions, finds a coherent correspondence between some of their parts and relations, and then, based on that correspondence, transfers further properties from one structure to the other. For example, if the structures are the solar system and the atom, we can map planets to electrons and the sun to the nucleus and conclude, as Bohr did, that electrons revolve around the nucleus. The truth is more subtle, of course, and we often need to refine analogies after we make them. But being able to learn from a single example like this is surely a key attribute of a universal <a id="babilu_link-70"></a> learner. When we’re confronted with a new type of cancer—and that happens all the time because cancers keep mutating—the models we’ve learned for previous ones don’t apply. Neither do we have time to gather data on the new cancer from a lot of patients; there may be only one, and she urgently needs a cure. Our best hope is then to compare the new cancer with known ones and try to find one whose behavior is similar enough that some of the same lines of attack will work.</p>

<p class="noindent chinese">然而，模拟者最巧妙的技巧是跨问题领域的学习。人类一直在这样做：一个高管可以从媒体公司转到消费品公司，而不必从头开始，因为许多相同的管理技能仍然适用。华尔街雇佣了很多物理学家，因为物理和金融问题，虽然表面上非常不同，但往往有类似的数学结构。然而，到目前为止，如果我们训练他们预测布朗运动，然后要求他们预测股票市场，我们所看到的所有学习者都会落空。股票价格和悬浮在流体中的粒子的速度只是不同的变量，所以学习者甚至不知道从哪里开始。但是类比者可以使用结构映射来做到这一点，这是西北大学的心理学家戴德雷·根特纳发明的一种算法。结构映射需要两个描述，在它们的一些部分和关系之间找到一个连贯的对应关系，然后根据这种对应关系，将进一步的属性从一个结构转移到另一个结构。例如，如果结构是太阳系和原子，我们可以将行星映射为电子，将太阳映射为原子核，并像玻尔那样得出结论，电子围绕原子核旋转。当然，真相更加微妙，我们在做完类比之后，常常需要对其进行完善。但能够从这样一个单一的例子中学习，肯定是一个通用的学习者的关键属性。当我们面对一种新的癌症类型时 —— 这种情况一直在发生，因为癌症一直在变异 —— 我们为以前的癌症学到的模型并不适用。我们也没有时间从许多病人身上收集关于新癌症的数据；可能只有一个，而且她迫切需要治愈。我们最好的希望是将新的癌症与已知的癌症进行比较，并试图找到一个其行为足够相似的癌症，使一些相同的攻击路线能够发挥作用。</p>

<p class="noindent english">Is there anything analogy can’t do? Not according to Douglas Hofstadter, cognitive scientist and author of <i>Gödel, Escher, Bach: An Eternal Golden Braid</i> . Hofstadter, who looks a bit like the Grinch’s good twin, is probably the world’s best-known analogizer. In their book <i>Surfaces and Essences: Analogy as the Fuel and Fire of Thinking</i> , Hofstadter and his collaborator Emmanuel Sander argue passionately that all intelligent behavior reduces to analogy. Everything we learn or discover, from the meaning of everyday words like <i>mother</i> and <i>play</i> to the brilliant insights of geniuses like Albert Einstein and Évariste Galois, is the result of analogy in action. When little Tim sees women looking after other children like his mother looks after him, he generalizes the concept “mommy” to mean anyone’s mommy, not just his. That in turn is a springboard for understanding things like “mother ship” and “Mother Nature.” Einstein’s “happiest thought,” out of which grew the general theory of relativity, was an analogy between gravity and acceleration: if you’re in an elevator, you can’t tell whether your weight is due to one or the other because their effects are the same. We swim in a vast ocean of analogies, which we both manipulate for our ends and are unwittingly manipulated by. Books have analogies on every page (like the title of this section, or the previous one’s). <i>Gödel, Escher, Bach</i> is an extended analogy between Gödel’s theorem, Escher’s art, and Bach’s music. If the Master Algorithm is not analogy, it must surely be something like it.</p>

<p class="noindent chinese">有什么事情是类比做不到的吗？认知科学家、《<i>哥德尔、艾舍尔、巴赫：一条永恒的金线</i>》一书的作者道格拉斯·霍夫斯塔特认为不是这样的。霍夫斯塔德，看起来有点像格林奇的好兄弟，可能是世界上最有名的类比者。在他们的书《<i>表面与本质</i>》中 <i>。</i>霍夫斯塔德和他的合作者伊曼纽尔·桑德在他们的《<i>表面和本质：作为思维的燃料和火种的类比</i>》一书中热情洋溢地论证了所有的智能行为都可以归结为类比。我们所学习或发现的一切，从像<i>母亲</i>和<i>游戏</i>这样的日常用语的含义到像爱因斯坦和埃瓦利斯特·伽罗瓦这样的天才的杰出见解，都是类比作用的结果。当小蒂姆看到妇女像他的母亲一样照顾其他孩子时，他将 “妈妈” 这一概念概括为任何人的妈妈，而不仅仅是他的妈妈。这反过来又成为理解 “母船” 和 “大自然母亲” 等事物的跳板。爱因斯坦的 “最快乐的想法” 是重力和加速度之间的类比：如果你在电梯里，你无法分辨你的重量是由一个还是另一个造成的，因为它们的影响是一样的。我们在一个巨大的类比的海洋中游泳，我们既为自己的目的操纵它，也在不知不觉中被它操纵。书籍的每一页都有类比（如本节的标题，或前一节的标题）。《<i>哥德尔、埃舍尔、巴赫</i>》是哥德尔定理、埃舍尔的艺术和巴赫的音乐之间的一个扩展类比。如果 “算法大师” 不是类比，它肯定是类似的东西。</p>

<h1 id="babilu_link-436"><b>Rise and shine</b></h1>

<h1 id="babilu_link-436"><b>崛起与闪耀</b></h1>

<p class="noindent english">Cognitive science has seen a long-running debate between symbolists and analogizers. Symbolists point to something they can model that <a id="babilu_link-214"></a> analogizers can’t; then analogizers figure out how to do it, come up with something they can model that symbolists can’t, and the cycle repeats. Instance-based learning, as it’s sometimes called, is supposedly better for modeling how we remember specific episodes in our lives; rules are the putative choice for reasoning with abstract concepts like “work” and “love.” But when I was a graduate student, it struck me that these two are really just points on a continuum, and we should be able to learn across all of it. Rules are in effect generalized instances where we’ve “forgotten” some attributes because they didn’t matter. Conversely, instances are very specific rules, with a condition on every attribute. As we go through life, similar episodes gradually become abstracted into rule-based structures, like “eating at a restaurant.” You know that going to a restaurant involves ordering from a menu and leaving a tip, and you follow those “rules of conduct” every time you eat out, but you probably don’t remember the specific restaurants where you first became aware of them.</p>

<p class="noindent chinese">认知科学在象征者和类比者之间有一场长期的辩论。符号主义者指出他们可以建模的东西，而类比者不能；然后类比者想出如何做，想出他们可以建模的东西，而符号主义者不能，然后循环往复。基于实例的学习，有时被称为，据说更适合于模拟我们如何记住我们生活中的具体事件；规则是推理 “工作” 和 “爱情” 等抽象概念的假定选择。但当我还是个研究生的时候，我突然发现，这两者其实只是一个连续体上的点，而我们应该能够在所有的连续体上学习。规则实际上是泛化的实例，我们已经 “忘记” 了一些属性，因为它们并不重要。相反，实例是非常具体的规则，每个属性都有一个条件。在我们的生活中，类似的事件逐渐被抽象为基于规则的结构，比如 “在餐馆吃饭”。你知道去餐馆吃饭需要根据菜单点菜并留下小费，而且你每次出去吃饭都会遵守这些 “行为规则”，但你可能不记得你第一次意识到这些规则的具体餐馆。</p>

<p class="noindent english">In my PhD thesis, I designed an algorithm that unifies instance-based and rule-based learning in this way. A rule doesn’t just match entities that satisfy all its preconditions; it matches any entity that’s more similar to it than to any other rule, in the sense that it comes closer to satisfying its conditions. For instance, someone with a cholesterol level of 220 mg/dL comes closer than someone with 200 mg/dL to matching the rule <i>If your cholesterol is above 240 mg/dL, you’re at risk of a heart attack</i> . RISE, as I called the algorithm, learns by starting with each training example as a rule and then gradually generalizing each rule to absorb the nearest examples. The end result is usually a combination of very general rules, which between them match most examples, with more specific rules that match exceptions to those, and so on all the way to a “long tail” of specific memories. RISE made better predictions than the best rule-based and instance-based learners of the time, and my experiments showed that this was precisely because it combined the best features of both. Rules can be matched analogically, and so they’re no longer brittle. Instances can select different features in different regions of space and so combat the curse of dimensionality much better than nearest-neighbor, which can only select the same features everywhere.</p>

<p class="noindent chinese">在我的博士论文中，我设计了一种算法，以这种方式将基于实例和基于规则的学习统一起来。一个规则不只是匹配满足其所有前提条件的实体；它匹配任何与之更相似的实体，比其他规则更接近满足其条件。例如，一个胆固醇水平为 220 毫克/分升的人比一个胆固醇水平为 200 毫克/分升的人更接近于匹配规则<i>如果你的胆固醇高于 240 毫克/分升，你就有心脏病发作的风险</i>。我称该算法为 RISE，它的学习方式是将每个训练实例作为一个规则开始，然后逐渐泛化每个规则以吸收最近的实例。最终的结果通常是非常普遍的规则的组合，这些规则与大多数例子相匹配，更具体的规则与这些例子的例外情况相匹配，以此类推，一直到具体记忆的 “长尾”。RISE 比当时最好的基于规则和基于实例的学习者做出了更好的预测，而我的实验表明，这正是因为它结合了两者的最佳特征。规则可以被类比匹配，因此它们不再是脆性的。实例可以在空间的不同区域选择不同的特征，因此比最近的邻居更好地对抗维度的诅咒，因为最近的邻居只能在所有地方选择相同的特征。</p>

<p class="noindent english"><a id="babilu_link-305"></a> RISE was a step toward the Master Algorithm because it combined symbolic and analogical learning. It was only a small step, however, because it doesn’t have the full power of either of those paradigms, and it’s still missing the other three. RISE’s rules can’t be chained together in different ways; each rule just predicts the class of an example directly from its attributes. Also, the rules can’t talk about more than one entity at a time; for example, RISE can’t express a rule like <i>If A has the flu and B was in contact with A, B may have the flu as well</i> . On the analogical side, RISE just generalizes the simple nearest-neighbor algorithm; it can’t learn across domains using structure mapping or some such strategy. At the time I finished my PhD, I didn’t see a way to bring together in one algorithm the full power of all the five paradigms, and I set the problem aside for a while. But as I applied machine learning to problems like word-of-mouth marketing, data integration, programming by example, and website personalization, I kept seeing how each of the paradigms provided only part of the solution. There had to be a better way.</p>

<p class="noindent chinese">RISE 是向主算法迈出的一步，因为它结合了符号和模拟学习。然而，这只是一小步，因为它没有这两种范式的全部力量，而且它还缺少其他三种范式。RISE 的规则不能以不同的方式串联起来；每个规则只是直接从一个例子的属性来预测它的类别。另外，规则不能同时谈论一个以上的实体；例如，RISE 不能表达这样的规则：<i>如果 A 得了流感，B 与 A 接触过，B 也可能得了流感</i>。在类比方面，RISE 只是概括了简单的最近邻算法；它不能使用结构映射或一些类似的策略进行跨域学习。在我完成博士学位的时候，我还没有看到有什么方法可以将所有这五种范式的全部力量汇集到一个算法中，于是我将这个问题搁置了一段时间。但当我把机器学习应用于口碑营销、数据整合、实例编程和网站个性化等问题时，我不断看到每个范式都只提供了部分解决方案。一定会有更好的方法。</p>

<p class="noindent english">And so we have traveled through the territories of the five tribes, gathering their insights, negotiating the border crossings, wondering how the pieces might fit together. We know immensely more now than when we started out. But something is still missing. There’s a gaping hole in the center of the puzzle, making it hard to see the pattern. The problem is that all the learners we’ve seen so far need a teacher to tell them the right answer. They can’t learn to distinguish tumor cells from healthy ones unless someone labels them “tumor” or “healthy.” But humans can learn without a teacher; they do it from the day they’re born. Like Frodo at the gates of Mordor, our long journey will have been in vain if we don’t find a way around this barrier. But there is a path past the ramparts and the guards, and the prize is near. Follow me…</p>

<p class="noindent chinese">因此，我们在五个部落的领土上旅行，收集他们的见解，谈判边界过境点，想知道这些碎片如何拼凑起来。我们现在知道的东西比我们开始时多得多。但是仍然缺少一些东西。拼图的中心有一个空洞，使我们很难看到图案。问题是，到目前为止，我们所看到的所有学习者都需要老师来告诉他们正确的答案。他们无法学会区分肿瘤细胞和健康细胞，除非有人给它们贴上 “肿瘤” 或 “健康” 的标签。但是人类可以在没有老师的情况下学习；他们从出生那天起就开始学习了。就像在魔多之门的弗罗多一样，如果我们没有找到绕过这个障碍的方法，我们的漫长旅程将是徒劳的。但有一条路可以穿过城墙和守卫，而且奖品就在附近。跟着我继续… </p>

</section>

</div>

</div>

<div id="babilu_link-313">

<div>

<section id="babilu_link-5">

<h1><a id="babilu_link-232"></a> <a href="#babilu_link-314">CHAPTER EIGHT</a></h1>

<h1><a id="babilu_link-232"></a> <a href="#babilu_link-314">第八章</a></h1>

<h1><a href="#babilu_link-314">Learning Without a Teacher</a></h1>

<h1><a href="#babilu_link-314">没有老师的学习</a></h1>

<p class="noindent english">If you’re a parent, the entire mystery of learning unfolds before your eyes in the first three years of your child’s life. A newborn baby can’t talk, walk, recognize objects, or even understand that an object continues to exist when the baby isn’t looking at it. But month after month, in steps large and small, by trial and error and great conceptual leaps, the child figures out how the world works, how people behave, and how to communicate. By a child’s third birthday, all this learning has coalesced into a stable self, a stream of consciousness that will continue throughout life. Older children and adults can time-travel, aka remember things past, but only so far back. If we could revisit ourselves as infants and toddlers and see the world again through those newborn eyes, much of what puzzles us about learning—even about existence itself—would suddenly seem obvious. But as it is, the greatest mystery in the universe is not how it begins or ends, or what infinitesimal threads it’s woven from, it’s what goes on in a small child’s mind: how a pound of gray jelly can grow into the seat of consciousness.</p>

<p class="noindent chinese">如果你是父母，在孩子生命的前三年，整个学习的奥秘就在你眼前展开了。一个刚出生的婴儿不能说话，不能走路，不能识别物体，甚至不能理解一个物体在婴儿不看它时继续存在。但是，月复一月，在大大小小的步骤中，通过试验和错误以及巨大的概念飞跃，孩子弄清了世界是如何运作的，人们是如何行为的，以及如何沟通。到孩子三岁生日时，所有这些学习已经凝聚成一个稳定的自我，一个将持续一生的意识流。年长的儿童和成年人可以进行时间旅行，也就是记住过去的事情，但只能追溯到这么远。如果我们能重温自己的婴幼儿时期，通过那些新生儿的眼睛再次看到这个世界，那么许多让我们对学习 —— 甚至对存在本身 —— 感到困惑的东西就会突然变得很明显。但是现在，宇宙中最大的谜团不是它如何开始或结束，或者它是由什么无穷无尽的线编织而成的，而是在一个小孩子的头脑中发生了什么：一磅灰色的果冻如何能成长为意识的所在地。</p>

<p class="noindent english">The scientific study of children’s learning is still young, having begun in earnest only a few decades ago, but it has already come remarkably far. Infants can’t answer questionnaires or follow experimental <a id="babilu_link-306"></a> protocols, but we can infer a surprising amount about what goes on in their minds by videotaping and studying their reactions during experiments. A coherent picture emerges: an infant’s mind isn’t just the unfolding of a predefined genetic program or a biological device for recording correlations in sense data; rather, the infant’s mind actively synthesizes his or her reality, and this reality changes quite radically over time.</p>

<p class="noindent chinese">对儿童学习的科学研究还很年轻，几十年前才开始认真研究，但已经取得了显著的进展。婴儿不能回答问卷，也不能遵循实验但我们可以通过录像和研究他们在实验中的反应，推断出大量关于他们头脑中发生的事情。一幅连贯的画面出现了：婴儿的思想不仅仅是一个预定的遗传程序的展开，也不是一个记录感官数据相关性的生物装置；相反，婴儿的思想积极地综合了他或她的现实，并且这个现实随着时间的推移发生了相当大的变化。</p>

<p class="noindent english">Increasingly, and most relevant to us, cognitive scientists express their theories of children’s learning in the form of algorithms. Many machine-learning researchers take inspiration from this. Everything we need is right there in a child’s mind, if only we can somehow capture its essence in computer code. Some researchers even argue that the way to create intelligent machines is to build a robot baby and let him experience the world as a human baby does. We, the researchers, would be his parents (perhaps even with an assist from crowdsourcing, giving a whole new meaning to the term <i>global village</i>). Little Robby—let’s call him that, in honor of the chubby but much taller robot in <i>Forbidden Planet</i> —is the only robot baby we’ll ever have to build. Once he has learned everything a three-year-old knows, the AI problem is solved. We can copy the contents of his mind into as many other robots as we like, and they’ll take it from there, the hardest part already accomplished.</p>

<p class="noindent chinese">越来越多的，也是与我们最相关的，认知科学家以算法的形式表达他们关于儿童学习的理论。许多机器学习研究人员从中获得了灵感。我们所需要的一切就在孩子的头脑中，只要我们能以某种方式在计算机代码中捕获其本质。一些研究人员甚至认为，创造智能机器的方法是建造一个机器人婴儿，让他像人类婴儿那样体验世界。我们，研究人员，将成为他的父母（也许甚至可以通过众包来协助，为<i>地球村</i>这个词赋予全新的含义）。小罗比 —— 让我们这样称呼他，以纪念《<i>禁忌星球</i>》中那个胖乎乎但更高大的机器人 —— 是我们唯一需要建造的机器人宝宝。一旦他学会了一个三岁孩子的所有知识，人工智能问题就解决了。我们可以随心所欲地将他的思想内容复制到其他机器人身上，他们会从那里开始，最困难的部分已经完成。</p>

<p class="noindent english">The question, of course, is what algorithm should be running in Robby’s brain at birth. Researchers influenced by child psychology look askance at neural networks because the microscopic workings of a neuron seem a million miles from the sophistication of even a child’s most basic behaviors, like reaching for an object, grasping it, and inspecting it with wide, curious eyes. We need to model the child’s learning at a higher level of abstraction, lest we miss the planet for the trees. Above all, even though children certainly get plenty of help from their parents, they learn mostly on their own, without supervision, and that’s what seems most miraculous. None of the algorithms we’ve seen so far can do it, but we’re about to see several that can—bringing us one step closer to the Master Algorithm.</p>

<p class="noindent chinese">当然，问题是，罗比出生时应该在大脑中运行什么算法。受儿童心理学影响的研究人员对神经网络嗤之以鼻，因为一个神经元的微观运作似乎与儿童最基本的行为的复杂性相差甚远，比如伸手去拿一个物体，抓住它，用好奇的大眼睛去观察它。我们需要在更高的抽象水平上为儿童的学习建立模型，以免我们为了树木而错过了地球。最重要的是，尽管孩子们肯定从他们的父母那里得到了大量的帮助，但他们大多是在没有监督的情况下自己学习，这就是看起来最神奇的地方。到目前为止，我们所看到的算法没有一个能做到这一点，但我们即将看到几个能做到的算法 —— 使我们离主算法又近了一步。</p>

<h1 id="babilu_link-437"><b><a id="babilu_link-181"><b></b></a> Putting together birds of a feather</b></h1>

<h1 id="babilu_link-437"><b><a id="babilu_link-181"><b></b></a>团结一致的鸟儿</b></h1>

<p class="noindent english">We flip the “on” switch, and Robby’s video eyes open for the very first time. At once he’s flooded with what William James memorably called the “blooming, buzzing confusion” of the world. With new images streaming in at a rate of dozens per second, one of the first things he must do is learn to organize them into larger chunks. The real world is made up of objects that persist over time, not random pixels changing arbitrarily from one moment to the next. Mommy isn’t replaced by a smaller Mommy when she walks away. Putting a dish on the table doesn’t make a white hole in it. A young baby is not surprised if a teddy bear passes behind a screen and reemerges as an airplane, but a one-year-old is. Somehow, he’s figured out that teddy bears are different from airplanes and don’t spontaneously transmute. Soon afterward, he’ll figure out that some objects are more alike than others and start forming categories. Given a pile of toy horses and pencils to play with, a nine-month-old doesn’t think to sort them into separate piles of horses and pencils, but an eighteen-month-old does.</p>

<p class="noindent chinese">我们打开 “开关”，罗比的视频眼睛第一次打开。一下子，他就被威廉·詹姆斯所称的世界的 “绽放、嗡嗡作响的混乱” 所淹没。随着新图像以每秒几十幅的速度涌入，他必须做的第一件事就是学会将它们组织成更大的块状。真实的世界是由随着时间推移而持续存在的物体组成的，而不是从这一刻到下一刻任意变化的随机像素。当妈妈离开时，她并没有被一个更小的妈妈取代。把一个盘子放在桌子上并不会在上面形成一个白洞。如果一只泰迪熊从屏幕后面经过，再出现时是一架飞机，小婴儿不会感到惊讶，但一岁的孩子会感到惊讶。不知何故，他已经明白了泰迪熊与飞机不同，不会自发地发生转变。不久之后，他就会发现有些物体比其他物体更相似，并开始形成分类。给他一堆玩具马和铅笔玩，9 个月大的孩子不会想到把它们分成独立的马和铅笔堆，但 18 个月大的孩子会。</p>

<p class="noindent english">Organizing the world into objects and categories is second nature to an adult but not to an infant, and even less to Robby the robot. We could endow him with a visual cortex in the form of a multilayer perceptron and show him labeled examples of all the objects and categories in the world—here’s Mommy close up, here’s Mommy far away—but we’d never be done. What we need is an algorithm that will spontaneously group together similar objects, or different images of the same object. This is the problem of clustering, and it’s one of the most intensively studied in machine learning.</p>

<p class="noindent chinese">将世界组织成物体和类别对成年人来说是第二天性，但对婴儿来说不是，对机器人罗比来说更是如此。我们可以赋予他一个多层感知器形式的视觉皮层，并向他展示世界上所有物体和类别的标记例子 —— 这里是妈妈的近照，这里是妈妈的远照 —— 但我们永远也做不到。我们需要的是一种算法，它能自发地将相似的物体或同一物体的不同图像组合在一起。这就是聚类的问题，也是机器学习中研究最深入的问题之一。</p>

<p class="noindent english">A cluster is a set of similar entities, or at a minimum, a set of entities that are more similar to each other than to members of other clusters. It’s human nature to cluster things, and it’s often the first step on the road to knowledge. When we look up at the night sky, we can’t help seeing clusters of stars, and then we fancifully name them after shapes they resemble. Noticing that certain sets of elements had very similar chemical properties was the first step in discovering the periodic table. Each <a id="babilu_link-96"></a> of those sets is now a column in it. Everything we perceive is a cluster, from friends’ faces to speech sounds. Without them, we’d be lost: children can’t learn a language before they learn to identify the characteristic sounds it’s made of, which they do in their first year of life, and all the words they then learn mean nothing without the clusters of real things they refer to. Confronted with big data—a very large number of objects—our first recourse is to group them into a more manageable number of clusters. A whole market is too coarse, and individual customers are too fine, so marketers divide markets into segments, which is their word for clusters. Even objects themselves are at bottom clusters of their observations, from all the different angles light falls on Mommy’s face to all the different sound waves baby hears as the word <i>mommy</i> . And we can’t think without objects, which is perhaps why quantum mechanics is so unintuitive: we want to visualize the subatomic world as particles colliding, or waves interfering, but it’s not really either.</p>

<p class="noindent chinese">集群是一组相似的实体，或者至少是一组实体，它们之间的相似度高于其他集群的成员。对事物进行聚类是人类的天性，而且这往往是通往知识之路的第一步。当我们仰望夜空时，我们会情不自禁地看到星星的集群，然后我们幻想着用它们类似的形状来命名它们。注意到某些元素组具有非常相似的化学性质是发现周期表的第一步。这些元素集的每一个现在是其中的一列。我们所感知到的一切，从朋友的脸到说话的声音，都是一个集群。如果没有它们，我们就会迷失方向：孩子们在学会识别语言的特征声音之前是无法学习语言的，他们在出生后的第一年就学会了，而他们随后学到的所有单词，如果没有它们所指的真实事物的群集，就毫无意义。面对大数据 —— 非常多的物体 —— 我们首先要做的是把它们分成更容易管理的集群数量。整个市场太粗了，而单个客户又太细了，所以营销人员将市场划分为细分市场，也就是他们所说的聚类。即使是物体本身也是它们观察到的最基本的集群，从光线落在妈妈脸上的所有不同角度到宝宝听到的<i>妈妈</i>这个词的所有不同声波。没有物体我们就无法思考，这也许就是为什么量子力学如此不直观：我们想把亚原子世界想象成粒子碰撞或波的干扰，但其实两者都不是。</p>

<p class="noindent english">We can represent a cluster by its prototypical element: the image of your mother that you see with your mind’s eye or the quintessential cat, sports car, country house, or tropical beach. Peoria, Illinois, is the average American town, according to marketing lore. Bob Burns, a fifty-three-year-old building maintenance supervisor in Windham, Connecticut, is America’s most ordinary citizen—at least if you believe Kevin O’Keefe’s book <i>The Average American</i> . Anything described by numeric attributes—say, people’s heights, weights, girths, shoe sizes, hair lengths, and so on—makes it easy to compute the average member: his height is the average height of all the cluster members, his weight the average of all the weights, and so on. For categorical attributes, like gender, hair color, zip code, or favorite sport, the “average” is simply the most frequent value. The average member described by this set of attributes may or may not be a real person, but either way it’s a useful reference to have: if you’re brainstorming how to market a new product, picturing Peoria as the town where you’re launching it or Bob Burns as your target customer beats thinking of abstract entities like “the market” or “the consumer.”</p>

<p class="noindent chinese">我们可以通过原型元素来代表一个集群：你脑海中看到的你母亲的形象，或典型的猫、跑车、乡村别墅或热带海滩。根据市场营销的传说，伊利诺伊州的皮奥里亚是一个普通的美国城镇。鲍勃·伯恩斯，康涅狄格州温德姆市一位 53 岁的建筑维修主管，是美国最普通的公民 —— 至少如果你相信凯文·奥基夫的《<i>普通美国人</i>》一书。任何由数字属性描述的东西 —— 例如，人们的身高、体重、腰围、鞋码、头发长度等等 —— 都很容易计算出平均成员：他的身高是所有集群成员的平均身高，他的体重是所有体重的平均值，等等。对于分类属性，如性别、头发颜色、邮政编码或最喜欢的运动，“平均” 只是最常见的值。这组属性所描述的平均成员可能是也可能不是一个真实的人，但无论如何，它都是一个有用的参考：如果你在脑力激荡如何推销一个新产品，把皮奥里亚想象成你要推出产品的城镇，或把鲍勃·伯恩斯想象成你的目标客户，这比思考 “市场” 或 “消费者” 这样的抽象实体要好。</p>

<p class="noindent english"><a id="babilu_link-183"></a> As useful as such averages are, we can do even better; indeed the whole point of big data and machine learning is to avoid thinking at such a coarse level. Our clusters can be very specialized sets of people or even different aspects of the same person: Alice buying books for work, for leisure, or as Christmas presents; Alice in a good mood versus Alice with the blues. Amazon would like to distinguish the books Alice buys for herself from the ones she buys for her boyfriend, as this would allow it to make appropriate recommendations at appropriate times. Unfortunately, purchases don’t come labeled with “self-gift” or “for Bob,” and Amazon needs to figure out how to group them.</p>

<p class="noindent chinese">尽管这样的平均数很有用，但我们可以做得更好；事实上，大数据和机器学习的全部意义在于避免在这样一个粗略的层面上思考。我们的集群可以是非常专业的人的集合，甚至是同一个人的不同方面。爱丽丝为工作、休闲或作为圣诞礼物买书；心情好的爱丽丝与有忧郁症的爱丽丝。亚马逊希望区分爱丽丝为自己买的书和为男友买的书，因为这将使它能够在适当的时候做出适当的推荐。不幸的是，购买的东西并没有标明 “自己的礼物” 或 “给鲍勃的”，亚马逊需要想办法把它们分组。</p>

<p class="noindent english">Suppose the entities in Robby’s world fall into five clusters (people, furniture, toys, food, and animals), but we don’t know which things belong to which clusters. This is the type of problem that Robby faces when we switch him on. One simple option for sorting entities into clusters is to pick five random objects as the cluster prototypes and then compare each entity with each prototype and assign it to the most similar prototype’s cluster. (As in analogical learning, the choice of similarity measure is important. If the attributes are numeric, it can be as simple as Euclidean distance, but there are many other options.) We now need to update the prototypes. After all, a cluster’s prototype is supposed to be the average of its members, and although that was necessarily the case when each cluster had only one member, it generally won’t be after we have added a bunch of new members to each cluster. So for each cluster, we compute the average properties of its members and make that the new prototype. At this point, we need to update the cluster memberships again: since the prototypes have moved, the closest prototype to a given entity may also have changed. Let’s imagine the prototype of one category was a teddy bear and the prototype of another was a banana. Perhaps on our first run we grouped an animal cracker with the bear, but on the second we grouped it with the banana. An animal cracker initially looked like a toy, but now it looks more like food. Once I reclassify animal crackers in the banana group, perhaps the prototypical item for that group also changes, from a banana to a cookie. This virtuous <a id="babilu_link-287"></a> cycle, with entities assigned to better and better clusters, continues until the assignment of entities to clusters doesn’t change (and therefore neither do the cluster prototypes).</p>

<p class="noindent chinese">假设罗比世界中的实体分为五个群组（人、家具、玩具、食物和动物），但我们不知道哪些东西属于哪个群组。这就是罗比在我们开启他时面临的问题类型。将实体分类到集群中的一个简单选择是随机挑选五个物体作为集群原型，然后将每个实体与每个原型进行比较，并将其分配到最相似原型的集群中。（如同在类比学习中，相似度的选择很重要。如果属性是数字，它可以像欧氏距离那样简单，但也有许多其他选择）。我们现在需要更新原型。毕竟，一个聚类的原型应该是其成员的平均数，尽管在每个聚类只有一个成员的时候，情况必然如此，但在我们为每个聚类添加了一堆新成员之后，情况一般就不会如此。因此，对于每个集群，我们计算其成员的平均属性，并将其作为新的原型。在这一点上，我们需要再次更新集群成员：因为原型已经移动了，离某个实体最近的原型也可能已经改变了。让我们想象一下，一个类别的原型是一只泰迪熊，另一个类别的原型是一只香蕉。也许在我们第一次运行时，我们把动物饼干和熊放在一起，但在第二次运行时，我们把它和香蕉放在一起。动物饼干最初看起来像玩具，但现在它看起来更像食物。一旦我把动物饼干重新归入香蕉组，也许该组的原型物品也会改变，从香蕉变成饼干。这种良性的循环，实体被分配到越来越好的群组中，一直持续到实体被分配到群组中不会改变（因此群组原型也不会改变）。</p>

<p class="noindent english">This algorithm is called <i>k</i> -means, and its origins go back to the fifties. It’s nice and simple and quite popular, but it has several shortcomings, some of which are easier to solve than others. For one, we need to fix the number of clusters in advance, but in the real world, Robby is always running into new kinds of objects. One option is to let an object start a new cluster if it’s too different from the existing ones. Another is to allow clusters to split and merge as we go along. Either way, we probably want the algorithm to include a preference for fewer clusters, lest we wind up with each object as its own cluster (hard to beat if we want clusters to consist of similar objects, but clearly not the goal).</p>

<p class="noindent chinese">这种算法被称为 <i>K</i>-means，其起源可以追溯到 50 年代。它很好，很简单，而且相当流行，但它有几个缺点，其中一些比其他的更容易解决。其一，我们需要事先固定集群的数量，但在现实世界中，罗比总是会碰到新的种类的物体。一种选择是，如果一个物体与现有的物体有太大的不同，就让它开始一个新的集群。另一个办法是允许集群在我们前进的过程中分裂和合并。无论哪种方式，我们可能都希望算法包括对较少的集群的偏好，以免我们最终把每个对象作为它自己的集群（如果我们希望集群由类似的对象组成，那就很难做到，但显然不是目标）。</p>

<p class="noindent english">A bigger issue is that <i>k</i> -means only works if the clusters are easy to tell apart: each cluster is roughly a spherical blob in hyperspace, the blobs are far from each other, and they all have similar volumes and include a similar number of objects. If any of these fails, ugly things can happen: an elongated cluster is split into two different ones, a smaller cluster is absorbed into a larger one nearby, and so on. Luckily, there’s a better option.</p>

<p class="noindent chinese">更大的问题是，<i>K</i>-means 只有在集群容易区分的情况下才会起作用：每个集群大致是超空间中的一个球形球体，球体之间相距甚远，而且它们都有相似的体积，包括相似数量的物体。如果其中任何一项失败，丑陋的事情就会发生：一个拉长的星团被分割成两个不同的星团，一个较小的星团被吸收到附近一个较大的星团中，等等。幸运的是，有一个更好的选择。</p>

<p class="noindent english">Suppose we decide that letting Robby roam around in the real world is too slow and cumbersome a way to learn. Instead, like a would-be pilot learning in a flight simulator, we’ll have him look at computer-generated images. We know what clusters the images come from, but we’re not telling Robby. Instead, we create each image by first choosing a cluster at random (toys, say) and then synthesizing an example of that cluster (small, fluffy, brown teddy bear with big black eyes, round ears, and a bow tie). We also choose the properties of the example at random: the size comes from a normal distribution with a mean of ten inches, the fur is brown with 80 percent probability and white otherwise, and so on. After Robby has seen lots of images generated in this way, he should have learned to cluster them into people, furniture, toys, and so on, because people are more like people than furniture and so on. But the interesting question is: If we look at it from Robby’s point of view, <a id="babilu_link-177"></a> what’s the best algorithm to discover the clusters? The answer is surprising: Naïve Bayes, which we first met as an algorithm for supervised learning. The difference is that now Robby doesn’t know the classes, so he’ll have to guess them!</p>

<p class="noindent chinese">假设我们决定让罗比在现实世界中漫游，这种学习方式太慢，太麻烦了。相反，就像未来的飞行员在飞行模拟器中学习一样，我们会让他看电脑生成的图像。我们知道这些图像来自什么群组，但我们不会告诉罗比。相反，我们在创建每个图像时，首先随机选择一个群组（比如说玩具），然后合成该群组的一个例子（小的、蓬松的、棕色的泰迪熊，有大黑眼、圆耳朵和领结）。我们还随机选择了这个例子的属性：尺寸来自一个平均为 10 英寸的正态分布，毛发是棕色的，有 80% 的概率是白色的，等等。在罗比看过很多以这种方式生成的图像后，他应该已经学会了将这些图像聚类为人、家具、玩具等，因为人比家具等更像人。但有趣的问题是：如果我们从罗比的角度来看，什么是发现聚类的最佳算法？答案是令人惊讶的。天真贝叶斯，我们第一次见到它是作为监督学习的一种算法。不同的是，现在罗比不知道这些类，所以他必须猜测它们！</p>

<p class="noindent english">Clearly, if Robby did know them, it would be smooth sailing: as in Naïve Bayes, each cluster would be defined by its probability (17 percent of the objects generated were toys), and by the probability distribution of each attribute among the cluster’s members (for example, 80 percent of the toys are brown). Robby could estimate these probabilities just by counting the number of toys in the data, the number of brown toys, and so on. But in order to do that, we would need to know which objects are toys. This seems like a tough nut to crack, but it turns out we already know how to do it as well. If Robby has a Naïve Bayes classifier and needs to figure out the class of a new object, all he needs to do is apply the classifier and compute the probability of each class given the object’s attributes. (Small, fluffy, brown, bear-like, with big eyes, and a bow tie? Probably a toy but possibly an animal.)</p>

<p class="noindent chinese">显然，如果罗比真的知道它们，就会一帆风顺：就像在天真贝叶斯中一样，每个聚类将由其概率（生成的物体中 17% 是玩具）和聚类成员中每个属性的概率分布来定义（例如，80% 的玩具是棕色的）。罗比可以仅仅通过计算数据中玩具的数量、棕色玩具的数量等来估计这些概率。但为了做到这一点，我们需要知道哪些物体是玩具。这似乎是一个难以破解的难题，但事实证明，我们也已经知道如何做到这一点。如果罗比有一个天真贝叶斯分类器，并需要弄清楚一个新物体的类别，他所需要做的就是应用分类器，并根据物体的属性计算每个类别的概率。小巧、毛茸茸、棕色、像熊、有大眼睛、打着领结？可能是一个玩具，但也可能是一种动物）。</p>

<p class="noindent english">So Robby is faced with a chicken-and-egg problem: if he knew the objects’ classes, he could learn the classes’ models by counting, and if he knew the models, he could infer the objects’ classes. We seem to be stuck again, but far from it: just start by guessing a class for each object any way you want—even at random—and you’re off to the races. From those classes and the data, you can learn the class models; based on these models you can reinfer the classes and so on. At first sight this looks like a crazy scheme: it may never finish, circling forever between inferring the classes from the models and the models from the classes, and even if it does finish, there’s no reason to believe it will settle on meaningful clusters. But in 1977 a trio of Harvard statisticians (Arthur Dempster, Nan Laird, and Donald Rubin) showed that the crazy scheme actually works: every time we go around the loop, the cluster model gets better, and the loop ends when the model is a local maximum of the likelihood. They called this scheme the EM algorithm, where the E stands for expectation (inferring the expected probabilities) and the M for maximization (estimating the maximum-likelihood parameters). They <a id="babilu_link-182"></a> also showed that many previous algorithms were special cases of EM. For example, to learn hidden Markov models, we alternate between inferring the hidden states and estimating the transition and observation probabilities based on them. Whenever we want to learn a statistical model but are missing some crucial information (e.g., the classes of the examples), we can use EM. This makes it one of the most popular algorithms in all of machine learning.</p>

<p class="noindent chinese">因此，罗比面临着一个鸡生蛋蛋生鸡的问题：如果他知道物体的类别，他可以通过计数来学习类别的模型，如果他知道模型，他可以推断出物体的类别。我们似乎又被卡住了，但远非如此：只要一开始就以任何方式为每个物体猜测一个类别 —— 甚至是随机的 —— 你就可以开始行动了。从这些类和数据中，你可以学习类的模型；基于这些模型，你可以重新推断类，如此反复。乍一看，这似乎是一个疯狂的计划：它可能永远不会完成，永远在从模型推断出类和从类推断出模型之间徘徊，即使它完成了，也没有理由相信它会在有意义的群组上定居。但在 1977 年，哈佛大学的三位统计学家（阿瑟·邓普斯特、南·莱尔德和唐纳德·鲁宾）表明，这个疯狂的方案实际上是有效的：每当我们绕过这个循环，集群模型就会变得更好，当模型是可能性的局部最大值时，这个循环就会结束。他们把这个方案称为 EM 算法，其中 E 代表期望（推断期望概率），M 代表最大化（估计最大似然参数）。他们还表明许多以前的算法是 EM 的特例。例如，为了学习隐马尔科夫模型，我们在推断隐性状态和根据隐性状态估计过渡和观察概率之间交替进行。每当我们想学习一个统计模型，但缺少一些关键信息（如例子的类别），我们可以使用 EM。这使得它成为所有机器学习中最受欢迎的算法之一。</p>

<p class="noindent english">You might have noticed a certain resemblance between <i>k</i> -means and EM, in that they both alternate between assigning entities to clusters and updating the clusters’ descriptions. This is not an accident: <i>k</i> -means itself is a special case of EM, which you get when all the attributes have “narrow” normal distributions, that is, normal distributions with very small variance. When clusters overlap a lot, an entity could belong to, say, cluster A with a probability of 0.7 and cluster B with a probability of 0.3, and we can’t just decide that it belongs to cluster A without losing information. EM takes this into account by fractionally assigning the entity to the two clusters and updating their descriptions accordingly. If the distributions are very concentrated, however, the probability that an entity belongs to the nearest cluster is always approximately 1, and all we have to do is assign entities to clusters and average the entities in each cluster to obtain its mean, which is just the <i>k</i> -means algorithm.</p>

<p class="noindent chinese">你可能已经注意到 <i>k</i>-means 和 EM 之间有某种相似之处，因为它们都是在将实体分配到聚类和更新聚类的描述之间交替进行。这不是偶然的：<i>K</i>-means 本身就是 EM 的一个特例，当所有的属性都具有 “窄” 正态分布时，也就是具有非常小的方差的正态分布时，你会得到 EM。当聚类大量重叠时，一个实体可能属于，例如，聚类 A 的概率为 0.7，聚类 B 的概率为 0.3，我们不能只决定它属于聚类 A 而不丢失信息。EM 考虑到这一点，将实体分给两个聚类，并相应地更新其描述。然而，如果分布非常集中，一个实体属于最近的聚类的概率总是近似于 1，我们所要做的就是将实体分配到聚类中，并对每个聚类中的实体进行平均，以获得其平均值，这只是 <i>k</i>-means 算法。</p>

<p class="noindent english">So far we’ve only seen how to learn one level of clusters, but the world is, of course, much richer than that, with clusters within clusters all the way down to individual objects: living things cluster into plants and animals, animals into mammals, birds, fishes, and so on, all the way down to Fido the family dog. No problem: once we’ve learned one set of clusters, we can treat them as objects and cluster them in turn, and so on up to the cluster of all things. Alternatively, we can start with a coarse clustering and then further divide each cluster into subclusters: Robby’s toys divide into stuffed animals, constructions toys, and so on; stuffed animals into teddy bears, plush kittens, and so on. Children seem to start out in the middle and then work their way up and down. For example, they learn <i>dog</i> before they learn <i>animal</i> or <i>beagle</i> . This might be a good strategy for Robby, as well.</p>

<p class="noindent chinese">到目前为止，我们只看到了如何学习一个层次的聚类，但世界当然要比这丰富得多，聚类中的聚类一直到单个物体：生物聚类为植物和动物，动物聚类为哺乳动物、鸟类、鱼类，等等，一直到家狗 Fido。没问题：一旦我们学会了一组聚类，我们就可以把它们当作物体，并依次对它们进行聚类，以此类推，直到所有事物的聚类。另外，我们可以从一个粗略的聚类开始，然后进一步将每个聚类分为子聚类。罗比的玩具分为毛绒动物、建筑玩具等；毛绒动物分为泰迪熊、毛绒小猫等。孩子们似乎都是从中间开始的，然后再向上和向下努力。例如，他们在学习<i>动物</i>或<i>小猎犬</i>之前先学习 <i>狗</i>。这对罗比来说可能也是一个好策略。</p>

<h1 id="babilu_link-438"><b><a id="babilu_link-50"><b></b></a> Discovering the shape of the data</b></h1>

<h1 id="babilu_link-438"><b><a id="babilu_link-50"><b></b></a>发现数据的形状</b></h1>

<p class="noindent english">Whether it’s data pouring into Robby’s brain through his senses or the click streams of millions of Amazon customers, grouping a large number of entities into a smaller number of clusters is only half the battle. The other half is shortening the description of each entity. The very first picture of Mom that Robby sees comprises perhaps a million pixels, each with its own color, but you hardly need a million variables to describe a face. Likewise, each thing you click on at Amazon provides an atom of information about you, but what Amazon would really like to know is your likes and dislikes, not your clicks. The former, which are fairly stable, are somehow immanent in the latter, which grow without limit as you use the site. Little by little, all those clicks should add up to a picture of your taste, in the same way that all those pixels add up to a picture of your face. The question is how to do the adding.</p>

<p class="noindent chinese">无论是通过感官涌入罗比大脑的数据，还是数以百万计的亚马逊客户的点击流，将大量的实体分组为较少的集群，只是战斗的一半。另一半是缩短每个实体的描述。罗比看到的妈妈的第一张照片可能包括一百万个像素，每个像素都有自己的颜色，但你几乎不需要一百万个变量来描述一张脸。同样，你在亚马逊上点击的每件东西都提供了一个关于你的信息原子，但亚马逊真正想知道的是你的喜好和厌恶，而不是你的点击次数。前者是相当稳定的，在某种程度上是后者的一部分，而后者随着你对网站的使用而无限制地增长。渐渐地，所有这些点击都应该累积成你的品味，就像所有这些像素累积成你的脸一样。问题是如何进行添加。</p>

<p class="noindent english">A face has only about fifty muscles, so fifty numbers should suffice to describe all possible expressions, with plenty of room to spare. The shape of the eyes, nose, mouth, and so on—the features that let you tell one person from another—shouldn’t take more than a few dozen numbers, either. After all, with only ten choices for each facial feature, a police artist can put together a sketch of a suspect that’s good enough to recognize him. You can add a few more numbers to specify lighting and pose, but that’s about it. So if you give me a hundred numbers or so, that should be enough to re-create a picture of a face. Conversely, Robby’s brain should be able to take in a picture of a face and quickly reduce it to the hundred numbers that really matter.</p>

<p class="noindent chinese">一张脸只有大约 50 块肌肉，所以 50 个数字应该足以描述所有可能的表情，而且还有足够的空间。眼睛、鼻子、嘴巴等的形状 —— 让你区分一个人和另一个人的特征 —— 也不需要超过几十个数字。毕竟，每个面部特征只有 10 个选择，警察艺术家就能画出一张足以识别嫌疑人的素描。你可以多加几个数字来指定灯光和姿势，但也就这样了。因此，如果你给我一百个左右的数字，这应该足以重新创造一张脸的照片。相反，罗比的大脑应该能够接受一张脸的照片，并迅速将其还原为真正重要的一百个数字。</p>

<p class="noindent english">Machine learners call this process dimensionality reduction because it reduces a large number of visible dimensions (the pixels) to a few implicit ones (expression, facial features). Dimensionality reduction is essential for coping with big data—like the data coming in through your senses every second. A picture may be worth a thousand words, but it’s also a million times more costly to process and remember. Yet somehow your visual cortex does a pretty good job of whittling it down to a manageable amount of information, enough to navigate the world, <a id="babilu_link-262"></a> recognize people and things, and remember what you saw. It’s one of the great miracles of cognition and so natural you’re not even conscious of doing it.</p>

<p class="noindent chinese">机器学习者称这一过程为降维，因为它将大量的可见维度（像素）减少到少数隐含维度（表情、面部特征）。降维对于处理大数据 —— 比如每秒钟通过你的感官传来的数据 —— 至关重要。一张图片可能胜过千言万语，但它的处理和记忆成本也要高出一百万倍。然而，不知何故，你的视觉皮层能很好地将其缩减为可管理的信息量，足以浏览世界，识别人和物，并记住你看到的东西。这是认知的伟大奇迹之一，而且是如此自然，你甚至没有意识到它的存在。</p>

<p class="noindent english">When you arrange books on a shelf so that books on similar topics are close to each other, you’re doing a kind of dimensionality reduction, from the vast space of topics to the one-dimensional shelf. Unavoidably, some books that are closely related will wind up far apart on the shelf, but you can still order them in a way that minimizes such occurrences. That’s what dimensionality reduction algorithms do.</p>

<p class="noindent chinese">当你把书放在书架上，使类似主题的书相互靠近时，你就是在做一种降维，从庞大的主题空间到一维的书架。不可避免的是，一些密切相关的书籍会在书架上相距甚远，但你仍然可以以一种最小化这种情况发生的方式来排列它们。这就是降维算法的作用。</p>

<p class="noindent english">Suppose I give you the GPS coordinates of all the shops in Palo Alto, California, and you plot a few of them on a piece of paper:</p>

<p class="noindent chinese">假设我给你加利福尼亚州帕洛阿尔托市所有商店的 GPS 坐标，你在一张纸上画出其中的几个。</p>

<div>

<div>

<img alt="image" src="images/000007.jpg"/>

</div>

</div>

<p class="noindent english">You can probably tell just by looking at this plot that the main street in Palo Alto runs southwest–northeast. You didn’t draw a street, but you can intuit that it’s there from the fact that all the points fall along a straight line (or close to it—they can be on different sides of the street). Indeed, the street is University Avenue, and if you want to shop or eat out in Palo Alto, that’s the place to go. As a bonus, once you know that the shops are on University Avenue, you don’t need two numbers to locate them, just one: the street number (or, if you wanted to be really precise, the distance from the shop to the Caltrain station, on the southwest corner, which is where University Avenue begins).</p>

<p class="noindent chinese">光看这个图，你大概就能知道，帕洛阿尔托的主要街道是西南·东北走向。你没有画出一条街道，但你可以从所有的点都沿着一条直线落下（或接近直线 —— 它们可以在街道的不同侧面）这一事实中直觉到它的存在。事实上，这条街道就是大学路，如果你想在帕洛阿尔托购物或吃饭，那就是你要去的地方。作为奖励，一旦你知道商店在大学大道上，你就不需要两个数字来定位它们，只需要一个：街道号码（或者，如果你想真正精确，从商店到加州火车站的距离，在西南角，那是大学大道的起点）。</p>

<p class="noindent english">If you plot more shops, you’ll probably notice that some are on cross streets, a little bit off University Avenue, and a few are elsewhere entirely:</p>

<p class="noindent chinese">如果你绘制更多的商店，你可能会注意到，有些是在十字街头，有些是在大学路边，还有一些完全在其他地方。</p>

<div>

<div>

<img alt="image" src="images/000001.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-439"></a> Nevertheless, it’s still the case that most shops are pretty close to University Avenue, and if you were allowed only one number to locate a shop, its distance from the Caltrain station along the avenue would be a pretty good choice: after walking that distance, looking around is probably enough to find the shop. So you’ve just reduced the dimensionality of “shop locations in Palo Alto” from two to one.</p>

<p class="noindent chinese">尽管如此，大多数商店仍然离大学路很近，如果只允许你用一个数字来定位一家商店，那么它与沿街的加州火车站的距离将是一个相当不错的选择：走完这个距离后，四处看看可能就能找到这家商店。所以你刚刚把 “帕洛阿尔托的商店位置” 的维度从两个减少到一个。</p>

<p class="noindent english">Robby doesn’t have the benefit of your highly evolved visual system, though, so if you want him to go fetch your dry cleaning from Elite Cleaners and you only allow his map of Palo Alto to have one coordinate, he needs an algorithm to “discover” University Avenue from the GPS coordinates of the shops. The key to this is to notice that, if you put the origin of the <i>x,y</i> plane at the average of the shops’ locations and slowly rotate the axes, the shops are closest to the <i>x</i> axis when you’ve turned it by about 60 degrees, that is, when it lines up with University Avenue:</p>

<p class="noindent chinese">不过，罗比没有你的高度进化的视觉系统，所以如果你想让他去 Elite Cleaners 拿你的干洗衣服，而你只允许他的帕洛阿尔托地图有一个坐标，他需要一个算法来从商店的 GPS 坐标中 “发现” 大学大道。这方面的关键是注意到，如果你把<i>x,y</i>平面的原点放在商店位置的平均值上，然后慢慢旋转轴，当你把轴转了大约 60 度时，商店离 <i>x</i> 轴最近，也就是说，当它与大学大道对齐时。</p>

<div>

<div>

<img alt="image" src="images/000028.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-219"></a> This direction—known as the first principal component of the data—is also the direction along which the spread of the data is greatest. (Notice how, if you project the shops onto the <i>x</i> axis, they’re farther apart in the right figure than in the left one.) After you’ve found the first principal component, you can look for the second one, which in this case is the direction of greatest variation at right angles to University Avenue. On a map, there’s only one possible direction left (the direction of the cross streets). But if Palo Alto was on a hillside, one or both of the two first principal components would be partly uphill, and the third and last one would be up into the air. We can apply the same idea to data in thousands or millions of dimensions, like face images, successively looking for the directions of greatest variation until the remaining variability is small, at which point we can stop. For example, after rotating the axes in the figure above, most shops have <i>y</i> = 0, so the average <i>y</i> is very small, and we don’t lose too much information by ignoring the <i>y</i> coordinate altogether. And if we decide to keep <i>y</i> , surely <i>z</i> (up into the air) is insignificant. As it turns out, the whole process of finding the principal components can all be accomplished in one shot with a bit of linear algebra. Best of all, a few dimensions often account for the bulk of the variation in even very high-dimensional data. Even if that’s not the case, eyeballing the data in the top two or three dimensions often yields a lot of insight because it takes advantage of your visual system’s amazing powers of perception.</p>

<p class="noindent chinese">这个方向被称为数据的第一主成分，也是数据分布最大的方向。（注意，如果你把商店投射到 <i>X</i> 轴上，右图中的商店比左图中的商店相距更远）。找到第一个主成分后，你可以寻找第二个主成分，在这种情况下，它是与大学路成直角的最大变化方向。在地图上，只剩下一个可能的方向（十字街的方向）。但如果帕洛阿尔托是在山坡上，那么两个第一主成分中的一个或两个都会有一部分是上坡，而第三个也就是最后一个主成分则是上到空中。我们可以将同样的想法应用于数千或数百万维度的数据，比如人脸图像，依次寻找变化最大的方向，直到剩余的变异性很小，这时我们可以停止。例如，在旋转上图中的坐标轴后，大多数商店的 <i>y</i> = 0，所以平均 <i>y</i> 非常小，我们完全忽略 <i>y</i> 坐标也不会损失太多的信息。而且，如果我们决定保留 <i>y</i>，肯定 <i>z</i>（上升到空中）是无足轻重的。事实证明，寻找主成分的整个过程都可以通过一点线性代数一次性完成。最重要的是，即使是非常高维的数据，几个维度也往往占了大部分的变化。即使情况并非如此，用眼睛观察前两三个维度的数据也往往能获得很多启示，因为它利用了你的视觉系统的惊人感知能力。</p>

<p class="noindent english">Principal-component analysis (PCA), as this process is known, is one of the key tools in the scientist’s toolkit. You could say PCA is to unsupervised learning what linear regression is to the supervised variety. The famous hockey-stick curve of global warming, for example, is the result of finding the principal component of various temperature-related data series (tree rings, ice cores, etc.) and assuming it’s the temperature. Biologists use PCA to summarize the expression levels of thousands of different genes into a few pathways. Psychologists have found that personality boils down to five dimensions—extroversion, agreeableness, conscientiousness, neuroticism, and openness to experience—which they can infer from your tweets and blog posts. (Chimps <a id="babilu_link-184"></a> supposedly have one more dimension—reactivity—but Twitter data for them is not available.) Applying PCA to congressional votes and poll data shows that, contrary to popular belief, politics is not mainly about liberals versus conservatives. Rather, people differ along two main dimensions: one for economic issues and one for social ones. Collapsing these into a single axis mixes together populists and libertarians, who are polar opposites, and creates the illusion of lots of moderates in the middle. Trying to appeal to them is an unlikely winning strategy. On the other hand, if liberals and libertarians overcame their mutual aversion, they could ally themselves on social issues, where both favor individual freedom.</p>

<p class="noindent chinese">主成分分析（PCA），正如人们所知道的那样，是科学家工具箱中的关键工具之一。你可以说 PCA 对无监督学习的作用就像线性回归对有监督学习的作用。例如，著名的全球变暖曲棍球曲线就是找到各种温度相关数据系列（树环、冰芯等）的主成分并假设它是温度的结果。生物学家用 PCA 将数千个不同基因的表达水平概括为几个路径。心理学家发现，人格可以归结为五个维度 —— 外向性、合群性、自觉性、神经质和经验开放性 —— 他们可以从你的推特和博客文章中推断出这些。（黑猩猩据说还有一个维度 —— 反应性，但它们的推特数据不可用）。将 PCA 应用于国会投票和民意调查数据显示，与流行的看法相反，政治主要不是关于自由派与保守派。相反，人们在两个主要方面存在差异：一个是经济问题，另一个是社会问题。把这些问题归结为一个轴心，就会把民粹主义者和自由主义者混为一谈，他们是两极对立的，并造成中间有很多温和派的错觉。试图吸引他们是一个不太可能的获胜策略。另一方面，如果自由主义者和自由主义者克服了他们的相互厌恶，他们可以在社会问题上结盟，双方都赞成个人自由。</p>

<p class="noindent english">When he grows up, Robby can use a variant of PCA to solve the “cocktail party” problem, which is to pick out individual voices from the babble of the crowd. A related method can help him learn to read. If each word is a dimension, then a text is a point in the space of words, and the main directions of that space turn out to be elements of meaning. For example, <i>President Obama</i> and <i>the White House</i> are far apart in word space but close together in meaning space, because they tend to appear in similar contexts. Believe it or not, this type of analysis is all it takes for computers to grade SAT essays as well as humans do. Netflix uses a similar idea. Instead of just recommending movies that users with similar tastes liked, it first projects both users and movies into a lower-dimensional “taste space” and recommends a movie if it’s close to you in this space. That way it can find movies for you that you never knew you’d love.</p>

<p class="noindent chinese">当他长大后，罗比可以用 PCA 的一个变种来解决 “鸡尾酒会” 的问题，即从人群的胡言乱语中挑出个别声音。一个相关的方法可以帮助他学习阅读。如果每个词都是一个维度，那么一个文本就是词的空间中的一个点，而这个空间的主要方向变成了意义的元素。例如，<i>奥巴马总统</i>和<i>白宫</i>在单词空间中相距甚远，但在意义空间中却相距甚近，因为它们往往出现在相似的语境中。信不信由你，这种类型的分析是计算机对 SAT 论文进行评分的全部条件，就像人类一样。Netflix 也采用了类似的想法。它不只是推荐口味相似的用户喜欢的电影，而是首先将用户和电影都投射到一个较低维度的 “口味空间” 中，如果电影在这个空间中与你接近，它就会推荐你。这样，它就能为你找到你从来不知道你会喜欢的电影。</p>

<p class="noindent english">You’d probably be disappointed if you looked at the principal components of a face data set, though. They’re not what you’d expect, such as facial expressions or features, but more like ghostly faces, blurred beyond recognition. This is because PCA is a linear algorithm, and so all that the principal components can be is weighted pixel-by-pixel averages of real faces. (Also known as eigenfaces because they’re eigenvectors of the centered covariance matrix of the data—but I digress.) To really understand faces, and most shapes in the world, we need something else: nonlinear dimensionality reduction.</p>

<p class="noindent chinese">不过，如果你看一下人脸数据集的主成分，你可能会感到失望。它们不是你所期望的那样，比如面部表情或特征，而更像是幽灵般的面孔，模糊得无法辨认。这是因为 PCA 是一种线性算法，所以主成分只能是真实面孔的逐像素加权平均数。（也被称为特征脸，因为它们是数据的中心协方差矩阵的特征向量 —— 但我想说的是。）为了真正理解人脸，以及世界上大多数形状，我们需要其他东西：非线性降维。</p>

<p class="noindent english"><a id="babilu_link-263"></a> Suppose we zoom out from Palo Alto, and I give you the GPS coordinates of the main cities in the Bay Area:</p>

<p class="noindent chinese">假设我们从帕洛阿尔托放大，我给你湾区主要城市的 GPS 坐标。</p>

<div>

<div>

<img alt="image" src="images/000020.jpg"/>

</div>

</div>

<p class="noindent english">Again, you can probably surmise just by looking at this plot that the cities are on a bay, and if you draw a line running through them, you can locate each city using just one number: how far it is from San Francisco along that line. But PCA can’t find this curve; instead, it draws a straight line running down the middle of the bay, where there are no cities at all. Far from elucidating the shape of the data, PCA obscures it.</p>

<p class="noindent chinese">同样，你可以通过观察这幅图推测出，这些城市都在一个海湾上，如果你画一条贯穿它们的线，你可以只用一个数字来定位每个城市：沿着这条线它离旧金山有多远。但是 PCA 无法找到这条曲线；相反，它在海湾的中间画了一条直线，那里根本就没有城市。PCA 不仅没有阐明数据的形状，反而掩盖了它。</p>

<p class="noindent english">Instead, imagine for a moment that we’re going to develop the Bay Area from scratch. We’ve decided where each city will be located, and our budget allows us to build a single road connecting them. Naturally, we lay down a road that goes from San Francisco to San Bruno, from there to San Mateo, and so on all the way to Oakland. This road is a pretty good one-dimensional representation of the Bay Area and can be found by a simple algorithm: build a road between each pair of nearby cities. Of course, in general this will result in a network of roads, not a single road running by every city. But we can force the latter by building the single road that best approximates the network, in the sense that the distances between cities along this road are as close as possible to the distances along the network.</p>

<p class="noindent chinese">相反，想象一下，我们要从头开始开发湾区。我们已经决定了每个城市的位置，而且我们的预算允许我们建造一条连接它们的道路。自然，我们铺设一条道路，从旧金山到圣布鲁诺，从那里到圣马特奥，以此类推，一直到奥克兰。这条路是对湾区相当好的一维表示，可以通过一个简单的算法找到：在每一对附近的城市之间建立一条路。当然，在一般情况下，这将导致一个道路网络，而不是一条贯穿每个城市的道路。但我们可以通过建立最接近网络的单一道路来迫使后者出现，即沿这条道路的城市之间的距离尽可能接近网络的距离。</p>

<p class="noindent english"><a id="babilu_link-284"></a> One of the most popular algorithms for nonlinear dimensionality reduction, called Isomap, does just this. It connects each data point in a high-dimensional space (a face, say) to all nearby points (very similar faces), computes the shortest distances between all pairs of points along the resulting network and finds the reduced coordinates that best approximate these distances. In contrast to PCA, faces’ coordinates in this space are often quite meaningful: one may represent which direction the face is facing (left profile, three quarters, head on, etc.); another how the face looks (very sad, a little sad, neutral, happy, very happy, etc.); and so on. From understanding motion in video to detecting emotion in speech, Isomap has a surprising ability to zero in on the most important dimensions of complex data.</p>

<p class="noindent chinese">最流行的非线性降维算法之一，称为 Isomap，就是这样做的。它将高维空间中的每个数据点（比如说一张脸）与所有附近的点（非常相似的脸）连接起来，计算出所有成对的点之间的最短距离，并找到最接近这些距离的缩小的坐标。与 PCA 相比，人脸在这个空间中的坐标往往是相当有意义的：一个可能代表人脸朝向哪个方向（左侧轮廓、四分之三、正面，等等）；另一个代表人脸的样子（非常悲伤、有点悲伤、中性、高兴、非常高兴，等等）；等等。从理解视频中的运动到检测语音中的情感，Isomap 有一种令人惊讶的能力，可以将复杂数据中最重要的维度归零。</p>

<p class="noindent english">Here’s an interesting experiment. Take the video stream from Robby’s eyes, treat each frame as a point in the space of images, and reduce that set of images to a single dimension. What will you discover? Time. Like a librarian arranging books on a shelf, time places each image next to its most similar ones. Perhaps our perception of it is just a natural result of our brains’ dimensionality reduction prowess. In the road network of memory, time is the main thoroughfare, and we soon find it. Time, in other words, is the principal component of memory.</p>

<p class="noindent chinese">这里有一个有趣的实验。从罗比的眼睛中获取视频流，将每一帧视为图像空间中的一个点，并将这组图像还原为一个单一维度。你会发现什么？时间。就像图书管理员在书架上摆放书籍一样，时间将每个图像放在最相似的图像旁边。也许我们对它的感知只是我们大脑降维能力的一个自然结果。在记忆的道路网络中，时间是主要的大道，我们很快就能找到它。换句话说，时间是记忆的主要组成部分。</p>

<h1 id="babilu_link-440"><b>The hedonistic robot</b></h1>

<h1 id="babilu_link-440"><b>享乐主义的机器人</b></h1>

<p class="noindent english">Clustering and dimensionality reduction get us closer to human learning, but there’s still something very important missing. Children don’t just passively observe the world; they do things. They pick up objects they see, play with them, run around, eat, cry, and ask questions. Even the most advanced visual system is of no use to Robby if it doesn’t help him interact with the environment. Robby needs to know not just what’s where but what to do at each moment. In principle we could teach him using step-by-step instructions, pairing sensor readings with the appropriate actions to take in response, but this is viable only for narrow tasks. The actions you take depend on your goals, not just whatever <a id="babilu_link-238"></a> you are currently perceiving, and those goals can be far in the future. Step-by-step supervision shouldn’t be needed, in any case. Parents don’t teach their children to crawl, walk, or run; they figure it out on their own. But none of the learning algorithms we’ve seen so far can do this.</p>

<p class="noindent chinese">聚类和降维使我们更接近人类的学习，但仍然缺少非常重要的东西。儿童不只是被动地观察世界；他们会做事。他们拿起他们看到的物体，和它们一起玩，到处跑，吃东西，哭，问问题。如果不能帮助罗比与环境互动，即使最先进的视觉系统对罗比也没有用。罗比不仅需要知道什么在哪里，而且需要知道在每个时刻该做什么。原则上，我们可以用分步指示来教他，将传感器的读数与相应的行动配对起来，但这只对狭窄的任务是可行的。你所采取的行动取决于你的目标，而不仅仅是你目前感知到的任何而且这些目标可能是未来的。在任何情况下，都不应该需要一步步的监督。父母不会教他们的孩子爬行、走路或跑步；他们自己会想出办法。但是到目前为止，我们所看到的学习算法没有一个能做到这一点。</p>

<p class="noindent english">Humans do have one constant guide: their emotions. We seek pleasure and avoid pain. When you touch a hot stove, you instinctively recoil. That’s the easy part. The hard part is learning not to touch the stove in the first place. That requires moving to avoid a sharp pain that you have not yet felt. Your brain does this by associating the pain not just with the moment you touch the stove, but with the actions leading up to it. Edward Thorndike called this the law of effect: actions that lead to pleasure are more likely to be repeated in the future; actions that lead to pain, less so. Pleasure travels back through time, so to speak, and actions can eventually become associated with effects that are quite remote from them. Humans can do this kind of long-range reward seeking better than any other animal, and it’s crucial to our success. In a famous experiment, children were presented with a marshmallow and told that if they resisted eating it for a few minutes, they could have two. The ones who succeeded went on to do better in school and adult life. Perhaps less obviously, companies using machine learning to improve their websites or their business practices face a similar problem. A company may make a change that brings in more revenue in the short term—like selling an inferior product that costs less to make for the same price as the original superior product—but miss seeing that doing this will lose customers in the longer term.</p>

<p class="noindent chinese">人类确实有一个不变的指南：他们的情感。我们寻求快乐，避免痛苦。当你触摸一个热炉子时，你会本能地退缩。这是最容易的部分。困难的部分是首先要学会不碰炉子。这需要移动以避免你还没有感觉到的剧痛。你的大脑通过将疼痛不仅与你触摸炉子的那一刻联系起来，而且与导致它的行动联系起来来实现这一点。爱德华·桑代克将此称为效果法则：导致快乐的行为在未来更有可能被重复；导致痛苦的行为则较少。可以说，快乐会穿越时空，而行动最终会与那些与之相距甚远的效果联系起来。人类比其他任何动物都能更好地进行这种远距离的奖励寻求，这对我们的成功至关重要。在一个著名的实验中，孩子们得到了一个棉花糖，并被告知如果他们在几分钟内忍住不吃，就可以吃两个。那些成功的孩子在学校和成年生活中表现得更好。也许不那么明显，使用机器学习来改善他们的网站或商业行为的公司也面临类似的问题。一家公司可能会做出改变，在短期内带来更多的收入，比如以与原来的优质产品相同的价格销售成本更低的劣质产品，但却没有看到这样做会在长期内失去客户。</p>

<p class="noindent english">The learners we saw in the previous chapters are all guided by instant gratification: every action, whether it’s flagging a spam e-mail or buying a stock, gets an immediate reward or punishment from the teacher. But there’s a whole subfield of machine learning dedicated to algorithms that explore on their own, flail, hit on rewards, and figure out how to get them again in the future, much like babies crawling around and putting things in their mouths.</p>

<p class="noindent chinese">我们在前几章看到的学习者都是由即时满足引导的：每一个行动，无论是标记垃圾邮件还是购买股票，都会立即得到老师的奖励或惩罚。但是，机器学习有一个完整的子领域，专门研究算法，它们自己探索、摇摆、击中奖励，并想办法在未来再次获得奖励，就像婴儿爬来爬去，把东西放进嘴里。</p>

<p class="noindent english">It’s called reinforcement learning, and your first housebot will probably use it a lot. If you ask Robby to make eggs and bacon for you right <a id="babilu_link-143"></a> after you’ve unpacked him and turned him on, it may take a while. But then, while you’re at work, he will explore the kitchen, noting where various things are and what kind of stove you have. By the time you get back, dinner will be ready.</p>

<p class="noindent chinese">这叫做强化学习，你的第一个家庭机器人可能会经常使用它。如果你让罗比为你做鸡蛋和熏肉，就在你打开他的包装并打开他的电源后，可能需要一段时间。但随后，当你在工作时，他会探索厨房，注意到各种东西的位置，以及你的炉子是什么样的。当你回来的时候，晚餐就已经准备好了。</p>

<p class="noindent english">An important precursor of reinforcement learning was a checkers-playing program created by Arthur Samuel, an IBM researcher, in the 1950s. Board games are a great example of a reinforcement learning problem: you have to make a long series of moves without any feedback, and the whole reward or punishment comes at the very end, in the form of a win or loss. Yet Samuel’s program was able to teach itself to play as well as most humans. It did not directly learn which move to make in each board position because that would have been too difficult. Rather, it learned how to evaluate each board position—how likely am I to win starting from this position?—and chose the move that led to the best position. Initially, the only positions it knew how to evaluate were the final ones: a win, a tie, or a loss. But once it knew that a certain position was a win, it also knew that positions from which it could move to it were good, and so on. Thomas J. Watson Sr., IBM’s president, predicted that when the program was demonstrated IBM stock would go up by fifteen points. It did. The lesson was not lost on IBM, which went on to build a chess champion and a <i>Jeopardy!</i> one.</p>

<p class="noindent chinese">强化学习的一个重要先驱是 IBM 的研究员阿瑟·塞缪尔在 20 世纪 50 年代创建的跳棋游戏程序。棋盘游戏是强化学习问题的一个很好的例子：你必须在没有任何反馈的情况下做出一长串的动作，而整个奖励或惩罚是在最后，以赢或输的形式出现。然而，塞缪尔的程序能够像大多数人类一样教自己下棋。它没有直接学习在每个棋盘位置走哪一步，因为这太难了。相反，它学会了如何评估每个棋盘位置 —— 我从这个位置开始赢的可能性有多大 —— 并选择导致最佳位置的那一步。最初，它知道如何评估的唯一位置是最后的位置：赢、平或输。但是，一旦它知道某个位置是赢的，它也知道可以移动到这个位置的位置是好的，以此类推。IBM 总裁托马斯·J·沃森预言，当这个程序被展示出来时，IBM 的股票将上涨 15 个点。它确实上涨了。这个教训对 IBM 来说并非一无是处，它后来又制造了一个国际象棋冠军和一个<i>危险游戏</i>冠军。</p>

<p class="noindent english">The notion that not all states have rewards (positive or negative) but every state has a value is central to reinforcement learning. In board games, only final positions have a reward (1, 0, or −1 for a win, tie, or loss, say). Other positions give no immediate reward, but they have value in that they can lead to rewards later. A chess position from which you can force checkmate in some number of moves is practically as good as a win and therefore has high value. We can propagate this kind of reasoning all the way to good and bad opening moves, even if at that distance the connection is far from obvious. In video games, the rewards are usually points, and the value of a state is the number of points you can accumulate starting from that state. In real life, a reward now is better than a reward later, so future rewards can be discounted by some rate of return, like investments. Of course, the rewards depend <a id="babilu_link-133"></a> on what actions you choose, and the goal of reinforcement learning is to always choose the action that leads to the greatest rewards. Should you pick up the phone and ask your friend for a date? It could be the start of a beautiful relationship or just the route to a painful rejection. Even if your friend agrees to go on a date, that date may turn out well or not. Somehow, you have to abstract over all the infinite paths the future could take and make a decision now. Reinforcement learning does that by estimating the value of each state—the sum total of the rewards you can expect to get starting from that state—and choosing the actions that maximize it.</p>

<p class="noindent chinese">不是所有的状态都有奖励（正或负），但每个状态都有一个值，这个概念是强化学习的核心。在棋盘游戏中，只有最后的位置才有奖励（比如说，赢、平、输，分别为 1、0 或 -1）。其他位置没有直接的奖励，但它们有价值，因为它们可以导致以后的奖励。一个你能在若干步内强行将死的国际象棋局面，实际上和胜利一样好，因此具有很高的价值。我们可以将这种推理一直推广到好的和坏的开局棋，即使在这个距离上，这种联系还很不明显。在视频游戏中，奖励通常是积分，一个状态的价值是你从这个状态开始可以积累的积分数。在现实生活中，现在的奖励比以后的奖励要好，所以未来的奖励可以用一些回报率来折算，就像投资一样。当然，奖励取决于你选择什么行动，强化学习的目标是始终选择导致最大奖励的行动。你是否应该拿起电话，向你的朋友提出约会？这可能是一段美好关系的开始，也可能只是通往痛苦的拒绝之路。即使你的朋友同意去约会，这次约会可能会有好的结果，也可能没有。不知何故，你必须抽象出所有未来可能采取的无限路径，现在就做出决定。强化学习通过估计每个状态的价值 —— 即你从该状态开始可望得到的奖励的总和，并选择使其最大化的行动。</p>

<p class="noindent english">Suppose you’re moving along a tunnel, Indiana Jones–like, and you come to a fork. Your map says the left tunnel leads to a treasure and the right one to a snake pit. The value of where you’re standing—right before the fork—is the value of the treasure because you’ll choose to go left. If you always choose the best possible action, then the value of a state differs from the value of the succeeding state only by the immediate reward (if any) that you’ll get by performing that action. If we know each state’s immediate reward, we can use this observation to update the values of neighboring states, and so on, until all states have consistent values. The treasure’s value propagates backward along the tunnel until it reaches the fork and beyond. Once you know the value of each state, you also know which action to choose in each state (the one that maximizes the combination of immediate reward and value of the resulting state). This much was worked out in the 1950s by the control theorist Richard Bellman. But the real problem in reinforcement learning is when you don’t have a map of the territory. Then your only choice is to explore and discover what rewards are where. Sometimes you’ll discover a treasure, and other times you’ll fall into a snake pit. Every time you take an action, you note the immediate reward and the resulting state. That much could be done by supervised learning. But you also update the value of the state you just came from to bring it into line with the value you just observed, namely the reward you got plus the value of the new state you’re in. Of course, that value may not yet be the correct one, but if you wander around doing <a id="babilu_link-112"></a> this for long enough, you’ll eventually settle on the right values for all the states and the corresponding actions. That’s reinforcement learning in a nutshell.</p>

<p class="noindent chinese">假设你正沿着一条隧道前进，像印第安纳·琼斯一样，你来到了一个岔路口。你的地图说左边的隧道通向宝藏，右边的隧道通向蛇坑。你所站的位置 —— 在岔路口之前 —— 的价值就是宝藏的价值，因为你会选择向左走。如果你总是选择最好的行动，那么一个状态的价值与后一个状态的价值的差别只在于你通过执行该行动所得到的即时奖励（如果有的话）。如果我们知道每个状态的即时奖励，我们就可以用这个观察结果来更新相邻状态的价值，以此类推，直到所有状态的价值都一致。宝藏的价值沿着隧道向后传播，直到它到达分叉处及以后。一旦你知道了每个状态的价值，你也就知道了在每个状态下应该选择哪种行动（使即时奖励和结果状态的价值组合最大化的行动）。这一点在 20 世纪 50 年代由控制理论家理查德·贝尔曼研究出来。但强化学习的真正问题是，当你没有一张领土地图的时候。那么你唯一的选择就是去探索，发现哪里有什么奖励。有时你会发现一个宝藏，有时你会掉进一个蛇坑。每当你采取一个行动，你就会注意到即时的奖励和由此产生的状态。这一点可以通过监督学习来完成。但你也要更新你刚来时的状态的价值，使其与你刚观察到的价值一致，即你得到的奖励加上你所处的新状态的价值。当然，这个值可能还不是正确的，但如果你在做了足够长的时间，你最终会确定所有状态和相应行动的正确值。这就是强化学习的简述。</p>

<p class="noindent english">Notice how reinforcement learners face the same exploration-exploitation dilemma we met in <a href="#babilu_link-315">Chapter 5</a> : to maximize your rewards, you’ll naturally want to always pick the action leading to the highest-value state, but that prevents you from potentially discovering even higher rewards elsewhere. Reinforcement learners solve this by sometimes choosing the best action and sometimes a random one. (The brain even seems to have a “noise generator” for this purpose.) Early on, when there’s much to learn, it makes sense to explore a lot. Once you know the territory, it’s best to concentrate on exploiting it. That’s what humans do over their lifetimes: children explore, and adults exploit (except for scientists, who are eternal children). Children’s play is a lot more serious than it looks; if evolution made a creature that is helpless and a heavy burden on its parents for the first several years of its life, that extravagant cost must be for the sake of an even bigger benefit. In effect, reinforcement learning is a kind of speeded-up evolution—trying, discarding, and refining actions within a single lifetime instead of over generations—and by that standard it’s extremely efficient.</p>

<p class="noindent chinese">注意到强化学习者如何面对我们在<a href="#babilu_link-315">第五章</a>中遇到的同样的探索·开发困境：为了使你的回报最大化，你自然希望总是选择导致最高价值状态的行动，但这阻碍了你在其他地方发现更高的回报的可能性。强化学习者通过有时选择最佳行动、有时选择随机行动来解决这个问题。（大脑甚至似乎有一个 “噪音发生器” 用于此目的。）在早期，当有很多东西需要学习时，探索很多是有意义的。一旦你知道了这个领域，最好是集中精力去利用它。这就是人类在其一生中所做的事情：儿童探索，成人利用（除了科学家，他们是永恒的儿童）。孩子们的游戏比它看起来要严肃得多；如果进化造就了一种无助的生物，在其生命的头几年里是父母的沉重负担，那么这种奢侈的代价一定是为了更大的利益。实际上，强化学习是一种加速的进化 —— 在一个人的一生中尝试、抛弃和完善行动，而不是经过几代人的努力 —— 按照这个标准，它是非常有效的。</p>

<p class="noindent english">Research on reinforcement learning started in earnest in the early 1980s, with the work of Rich Sutton and Andy Barto at the University of Massachusetts. They felt that learning depends crucially on interacting with the environment, but supervised algorithms didn’t capture this, and they found inspiration instead in the psychology of animal learning. Sutton went on to become the leading proponent of reinforcement learning. Another key step happened in 1989, when Chris Watkins at Cambridge, initially motivated by his experimental observations of children’s learning, arrived at the modern formulation of reinforcement learning as optimal control in an unknown environment.</p>

<p class="noindent chinese">强化学习的研究始于 20 世纪 80 年代初，由马萨诸塞大学的里奇·萨顿和安迪·巴托的工作开始。他们认为，学习在很大程度上取决于与环境的互动，但监督算法并没有抓住这一点，因此他们从动物学习的心理学中找到了灵感。萨顿后来成为强化学习的主要倡导者。另一个关键步骤发生在 1989 年，剑桥大学的克里斯·沃特金斯最初受他对儿童学习的实验观察的启发，得出了强化学习的现代表述，即在未知环境中的最佳控制。</p>

<p class="noindent english">Reinforcement learners as we’ve seen them so far are not very realistic, however, because they don’t know what to do in a state unless they’ve been there before, and in the real world no two situations are ever exactly alike. We need to be able to generalize from previously <a id="babilu_link-105"></a> visited states to new ones. Luckily, we already know how to do that: all we have to do is wrap reinforcement learning around one of the supervised learners we’ve met before, such as a multilayer perceptron. The neural network’s job is now to predict the value of a state, and the error signal for backpropagation is the difference between the predicted and observed values. There’s a problem, however. In supervised learning the target value for a state is always the same, but in reinforcement learning, it keeps changing as a consequence of updates to nearby states. As a result, reinforcement learning with generalization often fails to settle on a stable solution, unless the inner learner is something very simple, like a linear function. Nevertheless, reinforcement learning with neural networks has had some notable successes. An early one was a human-level backgammon player. More recently, a reinforcement learner from DeepMind, a London-based startup, beat an expert human player at Pong and other simple arcade games. It used a deep network to predict actions’ values from the console screen’s raw pixels. With its end-to-end vision, learning, and control, the system bore at least a passing resemblance to an artificial brain. This may help explain why Google paid half a billion dollars for DeepMind, a company with no products, no revenues, and few employees.</p>

<p class="noindent chinese">然而，到目前为止，我们所看到的强化学习者并不是非常现实的，因为他们不知道在一个状态下该做什么，除非他们曾经去过那里，而在现实世界中，没有两种情况是完全相同的。我们需要能够从以前的到新的状态中进行归纳。幸运的是，我们已经知道如何做到这一点：我们所要做的就是将强化学习包裹在我们之前遇到的监督学习器中，比如多层感知器。现在神经网络的工作是预测一个状态的值，反向传播的误差信号是预测值和观察值之间的差异。然而，有一个问题。在监督学习中，一个状态的目标值总是相同的，但在强化学习中，由于附近状态的更新，它一直在变化。因此，除非内部学习者是非常简单的东西，如线性函数，否则具有概括性的强化学习往往无法确定一个稳定的解决方案。尽管如此，神经网络的强化学习还是取得了一些引人注目的成功。早期的一个例子是一个人类水平的双陆棋手。最近，一家位于伦敦的创业公司 DeepMind 的强化学习器在乒乓球和其他简单的街机游戏中击败了人类专家。它使用了一个深度网络，从控制台屏幕的原始像素预测动作的价值。凭借其端到端的视觉、学习和控制，该系统至少与人工大脑有几分相似。这可能有助于解释为什么谷歌为 DeepMind 支付了 5 亿美元，这家公司没有产品，没有收入，员工也很少。</p>

<p class="noindent english">Gaming aside, researchers have used reinforcement learning to balance poles, control stick-figure gymnasts, park cars backward, fly helicopters upside down, manage automated telephone dialogues, assign channels in cell phone networks, dispatch elevators, schedule space-shuttle cargo loading, and much else. Reinforcement learning has also influenced psychology and neuroscience. The brain does it, using the neurotransmitter dopamine to propagate differences between expected and actual rewards. Reinforcement learning explains Pavlovian conditioning, but unlike behaviorism, it allows animals to have internal mental states. Foraging bees use it, as do mice finding cheese in mazes. Your daily life is a stream of little-noticed miracles made possible in part by reinforcement learning. You get up, get dressed, eat breakfast, and drive to work, all the while thinking about something else. Below the surface, reinforcement learning continually orchestrates and <a id="babilu_link-175"></a> fine-tunes this prodigious symphony of motion. Snippets of reinforcement learning, also known as habits, make up most of what you do. You feel hungry, walk to the fridge, and grab a snack. As Charles Duhigg shows in <i>The Power of Habit</i> , understanding and controlling this cycle of cue, routine, and reward is key to success, not just for individuals but for businesses and even whole societies.</p>

<p class="noindent chinese">除游戏外，研究人员还将强化学习用于平衡杆子、控制棒状体操运动员、倒车、倒飞直升机、管理自动电话对话、分配手机网络中的频道、调度电梯、安排航天飞机货物装载，以及其他许多方面。强化学习也影响了心理学和神经科学。大脑是这样做的，利用神经递质多巴胺来传播预期和实际奖励之间的差异。强化学习解释了巴甫洛夫条件反射，但与行为主义不同的是，它允许动物有内部心理状态。觅食的蜜蜂使用它，老鼠在迷宫中寻找奶酪也是如此。你的日常生活是一连串鲜为人知的奇迹，这部分是由强化学习造成的。你起床、穿衣、吃早餐、开车去上班，所有这些都是在思考其他事情。在表面之下，强化学习不断地对这一惊人的运动交响曲进行协调和微调。强化学习的片段，也被称为习惯，构成了你的大部分行为。你觉得饿了，就走到冰箱前，拿起一个零食。正如查尔斯·杜希格在《<i>习惯的力量</i>》中所展示的，理解和控制这种提示、常规和奖励的循环是成功的关键，不仅对个人如此，对企业甚至整个社会也是如此。</p>

<p class="noindent english">Of reinforcement learning’s founders, Rich Sutton is the most gung ho. For him, reinforcement learning is the Master Algorithm and solving it is tantamount to solving AI. Chris Watkins, on the other hand, is dissatisfied. He sees many things children can do that reinforcement learners can’t: solve problems, solve them better after a few attempts, make plans, acquire increasingly abstract knowledge. Luckily, we also have learning algorithms for these higher-level abilities, the most important of which is chunking.</p>

<p class="noindent chinese">在强化学习的创始人中，里奇·萨顿是最有魄力的一个。对他来说，强化学习是主算法，解决了它就等于解决了人工智能。另一方面，克里斯·沃特金斯则不满意。他看到许多儿童能做的事情，而强化学习者却做不到：解决问题，在尝试几次后能更好地解决问题，制定计划，获得越来越抽象的知识。幸运的是，我们也有针对这些高层次能力的学习算法，其中最重要的就是分块。</p>

<h1 id="babilu_link-441"><b>Practice makes perfect</b></h1>

<h1 id="babilu_link-441"><b>实践出真知</b></h1>

<p class="noindent english">To learn is to get better with practice. You may barely remember it now, but learning to tie your shoelaces was really hard. At first you couldn’t do it at all, despite your five years of age. Then your laces probably came undone faster than you could tie them. But little by little you learned to tie them faster and better until it became completely automatic. The same happens with lots of other things, like crawling, walking, running, riding a bike, and driving a car; reading, writing, and arithmetic; playing an instrument and practicing a sport; cooking and using a computer. Ironically, you learn the most when it’s most painful: early on, when every step is difficult, you keep failing, and even when you succeed, the results are not very pretty. After you’ve mastered your golf swing or tennis serve, you can spend years perfecting it, but all those years make less difference than the first few weeks did. You get better with practice, but not at a constant rate: at first you improve quickly, then not so quickly, then very slowly. Whether it’s playing games or the guitar, the curve of performance improvement over time—how well you do something or how long it takes you to do it—has a very specific form:</p>

<p class="noindent chinese">学习是为了通过练习变得更好。你现在可能几乎不记得了，但学习系鞋带真的很困难。起初你根本做不到，尽管你只有五岁。然后你的鞋带解开的速度可能比你系的速度快。但渐渐地，你学会了更快、更好地系鞋带，直到它成为完全自动的。同样的情况也发生在很多其他事情上，比如爬行、走路、跑步、骑车和开车；阅读、写作和算术；演奏乐器和练习运动；烹饪和使用电脑。具有讽刺意味的是，在最痛苦的时候，你学得最多：早期，每一步都很困难，你不断失败，即使成功了，结果也不太漂亮。在你掌握了高尔夫挥杆或网球发球后，你可以花几年时间来完善它，但所有这些年的差别都不如最初几个星期的差别。你通过练习变得更好，但不是以恒定的速度：一开始你进步很快，然后不那么快，然后很慢。无论是玩游戏还是弹吉他，随着时间的推移，性能改进的曲线，即你做某事的好坏或你花多长时间去做，都有一个非常具体的形式。</p>

<div>

<div>

<img alt="image" src="images/000011.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-303"></a> This type of curve is called a power law, because performance varies as time raised to some negative power. For example, in the figure above, time to completion is proportional to the number of trials raised to minus two (or equivalently, one over the number of trials squared). Pretty much every human skill follows a power law, with different powers for different skills. (In contrast, Windows never gets faster with practice—something for Microsoft to work on.)</p>

<p class="noindent chinese">这种类型的曲线被称为幂律，因为性能随着时间上升到某个负数而变化。例如，在上图中，完成任务的时间与试验次数升至负 2 成正比（或者相当于试验次数的平方为 1）。几乎所有的人类技能都遵循幂律，不同的技能有不同的幂。（相比之下，Windows 从来没有因为练习而变得更快 —— 这是微软需要努力的事情）。</p>

<p class="noindent english">In 1979, Allen Newell and Paul Rosenbloom started wondering what could be the reason for this so-called power law of practice. Newell was one of the founders of AI and a leading cognitive psychologist, and Rosenbloom was one of his graduate students at Carnegie Mellon University. At the time, none of the existing models of practice could explain the power law. Newell and Rosenbloom suspected it might have something to do with chunking, a concept from the psychology of perception and memory. We perceive and remember things in chunks, and we can only hold so many chunks in short-term memory at any given time (seven plus or minus two, according to the classic paper by George Miller). Crucially, grouping things into chunks allows us to process <a id="babilu_link-302"></a> much more information than we otherwise could. That’s why telephone numbers have hyphens: 1-723-458-3897 is much easier to remember than 17234583897. Herbert Simon, Newell’s longtime collaborator and AI cofounder, had earlier found that the main difference between novice and expert chess players is that novices perceive chess positions one piece at a time while experts see larger patterns involving multiple pieces. Getting better at chess mainly involves acquiring more and larger such chunks. Newell and Rosenbloom hypothesized that a similar process is at work in all skill acquisition, not just chess.</p>

<p class="noindent chinese">1979 年，艾伦·纽维尔和保罗·罗森布卢姆开始想知道这种所谓的实践幂律的原因可能是什么。纽维尔是人工智能的创始人之一，也是著名的认知心理学家，而罗森布卢姆是他在卡耐基梅隆大学的研究生之一。当时，没有一个现有的实践模型能够解释幂律。纽维尔和罗森布卢姆怀疑它可能与 “分块” 有关，这是一个来自感知和记忆心理学的概念。我们对事物的感知和记忆是分块的，而我们在任何时候都只能在短时记忆中保留这么多块（根据乔治·米勒的经典论文，是 7 加 2 减）。至关重要的是，将事物分组，使我们能够处理比我们在其他情况下所能处理的信息多得多。这就是为什么电话号码有连字符：1-723-458-3897 比 17234583897 更容易记住。纽维尔的长期合作者和人工智能的共同创始人赫伯特·西蒙（Herbert Simon）早些时候发现，新手和专家棋手之间的主要区别是，新手每次只感知一个棋子的位置，而专家则看到涉及多个棋子的更大的模式。变得更好的国际象棋主要涉及获得更多和更大的这种块状物。Newell 和 Rosenbloom 假设，一个类似的过程在所有的技能获得中起作用，而不仅仅是象棋。</p>

<p class="noindent english">In perception and memory, a chunk is just a symbol that stands for a pattern of other symbols, like AI stands for artificial intelligence. Newell and Rosenbloom adapted this notion to the theory of problem solving that Newell and Simon had developed earlier. Newell and Simon asked experimental subjects to solve problems—for example, derive one mathematical formula from another on the blackboard—while narrating aloud how they were going about it. They found that humans solve problems by decomposing them into subproblems, subsubproblems, and so on and systematically reducing the differences between the initial state (the first formula, say) and the goal state (the second formula). Doing so requires searching for a sequence of actions that will work, however, and that takes time. Newell and Rosenbloom’s hypothesis was that each time we solve a subproblem, we form a chunk that allows us to go directly from the state before we solve it to the state after. A chunk in this sense has two parts: the stimulus (a pattern you recognize in the external world or in your short-term memory) and the response (the sequence of actions you execute as a result). Once you’ve learned a chunk, you store it in long-term memory. Next time you have to solve the same subproblem, you can just apply the chunk, and save the time spent searching. This happens at all levels until you have a chunk for the whole problem and can solve it automatically. To tie your shoelaces, you tie the starting knot, make a loop with one end, wrap the other end around it, and pull it through the hole in the middle. Each of these is far from trivial for a five-year-old, but once you’ve acquired the corresponding chunks, you’re almost there.</p>

<p class="noindent chinese">在感知和记忆中，一个块只是一个符号，代表着其他符号的模式，就像 AI 代表着人工智能。纽厄尔和罗森布洛姆将这一概念改编为纽厄尔和西蒙早先提出的问题解决的理论。纽厄尔和西蒙要求实验对象解决问题 —— 例如，从黑板上的另一个数学公式推导出一个数学公式 —— 同时大声讲述他们是如何进行的。他们发现，人类解决问题的方式是将问题分解为子问题、子子问题等，并系统地减少初始状态（例如第一个公式）和目标状态（第二个公式）之间的差异。然而，这样做需要寻找一连串有效的行动，而这需要时间。Newell 和 Rosenbloom 的假设是，每次我们解决一个子问题时，都会形成一个块，使我们能够直接从解决前的状态到解决后的状态。在这个意义上，一个大块有两个部分：刺激（你在外部世界或短期记忆中认识到的模式）和反应（你作为结果执行的行动序列）。一旦你学会了一个大块，你就把它储存在长期记忆中。下次你要解决同样的子问题时，你就可以直接应用这块内容，而节省了搜索的时间。这种情况在各个层面都会发生，直到你对整个问题有了一个大块，并能自动解决它。要系鞋带，你要打起始结，用一端打一个圈，把另一端绕在上面，然后把它拉过中间的洞。对于一个五岁的孩子来说，其中的每一项都远非小事，但一旦你获得了相应的块，你就几乎成功了。</p>

<p class="noindent english"><a id="babilu_link-185"></a> Rosenbloom and Newell set their chunking program to work on a series of problems, measured the time it took in each trial, and lo and behold, out popped a series of power law curves. But that was only the beginning. Next they incorporated chunking into Soar, a general theory of cognition that Newell had been working on with John Laird, another one of his students. Instead of working only within a predefined hierarchy of goals, the Soar program could define and solve a new subproblem every time it hit a snag. Once it formed a new chunk, Soar generalized it to apply to similar problems, in a manner similar to inverse deduction. Chunking in Soar turned out to be a good model of lots of learning phenomena besides the power law of practice. It could even be applied to learning new knowledge by chunking data and analogies. This led Newell, Rosenbloom, and Laird to hypothesize that chunking is the <i>only</i> mechanism needed for learning—in other words, the Master Algorithm.</p>

<p class="noindent chinese">Rosenbloom 和 Newell 将他们的分块程序设定为在一系列问题上工作，测量它在每次试验中花费的时间，瞧，跳出了一系列幂律曲线。但这仅仅是个开始。接下来，他们将分块法纳入了 Soar，这是纽维尔与他的另一个学生约翰·莱尔德一起研究的认知的一般理论。Soar 程序不是只在预定的目标层次中工作，而是在每次遇到困难时都能定义并解决一个新的子问题。一旦它形成了一个新的分块，Soar 就会以类似于反推法的方式将其泛化，以适用于类似的问题。Soar 中的分块法被证明是除了实践的幂律之外的许多学习现象的一个很好的模型。它甚至可以应用于通过数据和类比的分块来学习新知识。这导致纽维尔、罗森布洛姆和莱尔德假设，分块是学习所需的<i>唯一</i>机制 —— 换句话说，就是主算法。</p>

<p class="noindent english">Being classic AI types, Newell, Simon, and their students and followers were strong believers in the primacy of problem solving. If the problem solver is powerful, the learner can piggyback on it and be simple. Indeed, learning is just another kind of problem solving. Newell and company made a concerted effort to reduce all learning to chunking and all cognition to Soar, but in the end they failed. One problem was that, as the problem solver learned more chunks, and more complicated ones, the cost of trying them often became so high that the program got slower instead of faster. Somehow humans avoid this, but so far researchers in this area have not figured out how. On top of that, trying to reduce reinforcement learning, supervised learning, and everything else to chunking ultimately created more problems than it solved. Eventually, the Soar researchers conceded defeat and incorporated those other types of learning into Soar as separate mechanisms. Nevertheless, chunking remains a preeminent example of a learning algorithm inspired by psychology, and the true Master Algorithm, whatever it turns out to be, must surely share its ability to improve with practice.</p>

<p class="noindent chinese">作为典型的人工智能类型，纽维尔、西蒙以及他们的学生和追随者都坚信问题解决的首要地位。如果问题解决者是强大的，那么学习者就可以借助于它而变得简单。事实上，学习只是另一种问题解决方式。纽厄尔和公司做出了一致的努力，将所有的学习简化为分块，将所有的认知简化为 Soar，但最终他们失败了。一个问题是，随着问题解决者学习更多的块，以及更复杂的块，尝试这些块的成本往往变得非常高，以至于程序变得更慢而不是更快。人类以某种方式避免了这一点，但到目前为止，该领域的研究人员还没有弄清楚如何避免。除此之外，试图将强化学习、监督学习和其他一切都简化为分块学习，最终造成的问题比解决的问题更多。最终，Soar 的研究人员认输了，将其他类型的学习作为单独的机制纳入 Soar。尽管如此，分块学习仍然是一个受心理学启发的学习算法的杰出例子，而真正的主算法，不管它是什么，肯定都会分享它在实践中改进的能力。</p>

<p class="noindent english">Chunking and reinforcement learning are not as widely used in business as supervised learning, clustering, or dimensionality reduction, but <a id="babilu_link-21"></a> a simpler type of learning by interacting with the environment is: learning the effects of your actions (and acting accordingly). If the background color of your e-commerce site’s home page is currently blue and you’re wondering whether making it red would increase sales, try it out on a hundred thousand randomly chosen customers and compare the results with those of the regular site. This technique, called A/B testing, was at first used mainly in drug trials but has since spread to many fields where data can be gathered on demand, from marketing to foreign aid. It can also be generalized to try many combinations of changes at once, without losing track of which changes lead to which gains (or losses). Companies like Amazon and Google swear by it; you’ve probably participated in thousands of A/B tests without realizing it. A/B testing gives the lie to the oft-heard criticism that big data is only good for finding correlations, not causation. Philosophical fine points aside, learning causality is learning the effects of your actions, and anyone with a stream of data they can affect can do it—from a one-year-old splashing around in the bathtub to a president campaigning for reelection.</p>

<p class="noindent chinese">分块学习和强化学习在商业中的应用并不像监督学习、聚类或降维那样广泛，但通过与环境的互动，一种更简单的学习方式是：学习你的行为的效果（并采取相应的行动）。如果你的电子商务网站首页的背景颜色目前是蓝色的，你想知道把它变成红色是否会增加销售量，那就在十万个随机选择的客户身上试一试，把结果与普通网站的结果进行比较。这种技术被称为 A/B 测试，起初主要用于药物试验，但后来扩展到许多可以按需收集数据的领域，从市场营销到对外援助。它也可以被概括为一次尝试许多变化的组合，而不会失去对哪些变化导致哪些收益（或损失）的跟踪。像亚马逊和谷歌这样的公司对它发誓；你可能已经参与了成千上万的 A/B 测试而没有意识到。A/B 测试掩盖了人们经常听到的批评，即大数据只适合于寻找相关性，而不是因果关系。撇开哲学上的细枝末节不谈，学习因果关系就是学习你的行为的影响，任何拥有他们可以影响的数据流的人都可以这样做 —— 从一岁的孩子在浴缸里嬉戏到总统竞选连任的时候。</p>

<h1 id="babilu_link-442"><b>Learning to relate</b></h1>

<h1 id="babilu_link-442"><b>学会与人交往</b></h1>

<p class="noindent english">If we endow Robby the robot with all the learning abilities we’ve seen so far in this book, he’ll be pretty smart but still a bit autistic. He’ll see the world as a bunch of separate objects, which he can identify, manipulate, and even make predictions about, but he won’t understand that the world is a web of interconnections. Robby the doctor would be very good at diagnosing someone with the flu based on his symptoms but unable to suspect that the patient has swine flu because he has been in contact with someone infected with it. Before Google, search engines decided whether a web page was relevant to your query by looking at its content—what else? Brin and Page’s insight was that the strongest sign a page is relevant is that relevant pages link to it. Similarly, if you want to predict whether a teenager is at risk of starting to smoke, by far the best thing you can do is check whether her close friends smoke. An enzyme’s shape is as inseparable from the shapes of the molecules it <a id="babilu_link-289"></a> brings together as a lock is from its key. Predator and prey have deeply entwined properties, each evolved to defeat the other’s properties. In all of these cases, the best way to understand an entity—whether it’s a person, an animal, a web page, or a molecule—is to understand how it relates to other entities. This requires a new kind of learning that doesn’t treat the data as a random sample of unrelated objects but as a glimpse into a complex network. Nodes in the network interact; what you do to one affects the others and comes back to affect you. Relational learners, as they’re called, may not quite have social intelligence, but they’re the next best thing. In traditional statistical learning, every man is an island, entire of itself. In relational learning, every man is a piece of the continent, a part of the main. Humans are relational learners, wired to connect, and if we want Robby to grow into a perceptive, socially adept robot, we need to wire him to connect, too.</p>

<p class="noindent chinese">如果我们赋予机器人罗比所有的学习能力，他就会相当聪明，但仍有一点自闭症。他把世界看成是一堆独立的物体，他可以识别、操纵，甚至做出预测，但他不会理解世界是一个相互联系的网络。医生罗比会非常善于根据某人的症状来诊断他是否患有流感，但却无法怀疑病人是否患有猪流感，因为他曾与感染猪流感的人接触过。在谷歌之前，搜索引擎决定一个网页是否与你的查询相关，是看它的内容 —— 还有什么？布林和佩奇的见解是，一个网页是相关的最有力的标志是相关的网页链接到它。同样，如果你想预测一个青少年是否有开始吸烟的风险，到目前为止，你能做的最好的事情就是检查她身边的朋友是否吸烟。酶的形状与它的分子形状密不可分，就像一把锁与它的钥匙一样，。捕食者和猎物的特性深深地纠缠在一起，每一个都是为了打败对方的特性而进化的。在所有这些情况下，理解一个实体 —— 无论是一个人、一个动物、一个网页，还是一个分子 —— 的最好方式是理解它与其他实体的关系。这需要一种新的学习方法，它不把数据当作不相关对象的随机样本，而是当作对一个复杂网络的窥视。网络中的节点是相互作用的；你对一个节点所做的事情会影响到其他节点，并会回来影响你。关系型学习者，正如他们所称，可能不完全具有社会智能，但他们是下一个最好的东西。在传统的统计学习中，每个人都是一个岛屿，都是自己的一部分。在关系学习中，每个人都是大陆的一部分，是主体的一部分。人类是关系型学习者，是连接的纽带，如果我们想让罗比成长为一个有洞察力的、善于社交的机器人，我们也需要把他连接起来。</p>

<p class="noindent english">The first difficulty we face is that, when the data is all one big network, we no longer seem to have many examples to learn from, just one—and that’s not enough. Naïve Bayes learns that a fever is a symptom of the flu by counting the number of fever-stricken flu patients. If it could only see one patient, it would either conclude that flu always causes fever or that it never does, both of which are wrong. We would like to learn that the flu is contagious by looking at the pattern of infections in a social network—a clump of infected people here, a clump of uninfected ones there—but we only have one pattern to look at, even if it’s in a network of seven billion people, so it’s not clear how to generalize. The key is to notice that, embedded in that big network, we have many examples of <i>pairs</i> of people. If acquaintances are more likely to both have the flu than pairs of people who have never met, then being acquainted with a flu patient makes you more likely to be one as well. Unfortunately, however, we can’t just count how many pairs of acquaintances in the data both have the flu and turn those counts into probabilities. This is because a person has many acquaintances, and all the pairwise probabilities don’t add up to a coherent model that lets us, for example, compute how likely someone is to have the flu given which of their acquaintances do. We didn’t have this problem when the examples <a id="babilu_link-298"></a> were all separate, and we wouldn’t have it in, say, a society of childless couples, each living on their own desert island. But that’s not the real world, and there wouldn’t be any epidemics in it, anyway.</p>

<p class="noindent chinese">我们面临的第一个困难是，当数据都是一个大网络时，我们似乎不再有很多例子可以学习，只有一个，这还不够。天真贝叶斯通过计算发烧的流感病人的数量来学习发烧是流感的症状。如果它只能看到一个病人，它要么得出结论说流感总是导致发烧，要么得出结论说它从不导致发烧，这两个结论都是错的。我们想通过观察社会网络中的感染模式来了解流感的传染性 —— 这里有一群受感染的人，那里有一群未受感染的人 —— 但我们只有一个模式可看，即使是在一个有 70 亿人的网络中，所以不清楚如何归纳。关键是要注意到，在这个大网络中，我们有许多<i>成对</i>的例子。如果熟人比从未见过面的人更有可能同时患流感，那么与流感患者相识就更有可能成为流感患者。然而，不幸的是，我们不能只计算数据中的多少对熟人都患有流感，并将这些计数变成概率。这是因为一个人有很多熟人，而所有成对的概率加起来并不能形成一个连贯的模型，例如，让我们计算出某人有多大可能在其熟人中患上流感。当例子我们没有这个问题，而且我们也不会有这个问题，比如说，一个由无子女夫妇组成的社会，每个人都生活在自己的荒岛上。但那不是现实世界，而且无论如何也不会有任何流行病。</p>

<p class="noindent english">The solution is to have a set of features and learn their weights, as in Markov networks. For every person X, we can have the feature <i>X has the flu</i> ; for every pair of acquaintances X and Y, the feature <i>X and Y both have the flu</i> ; and so on. As in Markov networks, the maximum-likelihood weights are the ones that make each feature occur with the frequency observed in the data. The weight of <i>X has the flu</i> will be high if a lot of people have the flu. The weight of <i>X and Y both have the flu</i> will be high if, when person X has the flu, the odds that acquaintance Y also has the flu are higher than for a randomly chosen member of the network. If 40 percent of people have the flu and so do 16 percent of all acquaintance pairs, then the weight of <i>X and Y both have the flu</i> will be zero, because we don’t need that feature to correctly reproduce the data’s statistics (0.4 × 0.4 = 0.16). But if the feature has a positive weight, flu is more likely to occur in clumps than to just infect people at random, and you’re more likely to have the flu if your acquaintances do.</p>

<p class="noindent chinese">解决方案是有一组特征并学习它们的权重，就像在马尔可夫网络中一样。对于每个人 X，我们可以有<i>X 患有流感</i>的特征；对于每一对熟人 X 和 Y，<i>X 和 Y 都患有流感的</i>特征；等等。与马尔可夫网络一样，最大可能性权重是使每个特征以数据中观察到的频率出现。如果有很多人得了流感，那么<i>X 得了</i>流感的权重就会很高。如果 X<i>和 Y 都得了流感</i>，那么 X 的权重就会很高，因为当 X 得了流感时，熟人 Y 也得了流感的几率比随机选择的网络成员高。如果 40% 的人得了流感，16% 的熟人对也得了流感，那么<i>X 和 Y 都得了流感</i>的权重将为零，因为我们不需要这个特征来正确重现数据的统计数据（0.4 × 0.4 = 0.16）。但是，如果这个特征的权重是正的，那么流感就更有可能成群结队地发生，而不是随机地感染人们，如果你的熟人有流感，你就更有可能得流感。</p>

<p class="noindent english">Notice that the network has a separate feature for each pair of people: <i>Alice and Bob both have the flu, Alice and Chris both have the flu,</i> and so on. But we can’t learn a separate weight for each pair, because we only have one data point per pair (whether it’s infected or not), and we wouldn’t be able to generalize to members of the network we haven’t diagnosed yet (do Yvette and Zach both have the flu?). What we can do instead is learn a single weight for all features of the same form, based on all the instances of it that we’ve seen. In effect, <i>X and Y have the flu</i> is a template for features that can be instantiated with each pair of acquaintances (Alice and Bob, Alice and Chris, etc.). The weights for all the instances of a template are “tied together,” in the sense that they all have the same value, and that’s how we can generalize despite having only one example (the whole network). In nonrelational learning, the parameters of a model are tied in only one way: across all the independent examples (e.g., all the patients we’ve diagnosed). In relational learning, every feature template we create ties the parameters of all its instances.</p>

<p class="noindent chinese">请注意，该网络对每一对人都有一个单独的特征。<i>爱丽丝和鲍勃都得了流感，爱丽丝和克里斯都得了流感，</i>以此类推。但我们不能为每对人学习一个单独的权重，因为我们每对人只有一个数据点（是否被感染），而且我们将不能概括到我们还没有诊断出的网络成员（Yvette 和 Zach 是否都有流感？）我们可以做的是，根据我们所见过的所有实例，为所有相同形式的特征学习一个单一的权重。实际上，<i>X 和 Y 得了流感</i>是一个特征模板，可以用每一对熟人（Alice 和 Bob，Alice 和 Chris，等等）来实例化。模板的所有实例的权重是 “绑在一起的”，在这个意义上，它们都有相同的值，这就是我们如何在只有一个例子（整个网络）的情况下进行概括。在非关系型学习中，一个模型的参数只有一种捆绑方式：在所有独立的例子中（例如，我们诊断过的所有病人）。在关系型学习中，我们创建的每个特征模板都与所有实例的参数挂钩。</p>

<p class="noindent english"><a id="babilu_link-249"></a> We’re not limited to pairwise or individual features. Facebook wants to predict who your friends are so it can recommend them to you. It can use the rule <i>Friends of friends are likely to be friends</i> for that, but each instance of it involves three people: if Alice and Bob are friends, and Bob and Chris are also friends, then Alice and Chris are potential friends. H. L. Mencken’s quip that a man is wealthy if he makes more than his wife’s sister’s husband involves four people. Each of these rules can be turned into a feature template in a relational model, and a weight for it can be learned based on how often the feature occurs in the data. As in Markov networks, the features themselves can also be learned from the data.</p>

<p class="noindent chinese">我们并不局限于成对的或单独的特征。Facebook 想预测你的朋友是谁，这样它就可以向你推荐他们。它可以使用<i>朋友的朋友很可能是朋友</i>这一规则，但它的每个实例都涉及三个人：如果爱丽丝和鲍勃是朋友，而鲍勃和克里斯也是朋友，那么爱丽丝和克里斯就是潜在的朋友。门肯的调侃说，如果一个人赚的钱比他妻子的妹妹的丈夫多，那么他就是富有的，这涉及到四个人。这些规则中的每一条都可以变成关系模型中的一个特征模板，并且可以根据该特征在数据中出现的频率来学习它的权重。与马尔科夫网络一样，特征本身也可以从数据中学习。</p>

<p class="noindent english">Relational learners can generalize from one network to another (e.g., learn a model of how flu spreads in Atlanta and apply it in Boston). They can also learn on more than one network (e.g., Atlanta and Boston, assuming, unrealistically, that no one in Atlanta is ever in contact with anyone in Boston). But unlike “regular” learning, where all examples must have exactly the same number of attributes, in relational learning networks can vary in size; a larger network will just have more instances of the same templates than a smaller one. Of course, the generalization from a smaller network to a larger one may or may not be accurate, but the point is that nothing prevents it; and large networks often do behave locally like small ones.</p>

<p class="noindent chinese">关系型学习者可以从一个网络归纳到另一个网络（例如，学习一个关于流感如何在亚特兰大传播的模型并将其应用于波士顿）。他们也可以在一个以上的网络中学习（例如，亚特兰大和波士顿，不切实际地假设亚特兰大没有人与波士顿的人接触过）。但与 “常规” 学习不同的是，在 “常规” 学习中，所有的例子都必须有完全相同数量的属性，而在关系学习中，网络的大小可以不同；一个较大的网络只是比一个较小的网络有更多相同模板的实例。当然，从一个较小的网络归纳到一个较大的网络可能是准确的，也可能是不准确的，但关键是没有什么可以阻止它；而且大的网络经常在局部表现得像小的网络。</p>

<p class="noindent english">The neatest trick a relational learner can do is to turn a sporadic teacher into an assiduous one. For an ordinary classifier, examples without classes are useless. If I’m given a patient’s symptoms, but not the diagnosis, that doesn’t help me learn to diagnose. But if I know that some of the patient’s friends have the flu, that’s indirect evidence that he may have the flu as well. Diagnosing a few people in a network and then propagating those diagnoses to their friends, and their friends’ friends, is the next best thing to diagnosing everyone. The inferred diagnoses may be noisy, but the overall statistics of how symptoms correlate with the flu will probably be a lot more accurate and complete than if I had only a handful of isolated diagnoses to draw on. Children are very good <a id="babilu_link-121"></a> at making the most of the sporadic supervision they get (provided they don’t choose to ignore it). Relational learners share some of that ability.</p>

<p class="noindent chinese">一个关系型学习者能做的最巧妙的把戏就是把一个零星的老师变成一个勤奋的老师。对于一个普通的分类器来说，没有类别的例子是没有用的。如果我得到了一个病人的症状，但没有得到诊断，这并不能帮助我学习诊断。但如果我知道病人的一些朋友得了流感，这就是他可能也得了流感的间接证据。诊断网络中的几个人，然后将这些诊断传播给他们的朋友，以及他们朋友的朋友，是诊断所有人的下一个最好的方法。推断出来的诊断可能是有噪音的，但是关于症状与流感的相关性的总体统计可能会比我只有少数几个孤立的诊断可以借鉴的情况要准确和完整得多。孩子们非常善于能够充分利用他们得到的零星监督（只要他们不选择忽视它）。关系型学习者也有一些这种能力。</p>

<p class="noindent english">All this power comes at a cost, however. In an ordinary classifier, such as a decision tree or a perceptron, inferring an entity’s class from its attributes is a matter of a few lookups and a bit of arithmetic. In a network, each node’s class depends indirectly on all the others’, and we can’t infer it in isolation. We can resort to the same kinds of inference techniques we used for Bayesian networks, like loopy belief propagation or MCMC, but the scale is different. A typical Bayesian network has perhaps thousands of variables, but a typical social network has millions of nodes or more. Luckily, because the model of the network consists of many repetitions of the same features with the same weights, we can often condense the network into “supernodes,” each consisting of many nodes that we know will have the same probabilities, and solve a much smaller problem with the same result.</p>

<p class="noindent chinese">然而，所有这些能力都是有代价的。在一个普通的分类器中，如决策树或感知器，从一个实体的属性中推断出它的类别只是几个查询和一点算术的问题。在一个网络中，每个节点的类别都间接地取决于所有其他节点的类别，我们不能孤立地推断它。我们可以采用与贝叶斯网络相同的推理技术，如循环信念传播或 MCMC，但规模是不同的。一个典型的贝叶斯网络也许有数千个变量，但一个典型的社会网络有数百万个节点或更多。幸运的是，由于网络的模型由许多重复的相同特征和相同的权重组成，我们通常可以将网络浓缩为 “超级节点”，每个节点由许多我们知道会有相同概率的节点组成，并以同样的结果解决一个小得多的问题。</p>

<p class="noindent english">Relational learning has a long history, going back to at least the seventies and symbolist techniques like inverse deduction. But it acquired a new impetus with the advent of the Internet. Suddenly networks were everywhere, and modeling them was urgent. One phenomenon I found particularly intriguing was word of mouth. How does information propagate in a social network? Can we measure each member’s influence and target just enough of the most influential members to set off a wave of word of mouth? With my student Matt Richardson, I designed an algorithm that did just that. We applied it to Epinions, a product review site that allowed members to say whose reviews they trusted. We found, among other things, that marketing a product to the single most influential member—trusted by many followers who were in turn trusted by many others, and so on—was as good as marketing to a third of all the members in isolation. An avalanche of other research on this problem followed. Since then, I’ve applied relational learning to many others, including predicting who will form links in a social network, integrating databases, and enabling robots to build maps of their surroundings.</p>

<p class="noindent chinese">关系学习有很长的历史，至少可以追溯到七十年代和象征主义技术，如逆向推导。但随着互联网的出现，它获得了新的推动力。突然间，网络无处不在，对它们进行建模是很紧迫的。我发现一个特别有趣的现象是口碑。信息是如何在社会网络中传播的？我们能不能衡量每个成员的影响力，并瞄准足够多的最有影响力的成员来掀起一波口碑传播？我和我的学生马特·理查德森一起设计了一种算法，就是这样做的。我们将其应用于 Epinions，一个允许会员说出他们信任的评论的产品评论网站。我们发现，除其他外，向最具影响力的单一会员 —— 受到许多追随者的信任，而这些追随者又受到许多其他人的信任，以此类推 —— 推销产品的效果与向所有会员中的三分之一单独推销产品的效果一样好。随后，关于这个问题的其他研究如雨后春笋般涌现。从那时起，我将关系学习应用于许多其他方面，包括预测谁将在社会网络中形成链接，整合数据库，以及使机器人能够建立其周围环境的地图。</p>

<p class="noindent english"><a id="babilu_link-82"></a> If you want to understand how the world works, relational learning is a good tool to have. In Isaac Asimov’s <i>Foundation</i> , the scientist Hari Seldon manages to mathematically predict the future of humanity and thereby save it from decadence. Paul Krugman, among others, has confessed that this seductive dream was what made him become an economist. According to Seldon, people are like molecules in a gas, and the law of large numbers ensures that even if individuals are unpredictable, whole societies aren’t. Relational learning reveals why this is not the case. If people were independent, each making decisions in isolation, societies would indeed be predictable, because all those random decisions would add up to a fairly constant average. But when people interact, larger assemblies can be less predictable than smaller ones, not more. If confidence and fear are contagious, each will dominate for a while, but every now and then an entire society will swing from one to the other. It’s not all bad news, though. If we can measure how strongly people influence each other, we can estimate how long it will be before a swing occurs, even if it’s the first one—another way in which black swans are not necessarily unpredictable.</p>

<p class="noindent chinese">如果你想了解世界是如何运作的，关系学习是一个很好的工具。在艾萨克·阿西莫夫的《<i>基础</i>》中，科学家哈瑞·塞尔登成功地用数学方法预测了人类的未来，从而将其从颓废中拯救出来。保罗·克鲁格曼等人承认，这个诱人的梦想是使他成为一名经济学家的原因。根据塞尔登的说法，人就像气体中的分子，大数法则确保即使个人是不可预测的，整个社会也是不可预测的。关系学习揭示了为什么情况不是这样的。如果人们是独立的，每个人都在孤立地做决定，社会确实是可预测的，因为所有这些随机的决定加起来是一个相当稳定的平均值。但是，当人们互动时，更大的集会可能比小集会的可预测性更低，而不是更高。如果信心和恐惧是可以传染的，那么每一种都会在一段时间内占主导地位，但每当这个时候，整个社会就会从一个摆到另一个。不过，这并不全是坏消息。如果我们能够衡量人们对彼此的影响有多大，我们就可以估计出多久后才会发生摇摆，即使这是第一次摇摆，这也是黑天鹅不一定不可预测的另一种方式。</p>

<p class="noindent english">A common complaint about big data is that the more data you have, the easier it is to find spurious patterns in it. This may be true if the data is just a huge set of disconnected entities, but if they’re interrelated, the picture changes. For example, critics of using data mining to catch terrorists argue that, ethical issues aside, it will never work because there are too many innocents and too few terrorists and so mining for suspicious patterns will either cause too many false alarms or never catch anyone. Is someone videotaping the New York City Hall a tourist or a terrorist scoping out a bombing site? And is someone buying large quantities of ammonium nitrate a farmer or a bomb maker? Each of these looks innocent enough in isolation, but if the “tourist” and the “farmer” have been in close phone contact, and the latter just drove his heavily laden pickup into Manhattan, maybe it’s time for someone to take a closer look. The NSA likes to mine records of who called whom not just because it’s arguably legal, but because they’re often more <a id="babilu_link-167"></a> informative to the prediction algorithms than the content of the calls, which it would take a human to understand.</p>

<p class="noindent chinese">关于大数据的一个常见抱怨是，你拥有的数据越多，就越容易在其中找到虚假的模式。如果数据只是一组巨大的互不相干的实体，这可能是真的，但如果它们是相互关联的，情况就会改变。例如，批评使用数据挖掘来抓捕恐怖分子的人认为，撇开道德问题不谈，这永远不会奏效，因为无辜者太多，恐怖分子太少，所以挖掘可疑的模式要么会造成太多的错误警报，要么永远抓不到人。对纽约市政厅进行录像的人是游客还是恐怖分子在寻找爆炸地点？而购买大量硝酸铵的人是农民还是炸弹制造者？每一个人在孤立的情况下看起来都很无辜，但如果 “游客” 和 “农民” 一直有密切的电话联系，而且后者刚刚把他的重载皮卡开进曼哈顿，也许是时候让人仔细看看了。国家安全局喜欢挖掘谁给谁打电话的记录，不仅仅是因为这可以说是合法的，还因为它们对预测算法来说往往比通话内容更有参考价值，而后者需要人去理解。</p>

<p class="noindent english">Social networks aside, the killer app of relational learning is understanding how living cells work. A cell is a complex metabolic network with genes coding for proteins that regulate other genes, long interlocking chains of chemical reactions, and products migrating from one organelle to another. Independent entities, doing their work in isolation, are nowhere to be seen. A cancer drug must disrupt cancer cells’ workings without interfering with normal ones’. If we have an accurate relational model of both, we can try many different drugs <i>in silico</i> , letting the model infer their good and bad effects and keeping only the best ones to try <i>in vitro</i> and finally <i>in vivo</i> .</p>

<p class="noindent chinese">撇开社交网络不谈，关系学习的杀手级应用是了解活体细胞如何工作。细胞是一个复杂的新陈代谢网络，其基因为调节其他基因的蛋白质编码，化学反应的长链环环相扣，产品从一个细胞器迁移到另一个。独立的实体，孤立地做他们的工作，是无处可寻的。一种抗癌药物必须在不干扰正常细胞的情况下破坏癌细胞的工作。如果我们对这两者有一个准确的关系模型，我们就可以<i>在硅谷</i>尝试许多不同的药物，让模型推断它们的好坏影响，只保留最好的药物在<i>体外</i>尝试，最后<i>在体内</i>尝试。</p>

<p class="noindent english">Like human memory, relational learning weaves a rich web of associations. It connects percepts, which a robot like Robby can acquire by clustering and dimensionality reduction, with skills, which he can learn by reinforcement and chunking, and with the higher-level knowledge that comes from reading, going to school, and interacting with humans. Relational learning is the last piece of the puzzle, the final ingredient we need for our alchemy. And now it’s time to repair to the lab and transmute all these elements into the Master Algorithm.</p>

<p class="noindent chinese">像人类的记忆一样，关系学习编织了一个丰富的关联网络。它将像罗比这样的机器人可以通过聚类和降维获得的概念与他可以通过强化和分块学习的技能，以及来自阅读、上学和与人类互动的更高层次的知识联系起来。关系学习是拼图的最后一块，是我们的炼金术需要的最后成分。现在是时候修复实验室，将所有这些元素转化为主算法了。</p>

</section>

</div>

</div>

<div id="babilu_link-2">

<div>

<section id="babilu_link-12">

<h1><a id="babilu_link-218"></a> <a href="#babilu_link-3">CHAPTER NINE</a></h1>

<h1><a id="babilu_link-218"></a> <a href="#babilu_link-3">第九章</a></h1>

<h1><a href="#babilu_link-3">The Pieces of the Puzzle Fall into Place</a></h1>

<h1><a href="#babilu_link-3">拼图的碎片落到了地上</a></h1>

<p class="noindent english">Machine learning is both a science and a technology, and both characteristics give us hints on how to unify it. On the science side, unifying theories often begin with a deceptively simple observation. Two seemingly unrelated phenomena turn out to be just two faces of the same coin, and like the first domino to fall, that realization sets off a cascade of others. An apple falling to the ground, the moon hanging in the sky: both are caused by gravity, and—apocryphal story or not—once Newton figured out how, gravity turned out to also account for the tides, the precession of the equinoxes, the trajectories of comets, and much else. In everyday experience, electricity and magnetism are never seen together: a lightning spark here, a rock that attracts iron objects there, both quite rare. But once Maxwell figured out how a changing electric field gives rise to magnetism and vice versa, it became clear that light itself is an intimate marriage of the two, and today we know that, far from rare, electromagnetism pervades all matter. Mendeleev’s periodic table not only organized all the known elements into just two dimensions, it also predicted where new elements would be found. Darwin’s observations aboard the <i>Beagle</i> suddenly began to make sense when Malthus’s <i>Essay on Population</i> suggested natural selection as the organizing principle. <a id="babilu_link-197"></a> When Crick and Watson hit on the double helix structure as an explanation for the puzzling properties of DNA, they immediately saw how it might replicate itself, and biology’s transition from stamp collecting (in Rutherford’s pejorative words) to unified science had begun. In each of these cases, a bewildering variety of observations turned out to have a common cause, and once scientists identified it, they could in turn use it to predict many new phenomena. Similarly, even though the learners we’ve met in this book seem quite disparate—some based on the brain, some on evolution, some on abstract mathematical principles—they in fact have much in common, and the resulting theory of learning yields many new insights.</p>

<p class="noindent chinese">机器学习既是一门科学，也是一门技术，这两个特点都给了我们如何统一它的提示。在科学方面，统一的理论往往从一个看似简单的观察开始。两个看似不相关的现象原来只是同一枚硬币的两面，就像第一张多米诺骨牌倒下一样，这一认识引发了一连串的其他现象。一个苹果掉到地上，月亮挂在天上：两者都是由引力引起的，而且 —— 不管是否是虚构的故事 —— 一旦牛顿弄清楚了方法，引力也被证明可以解释潮汐、赤道的前行、彗星的轨迹，以及其他许多东西。在日常经验中，电和磁从来没有一起出现过：这里有一个闪电火花，那里有一块吸引铁器的石头，两者都很罕见。但是，一旦麦克斯韦弄清了变化的电场是如何产生磁力的，反之亦然，就很清楚，光本身就是两者的亲密结合，而今天我们知道，电磁学远非罕见，它充斥着所有物质。门捷列夫的元素周期表不仅将所有已知的元素整理成两个维度，而且还预测了新元素的发现地点。当马尔萨斯的《<i>人口论</i>》提出自然选择是组织原则时，达尔文在<i>贝格尔</i>号上的观察突然开始有了意义。当克里克和沃森发现双螺旋结构是对 DNA 令人困惑的特性的解释时，他们立即看到了它如何进行自我复制，生物学从集邮（用卢瑟福的贬义词）到统一科学的转变已经开始。在每一个案例中，各种令人困惑的观察结果都有一个共同的原因，一旦科学家确定了这个原因，他们又可以用它来预测许多新现象。同样，即使我们在本书中遇到的学习者看起来很不一样 —— 有些基于大脑，有些基于进化，有些基于抽象的数学原理 —— 他们实际上有很多共同点，由此产生的学习理论产生了很多新的见解。</p>

<p class="noindent english">Although it is less well known, many of the most important technologies in the world are the result of inventing a unifier, a single mechanism that does what previously required many. The Internet, as the name implies, is a network that interconnects networks. Without it, every type of network would need a different protocol to talk to every other, much like we need a different dictionary for every pair of languages in the world. The Internet’s protocols are an Esperanto that gives each computer the illusion of talking directly to any other and that allows e-mail and the web to ignore the details of the physical infrastructure they flow over. Relational databases do something similar for enterprise applications, allowing developers and users to think in terms of the abstract relational model and ignore the different ways computers go about answering queries. A microprocessor is an assembly of digital electronic components that can mimic any other assembly. Virtual machines allow the same computer to pose as a hundred different computers to a hundred different people at the same time, and help make the cloud possible. Graphical user interfaces let us edit documents, spreadsheets, slide decks, and much else using a common language of windows, menus, and mouse clicks. The computer itself is a unifier: a single device capable of solving any logical or mathematical problem, provided we know how to program it. Even plain old electricity is a kind of unifier: you can generate it from many different sources—coal, gas, nuclear, hydro, wind, solar—and consume it in an infinite variety of ways. <a id="babilu_link-230"></a> A power station doesn’t know or care how the electricity it produces will be consumed, and your porch light, dishwasher, or brand-new Tesla are oblivious to where their electricity supply comes from. Electricity is the Esperanto of energy. The Master Algorithm is the unifier of machine learning: it lets any application use any learner, by abstracting the learners into a common form that is all the applications need to know.</p>

<p class="noindent chinese">虽然不太为人所知，但世界上许多最重要的技术都是发明统一器的结果，一个单一的机制可以做以前需要很多的事情。互联网，顾名思义，是一个将网络互连的网络。如果没有它，每一种类型的网络都需要不同的协议来与其他网络对话，就像我们需要为世界上的每一对语言编写不同的字典一样。互联网的协议是一种世界语，使每台计算机都有一种直接与其他计算机对话的错觉，并允许电子邮件和网络忽略它们流经的物理基础设施的细节。关系型数据库为企业应用做了类似的事情，允许开发者和用户在抽象的关系模型方面进行思考，而忽略了计算机回答查询的不同方式。一个微处理器是一个数字电子元件的组件，可以模仿任何其他组件。虚拟机允许同一台计算机在同一时间向一百个不同的人冒充一百台不同的计算机，并有助于使云成为可能。图形用户界面让我们能够使用一种由窗口、菜单和鼠标点击组成的通用语言来编辑文档、电子表格、幻灯片和其他许多东西。计算机本身就是一个统一体：一个能够解决任何逻辑或数学问题的单一设备，只要我们知道如何对其进行编程。即使是普通的电也是一种统一体：你可以从许多不同的来源产生它 —— 煤炭、天然气、核能、水力、风能、太阳能 —— 并以无限的方式消耗它。一个发电站不知道也不关心它产生的电将如何被消耗，而你的门廊灯、洗碗机或全新的特斯拉都不知道它们的电力供应来自哪里。电力是能源的世界语。主算法是机器学习的统一体：它让任何应用程序使用任何学习者，通过将学习者抽象为一种共同的形式，这是所有应用程序需要知道的。</p>

<p class="noindent english">Our first step toward the Master Algorithm will be surprisingly simple. As it turns out, it’s not hard to combine many different learners into one, using what is known as metalearning. Netflix, Watson, Kinect, and countless others use it, and it’s one of the most powerful arrows in the machine learner’s quiver. It’s also a stepping-stone to the deeper unification that will follow.</p>

<p class="noindent chinese">我们迈向主算法的第一步将出奇地简单。事实证明，利用所谓的 metalearning，将许多不同的学习者结合为一体并不难。Netflix、Watson、Kinect 和无数的其他公司都在使用它，它是机器学习者的箭筒中最强大的箭之一。这也是通往后续更深层次统一的垫脚石。</p>

<h1 id="babilu_link-443"><b>Out of many models, one</b></h1>

<h1 id="babilu_link-443"><b>在许多模型中，有那么一个</b></h1>

<p class="noindent english">Here’s a challenge: you have fifteen minutes to combine decision trees, multilayer perceptrons, classifier systems, Naïve Bayes, and SVMs into a single algorithm possessing the best properties of each. Quick—what can you do? Clearly, it can’t involve the details of the individual algorithms; there’s no time for that. But how about the following? Think of each learner as an expert on a committee. Each looks carefully at the instance to be classified—what is the diagnosis for this patient?—and confidently makes its prediction. You’re not an expert yourself, but you’re the chair of the committee, and your job is to combine their recommendations into a final decision. What you have on your hands is in fact a new classification problem, where instead of the patient’s symptoms, the input is the experts’ opinions. But you can apply machine learning to this problem in the same way the experts applied it to the original one. We call this metalearning because it’s learning about the learners. The metalearner can itself be any learner, from a decision tree to a simple weighted vote. To learn the weights, or the decision tree, we replace the attributes of each original example by the learners’ predictions. Learners that often predict the correct class will get high weights, and inaccurate ones will tend to be ignored. With a decision tree, the choice of whether <a id="babilu_link-106"></a> to use a learner can be contingent on other learners’ predictions. Either way, to obtain a learner’s prediction for a given training example, we must first apply it to the original training set <i>excluding that example</i> and use the resulting classifier—otherwise the committee risks being dominated by learners that overfit, since they can predict the correct class just by remembering it. The Netflix Prize winner used metalearning to combine hundreds of different learners. Watson uses it to choose its final answer from the available candidates. Nate Silver combines polls in a similar way to predict election results.</p>

<p class="noindent chinese">这里有一个挑战：你有 15 分钟的时间将决策树、多层感知器、分类器系统、天真贝叶斯和 SVMs 结合到一个拥有各自最佳特性的单一算法中。快 —— 你能做什么？显然，它不能涉及各个算法的细节；没有时间去做这个。但下面的方法如何呢？把每个学习者看作是一个委员会的专家。每个人都仔细观察要分类的实例 —— 这个病人的诊断是什么 —— 并自信地做出预测。你自己不是专家，但你是委员会的主席，你的工作是把他们的建议合并成一个最终决定。你手上的问题实际上是一个新的分类问题，输入的不是病人的症状，而是专家的意见。但你可以将机器学习应用于这个问题，就像专家应用于原来的问题一样。我们把这称为金属学习，因为它是关于学习者的学习。金属学习器本身可以是任何学习器，从决策树到简单的加权投票。为了学习权重或决策树，我们用学习者的预测取代每个原始例子的属性。经常预测正确类别的学习者将获得较高的权重，而不准确的学习者将倾向于被忽略。在决策树中，是否使用一个学习者的选择可以取决于其他学习者的预测。无论哪种方式，要想获得一个学习者对某一训练实例的预测，我们必须首先将其应用于<i>不包括该实例</i>的原始训练集，并使用产生的分类器 —— 否则，委员会就有可能被过度拟合的学习者所支配，因为他们只要记住正确的类别就可以预测。Netflix 奖得主使用 metalearning 来结合数百个不同的学习者。沃森用它来从现有的候选人中选择最终答案。内特·西尔弗以类似的方式结合民意调查来预测选举结果。</p>

<p class="noindent english">This type of metalearning is called stacking and is the brainchild of David Wolpert, whom we met in <a href="#babilu_link-4">Chapter 3</a> as the author of the “no free lunch” theorem. An even simpler metalearner is bagging, invented by the statistician Leo Breiman. Bagging generates random variations of the training set by resampling, applies the same learner to each one, and combines the results by voting. The reason to do this is that it reduces variance: the combined model is much less sensitive to the vagaries of the data than any single one, making this a remarkably easy way to improve accuracy. If the models are decision trees and we further vary them by withholding a random subset of the attributes from consideration at each node, the result is a so-called random forest. Random forests are some of the most accurate classifiers around. Microsoft’s Kinect uses them to figure out what you’re doing, and they regularly win machine-learning competitions.</p>

<p class="noindent chinese">这种类型的金属学习被称为堆积，是大卫·沃珀特的创意，我们在<a href="#babilu_link-4">第三章</a>中见过他，他是 “没有免费午餐” 定理的作者。一个更简单的金属学习法是袋法，由统计学家利奥·布雷曼发明的。布袋法通过重新抽样产生训练集的随机变化，对每个学习者应用相同的学习器，并通过投票将结果合并。这样做的原因是，它可以减少方差：与任何一个单一的模型相比，综合模型对数据的变化不那么敏感，这使得它成为提高准确性的一个非常容易的方法。如果模型是决策树，并且我们通过在每个节点不考虑一个随机的属性子集来进一步改变它们，结果就是所谓的随机森林。随机森林是一些最准确的分类器。微软的 Kinect 使用它们来弄清你在做什么，而且它们经常赢得机器学习比赛。</p>

<p class="noindent english">One of the cleverest metalearners is boosting, created by two learning theorists, Yoav Freund and Rob Schapire. Instead of combining different learners, boosting repeatedly applies the same classifier to the data, using each new model to correct the previous ones’ mistakes. It does this by assigning weights to the training examples; the weight of each misclassified example is increased after each round of learning, causing later rounds to focus more on it. The name <i>boosting</i> comes from the notion that this process can boost a classifier that’s only slightly better than random guessing, but consistently so, into one that’s almost perfect.</p>

<p class="noindent chinese">最聪明的金属学习法之一是提升法，由两位学习理论家约阿夫·弗罗伊德和罗伯·沙佩尔创造。提升法不是结合不同的学习者，而是将同一个分类器重复应用于数据，用每个新的模型来纠正之前的错误。它通过给训练实例分配权重来做到这一点；每轮学习后，每个分类错误的实例的权重都会增加，导致后面的几轮学习更加关注它。<i>提升</i>的名字来自于这样一个概念：这个过程可以将一个只比随机猜测稍好的分类器提升为一个几乎完美的分类器，但也是持续的。</p>

<p class="noindent english"><a id="babilu_link-246"></a> Metalearning is remarkably successful, but it’s not a very deep way to combine models. It’s also expensive, requiring as it does many runs of learning, and the combined models can be quite opaque. (“I believe you have prostate cancer because the decision tree, the genetic algorithm, and Naïve Bayes say so, although the multilayer perceptron and the SVM disagree.”) Moreover, all the combined models are really just one big, messy model. Can’t we have a single learner that does the same job? Yes we can.</p>

<p class="noindent chinese">Metalearning 是非常成功的，但它不是一个非常深入的组合模型的方法。它也很昂贵，需要进行多次学习，而且组合的模型可能是相当不透明的。（“我相信你有前列腺癌，因为决策树、遗传算法和天真贝叶斯这么说，尽管多层感知器和 SVM 不同意。”）此外，所有的组合模型实际上只是一个大的、混乱的模型。难道我们不能有一个单一的学习者来做同样的工作吗？是的，我们可以。</p>

<h1 id="babilu_link-444"><b>The Master Algorithm</b></h1>

<h1 id="babilu_link-444"><b>主算法</b></h1>

<p class="noindent english">Our unified learner is perhaps best introduced through an extended allegory. If machine learning is a continent divided into the territories of the five tribes, the Master Algorithm is its capital city, standing on the unique spot where the five territories meet. As you approach it from a distance, you can see that the city is made up of three concentric circles, each bounded by a wall. The outer and by far widest circle is Optimization Town. Each house here is an algorithm, and they come in all shapes and sizes. Some are under construction, the locals busy around them; some are gleaming new; and some look old and abandoned. Higher up the hill lies the Citadel of Evaluation. From its mansions and palaces orders issue continuously to the algorithms below. Above all, silhouetted against the sky, rise the Towers of Representation. Here live the rulers of the city. Their immutable laws set forth what can and cannot be done not just in the city but throughout the continent. Atop the central, tallest tower flies the flag of the Master Algorithm, red and black, with a five-pointed star surrounding an inscription that you cannot yet make out.</p>

<p class="noindent chinese">通过一个扩展的寓言故事来介绍我们的统一的学习者也许是最好的。如果机器学习是一块大陆，分为五个部落的领土，那么主算法就是它的首都，站在五个领土交汇的独特位置。当你从远处走近它时，你可以看到这座城市是由三个同心圆组成的，每个圆都有一堵墙为界。外圈和迄今为止最宽的圈是优化镇。这里的每栋房子都是一种算法，它们有各种形状和大小。有些正在建设中，当地人在周围忙碌着；有些是闪闪发光的新房子；有些看起来很旧，被遗弃。山上更高的地方是评价的城堡。从它的豪宅和宫殿中，不断有命令传到下面的算法中。在所有的上方，在天空的映衬下，耸立着代表之塔。这里住着这个城市的统治者。他们不变的法律规定了什么可以做，什么不可以做，不仅在城市，而且在整个大陆。在中央最高的塔顶上，飘扬着主算法的旗帜，红黑相间，一个五角星围绕着一个你还看不出来的铭文。</p>

<p class="noindent english">The city is divided into five sectors, each belonging to one of the five tribes. Each sector stretches down from its Tower of Representation to the city’s outer walls, encompassing the tower, a clutch of palaces in the Citadel of Evaluation, and the streets and houses in Optimization Town they overlook. The five sectors and three rings divide the city into fifteen districts, fifteen shapes, fifteen pieces of the puzzle you need to solve:</p>

<p class="noindent chinese">这座城市被划分为五个区，每个区都属于五个部落之一。每个区从代表塔一直延伸到城市的外墙，包括代表塔、评价城堡中的一系列宫殿，以及它们所俯瞰的优化镇的街道和房屋。五个区和三个环将城市分为十五个区，十五个形状，十五块你需要解决的拼图。</p>

<div>

<div>

<img alt="image" src="images/000002.jpg"/>

</div>

</div>

<p class="noindent english"><a id="babilu_link-67"></a> You gaze intently at the map, trying to decipher its secret. The fifteen pieces all match quite precisely, but you need to figure out how they combine to form just three: the representation, evaluation, and optimization components of the Master Algorithm. Every learner has these three elements, but they vary from tribe to tribe.</p>

<p class="noindent chinese">你紧紧盯着地图，试图破译它的秘密。十五块碎片都非常精确地匹配，但你需要弄清楚它们是如何组合成三个：主算法的表示、评估和优化部分。每个学习者都有这三个要素，但它们因部落而异。</p>

<p class="noindent english">Representation is the formal language in which the learner expresses its models. The symbolists’ formal language is logic, of which rules and decision trees are special cases. The connectionists’ is neural networks. The evolutionaries’ is genetic programs, including classifier systems. The Bayesians’ is graphical models, an umbrella term for Bayesian networks and Markov networks. The analogizers’ is specific instances, possibly with weights, as in an SVM.</p>

<p class="noindent chinese">表征是学习者表达其模型的形式语言。符号主义者的形式语言是逻辑，其中规则和决策树是特殊情况。连接主义者的语言是神经网络。进化论者的语言是遗传程序，包括分类器系统。贝叶斯派是图形模型，是贝叶斯网络和马尔科夫网络的总称。模拟者是具体的实例，可能带有权重，如 SVM。</p>

<p class="noindent english"><a id="babilu_link-25"></a> The evaluation component is a scoring function that says how good a model is. Symbolists use accuracy or information gain. Connectionists use a continuous error measure, such as squared error, which is the sum of the squares of the differences between the predicted values and the true ones. Bayesians use the posterior probability. Analogizers (at least of the SVM stripe) use the margin. In addition to how well the model fits the data, all tribes take into account other desirable properties, such as the model’s simplicity.</p>

<p class="noindent chinese">评价部分是一个打分函数，说一个模型有多好。符号主义者使用准确性或信息增益。连接主义者使用连续的误差测量，如平方误差，即预测值和真实值之间的差异的平方之和。贝叶斯主义者使用后验概率。类比者（至少是 SVM 的类比者）使用边际。除了模型对数据的拟合程度外，所有部落都考虑到其他理想的属性，如模型的简单性。</p>

<p class="noindent english">Optimization is the algorithm that searches for the highest-scoring model and returns it. The symbolists’ characteristic search algorithm is inverse deduction. The connectionists’ is gradient descent. The evolutionaries’ is genetic search, including crossover and mutation. The Bayesians are unusual in this regard: they don’t just look for the best model, but average over all models, weighted by how probable they are. To do the weighting efficiently, they use probabilistic inference algorithms like MCMC. The analogizers (or more precisely, the SVM mavens) use constrained optimization to find the best model.</p>

<p class="noindent chinese">优化是搜索得分最高的模型并将其返回的算法。符号派的特色搜索算法是反推法。连接主义者的是梯度下降法。进化论者的是遗传搜索，包括交叉和变异。贝叶斯派在这方面是不寻常的：他们不只是寻找最好的模型，而是对所有的模型进行平均，根据它们的可能性进行加权。为了有效地进行加权，他们使用 MCMC 等概率推理算法。类比者（或更准确地说，SVM 大师）使用约束性优化来寻找最佳模型。</p>

<p class="noindent english">After a long day’s journey, the sun is rapidly nearing the horizon, and you need to hurry before it gets dark. The city’s outer wall has five massive gates, each controlled by one of the tribes and leading to its district in Optimization Town. Let us enter through the Gradient Descent Gate, after whispering the watchword—“deep learning”—to the guard, and spiral in toward the Towers of Representation. From the gate the street ascends steeply up the hill to the citadel’s Squared Error Gate, but instead you turn left toward the evolutionary sector. The houses in the gradient descent district are all smooth curves and densely intertwined patterns, almost more like a jungle than a city. But when gradient descent gives way to genetic search, the picture changes abruptly. Here the houses rise higher, with structure piled on structure, but the structures are spare, almost vacant, as if waiting to be filled in by gradient descent’s curves. That’s it: the way to combine the two is to use genetic search to find the structure of the model and let gradient descent fill in its parameters. This is what nature does: evolution creates brain structures, and individual experience modulates them.</p>

<p class="noindent chinese">经过一天的长途跋涉，太阳迅速接近地平线，你需要赶在天黑之前。城市的外墙有五个巨大的门，每个门都由一个部落控制，通向优化镇的区。让我们从梯度下降门进入，在对守卫小声说了 “深度学习” 之后，向代表之塔螺旋式前进。从大门开始，街道陡然上山，通往城堡的平方误差门，但你却向左转，走向进化区。梯度下降区的房屋都是平滑的曲线和密集交织的图案，几乎更像丛林而不是城市。但是当梯度下降让位于遗传搜索时，情况就会突然改变。在这里，房子升得更高，结构堆积在结构上，但结构是空闲的，几乎是空置的，仿佛等待着被梯度下降的曲线填满。就是这样：将两者结合起来的方法是使用遗传搜索来找到模型的结构，然后让梯度下降填补其参数。这就是自然界所做的：进化创造了大脑结构，而个人经验则对其进行调制。</p>

<p class="noindent english"><a id="babilu_link-127"></a> The first step accomplished, you hurry on to the Bayesian district. Even from a distance, you can see how it clusters around the Cathedral of Bayes’ Theorem. MCMC Alley zigzags randomly along the way. This is going to take a while. You take a shortcut onto Belief Propagation Street, but it seems to loop around forever. Then you see it: the Most Likely Avenue, rising majestically toward the Posterior Probability Gate. Rather than average over all models, you can head straight for the most probable one, confident that the resulting predictions will be almost the same. And you can let genetic search pick the model’s structure and gradient descent its parameters. With a sigh of relief, you realize that’s all the probabilistic inference you’ll need, at least until it’s time to answer questions using the model.</p>

<p class="noindent chinese">第一步完成了，你赶紧去贝叶斯区。即使从远处看，你也能看到它是如何聚集在贝叶斯定理大教堂的周围。MCMC 小巷沿途随机曲折。这要花点时间。你走了一条捷径到信仰传播街，但它似乎永远都在绕圈。然后你看到了它：最可能大道，雄伟地朝着后验概率门上升。与其在所有模型上求平均值，你可以直奔最可能的模型，相信由此产生的预测结果几乎是一样的。你可以让遗传搜索选择模型的结构和梯度下降的参数。你松了一口气，意识到这就是你需要的所有概率推理，至少在使用该模型回答问题之前是这样。</p>

<p class="noindent english">You keep going. The constrained optimization district is a maze of narrow alleys and dead ends, examples of all kinds standing cheek by jowl everywhere, with an occasional clearing around a support vector. Clearly, all you need to do to avoid bumping into examples of the wrong class is add constraints to the optimizer you’ve already assembled. But come to think of it, not even that is necessary. When we learn SVMs, we usually let margins be violated in order to avoid overfitting, provided each violation pays a penalty. In this case the optimal example weights can again be learned by a form of gradient descent. That was easy. You feel like you’re starting to get the hang of it.</p>

<p class="noindent chinese">你继续走。受限优化区是一个由狭窄的小巷和死胡同组成的迷宫，各种各样的例子比比皆是，偶尔会在一个支持向量周围出现空地。很明显，你需要做的就是在你已经组装好的优化器上添加约束条件，以避免撞上错误的例子。但仔细想想，甚至没有必要这样做。当我们学习 SVM 时，为了避免过度拟合，我们通常会让边际值被违反，前提是每次违反都要付出惩罚。在这种情况下，最佳的例子权重可以再次通过梯度下降的形式来学习。这很容易。你感觉你已经开始掌握它了。</p>

<p class="noindent english">The dense ranks of instances end abruptly, and you find yourself in the inverse deduction district, a place of broad avenues and ancient stone buildings. The architecture here is geometric, austere, made of straight lines and right angles. Even the severely pruned trees have rectangular trunks, and their leaves are meticulously labeled with class predictions. The denizens of this district seem to build their houses in a peculiar way: they start with the roof, which they label “Conclusions,” and gradually fill in the gaps between it and the ground, which they label “Premises.” One by one, they find a stone block that’s the right shape to fill in a particular gap and hoist it up to its place. But, you notice, many gaps have the same shape, and it would be faster to cut and combine blocks until they form that shape, and then repeat the process as <a id="babilu_link-26"></a> many times as necessary. In other words, you could use genetic search to do inverse deduction. Neat. It looks like you’ve boiled down the five optimizers to a simple recipe: genetic search for structure and gradient descent for parameters. And even that may be overkill. For a lot of problems, you can whittle genetic search down to hill climbing if you do three things: leave out crossover, try all possible point mutations in each generation, and always select the single best hypothesis to seed the next generation.</p>

<p class="noindent chinese">密集的实例行列突然结束，你发现自己在反向推理区，一个有宽阔大道和古老石头建筑的地方。这里的建筑是几何学的，朴素的，由直线和直角组成。即使是被严重修剪的树木也有矩形的树干，它们的叶子上都一丝不苟地贴着等级预测的标签。这个地区的居民似乎以一种奇特的方式建造他们的房子：他们从屋顶开始，标记为 “结论”，然后逐渐填补屋顶和地面之间的空隙，标记为 “前提”。他们一个接一个地找到形状合适的石块来填补某个特定的缝隙，然后把它吊到它的位置上。但是，你注意到，许多缝隙都有相同的形状，如果切割和组合石块，直到它们形成这种形状，然后根据需要多次重复这一过程，会更快。换句话说，你可以用遗传搜索来做反推理。很好。看起来你已经把五个优化器归结为一个简单的配方：结构的遗传搜索和参数的梯度下降。即使这样做也可能是矫枉过正。对于很多问题，如果你做了三件事，你可以把遗传搜索简化为爬坡：不考虑交叉，在每一代中尝试所有可能的点突变，并且总是选择单一的最佳假设作为下一代的种子。</p>

<p class="noindent english">What’s that statue up ahead? Aristotle, looking rather disapprovingly toward the tangled mess of the gradient descent quarter. You’ve come full circle. You have the unified optimizer you need for the Master Algorithm, but this is no time to congratulate yourself. Night has fallen, and you still have much to do. You enter the Citadel of Evaluation through the imposing but rather narrow Accuracy Gate. The inscription above it says “Abandon all hope of overfitting, ye who enter here.” As you circle past the palaces of the five tribes’ evaluators, you mentally snap the pieces into place. You use accuracy to evaluate yes-or-no predictions and squared error for continuous ones. Fitness is just the evolutionaries’ name for the scoring function; you can make it anything you want, including accuracy and squared error. Posterior probability reduces to squared error if you ignore the prior probability and the errors follow a normal distribution. The margin, if you allow it to be violated for a price, becomes a softer version of accuracy: instead of paying no penalty for a correct prediction and a penalty of one for an incorrect prediction, the penalty is zero until you get inside the margin, at which point it starts to steadily go up. Whew! Combining the evaluators was a lot easier than combining the optimizers. But the Towers of Representation, looming above you, fill you with a sense of foreboding.</p>

<p class="noindent chinese">前面的雕像是什么？亚里士多德，相当不赞同地看向纠缠在一起的梯度下降四分之一。你已经走了一圈。你有了主算法所需的统一优化器，但现在还不是祝贺自己的时候。夜幕已经降临，而你还有很多事情要做。你通过雄伟但相当狭窄的精确性大门进入评估城堡。门上的铭文写道：“进入这里的人，放弃所有超额的希望。” 当你绕过五个部落的评估者的宫殿时，你在心理上将这些碎片归位。你用准确性来评估是或不是的预测，用平方误差来评估连续的预测。适应性只是进化者对评分函数的称呼；你可以把它变成任何你想要的东西，包括准确性和平方误差。如果你忽略了先验概率，并且误差遵循正态分布，那么后验概率就会减少为误差平方。如果你允许它被违反的价格，保证金就变成了准确性的柔和版本：而不是对正确的预测不付惩罚，对错误的预测惩罚 1，惩罚是 0，直到你进入保证金内，在这一点上它开始稳步上升。呜呼！合并评估器比合并优化器要容易得多。但是，代表之塔在你头顶上若隐若现，让你充满了一种不祥的预感。</p>

<p class="noindent english">You’ve reached the final stage of your quest. You knock on the door of the Tower of Support Vectors. A menacing-looking guard opens it, and you suddenly realize that you don’t know the password. “Kernel,” you blurt out, trying to keep the panic from your voice. The guard bows and steps aside. Regaining your composure, you step in, mentally kicking yourself for your carelessness. The entire ground floor of the tower <a id="babilu_link-259"></a> is taken up by a lavishly appointed circular chamber, with what seems to be a marble representation of an SVM occupying pride of place at the center. As you walk around it, you notice a door on the far side. It must lead to the central tower—the Tower of the Master Algorithm. The door seems unguarded. You decide to take a shortcut. Slipping through the doorway, you walk down a short corridor and find yourself in an even larger pentagonal chamber, with a door in each wall. In the center, a spiral staircase rises as high as the eye can see. You hear voices above and duck into the doorway opposite. This one leads to the Tower of Neural Networks. Once again you’re in a circular chamber, this one with a sculpture of a multilayer perceptron as the centerpiece. Its parts are different from the SVM’s, but their arrangement is remarkably similar. Suddenly you see it: an SVM is just a multilayer perceptron with a hidden layer composed of kernels instead of S curves and an output that’s a linear combination instead of another S curve.</p>

<p class="noindent chinese">你已经到达了任务的最后阶段。你敲响了支持载体之塔的门。一个看起来气势汹汹的警卫打开了门，而你突然意识到你不知道密码。“Kernel”，你大声说，试图让自己的声音不那么惊慌。警卫鞠了一躬，走到一边。恢复平静后，你走了进去，在心理上为自己的粗心而自责。塔楼的整个底层被一个豪华的圆形房间占据，中间似乎是一个大理石的 SVM 代表，占据了最重要的位置。当你绕着它走时，你注意到远处有一扇门。它一定是通向中央塔楼 —— 主算法之塔。这扇门似乎无人看守。你决定走一条捷径。穿过门洞，你走过一条短的走廊，发现自己在一个更大的五边形房间里，每面墙上都有一个门。在中央，一个螺旋形的楼梯一直上升到肉眼可见的高度。你听到上面有声音，于是躲进对面的门洞。这扇门通往神经网络塔。你又一次进入了一个圆形的房间，这个房间的中心是一个多层感知器的雕塑。它的部件与 SVM 的不同，但它们的排列方式却非常相似。突然间你明白了：SVM 只是一个多层感知器，它的隐藏层是由核而不是 S 曲线组成的，输出是一个线性组合而不是另一个 S 曲线。</p>

<p class="noindent english">Could it be that the other representations also have a similar form? With rising excitement, you run back through the pentagonal chamber and into the Tower of Logic. Staring at the depiction of a set of rules in the center, you try to discern a pattern. Yes! Each rule is just a highly stylized neuron. For example, the rule <i>If it’s a giant reptile and breathes fire then it’s a dragon</i> is just a perceptron with weights of one for <i>it’s a giant reptile</i> and <i>breathes fire</i> and a threshold of 1.5. And a set of rules is a multilayer perceptron with a hidden layer containing one neuron for each rule and an output neuron to form the disjunction of the rules. There’s a nagging doubt in the back of your mind, but you don’t have time for it right now. As you cross the pentagonal chamber to the Tower of Genetic Programs, you can already see how to bring them into the fold. Genetic programs are just programs, and programs are just logic constructs. The sculpture of a genetic program in the chamber is in the shape of a tree, subroutines branching into more subroutines, and when you look closely at the leaves, you can see that they’re just simple rules. So programs boil down to rules, and if rules can be reduced to neurons, so can programs.</p>

<p class="noindent chinese">莫非其他代表也有类似的形式？带着不断上升的兴奋，你跑回五边形的房间，进入逻辑之塔。盯着中间的一套规则的描述，你试图找出一个模式。是的！每条规则只是一个高度风格化的神经元。例如，<i>如果它是一个巨大的爬行动物并且会喷火，那么它就是一条龙</i>，这只是一个权重为 1 的感知器，<i>它是一个巨大的爬行动</i>物并且<i>会喷火</i>，阈值为 1.5。而一组规则是一个多层感知器，其隐藏层包含每个规则的一个神经元和一个输出神经元，以形成规则的分离。在你的脑海中有一个唠叨的疑问，但你现在没有时间去想它。当你穿过五边形的密室来到遗传程序之塔时，你已经可以看到如何将它们带入其中。遗传程序只是程序，而程序只是逻辑构造。遗传程序在密室中的雕塑是一个树的形状，子程序分支到更多的子程序，当你仔细观察叶子时，你可以看到它们只是简单的规则。因此，程序可以归结为规则，如果规则可以简化为神经元，那么程序也可以。</p>

<p class="noindent english"><a id="babilu_link-120"></a> On to the Tower of Graphical Models. Unfortunately, the sculpture in its circular chamber looks nothing like the others. A graphical model is a product of factors: conditional probabilities, in the case of Bayesian networks, and non-negative functions of the state, in the case of Markov networks. Try as you might, you just can’t see the connection to neural networks or sets of rules. Disappointment washes over you. But then you put on your “loggles,” which replace every function by its logarithm. Eureka—the product of factors is now a sum of terms, just like an SVM, a voting set of rules, or a multilayer perceptron without the output S curve. For example, you can translate a Naïve Bayes dragon classifier into a perceptron whose weight for <i>breathes fire</i> is the log of <i>P(breathes fire | dragon)</i> minus the log of <i>P(breathes fire | not dragon)</i> . But of course, graphical models are much more general than this because they can represent probability distributions over many variables, not just the distribution of one variable (the class) given the others (the attributes).</p>

<p class="noindent chinese">继续前往图形模型塔。不幸的是，它的圆形室中的雕塑看起来和其他的没什么两样。图形模型是各种因素的产物：在贝叶斯网络的情况下，是条件概率；在马尔科夫网络的情况下，是状态的非负函数。尽管你很努力，你就是看不出与神经网络或规则集的联系。失望涌上心头。但随后你戴上了你的 “loggles”，用它的对数代替每个函数。尤里卡 —— 因子的乘积现在是一个项的总和，就像一个 SVM，一个规则的投票集，或一个没有输出 S 曲线的多层感知器。例如，你可以把天真贝叶斯的龙分类器转化为感知器，其对<i>喷火</i>的权重是 <i>p(喷火 | 龙)</i> 的对数减去 <i>p(喷火 | 非龙)</i> 的对数。但当然，图形模型比这更普遍，因为它们可以表示许多变量的概率分布，而不仅仅是一个变量（类别）在其他变量（属性）下的分布。</p>

<p class="noindent english">You did it! Or did you? Absorbing SVMs into neural networks and neural networks into graphical models: that worked. So did absorbing genetic programs into logic. But combining logic and graphical models? Something is amiss there. Belatedly, you see the problem: logic has a dimension that graphical models lack and vice versa. The sculptures in the five chambers matched because they were simple allegories, but the reality doesn’t. Graphical models don’t let us represent rules involving more than one object, like <i>Friends of friends are friends</i> ; all their variables have to be properties of the same object. They also can’t represent arbitrary programs, which pass sets of variables from one subroutine to another. Logic can easily do both of these things, but on the other hand it can’t represent uncertainty, ambiguity, or degrees of similarity. And without a representation that can do all of these things, you don’t have a universal learner.</p>

<p class="noindent chinese">你成功了！或者你做到了？将 SVM 吸收到神经网络中，将神经网络吸收到图形模型中：这很有效。将遗传程序吸收到逻辑中也是如此。但是把逻辑和图形模型结合起来？那里有些不对劲。姗姗来迟，你看到了问题所在：逻辑有一个维度，而图形模型缺乏，反之亦然。五室的雕塑相匹配，因为它们是简单的寓言，但现实并不如此。图形模型不能让我们表示涉及一个以上对象的规则，比如<i>朋友的朋友就是朋友</i>；它们的所有变量都必须是同一对象的属性。它们也不能表示任意的程序，即从一个子程序传递到另一个子程序的变量集。逻辑可以很容易地做到这两点，但另一方面，它不能表示不确定性、模糊性或相似度。如果没有一个可以做所有这些事情的表示，你就没有一个通用的学习者。</p>

<p class="noindent english">You rack your brains for a solution, but the more you try, the harder it gets. Perhaps unifying logic and probability is just beyond human ability. Exhausted, you fall asleep. A deep growl jolts you awake. The <a id="babilu_link-36"></a> hydra-headed complexity monster pounces on you, jaws snapping, but you duck at the last moment. Slashing desperately at the monster with the sword of learning, the only one that can slay it, you finally succeed in cutting off all its heads. Before it can grow new ones, you run up the stairs.</p>

<p class="noindent chinese">你绞尽脑汁寻找解决方案，但你越是尝试，就越是困难。也许将逻辑和概率统一起来是人类无法做到的。疲惫不堪的你睡着了。一声深沉的咆哮将你惊醒。九头蛇头的复杂怪物向你扑来，下巴啪啪作响，但你在最后一刻躲开了。用学习之剑拼命地砍向怪物，这是唯一能杀死它的剑，你终于成功地砍下了它所有的头。在它能长出新头之前，你跑上了楼梯。</p>

<p class="noindent english">After an arduous climb, you reach the top. A wedding is in progress. Praedicatus, First Lord of Logic, ruler of the symbolic realm and Protector of the Programs, says to Markovia, Princess of Probability, Empress of Networks: “Let us unite our realms. To my rules thou shalt add weights, begetting a new representation that will spread far across the land.” The princess says, “And we shall call our progeny Markov logic networks.”</p>

<p class="noindent chinese">经过艰苦的攀登，你到达了顶部。一场婚礼正在进行中。Praedicatus，逻辑的第一领主，象征性领域的统治者和程序的保护者，对 Markovia，概率的公主，网络的皇后说。“让我们把我们的领域联合起来。在我的规则中，你要增加权重，产生一个新的代表，将在这片土地上传播很远。” 公主说：“而我们将把我们的后代称为马尔科夫逻辑网络。”</p>

<p class="noindent english">Your head is spinning. You go outside to the balcony. The sun has risen over the city. You gaze out over the rooftops to the countryside beyond. Forests of servers stretch away in all directions, humming quietly, waiting for the Master Algorithm. Convoys move along the roads, carrying gold from the data mines. Far to the west, the land gives way to a sea of information, dotted with ships. You look up at the flag of the Master Algorithm. You can now clearly see the inscription inside the five-pointed star:</p>

<p class="noindent chinese">你的头在旋转。你走到外面的阳台上。太阳已经从城市上空升起。你凝视着屋顶上的乡村。林立的服务器向四面八方延伸，安静地嗡嗡作响，等待着主算法。车队沿着公路行驶，从数据矿场运来黄金。在西部的远处，土地让位于信息的海洋，上面点缀着船只。你抬头看看算法大师的旗帜。你现在可以清楚地看到五角星里面的铭文。</p>

<div>

<p class="noindent english"><i>P</i> = <i>e<sup><i>w•n</i></sup></i> / <i>Z</i></p>

<p class="noindent chinese"><i>P</i> = <i>e<sup><i>w•n</i></sup></i> / <i>Z</i></p

</div>

<p class="noindent english">What could this mean, you wonder?</p>

<p class="noindent chinese">你想知道，这可能意味着什么？</p>

<h1 id="babilu_link-445"><b>Markov logic networks</b></h1>

<h1 id="babilu_link-445"><b>马尔科夫逻辑网络</b></h1>

<p class="noindent english">In 2003, I started thinking about the problem of how to unify logic and probability, together with my student Matt Richardson. At first we made little progress because we were trying to do it with Bayesian networks, and their rigid form—a strict order on variables, conditional distributions of children given parents—is incompatible with the flexibility of logic. But the day before Christmas Eve, I realized there was a much better way. If we switched to Markov networks, we could use <i>any</i> logical formula as a template for Markov network features, and that would unify logic and graphical models. Let’s see how.</p>

<p class="noindent chinese">2003 年，我和我的学生马特·理查森一起开始思考如何统一逻辑和概率的问题。起初我们没有取得什么进展，因为我们试图用贝叶斯网络来做这件事，而贝叶斯网络的僵硬形式 —— 变量的严格顺序，给定父母的子女的条件分布 —— 与逻辑的灵活性不相容。但在圣诞前夜，我意识到有一个更好的方法。如果我们改用马尔科夫网络，我们可以使用<i>任何</i>逻辑公式作为马尔科夫网络特征的模板，这将统一逻辑和图形模型。让我们来看看如何。</p>

<p class="noindent english"><a id="babilu_link-446"></a> Recall that a Markov network is defined by a weighted sum of features, much like a perceptron. Suppose we have a collection of photos of people. We pick a random one and compute features of it like <i>The person has gray hair, The person is old, The person is a woman</i> , and so on. In a perceptron, we pass the weighted sum of these features through a threshold to decide whether, say, the person is your grandmother or not. In a Markov network, we do something very different (at least at first sight): we exponentiate the weighted sum, turning it into a product of factors, and this product is the probability of choosing that particular picture from the collection, regardless of whether your grandmother is in it. If you have many pictures of old people, the weight of that feature goes up. If most of them are of men, the weight of <i>The person is a woman</i> goes down. The features can be anything we want, making Markov networks a remarkably flexible way to represent probability distributions.</p>

<p class="noindent chinese">回顾一下，马尔科夫网络是由特征的加权和定义的，很像一个感知器。假设我们有一个人的照片集。我们随机挑选一张，并计算其特征，如<i>这个人有白头发，这个人老了，这个人是个女人</i>，等等。在感知器中，我们将这些特征的加权和通过一个阈值来决定，比如说，这个人是否是你的祖母。在马尔科夫网络中，我们做了一些非常不同的事情（至少乍一看是这样）：我们对加权和进行指数化处理，把它变成一个因子的乘积，这个乘积就是在收藏品中选择那张特定照片的概率，不管你的祖母是否在里面。如果你有很多老人的照片，这个特征的权重就会上升。如果大多数都是男人，那么<i>这个人是女人</i>的比重就会下降。特征可以是我们想要的任何东西，这使得马尔科夫网络成为表示概率分布的一种非常灵活的方式。</p>

<p class="noindent english">Actually, I lied: the product of factors is not yet a probability because the probabilities of all pictures must add up to one, and there’s no guarantee that the products of factors for all pictures will do so. We need to normalize them, meaning divide each product by the sum of all of them. The sum of all the normalized products is then guaranteed to be one because it’s just a number divided by itself. The probability of a picture is thus the weighted sum of its features, exponentiated and normalized. If you look back at the equation in the five-pointed star, you’ll probably start to get an inkling of what it means. <i>P</i> is a probability, <strong><b><b><i>w</i></b></b></strong> is a vector of weights (notice it’s in boldface), <i>n</i> is a vector of numbers, and their dot product • is exponentiated and divided by <i>Z</i> , the sum of all products. If we let the first component of <i>n</i> be one if the first feature of the image is true and zero otherwise, and so on, <strong><b><b><i>w•n</i></b></b></strong> is just a shorthand for the weighted sum of features we’ve been talking about all along.</p>

<p class="noindent chinese">实际上，我撒了谎：因子的乘积还不是概率，因为所有图片的概率加起来必须是 1，而且不能保证所有图片的因子乘积都是这样的。我们需要对它们进行归一化处理，即用每个乘积除以所有乘积的总和。然后，所有归一化产品的总和保证是 1，因为它只是一个数字除以它自己。因此，一张图片的概率是其特征的加权总和，经过指数化和规范化处理。如果你回顾一下五角星中的方程式，你可能会开始隐约明白它的意思。<i>P</i> 是一个概率。<strong><b><b><i>w</i></b></b></strong>是一个权重向量（注意它是黑体字）。<i>n</i>是一个数字向量，它们的点积 —— 被指数化并除以 <i>Z</i>，即所有产品的总和。如果我们让 Z 的第一个分量 <i>n</i>的第一个分量是 1，如果图像的第一个特征是真实的，否则就是 0，以此类推。<strong><b><b><i>w-n</i></b></b></strong>只是我们一直在谈论的特征的加权总和的一个缩写。</p>

<p class="noindent english">So the equation gives the probability of an image (or whatever) according to a Markov network. But it’s more general than that because it’s not just the equation of a Markov network; rather, it’s the equation of a Markov logic network, as we call it. In a Markov logic network, or MLN for short, the numbers in <i>n</i> don’t have to be just zero or one, and they don’t refer to features—they refer to logical formulas. At the <a id="babilu_link-250"></a> end of <a href="#babilu_link-5">Chapter 8</a> , we saw how we can go beyond Markov networks to relational models, which are defined in terms of feature templates, not just features. <i>Alice and Bob both have the flu</i> is a feature specific to Alice and Bob. <i>X and Y both have the flu</i> is a feature template, which can be instantiated with Alice and Bob, Alice and Chris, and any other two people. A feature template is a powerful thing because it can summarize billions of features or more in a single short expression. But we need a formal language to define feature templates, and we have one readily available: logic.</p>

<p class="noindent chinese">因此，该方程根据马尔可夫网络给出了一个图像（或任何东西）的概率。但它比这更普遍，因为它不仅仅是马尔科夫网络的方程；相反，它是马尔科夫逻辑网络的方程，我们称之为马尔科夫网络。在马尔可夫逻辑网络中，简称 MLN，其中的数字不一定是零。<i>n</i> 中的数字不一定只是零或一，它们也不是指特征，而是指逻辑公式。在<a href="#babilu_link-5">第 8 章</a>的我们看到了我们如何超越马尔科夫网络到关系模型，它是以特征模板而不仅仅是特征来定义的。<i>Alice 和 Bob 都患有流感</i>，这是 Alice 和 Bob 的特有特征。<i>X 和 Y 都有流感</i>是一个特征模板，它可以被实例化为 Alice 和 Bob，Alice 和 Chris，以及其他任何两个人。一个特征模板是一个强大的东西，因为它可以在一个简短的表达中总结出数十亿甚至更多的特征。但我们需要一种正式的语言来定义特征模板，而我们有一种现成的语言：逻辑。</p>

<p class="noindent english">An MLN is just a set of logical formulas and their weights. When applied to a particular set of entities, it defines a Markov network over their possible states. For example, if the entities are Alice and Bob, a possible state is that Alice and Bob are friends, Alice has the flu, and so does Bob. Let’s suppose the MLN has two formulas: <i>Everyone has the flu</i> and <i>If someone has the flu, so do their friends.</i> In standard logic, this would be a pretty useless pair of statements: the first would rule out any state with even a single healthy person, and the second would be redundant. But in an MLN, the first formula just means that there’s a feature <i>X has the flu</i> for every person X, with the same weight as the formula. If people are likely to have the flu, the formula will have a high weight, and so will the corresponding features. A state with many healthy people is less probable than one with few, but not impossible. And because of the second formula, a state where someone has the flu and their friends don’t is less probable than one where healthy and infected people fall into separate clusters of friends.</p>

<p class="noindent chinese">一个 MLN 只是一组逻辑公式和它们的权重。当应用于一组特定的实体时，它在其可能的状态上定义了一个马尔可夫网络。例如，如果实体是 Alice 和 Bob，一个可能的状态是 Alice 和 Bob 是朋友，Alice 有流感，Bob 也有。我们假设 MLN 有两个公式。<i>每个人都得了流感</i>，<i>如果有人得了流感，他们的朋友也会得。</i>在标准逻辑中，这将是一对非常无用的语句：第一条将排除任何有一个健康人的状态，而第二条将是多余的。但是在 MLN 中，第一个公式只是意味着对于每个人 X 来说，都有一个<i>X 有流感</i>的特征，其权重与该公式相同。如果人们很可能得了流感，那么这个公式就会有很高的权重，相应的特征也会如此。一个有许多健康人的状态比一个有少数人的状态可能性小，但不是不可能。而且由于第二个公式，一个有人得了流感而他们的朋友没有得流感的状态比一个健康人和受感染的人分别属于不同的朋友群的状态可能性要小。</p>

<p class="noindent english">At this point you can probably guess what the <i>n</i> in the master equation is: its first component is the number of true instances of the first formula in the state, the second is the number of true instances of the second formula, and so on. If we’re looking at a group of ten friends and seven of them have the flu, the first component of <i>n</i> is seven, and so on. (Shouldn’t the probability be different if seven out of twenty instead of seven out of ten friends have the flu? Yes, and it is, because of <i>Z</i> .) In the limit, if we let all the weights go to infinity, Markov logic reduces to standard logic because violating a single instance of a formula then <a id="babilu_link-164"></a> causes the probability to collapse to zero, making the state impossible. On the probabilistic side, an MLN reduces to a Markov network when all the formulas talk about a single object. So Markov logic includes both logic and Markov networks as special cases, and it’s the unification we were looking for.</p>

<p class="noindent chinese">在这一点上，你也许能猜到 <i>n</i> 它的第一部分是状态中第一个公式的真实事例的数量，第二部分是第二个公式的真实事例的数量，以此类推。如果我们看一组 10 个朋友，其中 7 个得了流感，那么第一个分量为 <i>n</i> 的第一个分量是 7，以此类推。（如果二十个朋友中有七个而不是十个朋友中有七个得了流感，概率不是应该不同吗？是的，而且是不同的，因为有了 <i>Z</i>）。在极限情况下，如果我们让所有的权重都变成无穷大，马尔科夫逻辑就会还原成标准逻辑，因为违反一个公式的单个实例，那么导致概率崩溃为零，使状态不可能。在概率方面，当所有的公式都在谈论一个单一的对象时，一个 MLN 就会减少到马尔可夫网络。所以马尔科夫逻辑包括了逻辑和马尔科夫网络这两个特例，而这正是我们所寻找的统一。</p>

<p class="noindent english">Learning an MLN means discovering formulas that are true in the world more often than random chance would predict, and figuring out the weights for those formulas that cause their predicted probabilities to match their observed frequencies. Once we’ve learned an MLN, we can use it to answer questions like “What is the probability that Bob has the flu, given that he’s friends with Alice and she has the flu?” And guess what? It turns out that the probability is given by an S curve applied to the weighted sum of features, much as in a multilayer perceptron. And an MLN with long chains of rules can represent a deep neural network, with one layer per link in the chain.</p>

<p class="noindent chinese">学习 MLN 意味着发现那些在世界范围内比随机机会预测更频繁的真实公式，并计算出这些公式的权重，使其预测的概率与观察到的频率相匹配。一旦我们学会了 MLN，我们就可以用它来回答这样的问题：“考虑到鲍勃是爱丽丝的朋友，而且她得了流感，那么鲍勃得流感的概率是多少？” 你猜怎么着？事实证明，这个概率是由应用于特征加权和的 S 曲线给出的，就像多层感知器一样。而一个有长长的规则链的 MLN 可以代表一个深度神经网络，链中的每个环节都有一个层。</p>

<p class="noindent english">Of course, don’t be deceived by the simple MLN above for predicting the spread of flu. Picture instead an MLN for diagnosing and curing cancer. The MLN represents a probability distribution over the states of a cell. Every part of the cell, every organelle, every metabolic pathway, every gene and protein is an entity in the MLN, and the MLN’s formulas encode the dependencies between them. We can ask the MLN, “Is this cell cancerous?” and probe it with different drugs and see what happens. We don’t have an MLN like this yet, but later in this chapter I’ll envisage how it might come about.</p>

<p class="noindent chinese">当然，不要被上述预测流感传播的简单 MLN 所欺骗。想象一下用于诊断和治疗癌症的 MLN 吧。MLN 代表一个细胞状态的概率分布。细胞的每个部分、每个细胞器、每个代谢途径、每个基因和蛋白质都是 MLN 中的一个实体，而 MLN 的公式编码了它们之间的相关性。我们可以问 MLN：“这个细胞是癌症吗？” 然后用不同的药物探测它，看看会发生什么。我们现在还没有这样的 MLN，但在本章的后面，我将设想它是如何产生的。</p>

<p class="noindent english">To recap: the unified learner we’ve arrived at uses MLNs as the representation, posterior probability as the evaluation function, and genetic search coupled with gradient descent as the optimizer. If we want, we can easily replace the posterior by some other accuracy measure, or genetic search by hill climbing. We’ve ascended a high peak, and now we can enjoy the view. I wouldn’t be so rash as to call this learner the Master Algorithm, however. For one, the proof of the pudding is in the eating, and although over the last decade this algorithm (or variations of it) has been successfully applied in many areas, there are many more to which it hasn’t, and so it’s not yet clear just how general purpose it <a id="babilu_link-38"></a> is. Second, there are some important problems that it doesn’t solve. But before we look at them, let’s look at what it can do.</p>

<p class="noindent chinese">总结一下：我们得出的统一学习器使用 MLNs 作为表示，后验概率作为评价函数，遗传搜索加上梯度下降作为优化器。如果我们愿意，我们可以很容易地用一些其他的准确度来代替后验，或者用爬坡来代替遗传搜索。我们已经登上了一座高峰，现在我们可以欣赏美景了。然而，我不会如此轻率地把这个学习者称为主算法。首先，布丁的证明就在吃的过程中，尽管在过去的十年中，这个算法（或它的变体）已经成功地应用于许多领域，但还有许多领域没有应用，因此还不清楚它的通用性如何，。第二，有一些重要的问题它并没有解决。但在我们看这些问题之前，让我们先看看它能做什么。</p>

<h1 id="babilu_link-447"><b>From Hume to your housebot</b></h1>

<h1 id="babilu_link-447"><b>从休谟到你的家庭机器人</b></h1>

<p class="noindent english">You can download the learner I’ve just described from alchemy.cs .washington.edu. We christened it Alchemy to remind ourselves that, despite all its successes, machine learning is still in the alchemy stage of science. If you do download it, you’ll see that it includes a lot more than the basic algorithm I’ve described but also that it is still missing a few things I said the universal learner ought to have, like crossover. Nevertheless, let’s use the name Alchemy to refer to our candidate universal learner for simplicity.</p>

<p class="noindent chinese">你可以从 alchemy.cs.washington.edu 下载我刚才描述的学习器。我们把它命名为 “炼金术”，以提醒我们自己，尽管机器学习取得了所有的成功，但它仍然处于科学的炼金术阶段。如果你真的下载了它，你会发现它包含了比我描述的基本算法更多的东西，但它仍然缺少一些我说的通用学习器应该有的东西，比如交叉。尽管如此，为了简单起见，我们还是用炼金术这个名字来指代我们的候选通用学习器。</p>

<p class="noindent english">Alchemy addresses Hume’s original question by having another input besides the data: your initial knowledge, in the form of a set of logical formulas, with or without weights. The formulas can be inconsistent, incomplete, or even just plain wrong; the learning and probabilistic reasoning will take care of that. The key point is that Alchemy doesn’t have to learn from scratch. In fact, we can even tell Alchemy to keep the formulas unchanged and learn only the weights. In this case, giving Alchemy the appropriate formulas can turn it into a Boltzmann machine, a Bayesian network, an instance-based learner, and many other models. This explains why we can have a universal learner despite the “no free lunch” theorem. Rather, Alchemy is like an inductive Turing machine, which we can program to behave as a very powerful or a very restricted learner; it’s up to us. Alchemy provides a unifier for machine learning in the same way that the Internet provides one for computer networks, the relational model for databases, or the graphical user interface for everyday applications.</p>

<p class="noindent chinese">炼金术解决了休谟最初的问题，除了数据之外还有另一个输入：你的初始知识，以一组逻辑公式的形式，有或没有权重。这些公式可以是不一致的，不完整的，甚至只是简单的错误；学习和概率推理会处理这些问题。关键的一点是，炼金术不需要从头开始学习。事实上，我们甚至可以告诉炼金术保持公式不变，只学习权重。在这种情况下，给炼金术适当的公式可以把它变成一个玻尔兹曼机、一个贝叶斯网络、一个基于实例的学习者，以及许多其他模型。这就解释了为什么尽管有 “没有免费的午餐” 的定理，我们还是可以有一个通用的学习者。相反，炼金术就像一个归纳图灵机，我们可以将其编程为一个非常强大或非常有限的学习器；这取决于我们。炼金术为机器学习提供了一个统一的方式，就像互联网为计算机网络提供了一个统一的方式，为数据库提供了关系模型，为日常应用提供了图形用户界面。</p>

<p class="noindent english">Of course, even if you use Alchemy with no initial formulas (and you can), that doesn’t make it knowledge-free. The choice of formal language, score function, and optimizer implicitly encodes assumptions about the world. So it’s natural to ask whether we can have an even more general learner than Alchemy. What did evolution assume when <a id="babilu_link-276"></a> it began its long journey from the first bacteria to all the life-forms around today? I think there’s a simple assumption from which all else follows: the learner is part of the world. This means that the learner as a physical system obeys the same laws as its environment, whatever they are, and therefore already “knows” them implicitly and is primed to discover them. In the next section, we’ll see what this can mean concretely and how to embody it in Alchemy. But for the moment, let’s note that it’s perhaps the best answer we can ever give to Hume’s question. On the one hand, assuming the learner is part of the world <i>is</i> an assumption—in principle, the learner could obey different laws from those the world obeys—so it satisfies Hume’s dictum that learning is only possible with prior knowledge. On the other hand, it’s an assumption so basic and hard to disagree with that perhaps it’s all we need for this world.</p>

<p class="noindent chinese">当然，即使你在使用炼金术时没有初始公式（你可以这样做），这也不能使它没有知识。形式语言、评分函数和优化器的选择隐含地编码了对世界的假设。因此，我们很自然地要问，我们是否可以有一个比炼金术更通用的学习者。当它开始了从第一个细菌到今天周围所有生命形式的漫长旅程时，进化假设了什么？我认为有一个简单的假设，所有其他的东西都是由此而来：学习者是世界的一部分。这意味着学习者作为一个物理系统，遵守与环境相同的规律，无论这些规律是什么，因此已经隐含地 “知道” 这些规律，并准备好去发现它们。在下一节中，我们将看到这一点的具体含义以及如何在炼金术中体现它。但就目前而言，让我们注意到，这也许是我们对休谟的问题所能给出的最佳答案。一方面，假设学习者是世界的一部分<i>是</i>一种假设 —— 原则上，学习者可能会遵守与世界所遵守的不同的法则 —— 所以它满足了休谟的论断，即学习只有在先验知识的情况下才有可能。另一方面，这是一个非常基本的假设，很难有异议，也许这就是我们对这个世界的全部需求。</p>

<p class="noindent english">At the other extreme, knowledge engineers—the most determined critics of machine learning—have good reason to like Alchemy. Instead of a basic model structure or a few rough guesses, Alchemy can input a large, lovingly assembled knowledge base, if it’s available. Because probabilistic rules can interact in much richer ways than deterministic ones, manually encoded knowledge goes a longer way in Markov logic. And since knowledge bases in Markov logic don’t have to be self-consistent, they can be very large and accommodate many different contributors without falling apart—a goal that has so far eluded knowledge engineers.</p>

<p class="noindent chinese">在另一个极端，知识工程师 —— 机器学习最坚定的批评者 —— 有充分的理由喜欢炼金术。如果有一个基本的模型结构或一些粗略的猜测，炼金术可以输入一个大型的、精心组装的知识库，如果它是可用的。由于概率规则可以以比确定性规则更丰富的方式进行互动，手工编码的知识在马尔可夫逻辑中的作用更大。由于马尔可夫逻辑中的知识库不必是自洽的，因此它们可以非常大，并且可以容纳许多不同的贡献者而不至于崩溃 —— 这是知识工程师迄今为止一直无法达到的目标。</p>

<p class="noindent english">Most of all, though, Alchemy addresses the problems that each of the five tribes of machine learning has worked on for so long. Let’s look at each of them in turn.</p>

<p class="noindent chinese">但最重要的是，炼金术解决了机器学习五个部落中每个部落长期以来一直致力于解决的问题。让我们依次看一下它们的情况。</p>

<p class="noindent english">Symbolists combine different pieces of knowledge on the fly, in the same way that mathematicians combine axioms to prove theorems. This contrasts sharply with neural networks and other models with a fixed structure. Alchemy does it using logic, as symbolists do, but with a twist. To prove a theorem in logic, you need to find only one sequence of axiom applications that produces it. Because Alchemy reasons probabilistically, it does more: it finds multiple sequences of formulas that lead to the theorem or its negation and weighs them to compute the <a id="babilu_link-102"></a> theorem’s probability of being true. This way it can reason not just about mathematical universals, but about whether “the president” in a news story means “Barack Obama,” or what folder an e-mail should be filed in. The symbolists’ master algorithm, inverse deduction, postulates new logical rules needed to serve as steps between the data and a desired conclusion. Alchemy introduces new rules by hill climbing, starting with the initial rules and constructing rules that, combined with the initial ones and the data, make the conclusions more likely.</p>

<p class="noindent chinese">符号学家将不同的知识片断随心所欲地结合起来，就像数学家结合公理来证明定理一样。这与神经网络和其他具有固定结构的模型形成了鲜明的对比。炼金术是用逻辑来做的，就像符号学家所做的那样，但有一个转折。要在逻辑中证明一个定理，你只需要找到一个能产生该定理的公理应用序列。因为炼金术的推理是概率性的，所以它做得更多：它找到导致该定理或其否定的多个公式序列，并对它们进行权衡，以计算该定理为真的概率。这样，它不仅可以推理数学的普遍性，还可以推理新闻报道中的 “总统” 是否意味着 “巴拉克·奥巴马”，或者一封电子邮件应该归入哪个文件夹。象征主义者的主要算法 —— 逆向推理，提出了新的逻辑规则，作为数据和预期结论之间的步骤。炼金术通过爬坡引入新的规则，从最初的规则开始，构建规则，与最初的规则和数据相结合，使结论更有可能。</p>

<p class="noindent english">Connectionists’ models are inspired by the brain, with networks of S curves that correspond to neurons and weighted connections between them corresponding to synapses. In Alchemy, two variables are connected if they appear together in some formula, and the probability of a variable given its neighbors is an S curve. (Although I won’t show why, it’s a direct consequence of the master equation we saw in the previous section.) The connectionists’ master algorithm is backpropagation, which they use to figure out which neurons are responsible for which errors and adjust their weights accordingly. Backpropagation is a form of gradient descent, which Alchemy uses to optimize the weights of a Markov logic network.</p>

<p class="noindent chinese">连接主义者的模型受到大脑的启发，S 曲线的网络对应于神经元，它们之间的加权连接对应于突触。在炼金术中，如果两个变量一起出现在某个公式中，那么它们就是有联系的，而一个变量给定其邻居的概率就是一个 S 曲线。（虽然我不会说明原因，但这是我们在上一节看到的主方程的直接结果）。连接主义者的主要算法是反向传播，他们用它来找出哪些神经元负责哪些错误，并相应地调整其权重。逆向传播是梯度下降的一种形式，炼金术用它来优化马尔科夫逻辑网络的权重。</p>

<p class="noindent english">Evolutionaries use genetic algorithms to simulate natural selection. A genetic algorithm maintains a population of hypotheses and in each generation crosses over and mutates the fittest ones to produce the next generation. Alchemy maintains a population of hypotheses in the form of weighted formulas, modifies them in various ways at each step, and keeps the variations that most increase the posterior probability of the data (or some other score function). If the population is a single hypothesis, this reduces to hill climbing. The current open-source implementation of Alchemy does not include crossover, but this would be a straightforward addition. The evolutionaries’ master algorithm is genetic programming, which applies crossover and mutation to computer programs represented as trees of subroutines. Trees of subroutines can be represented by sets of logical rules, and the Prolog programming language does just that. In Prolog, each rule corresponds to a subroutine, and its antecedents are the subroutines it calls. So we can think <a id="babilu_link-65"></a> of Alchemy with crossover as genetic programming using a Prolog-like programming language, with the added advantage that the rules can be probabilistic.</p>

<p class="noindent chinese">进化论者使用遗传算法来模拟自然选择。遗传算法保持一个假设群，在每一代中交叉和变异最合适的假设以产生下一代。炼金术以加权公式的形式维护一个假说群，在每一步以各种方式修改它们，并保留最能增加数据后验概率（或其他评分函数）的变化。如果群体是一个单一的假设，这就简化为爬坡。目前炼金术的开源实现并不包括交叉，但这将是一个直接的补充。进化者的主要算法是遗传编程，它将交叉和变异应用于以子程序树表示的计算机程序。子程序树可以用逻辑规则集来表示，而 Prolog 编程语言正是这样做的。在 Prolog 中，每个规则都对应于一个子程序，其前因是它调用的子程序。因此，我们可以把炼金术与杂交看作是使用类似 Prolog 编程语言的遗传编程，其额外的优点是规则可以是概率性的。</p>

<p class="noindent english">Bayesians believe that modeling uncertainty is the key to learning and use formal representations like Bayesian networks and Markov networks to do so. As we already saw, Markov networks are a special type of MLN. Bayesian networks are also easily represented using the MLN master equation, with a feature for each possible state of a variable and its parents, and the logarithm of the corresponding conditional probability as its weight. (The normalization constant <i>Z</i> then conveniently reduces to 1, meaning we can ignore it.) Bayesians’ master algorithm is Bayes’ theorem, implemented using probabilistic inference algorithms like belief propagation and MCMC. As you may have noticed, Bayes’ theorem is a special case of the master equation, with <i>P</i> = <i>P(A|B)</i> , <i>Z</i> = <i>P(B)</i> , and features and weights corresponding to <i>P(A)</i> and <i>P(B|A).</i> The Alchemy system includes both belief propagation and MCMC for inference, generalized to handle weighted logical formulas. Using probabilistic inference over the proof paths provided by logic, Alchemy weighs the evidence for and against a conclusion and outputs the probability of the conclusion. This contrasts with the “plain vanilla” logic used by symbolists, which is all or none and so falls apart when given contradictory evidence.</p>

<p class="noindent chinese">贝叶斯主义者认为，对不确定性进行建模是学习的关键，并使用像贝叶斯网络和马尔科夫网络这样的正式表征来实现。正如我们已经看到的，马尔科夫网络是 MLN 的一种特殊类型。贝叶斯网络也很容易用 MLN 主方程来表示，一个变量的每个可能状态和它的父母都有一个特征，相应的条件概率的对数作为其权重。（归一化常数 <i>Z</i> 然后方便地减少到 1，意味着我们可以忽略它）。贝叶斯派的主要算法是贝叶斯定理，使用信仰传播和 MCMC 等概率推理算法实现。你可能已经注意到，贝叶斯定理是主方程的一个特例，<i>P</i> = <i>p(A|B)</i>，<i>Z</i> = <i>p(B)</i>，特征和权重对应于<i>p(A)</i> 和 <i>p(B|A)。</i>炼金术系统包括用于推理的信念传播和 MCMC，被泛化为处理加权逻辑公式。炼金术利用逻辑提供的证明路径进行概率推理，权衡支持和反对某一结论的证据，并输出结论的概率。这与符号学家使用的 “普通” 逻辑形成鲜明对比，后者是全有或全无的，因此在给出矛盾的证据时就会崩溃。</p>

<p class="noindent english">Analogizers learn by hypothesizing that entities with similar known properties have similar unknown ones: patients with similar symptoms have similar diagnoses, readers who bought the same books in the past will do so again in the future, and so on. MLNs can represent similarity between entities with formulas like <i>People with the same tastes buy the same books</i> . Then the more of the same books Alice and Bob have bought, the more likely they are to have the same tastes, and (applying the same formula in the opposite direction) the more likely Alice is to buy a book if Bob also did. Their similarity is represented by their probability of having the same tastes. To make this really useful, we can have different weights for different instances of the same rule: if Alice and Bob both bought a certain rare book, this is probably more <a id="babilu_link-176"></a> informative than if they both bought a best seller and should therefore have a higher weight. In this case the properties whose similarity we’re computing are discrete (bought/not bought), but we can also represent similarity between continuous properties, like the distance between two cities, by letting an MLN have these similarities as features. If the evaluation function is a margin-style score function instead of the posterior probability, the result is a generalization of SVMs, the analogizers’ master algorithm. A greater challenge for our master learner is reproducing structure mapping, the more powerful type of analogy that can make inferences from one domain (e.g., the solar system) to another (the atom). We can do this by learning formulas that don’t refer to any of the specific relations in the source domain. For example, <i>Friends of smokers also smoke</i> is about friendship and smoking, but <i>Related entities have similar properties</i> applies to any relation and property. We can learn it by generalizing from <i>Friends of friends also smoke</i> , <i>Coworkers of experts are also experts</i> , and other such patterns in a social network and then apply it to, say, the web, with instances like <i>Interesting pages link to interesting pages</i> , or to molecular biology, with instances like <i>Proteins that interact with gene-regulating proteins also regulate genes</i> . Researchers in my group and others have done all of these things, and more.</p>

<p class="noindent chinese">类比者通过假设具有类似已知属性的实体具有类似的未知属性来学习：具有类似症状的病人具有类似的诊断，过去购买相同书籍的读者在将来也会这样做，等等。MLNs 可以用公式来表示实体之间的相似性，比如<i>有相同品味的人买相同的书</i>。那么，Alice 和 Bob 买过的相同的书越多，他们就越有可能有相同的品味，而且（反过来应用相同的公式）如果 Bob 也买了一本书，Alice 就越有可能买。他们的相似性由他们拥有相同口味的概率来表示。为了使其真正有用，我们可以为同一规则的不同实例设置不同的权重：如果爱丽丝和鲍勃都买了某本稀有书籍，这可能比他们都买了一本畅销书更有因此应该有更高的权重。在这种情况下，我们要计算的相似性的属性是离散的（买了/没买），但是我们也可以通过让 MLN 把这些相似性作为特征来表示连续属性之间的相似性，比如两个城市之间的距离。如果评价函数是一个边际风格的分数函数，而不是后验概率，那么结果就是 SVM 的泛化，也就是模拟者的主算法。对我们的主学习器来说，更大的挑战是重现结构映射，这是一种更强大的类比类型，可以从一个领域（如太阳系）到另一个领域（原子）进行推论。我们可以通过学习不参考源域中任何特定关系的公式来做到这一点。例如，<i>吸烟者的朋友也吸烟</i>是关于友谊和吸烟的，但是<i>相关实体有类似的属性</i>适用于任何关系和属性。我们可以通过归纳<i>朋友的朋友也吸烟</i>、<i>专家的同事也是专家</i>以及社会网络中的其他此类模式来学习它，然后将其应用于，比如说网络，其中的实例包括<i>有趣的网页链接到有趣的网页</i>，或者分子生物学，其中的实例包括<i>与基因调节蛋白相互作用的蛋白质也调节基因</i>。我的小组和其他小组的研究人员已经做了所有这些事情，甚至更多。</p>

<p class="noindent english">Alchemy also enables the five types of unsupervised learning we saw in the previous chapter. It does relational learning, obviously, and in fact that’s where most of its applications to date have been. Alchemy uses logic to represent relations among entities and Markov networks to let them be uncertain. We can turn Alchemy into a reinforcement learner by wrapping delayed rewards around it and using it to learn the value of each state in the same way that traditional reinforcement learners use, say, a neural network. We can do chunking in Alchemy by adding a new operation that condenses chains of rules into single rules. (For example, <i>If A then B</i> and <i>If B then C</i> into <i>If A then C</i> .) An MLN with a single unobserved variable connected to all the observable ones does clustering. (An unobserved variable is a variable whose values we never see in the data; it’s “hidden,” so to speak, and can only be inferred.) MLNs with <a id="babilu_link-37"></a> more than one unobserved variable do a kind of discrete dimensionality reduction by inferring the values of those (fewer) variables from the (more numerous) observable ones. Alchemy can also handle MLNs with continuous unobserved variables, which would be needed to do things like principal-component analysis and Isomap. So Alchemy can in principle do all the things we want Robby the robot to do, or at least all the things we’ve discussed in this book. Indeed, we’ve used Alchemy to let a robot learn a map of its environment, figuring out from its sensors where the walls and doors are, their angles and distances, and so on, which is the first step in building a competent housebot.</p>

<p class="noindent chinese">炼金术还能实现我们在前一章中看到的五种无监督学习。很明显，它可以进行关系学习，事实上这也是它迄今为止的大部分应用。炼金术用逻辑来表示实体之间的关系，用马尔可夫网络来让它们变得不确定。我们可以把炼金术变成一个强化学习器，把延迟奖励包裹起来，用它来学习每个状态的值，就像传统强化学习器使用神经网络一样。我们可以在炼金术中通过添加一个新的操作，将一连串的规则浓缩成单一的规则来进行分块。（例如，<i>If A then B</i> 和 <i>If B then C</i> 变成 <i>If A then C</i>。）一个具有单个未观察变量的 MLN 与所有可观察变量相连，可以进行聚类。（未观察到的变量是一个我们在数据中从未看到的变量；可以说，它是 “隐藏的”，只能被推断出来）。多于一个未观察变量的 MLN 通过从（更多的）可观察变量中推断出这些（更少的）变量的值来进行一种离散降维。炼金术还可以处理具有连续未观察变量的 MLN，这将是进行主成分分析和 Isomap 等工作所需要的。因此，炼金术原则上可以做所有我们希望机器人罗比做的事情，或者至少是我们在本书中讨论的所有事情。事实上，我们已经用炼金术让机器人学习环境地图，从传感器中找出墙壁和门的位置、角度和距离，等等，这是建立一个合格的家庭机器人的第一步。</p>

<p class="noindent english">Finally, we can turn Alchemy into a metalearner like stacking by encoding the individual classifiers as MLNs and adding or learning formulas to combine them. This is what DARPA did in its PAL project. PAL, the Personalized Assistant that Learns, was the largest AI project in DARPA history and the progenitor of Siri. PAL’s goal was to build an automated secretary. It used Markov logic as its overarching representation, combining the outputs from different modules into the final decisions on what to do. This also allowed PAL’s modules to learn from each other by evolving toward a consensus.</p>

<p class="noindent chinese">最后，我们可以通过将单个分类器编码为 MLN，并添加或学习公式来组合它们，从而将炼金术变成一个类似于堆叠的金属 earner。这就是 DARPA 在其 PAL 项目中所做的事情。PAL，即学习型个性化助理，是 DARPA 历史上最大的人工智能项目，也是 Siri 的前身。PAL 的目标是建立一个自动秘书。它使用马尔可夫逻辑作为其总体代表，将不同模块的输出合并到关于做什么的最终决定中。这也使 PAL 的模块能够通过向共识的方向发展而相互学习。</p>

<p class="noindent english">One of Alchemy’s largest applications to date was to learn a semantic network (or knowledge graph, as Google calls it) from the web. A semantic network is a set of concepts (like planets and stars) and relations among those concepts (planets orbit stars). Alchemy learned over a million such patterns from facts extracted from the web (e.g., Earth orbits the sun). It discovered concepts like planet all by itself. The version we used was more advanced than the basic one I’ve described here, but the essential ideas are the same. Various research groups have used Alchemy or their own MLN implementations to solve problems in natural language processing, computer vision, activity recognition, social network analysis, molecular biology, and many other areas.</p>

<p class="noindent chinese">迄今为止，炼金术最大的应用之一是从网络中学习语义网络（或谷歌称之为知识图）。语义网络是一组概念（如行星和恒星）以及这些概念之间的关系（行星轨道上的恒星）。炼金术从网络中提取的事实（例如，地球绕着太阳运行）中学习了超过一百万个这样的模式。它自己就发现了行星这样的概念。我们使用的版本比我在这里描述的基本版本更先进，但其基本思想是相同的。各个研究小组已经使用炼金术或他们自己的 MLN 实现来解决自然语言处理、计算机视觉、活动识别、社会网络分析、分子生物学和许多其他领域的问题。</p>

<p class="noindent english">Despite its successes, Alchemy has some significant shortcomings. It does not yet scale to truly big data, and someone without a PhD in machine learning will find it hard to use. Because of these problems, it’s not yet ready for prime time. But let’s see what we can do about them.</p>

<p class="noindent chinese">尽管炼金术取得了成功，但它也有一些明显的缺点。它还不能扩展到真正的大数据，而且没有机器学习博士学位的人将发现它很难使用。由于这些问题，它还没有准备好进入黄金时间。但让我们看看我们能做些什么来解决这些问题。</p>

<h1 id="babilu_link-448"><b><a id="babilu_link-269"><b></b></a> Planetary-scale machine learning</b></h1>

<h1 id="babilu_link-448"><b><a id="babilu_link-269"><b></b></a>行星级的机器学习</b></h1>

<p class="noindent english">In computer science, a problem isn’t really solved until it’s solved efficiently. Knowing how to do something isn’t much use if you can’t do it within the available time and memory, and these can run out very quickly when you’re dealing with an MLN. We routinely learn MLNs with millions of variables and billions of features, but this is not as large as it seems because the number of variables grows very quickly with the number of entities in the MLN: if you have a social network with a thousand people, you already have a million possible pairs of friends and a billion instances of the formula <i>Friends of friends are friends</i> .</p>

<p class="noindent chinese">在计算机科学中，一个问题只有在得到有效解决后才算真正得到解决。如果你不能在可用的时间和内存内完成某件事，那么知道如何做就没有什么用了，而当你处理一个 MLN 时，这些可能很快就用完了。我们经常学习具有数百万变量和数十亿特征的 MLN，但这并不像它看起来那么大，因为变量的数量随着 MLN 中实体的数量而迅速增长：如果你有一个有一千个人的社交网络，你已经有一百万对可能的朋友和十亿个朋友<i>是朋友的</i>公式的实例。</p>

<p class="noindent english">Inference in Alchemy is a combination of logical and probabilistic inference. The former is done by proving theorems and the latter by belief propagation, MCMC, and the other methods we saw in <a href="#babilu_link-6">Chapter 6</a> . We’ve combined the two into probabilistic theorem proving, and the unified inference algorithm, capable of computing the probability of any logical formula, is a key part of the current Alchemy system. But it can be very computationally expensive. If your brain used probabilistic theorem proving, the proverbial tiger would eat you before you figured out to run away. That’s a high price to pay for the generality of Markov logic. Your brain, having evolved in the real world, must encode additional assumptions that allow it to do inference very efficiently. In the last few years, we’ve started to figure out what they might be and encode them into Alchemy.</p>

<p class="noindent chinese">炼金术中的推理是逻辑推理和概率推理的结合。前者是通过证明定理来完成的，后者是通过信念传播、MCMC 以及我们在<a href="#babilu_link-6">第六章</a>看到的其他方法来完成的。我们把这两者结合到概率定理证明中，统一的推理算法，能够计算任何逻辑公式的概率，是目前炼金术系统的一个关键部分。但它的计算成本可能非常高。如果你的大脑使用概率定理证明，在你想出逃跑之前，传说中的老虎会吃掉你。这是为马尔科夫逻辑的通用性所付出的高昂代价。你的大脑在现实世界中进化后，必须对额外的假设进行编码，使其能够非常有效地进行推理。在过去的几年里，我们已经开始弄清楚它们可能是什么，并将它们编码到炼金术中。</p>

<p class="noindent english">The world is not a random jumble of interactions; it has a hierarchical structure: galaxies, planets, continents, countries, cities, neighborhoods, your house, you, your head, your nose, a cell on its tip, the organelles in it, molecules, atoms, subatomic particles. The way to model it, then, is with an MLN that also has a hierarchical structure. This is an example of the assumption that the learner and its environment are alike. The MLN doesn’t have to know a priori which parts the world is composed of; all Alchemy has to do is assume that the world <i>has</i> parts and look for them, rather like a newly made bookshelf assumes that there are books but doesn’t yet know which ones will be <a id="babilu_link-178"></a> placed on it. Hierarchical structure helps make inference tractable because subparts of the world interact mostly with other subparts of the same part: neighbors talk more to each other than to people in another country, molecules produced in one cell react mostly with other molecules in that cell, and so on.</p>

<p class="noindent chinese">世界不是一个随机杂乱的互动；它有一个层次结构：星系、行星、大陆、国家、城市、社区、你的房子、你、你的头、你的鼻子、一个细胞的顶端、里面的细胞器、分子、原子、亚原子粒子。那么，建模的方式就是用一个同样具有层次结构的 MLN。这是一个假设学习者和它的环境是相似的例子。MLN 不必先验地知道世界是由哪些部分组成的；炼金术所要做的就是假设世界<i>有</i>部分，并寻找它们，就像一个新做的书架假设有书，但还不知道哪些书会被放在。层次结构有助于使推理变得切实可行，因为世界的各个部分大多与同一部分的其他部分相互作用：邻居之间的交谈多于与另一个国家的人的交谈，一个细胞中产生的分子大多与该细胞中的其他分子反应，等等。</p>

<p class="noindent english">Another property of the world that makes learning and inference easier is that the entities in it don’t come in arbitrary forms. Rather, they fall into classes and subclasses, with members of the same class being more alike than members of different ones. Alive or inanimate, animal or plant, bird or mammal, human or not: if we know all the distinctions relevant to the question at hand, we can lump together all the entities that lack them and that can save a lot of time. As before, the MLN doesn’t have to know a priori what the classes in the world are; it can learn them from data by hierarchical clustering.</p>

<p class="noindent chinese">世界的另一个使学习和推理更容易的属性是，其中的实体并不是以任意的形式出现的。相反，它们分为类和子类，同一类的成员比不同类的成员更相似。有生命或无生命，动物或植物，鸟或哺乳动物，人类或非人类：如果我们知道所有与手头问题相关的区别，我们就可以把所有缺乏这些区别的实体放在一起，这可以节省大量的时间。和以前一样，MLN 不需要先验地知道世界上的类是什么；它可以通过分层聚类从数据中学习它们。</p>

<p class="noindent english">The world has parts, and parts belong to classes: combining these two gives us most of what we need to make inference in Alchemy tractable. We can learn the world’s MLN by breaking it into parts and subparts, such that most interactions are between subparts of the same part, and then grouping the parts into classes and subclasses. If the world is a Lego toy, we can break it up into individual bricks, remembering which attaches to which, and group the bricks by shape and color. If the world is Wikipedia, we can extract the entities it talks about, group them into classes, and learn how classes relate to each other. Then if someone asks us “Is Arnold Schwarzenegger an action star?” we can answer yes, because he’s a star and he’s in action movies. Step-by-step, we can learn larger and larger MLNs, until we’re doing what a friend of mine at Google calls “planetary-scale machine learning”: modeling everyone in the world at once, with data continually streaming in and answers streaming out.</p>

<p class="noindent chinese">世界有部分，而部分属于类：将这两者结合起来，我们就可以得到在炼金术中进行推理所需的大部分内容。我们可以通过将世界分解成部分和子部分来学习世界的 MLN，这样大部分的交互是在同一部分的子部分之间进行的，然后将这些部分归入类和子类。如果这个世界是一个乐高玩具，我们可以把它分解成各个砖块，记住哪个砖块附着在哪个砖块上，并按形状和颜色对砖块进行分组。如果这个世界是维基百科，我们可以提取它所谈论的实体，将它们归类，并学习类之间的关系。然后，如果有人问我们：“阿诺德·施瓦辛格是一个动作明星吗？” 我们可以回答是的，因为他是一个明星，而且他在动作电影中。一步一步地，我们可以学习越来越大的 MLN，直到我们做一个我在谷歌的朋友称之为 “行星规模的机器学习”：同时对世界上的每个人进行建模，数据不断地流入，答案不断地流出。</p>

<p class="noindent english">Of course, learning on this scale requires much more than a direct implementation of the algorithms we’ve seen. For one, beyond a certain point a single processor is not enough; we have to distribute the learning over many servers. Researchers in both industry and academia have intensely investigated how to, for example, do gradient descent using <a id="babilu_link-139"></a> many computers in parallel. One option is to divide the data among the processors; another is to divide the model’s parameters. After each step, we combine the results and redistribute the work. Either way, doing this without letting the cost of communication overwhelm you, or the quality of the results suffer, is far from trivial. Another issue is that, if you have an endless stream of data coming in, you can’t wait to see it all before you commit to some decisions. One solution is to use the sampling principle: if you want to predict who will win the next presidential election, you don’t need to ask every voter who he or she will vote for; a sample of a few thousand suffices, if you’re willing to accept a little bit of uncertainty. The trick is to generalize this to complex models with millions of parameters. But we can do this by taking at each step just as many examples from the stream as we need to be pretty sure that we’re making the right decision and that the total uncertainty over all the decisions stays within bounds. That way we can effectively learn from infinite data in finite time, as I put it in an early paper proposing this approach.</p>

<p class="noindent chinese">当然，这种规模的学习需要比我们看到的算法的直接实现多得多。首先，超过一定程度后，单个处理器是不够的；我们必须将学习分布在许多服务器上。工业界和学术界的研究人员都在紧张地研究如何，例如，使用许多计算机并行地进行梯度下降。一种选择是在处理器之间划分数据；另一种是划分模型的参数。在每一步之后，我们把结果结合起来，重新分配工作。无论哪种方式，在不使通信成本压倒你或使结果的质量受到影响的情况下做到这一点远非易事。另一个问题是，如果你有无穷无尽的数据流进来，你就不能等到看到所有的数据后再承诺做出一些决定。一个解决方案是使用抽样原则：如果你想预测谁将赢得下一届总统选举，你不需要问每个选民他或她会投给谁；如果你愿意接受一点不确定性，几千个样本就足够了。诀窍是将其推广到有数百万个参数的复杂模型。但我们可以通过在每一步从数据流中抽取我们需要的尽可能多的例子来做到这一点，以确保我们正在做出正确的决定，并且所有决定的总不确定性保持在一定范围内。这样，我们就可以在有限的时间内有效地从无限的数据中学习，正如我在早期提出这种方法的论文中所说的那样。</p>

<p class="noindent english">Big-data systems are the Cecil B. DeMille productions of machine learning, with thousands of servers instead of thousands of extras. In the largest projects, just getting all the data together, verifying it, cleaning it up, and munging it into a form the learners can digest can make building the pyramids seem like a walk in the park. At the pharaonic end, Europe’s FuturICT project aims to build a model of—literally—the whole world. Societies, governments, culture, technology, agriculture, disease, the global economy: nothing is to be left out. This is surely premature, but it does foreshadow the shape of things to come. In the meantime, projects like this can help us find out where the limits of scalability are and how to overcome them.</p>

<p class="noindent chinese">大数据系统是机器学习的塞西尔·B·德米尔作品，有成千上万的服务器，而不是成千上万的临时演员。在最大的项目中，仅仅是将所有的数据集中起来，对其进行验证、清理，并将其转化为学习者可以消化的形式，就可以使建造金字塔看起来像在公园里散步。在法老式的末端，欧洲的 FuturICT 项目旨在建立一个模型 —— 实际上是整个世界。社会、政府、文化、技术、农业、疾病、全球经济：没有什么可以被遗漏。这当然是不成熟的，但它确实预示着未来的事物的形态。同时，像这样的项目可以帮助我们找出可扩展性的极限在哪里以及如何克服它们。</p>

<p class="noindent english">Computational complexity is one thing, but human complexity is another. If computers are like idiot savants, learning algorithms can sometimes come across like child prodigies prone to temper tantrums. That’s one reason humans who can wrangle them into submission are so highly paid. If you know how to expertly tweak the control knobs until they’re just right, magic can ensue, in the form of a stream of insights <a id="babilu_link-165"></a> beyond the learner’s years. And, not unlike the Delphic oracle, interpreting the learner’s pronouncements can itself require considerable skill. Turn the knobs wrong, though, and the learner may spew out a torrent of gibberish or clam up in defiance. Unfortunately, in this regard Alchemy is no better than most. Writing down what you know in logic, feeding in the data, and pushing the button is the fun part. When Alchemy returns a beautifully accurate and efficient MLN, you go down to the pub and celebrate. When it doesn’t—which is most of the time—the battle begins. Is the problem in the knowledge, the learning, or the inference? On the one hand, because of the learning and probabilistic inference, a simple MLN can do the job of a complex program. On the other, when it doesn’t work, it’s much harder to debug. The solution is to make it more interactive, able to introspect and explain its reasoning. That will take us another step closer to the Master Algorithm.</p>

<p class="noindent chinese">计算的复杂性是一回事，但人类的复杂性是另一回事。如果计算机像白痴一样，学习算法有时就像容易发脾气的神童。这也是能够使它们屈服的人类获得如此高薪的原因之一。如果你知道如何专业地调整控制旋钮，直到它们恰到好处，魔法就会随之而来，其形式是超越学习者年龄的洞察力流。而且，与德尔菲克神谕不同的是，解释学习者的声明本身也需要相当的技巧。但是，如果旋钮转错了，学习者可能会喷出大量的胡言乱语，或者不服气地闭口不言。不幸的是，在这方面，炼金术并不比大多数人好。用逻辑写下你知道的东西，输入数据，然后按下按钮，这才是有趣的部分。当炼金术返回一个漂亮的准确和有效的 MLN 时，你会去酒吧庆祝。当它没有这样的结果时 —— 这也是大多数时候，战斗开始了。问题是出在知识、学习还是推理上？一方面，由于学习和概率推理的存在，一个简单的 MLN 可以完成复杂程序的工作。另一方面，当它不工作时，它更难调试。解决方案是使它更具有互动性，能够自省并解释其推理。这将使我们离主算法又近了一步。</p>

<h1 id="babilu_link-449"><b>The doctor will see you now</b></h1>

<h1 id="babilu_link-449"><b>医生现在要见你</b></h1>

<p class="noindent english">The cure for cancer is a program that inputs the cancer’s genome and outputs the drug to kill it with. We can now picture what such a program—let’s call it CanceRx—will look like. Despite its outward simplicity, CanceRx is one of the largest and most complex programs ever built—indeed, so large and complex that it could only have been built with the help of machine learning. It is based on a detailed model of how living cells work, with a subclass for each type of cell in the human body and an overarching model of how they interact. This model, in the form of an MLN or something akin to it, combines knowledge of molecular biology with vast amounts of data from DNA sequencers, microarrays, and many other sources. Some of the knowledge was manually encoded, but most was automatically extracted from the biomedical literature. The model is continually evolving, incorporating the results of new experiments, data sources, and patient histories. Ultimately, it will know every pathway, regulatory mechanism, and chemical reaction in every type of human cell—the sum total of human molecular biology.</p>

<p class="noindent chinese">治愈癌症的方法是一个程序，输入癌症的基因组并输出杀死它的药物。我们现在可以想象这样一个程序 —— 我们叫它 CanceRx —— 的样子。尽管外表简单，但 CanceRx 是有史以来内置的最大和最复杂的程序之一，确实如此庞大和复杂，以至于它只能在机器学习的帮助下建立。它基于一个关于活细胞如何工作的详细模型，人体内每种类型的细胞都有一个子类，还有一个关于它们如何互动的总体模型。这个模型以 MLN 或类似的形式，将分子生物学知识与来自 DNA 测序仪、微阵列和许多其他来源的大量数据结合起来。有些知识是人工编码的，但大部分是自动从生物医学文献中提取的。该模型正在不断发展，纳入了新的实验结果、数据来源和病人历史。最终，它将知道每种类型的人类细胞中的每条通路、调节机制和化学反应 —— 人类分子生物学的总和。</p>

<p class="noindent english"><a id="babilu_link-450"></a> CanceRx spends most of its time querying the model with candidate drugs. Given a new drug, the model predicts its effect on both cancer cells and normal ones. When Alice is diagnosed with cancer, CanceRx instantiates its model with both her normal cells and the tumor’s and tries all available drugs until it finds one that kills the cancer cells without harming the healthy ones. If it can’t find a drug or combination of drugs that works, it sets about designing one that will, perhaps evolving it from existing ones using hill climbing or crossover. At each step in the search, it tries the candidate drugs on the model. If a drug stops the cancer but still has some harmful side effect, CanceRx tries to tweak it to get rid of the side effect. When Alice’s cancer mutates, it repeats the whole process. Even before the cancer mutates, the model predicts likely mutations, and CanceRx prescribes drugs that will stop them dead in their tracks. In the game of chess between humanity and cancer, CanceRx is checkmate.</p>

<p class="noindent chinese">CanceRx 将大部分时间用于查询候选药物的模型。给定一个新的药物，该模型预测其对癌细胞和正常细胞的影响。当爱丽丝被诊断出患有癌症时，CanceRx 用她的正常细胞和肿瘤细胞实例化它的模型，并尝试所有可用的药物，直到找到一种能杀死癌细胞而不伤害健康细胞的药物。如果它找不到有效的药物或药物组合，它就开始设计一个有效的药物，也许会用爬坡或交叉法从现有的药物中进化出来。在搜索的每一步，它在模型上尝试候选药物。如果一种药物能阻止癌症，但仍有一些有害的副作用，CanceRx 会尝试调整它，以消除副作用。当爱丽丝的癌症发生突变时，它会重复整个过程。甚至在癌症突变之前，模型就预测了可能的突变，而 CanceRx 则开出药物，将它们拦腰截断。在人类与癌症之间的棋局中，CanceRx 是将领。</p>

<p class="noindent english">Notice that machine learning isn’t going to give us CanceRx all by itself. It’s not as if we have a vast database of molecular biology ready to go, stream it into the Master Algorithm, and out pops the perfect model of a living cell. CanceRx would be the end result, after many iterations, of a worldwide collaboration between hundreds of thousands of biologists, oncologists, and data scientists. Most important, however, CanceRx would incorporate data from millions of cancer patients, with the help of their doctors and hospitals. Without that data, we can’t cure cancer; with it, we can. Contributing to this growing database would not only be in every cancer patient’s interest; it would be her ethical duty. In the world of CanceRx, discrete clinical trials are a thing of the past; new treatments proposed by CanceRx are continually being rolled out, and if they work, given to a widening circle of patients. Both successes and failures provide valuable data for CanceRx’s learning, in a virtuous circle of improvement. If you look at it one way, machine learning is only a small part of the CanceRx project, well behind data gathering and human contributions. But looked at another way, machine learning is the linchpin of the whole enterprise. Without it, we would have only fragmentary knowledge of cancer biology, scattered among thousands <a id="babilu_link-162"></a> of databases and millions of scientific articles, each doctor aware of only a small part. Assembling all this knowledge into a coherent whole is beyond the power of unaided humans, no matter how smart; only machine learning can do it. Because every cancer is different, it takes machine learning to find the common patterns. And because a single tissue can yield billions of data points, it takes machine learning to figure out what to do for each new patient.</p>

<p class="noindent chinese">请注意，机器学习不会自己给我们提供 CanceRx。这并不是说我们有一个庞大的分子生物学数据库，把它输入主算法，然后就会出现一个完美的活细胞模型。CanceRx 将是成千上万的生物学家、肿瘤学家和数据科学家在全球范围内合作，经过多次迭代的最终结果。然而，最重要的是，CanceRx 将在医生和医院的帮助下，纳入数百万癌症患者的数据。没有这些数据，我们无法治愈癌症；有了这些数据，我们就能治愈。为这个不断增长的数据库做出贡献，不仅符合每个癌症患者的利益，也是她的道德责任。在 CanceRx 的世界里，离散的临床试验已经成为过去；CanceRx 提出的新疗法不断被推出，如果它们有效，就会给越来越多的病人使用。成功和失败都为 CanceRx 的学习提供了宝贵的数据，形成了改进的良性循环。如果你从一个角度看，机器学习只是 CanceRx 项目的一个小部分，远远落后于数据收集和人类的贡献。但从另一个角度看，机器学习是整个企业的关键所在。没有它，我们将只有零星的癌症生物学知识，散落在数以千计的数据库和数以百万计的科学文章中，每个医生只知道一小部分。将所有这些知识组装成一个连贯的整体，这超出了无助的人类的能力，无论他们多么聪明；只有机器学习可以做到这一点。因为每一种癌症都是不同的，所以需要机器学习来找到共同的模式。而且因为一个组织可以产生数十亿个数据点，所以需要机器学习来找出对每个新病人的处理方法。</p>

<p class="noindent english">The effort to build what will ultimately become CanceRx is already under way. Researchers in the new field of systems biology model whole metabolic networks rather than individual genes and proteins. One group at Stanford has built a model of a whole cell. The Global Alliance for Genomics and Health promotes data sharing among researchers and oncologists, with a view to large-scale analysis. CancerCommons .org assembles cancer models and lets patients pool their histories and learn from similar cases. Foundation Medicine pinpoints the mutations in a patient’s tumor cells and suggests the most appropriate drugs. A decade ago, it wasn’t clear if, or how, cancer would ever be cured. Now we can see how to get there. The road is long, but we have found it.</p>

<p class="noindent chinese">建立最终将成为 CanceRx 的努力已经在进行中。系统生物学新领域的研究人员对整个代谢网络而不是单个基因和蛋白质进行建模。斯坦福大学的一个小组已经建立了一个整个细胞的模型。全球基因组学和健康联盟促进了研究人员和肿瘤学家之间的数据共享，以便进行大规模分析。CancerCommons.org 集合了癌症模型，让患者汇集他们的历史，并从类似的案例中学习。Foundation Medicine 指出了病人肿瘤细胞的突变，并建议最合适的药物。十年前，还不清楚癌症是否会被治愈，或如何被治愈。现在我们可以看到如何达到这个目标。这条路很漫长，但我们已经找到了它。</p>

</section>

</div>

</div>

<div id="babilu_link-335">

<div>

<section id="babilu_link-345">

<h1><a id="babilu_link-300"></a> <a href="#babilu_link-336">CHAPTER TEN</a></h1>

<h1><a id="babilu_link-300"></a> <a href="#babilu_link-336">第十章</a></h1>

<h1><a href="#babilu_link-336">This Is the World on Machine Learning</a></h1>

<h1><a href="#babilu_link-336">这就是机器学习的世界</a></h1>

<p class="noindent english">Now that you’ve toured the machine learning wonderland, let’s switch gears and see what it all means to you. Like the red pill in <i>The Matrix</i> , the Master Algorithm is the gateway to a different reality: the one you already live in but didn’t know it yet. From dating to work, from self-knowledge to the future of society, from data sharing to war, and from the dangers of AI to the next step in evolution, a new world is taking shape, and machine learning is the key that unlocks it. This chapter will help you make the most of it in your life and be ready for what comes next. Machine learning will not single-handedly determine the future, any more than any other technology; it’s what we decide to do with it that counts, and now you have the tools to decide.</p>

<p class="noindent chinese">现在你已经参观了机器学习的仙境，让我们换个角度，看看这一切对你意味着什么。就像《<i>黑客帝国</i>》中的红色药丸一样，主算法是通往不同现实的大门：你已经生活在其中，但还不知道。从约会到工作，从自我认识到社会的未来，从数据共享到战争，从人工智能的危险到进化的下一步，一个新世界正在形成，而机器学习是开启它的钥匙。本章将帮助你在生活中充分利用它，为接下来的事情做好准备。机器学习不会像其他技术一样，单枪匹马地决定未来；重要的是我们决定如何利用它，而现在你有了决定的工具。</p>

<p class="noindent english">Chief among these tools is the Master Algorithm. Whether it arrives sooner or later, and whether or not it looks like Alchemy, is less important than what it encapsulates: the essential capabilities of a learning algorithm, and where they’ll take us. We can equally well think of the Master Algorithm as a composite picture of current and future learners, which we can conveniently use in our thought experiments in lieu of the specific algorithm inside product X or website Y, which the respective companies are unlikely to share with us anyway. Seen in this light, <a id="babilu_link-194"></a> the learners we interact with every day are embryonic versions of the Master Algorithm, and our task is to understand them and shape their growth to better serve our needs.</p>

<p class="noindent chinese">这些工具中最主要的是主算法。不管它是早还是晚，也不管它看起来是否像炼金术，都不如它所囊括的东西重要：学习算法的基本能力，以及它们会把我们带到哪里去。我们同样可以把主算法看作是当前和未来学习者的综合图景，我们可以方便地在我们的思想实验中使用它，而不是产品 X 或网站 Y 中的具体算法，反正这些公司不太可能与我们分享。从这个角度来看，我们每天与之打交道的学习者都是主算法的雏形，我们的任务是了解他们并塑造他们的成长，以更好地满足我们的需求。</p>

<p class="noindent english">In the coming decades, machine learning will affect such a broad swath of human life that one chapter of one book cannot possibly do it justice. Nevertheless, we can already see a number of recurring themes, and it’s those we’ll focus on, starting with what psychologists call theory of mind—the computer’s theory of your mind, that is.</p>

<p class="noindent chinese">在未来的几十年里，机器学习将影响到人类生活的方方面面，以至于一本书的一个章节不可能对其做出公正的评价。不过，我们已经可以看到一些反复出现的主题，我们将重点讨论这些主题，首先是心理学家所说的心智理论 —— 计算机对你心智的理论，也就是说。</p>

<h1 id="babilu_link-451"><b>Sex, lies, and machine learning</b></h1>

<h1 id="babilu_link-451"><b>性、谎言和机器学习</b></h1>

<p class="noindent english">Your digital future begins with a realization: every time you interact with a computer—whether it’s your smart phone or a server thousands of miles away—you do so on two levels. The first one is getting what you want there and then: an answer to a question, a product you want to buy, a new credit card. The second level, and in the long run the most important one, is teaching the computer about you. The more you teach it, the better it can serve you—or manipulate you. Life is a game between you and the learners that surround you. You can refuse to play, but then you’ll have to live a twentieth-century life in the twenty-first. Or you can play to win. What model of you do you want the computer to have? And what data can you give it that will produce that model? Those two questions should always be in the back of your mind whenever you interact with a learning algorithm—as they are when you interact with other people. Alice knows that Bob has a mental model of her and seeks to shape it through her behavior. If Bob is her boss, she tries to come across as competent, loyal, and hardworking. If instead Bob is someone she’s trying to seduce, she’ll be at her most seductive. We could hardly function in society without this ability to intuit and respond to what’s on other people’s minds. The novelty in the world today is that computers, not just people, are starting to have theories of mind. Their theories are still primitive, but they’re evolving quickly, and they’re what we have to work with to get what we want—no less than with other people. And so <i>you</i> need a theory of <i>the computer</i> ’s mind, <a id="babilu_link-239"></a> and that’s what the Master Algorithm provides, after plugging in the score function (what you think the learner’s goals are, or more precisely its owner’s) and the data (what you think it knows).</p>

<p class="noindent chinese">你的数字未来始于一个认识：每次你与电脑互动 —— 不管是你的智能手机还是千里之外的服务器 —— 你都是在两个层面上进行的。第一个层次是在那里得到你想要的东西：一个问题的答案，一个你想购买的产品，一张新的信用卡。第二个层次，从长远来看也是最重要的一个层次，是让电脑了解你。你教得越多，它就越能为你服务 —— 或操纵你。生活是你和你周围的学习者之间的游戏。你可以拒绝游戏，但那样你将不得不在二十一世纪过着二十世纪的生活。或者你可以通过游戏来赢得胜利。你希望计算机拥有什么样的你的模型？你能给它什么数据来产生这个模型？当你与学习算法互动时，这两个问题应该始终在你的脑海里，正如你与其他人互动时一样。爱丽丝知道鲍勃对她有一个心理模型，并试图通过她的行为来塑造这个模型。如果鲍勃是她的老板，她会努力表现得能干、忠诚和勤奋。如果鲍勃是她想勾引的人，她就会表现得最有诱惑力。如果没有这种对他人想法的直觉和反应能力，我们很难在社会中发挥作用。当今世界的新奇之处在于，计算机，而不仅仅是人，开始有了思想理论。他们的理论仍然是原始的，但他们正在迅速发展，而且他们是我们必须用它来获得我们想要的东西 —— 不亚于与其他人。因此，<i>你</i> 需要一个<i>计算机</i>心智理论，这就是主算法所提供的，在插入分数函数（你认为学习者的目标是什么，或者更准确地说，它的主人的目标）和数据（你认为它知道什么）之后。</p>

<p class="noindent english">Take online dating. When you use Match.com, eHarmony, or OkCupid (suspend your disbelief, if necessary), your goal is simple: to find the best possible date you can. But chances are it will take a lot of work and several disappointing dates before you meet someone you really like. One hardy geek extracted twenty thousand profiles from OkCupid, did his own data mining, found the woman of his dreams on the eighty-eighth date, and told his odyssey to <i>Wired</i> magazine. To succeed with fewer dates and less work, your two main tools are your profile and your responses to suggested matches. One popular option is to lie (about your age, for example). This may seem unethical, not to mention liable to blow up in your face when your date discovers the truth, but there’s a twist. Savvy online daters already know that people lie about their age on their profiles and adjust accordingly, so if you state your true age, you’re effectively telling them you’re older than you really are! In turn, the learner doing the matching thinks people prefer younger dates than they really do. The logical next step is for people to lie about their age by even more, ultimately rendering this attribute meaningless.</p>

<p class="noindent chinese">以网上约会为例。当你使用 Match.com、eHarmony 或 OkCupid（如有必要，请暂停你的怀疑）时，你的目标很简单：找到你能找到的最佳约会对象。但是，在你遇到你真正喜欢的人之前，有可能需要大量的工作和几次令人失望的约会。一个顽强的极客从 OkCupid 提取了两万份资料，做了自己的数据挖掘，在第八十八次约会中找到了他的梦中情人，并向《<i>连线</i>》杂志讲述了他的艰辛历程。为了以较少的日期和较少的工作取得成功，你的两个主要工具是你的个人资料和你对建议的匹配对象的反应。一个流行的选择是撒谎（例如，关于你的年龄）。这似乎是不道德的，更不用说当你的约会对象发现真相时可能会让你大吃一惊，但还有一个转折点。精明的网上约会者已经知道，人们会在他们的个人资料中谎报年龄，并相应地进行调整，所以如果你说出你的真实年龄，你实际上是在告诉他们你比实际年龄要大！反过来，进行学习的人也会在他们的个人资料中说谎。反过来，进行匹配的学习者认为人们喜欢比他们真正年轻的约会。合乎逻辑的下一步是人们对自己的年龄撒谎更多，最终使这个属性变得毫无意义。</p>

<p class="noindent english">A better way for all concerned is to focus on your specific, unusual attributes that are highly predictive of a match, in the sense that they pick out people you like that not everyone else does, and therefore have less competition for. Your job (and your prospective date’s) is to provide these attributes. The matcher’s job is to learn from them, in the same way that an old-fashioned matchmaker would. Compared to a village matchmaker, Match.com’s algorithm has the advantage that it knows vastly more people, but the disadvantage is that it knows them much more superficially. A naïve learner, such as a perceptron, will be content with broad generalizations like “gentlemen prefer blondes.” A more sophisticated one will find patterns like “people with the same unusual musical tastes are often good matches.” If Alice and Bob both like Beyoncé, that alone hardly singles them out for each other. But if they both like Bishop Allen, that makes them at least a little bit more likely to <a id="babilu_link-48"></a> be potential soul mates. If they’re both fans of a band the learner does not know about, that’s even better, but only a relational algorithm like Alchemy can pick it up. The better the learner, the more it’s worth your time to teach it about you. But as a rule of thumb, you want to differentiate yourself enough so that it won’t confuse you with the “average person” (remember Bob Burns from <a href="#babilu_link-5">Chapter 8</a>), but not be so unusual that it can’t fathom you.</p>

<p class="noindent chinese">对所有相关人员来说，更好的方法是专注于你特定的、不寻常的属性，这些属性对匹配有很高的预测性，在这个意义上，它们会挑出你喜欢的人，而不是其他人都喜欢的人，因此竞争较少。你的工作（和你未来的约会对象）是提供这些属性。牵线人的工作是向他们学习，就像老式的媒人一样。与村里的媒人相比，Match.com 的算法的优势是它认识的人多得多，但劣势是它对他们的了解更肤浅。一个天真的学习者，如感知器，将满足于像 “绅士喜欢金发女郎” 这样的广泛概括。一个更复杂的学习者会发现一些模式，如 “具有相同的不寻常的音乐品味的人往往是很好的匹配”。如果爱丽丝和鲍勃都喜欢碧昂斯，单单这一点就很难让他们彼此相中。但如果他们都喜欢毕晓普·艾伦，这至少使他们更有可能成为潜在的灵魂伴侣。如果他们都是学习者不知道的乐队的粉丝，那就更好了，但只有像炼金术这样的关系型算法才能发现它。学习者越好，就越值得你花时间去教他了解你。但作为一条经验法则，你要把自己区分开来，使它不会把你和 “普通人” 混淆起来（记得<a href="#babilu_link-5">第八章</a>中的鲍勃·伯恩斯），但也不能太不寻常，以至于它无法理解你。</p>

<p class="noindent english">Online dating is in fact a tough example because chemistry is hard to predict. Two people who hit it off on a date may wind up falling in love and believing passionately that they were made for each other, but if their initial conversation takes a different turn, they might instead find each other annoying and never want to meet again. What a really sophisticated learner would do is run a thousand Monte Carlo simulations of a date between each pair of plausible matches and rank the matches by the fraction of dates that turned out well. Short of that, dating sites can organize parties and invite people who are each a likely match for many of the others, letting them accomplish in a few hours what would otherwise take weeks.</p>

<p class="noindent chinese">网上约会实际上是一个艰难的例子，因为化学反应是难以预测的。两个人在约会中一拍即合，最终可能会坠入爱河，并热切地相信他们是天生一对，但如果他们最初的谈话出现了不同的情况，他们可能反而会觉得对方很讨厌，再也不想见面了。一个真正复杂的学习者会做的是，在每一对貌似匹配的人之间进行一千次蒙特卡洛模拟约会，并根据约会结果的百分比对匹配的人进行排名。除此之外，约会网站可以组织聚会，邀请那些可能与其他许多人相匹配的人，让他们在几个小时内完成本来需要几周的工作。</p>

<p class="noindent english">For those of us who are not keen on online dating, a more immediately useful notion is to choose which interactions to record and where. If you don’t want your Christmas shopping to leave Amazon confused about your tastes, do it on other sites. (Sorry, Amazon.) If you watch different kinds of videos at home and for work, keep two accounts on YouTube, one for each, and YouTube will learn to make the corresponding recommendations. And if you’re about to watch some videos of a kind that you ordinarily have no interest in, log out first. Use Chrome’s incognito mode not for guilty browsing (which you’d never do, of course) but for when you don’t want the current session to influence future personalization. On Netflix, adding profiles for the different people using your account will spare you R-rated recommendations on family movie night. If you don’t like a company, click on their ads: this will not only waste their money now, but teach Google to waste it again in the future by showing the ads to people who are unlikely to buy the products. And if you have very specific queries that you want Google to <a id="babilu_link-308"></a> answer correctly in the future, take a moment to trawl through the later results pages for the relevant links and click on them. More generally, if a system keeps recommending the wrong things to you, try teaching it by finding and clicking on a bunch of the right ones and come back later to see if it did.</p>

<p class="noindent chinese">对于我们这些不热衷于网上约会的人来说，一个更直接有用的概念是选择记录哪些互动，在哪里记录。如果你不希望你的圣诞购物让亚马逊对你的品味感到困惑，那就在其他网站上做吧。（对不起，亚马逊。）如果你在家里和工作时看不同类型的视频，在 YouTube 上保留两个账户，各一个，YouTube 将学会做出相应的推荐。如果你要看一些你通常不感兴趣的视频，请先注销。使用 Chrome 浏览器的隐身模式，不是为了有罪的浏览（当然，你永远不会这样做），而是当你不想让当前会话影响未来的个性化。在 Netflix 上，为使用你账户的不同人添加档案，可以在家庭电影之夜为你提供 R 级推荐。如果你不喜欢一家公司，就点击他们的广告：这不仅会浪费他们现在的钱，而且会教谷歌在未来再次浪费钱，向那些不可能购买产品的人展示广告。如果你有非常具体的查询，你希望谷歌在未来能正确回答，花点时间在后来的结果页中浏览相关链接，并点击它们。更为普遍的是，如果一个系统一直向你推荐错误的东西，可以试着教它，找到并点击一堆正确的东西，以后再来看看它是否做到了。</p>

<p class="noindent english">That could be a lot of work, though. What all of these illustrate, unfortunately, is how narrow the communication channel between you and the learner is today. You should be able to tell it as much as you want about yourself, not just have it learn indirectly from what you do. More than that, you should be able to inspect the learner’s model of you and correct it as desired. The learner can still decide to ignore you, if it thinks you’re lying or are low on self-knowledge, but at least it would be able to take your input into account. For this, the model needs to be in a form that humans can understand, such as a set of rules rather than a neural network, and it needs to accept general statements as input in addition to raw data, as Alchemy does. All of which brings us to the question of how good a model of you a learner can have and what you’d want to do with that model.</p>

<p class="noindent chinese">不过，这可能是一个很大的工作。不幸的是，所有这些都说明，今天你和学习者之间的沟通渠道是多么狭窄。你应该能够尽可能多地告诉它你的情况，而不是让它间接地从你的工作中学习。不仅如此，你应该能够检查学习者对你的模型，并根据需要对其进行修正。如果学习者认为你在撒谎或自我认识不足，它仍然可以决定忽略你，但至少它能够考虑到你的意见。为此，模型需要采用人类可以理解的形式，比如一套规则而不是一个神经网络，而且除了原始数据之外，它还需要接受一般的陈述作为输入，就像炼金术那样。所有这些都给我们带来了一个问题：一个学习者可以拥有多好的模型，以及你想用这个模型做什么。</p>

<h1 id="babilu_link-452"><b>The digital mirror</b></h1>

<h1 id="babilu_link-452"><b>数字镜像</b></h1>

<p class="noindent english">Take a moment to consider all the data about you that’s recorded on all the world’s computers: your e-mails, Office docs, texts, tweets, and Facebook and LinkedIn accounts; your web searches, clicks, downloads, and purchases; your credit, tax, phone, and health records; your Fitbit statistics; your driving as recorded by your car’s microprocessors; your wanderings as recorded by your cell phone; all the pictures of you ever taken; brief cameos on security cameras; your Google Glass snippets—and so on and so forth. If a future biographer had access to nothing but this “data exhaust” of yours, what picture of you would he form? Probably a quite accurate and detailed one in many ways, but also one where some essential things would be missing. Why did you, one beautiful day, decide to change careers? Could the biographer have predicted it ahead of time? What about that person you met one day and secretly <a id="babilu_link-57"></a> never forgot? Could the biographer wind back through the found footage and say “Ah, there”?</p>

<p class="noindent chinese">花点时间考虑一下世界上所有电脑上记录的关于你的所有数据：你的电子邮件、Office 文档、短信、推特、Facebook 和 LinkedIn 账户；你的网络搜索、点击、下载和购买；你的信贷、税务、电话和健康记录；你的 Fitbit 统计数据；你的汽车微处理器所记录的你的驾驶情况；你的手机所记录的你的漫游；所有曾经拍摄的你的照片；安全摄像机上的短暂登场；你的谷歌眼镜片段，等等，等等。如果未来的传记作者只能获得你的这些 “数据资料”，他会对你形成怎样的印象？可能在许多方面是相当准确和详细的，但也会缺少一些基本的东西。为什么你会在一个美丽的日子里决定改变职业？传记作者能提前预测到这一点吗？有一天你遇到的那个人又是怎样的呢？秘密地永远不会忘记？传记作者能否在发现的片段中回过头来说 “啊，在那里”？</p>

<p class="noindent english">The sobering (or perhaps reassuring) thought is that no learner in the world today has access to all this data (not even the NSA), and even if it did, it wouldn’t know how to turn it into a real likeness of you. But suppose you took all your data and gave it to the—real, future—Master Algorithm, already seeded with everything we could teach it about human life. It would learn a model of you, and you could carry that model in a thumb drive in your pocket, inspect it at will, and use it for everything you pleased. It would surely be a wonderful tool for introspection, like looking at yourself in the mirror, but it would be a digital mirror that showed not just your looks but all things observable about you—a mirror that could come alive and converse with you. What would you ask it? Some of the answers you might not like, but that would be all the more reason to ponder them. And some would give you new ideas, new directions. The Master Algorithm’s model of you might even help you become a better person.</p>

<p class="noindent chinese">令人清醒的（或者说令人放心的）想法是，今天世界上没有任何学习者能够接触到所有这些数据（甚至是国家安全局），即使它知道，它也不知道如何把它变成你的真实形象。但是，假设你把你所有的数据都交给真正的、未来的主算法，它已经被植入了我们能教给它的关于人类生活的一切。它将学习一个你的模型，而你可以把这个模型放在你口袋里的优盘里，随意检查，并把它用于你喜欢的一切。它肯定会是一个极好的内省工具，就像照镜子一样，但它将是一面数字镜子，不仅显示你的长相，而且显示关于你的所有可观察到的东西 —— 一面可以活过来并与你交谈的镜子。你会问它什么？有些答案你可能不喜欢，但那将是更多的理由去思考它们。有些会给你带来新的想法，新的方向。算法大师对你的模型甚至可能帮助你成为一个更好的人。</p>

<p class="noindent english">Self-improvement aside, probably the first thing you’d want your model to do is negotiate the world on your behalf: let it loose in cyberspace, looking for all sorts of things for you. From all the world’s books, it would suggest a dozen you might want to read next, with more insight than Amazon could dream of. Likewise for movies, music, games, clothes, electronics—you name it. It would keep your refrigerator stocked at all times, natch. It would filter your e-mail, voice mail, Facebook posts, and Twitter feed and, when appropriate, reply on your behalf. It would take care of all the little annoyances of modern life for you, like checking credit-card bills, disputing improper charges, making arrangements, renewing subscriptions, and filling out tax returns. It would find a remedy for your ailment, run it by your doctor, and order it from Walgreens. It would bring interesting job opportunities to your attention, propose vacation spots, suggest which candidates to vote for on the ballot, and screen potential dates. And, after the match was made, it would team up with your date’s model to pick some restaurants you might both like. Which is where things start to get <i>really</i> interesting.</p>

<p class="noindent chinese">撇开自我提高不谈，你可能希望你的模型做的第一件事就是代表你与世界进行谈判：让它在网络空间放纵，为你寻找各种东西。从世界上所有的书籍中，它将为你推荐一打你可能想读的书，其洞察力比亚马逊所梦想的还要强。同样，电影、音乐、游戏、服装、电子产品 —— 你的名字。它将使你的冰箱随时保持库存，当然。它将过滤你的电子邮件、语音邮件、Facebook 帖子和 Twitter 信息，并在适当的时候代表你回复。它将为你处理现代生活中的所有小麻烦，如检查信用卡账单、对不正当收费提出异议、安排、续订和填写报税单。它可以为你的疾病找到补救办法，让你的医生进行检查，并从沃尔格林订购。它可以让你注意到有趣的工作机会，建议你去度假，建议你在选票上投票给哪个候选人，并筛选潜在的约会对象。而且，在配对成功后，它将与你的约会对象的模型合作，挑选一些你们都可能喜欢的餐馆。这就是事情开始变得<i>真正</i>有趣的地方。</p>

<h1 id="babilu_link-453"><b><a id="babilu_link-293"><b></b></a> A society of models</b></h1>

<h1 id="babilu_link-453"><b><a id="babilu_link-293"><b></b></a>模式的社会</b></h1>

<p class="noindent english">In this rapidly approaching future, you’re not going to be the only one with a “digital half” doing your bidding twenty-four hours a day. Everyone will have a detailed model of him- or herself, and these models will talk to each other all the time. If you’re looking for a job and company X is looking to hire, its model will interview your model. It will be a lot like a real, flesh-and-blood interview—your model will still be well advised to not volunteer negative information about you, and so on—but it will take only a fraction of a second. You’ll click on “Find Job” in your future LinkedIn account, and you’ll immediately interview for every job in the universe that remotely fits your parameters (profession, location, pay, etc.). LinkedIn will respond on the spot with a ranked list of the best prospects, and out of those, you’ll pick the first company that you want to have a chat with. Same with dating: your model will go on millions of dates so you don’t have to, and come Saturday, you’ll meet your top prospects at an OkCupid-organized party, knowing that you’re also one of <i>their</i> top prospects—and knowing, of course, that their <i>other</i> top prospects are also in the room. It’s sure to be an interesting night.</p>

<p class="noindent chinese">在这个迅速接近的未来，你不会是唯一一个有 “数字半身” 的人，一天二十四小时为你服务。每个人都会有一个他或她自己的详细模型，而这些模型将一直相互交谈。如果你在找工作，而 X 公司正在找人，它的模型会面试你的模型。这将很像一个真正的、有血有肉的面试 —— 你的模特仍然会被建议不要主动提供关于你的负面信息，等等 —— 但它只需要一小部分时间。你将在你未来的 LinkedIn 账户中点击 “寻找工作”，然后你将立即对宇宙中所有与你的参数（职业、地点、薪酬等）相匹配的工作进行面试。LinkedIn 会当场回应，并提供一份最佳前景的排名列表，在这些列表中，你会挑选出第一个你想与之交谈的公司。约会也是一样：你的模特会去参加数以百万计的约会，这样你就不用去了，到了周六，你会在 OkCupid 组织的派对上见到你的最佳潜在客户，知道你也是<i>他们的</i>最佳潜在客户之一 —— 当然，知道他们的<i>其他</i>最佳潜在客户也在这个房间里。这肯定会是一个有趣的夜晚。</p>

<p class="noindent english">In the world of the Master Algorithm, “my people will call your people” becomes “my program will call your program.” Everyone has an entourage of bots, smoothing his or her way through the world. Deals get pitched, terms negotiated, arrangements made, all before you lift a finger. Today, drug companies target your doctor, because he decides what drugs to prescribe to you. Tomorrow, the purveyors of every product and service you use, or might use, will target your model, because your model will screen them for you. Their bots’ job is to get your bot to buy. Your bot’s job is to see through their claims, just as you see through TV commercials, but at a much finer level of detail, one that you’d never have the time or patience for. Before you buy a car, the digital you will go over every one of its specs, discuss them with the manufacturer, and study everything anyone in the world has said about that car and its alternatives. Your digital half will be like power steering for your life: it goes where you want to go but with less effort from you. This does <a id="babilu_link-225"></a> not mean that you’ll end up in a “filter bubble,” seeing only what you reliably like, with no room for the unexpected; the digital you knows better than that. Part of its brief is to leave some things open to chance, to expose you to new experiences, and to look for serendipity.</p>

<p class="noindent chinese">在主算法的世界里，“我的人将呼叫你的人” 变成 “我的程序将呼叫你的程序”。每个人都有一个随行的机器人，使他或她在这个世界上走得更顺畅。交易的推销、条款的谈判、安排的达成，都在你动动手指之前。今天，制药公司以你的医生为目标，因为他决定给你开什么药。明天，你使用或可能使用的每一种产品和服务的提供者都将以你的模型为目标，因为你的模型将为你筛选它们。他们的机器人的工作是让你的机器人去购买。你的机器人的工作是看穿他们的说法，就像你看穿电视广告一样，但在一个更精细的层面上，你永远没有时间或耐心。在你买车之前，数字化的你会仔细研究它的每一项规格，与制造商讨论，并研究世界上任何人对该车及其替代品的所有评价。你的数字半边天将像你生活中的动力转向一样：它去你想去的地方，但你的努力要少一些。这并不意味着你会在一个 “过滤气泡” 中结束，只看到你可靠地喜欢的东西，而没有空间来应对意外；数字化的你比这更清楚。它的部分内容是把一些事情留给机会，让你接触新的体验，并寻找偶然性。</p>

<p class="noindent english">Even more interesting, the process doesn’t end when you find a car, a house, a doctor, a date, or a job. Your digital half is continually learning from its experiences, just as you would. It figures out what works and doesn’t, whether it’s in job interviews, dating, or real-estate hunting. It learns about the people and organizations it interacts with on your behalf and then (even more important) from your real-world interactions with them. It predicted Alice would be a great date for you, but you had an awkward time, so it hypothesizes possible reasons, which it will test on your next round of dating. It shares its most important findings with you. (“You believe you like X, but in reality you tend to go for Y.”) Comparing your experiences of various hotels with their reviews on TripAdvisor, it figures out what the really telling tidbits are and looks for them in the future. It learns not just which online merchants are more trustworthy but how to decode what the less trustworthy ones say. Your digital half has a model of the world: not just of the world in general but of the world as it relates to you. At the same time, of course, everyone else also has a continually evolving model of his or her world. Every party to an interaction learns from it and applies what it’s learned to its next interactions. You have your model of every person and organization you interact with, and they each have their model of you. As the models improve, their interactions become more and more like the ones you would have in the real world—except millions of times faster and in silicon. Tomorrow’s cyberspace will be a vast parallel world that selects only the most promising things to try out in the real one. It will be like a new, global subconscious, the collective id of the human race.</p>

<p class="noindent chinese">更有趣的是，当你找到一辆车、一所房子、一个医生、一个约会或一份工作时，这个过程并没有结束。你的数字半边天正在不断地从它的经验中学习，就像你一样。不管是在求职面试、约会还是在寻找房地产时，它都会找出有效和无效的方法。它代表你学习与之互动的人和组织，然后（更重要的是）从你与他们的现实世界互动中学习。它预测爱丽丝会是你的一个很好的约会对象，但你有一个尴尬的时间，所以它假设了可能的原因，它将在你的下一轮约会中测试这些原因。它与你分享它最重要的发现。（“你相信你喜欢 X，但实际上你倾向于选择 Y。”）将你对各种酒店的体验与 TripAdvisor 上的评论进行比较，它找出真正有说服力的花絮，并在未来寻找它们。它不仅学会了哪些在线商家更值得信赖，而且学会了如何解码那些不太值得信赖的商家的言论。你的数字半边天有一个世界的模型：不仅是一般的世界，而且是与你有关的世界。当然，与此同时，其他人也有一个不断发展的他或她的世界的模型。互动的每一方都会从中学习，并将学到的东西运用到下一次的互动中。你对与你互动的每个人和组织都有自己的模型，而他们对你也都有自己的模型。随着模型的改进，他们的互动变得越来越像你在现实世界中的互动 —— 只不过速度快了几百万倍，而且是在硅中。明天的网络空间将是一个巨大的平行世界，它只选择最有希望的东西在现实世界中进行试验。它将像一个新的、全球性的潜意识，是人类的集体偶像。</p>

<h1 id="babilu_link-454"><b>To share or not to share, and how and where</b></h1>

<h1 id="babilu_link-454"><b>分享或不分享，以及如何和在哪里分享</b></h1>

<p class="noindent english">Of course, learning about the world all by yourself is slow, even if your digital half does it orders of magnitude faster than the flesh-and-blood <a id="babilu_link-51"></a> you. If others learn about you faster than you learn about them, you’re in trouble. The answer is to share: a million people learn about a company or product a lot faster than a single one does, provided they pool their experiences. But who should you share data with? That’s perhaps the most important question of the twenty-first century.</p>

<p class="noindent chinese">当然，自己一个人了解世界是很慢的，即使你的数字部分比有血有肉的也要快上几个数量级。如果别人对你的了解比你对他们的了解更快，你就有麻烦了。答案是分享：一百万人了解一个公司或产品比一个人快得多，只要他们把他们的经验集中起来。但是，你应该与谁分享数据？这也许是二十一世纪最重要的问题。</p>

<p class="noindent english">Today your data can be of four kinds: data you share with everyone, data you share with friends or coworkers, data you share with various companies (wittingly or not), and data you don’t share. The first type includes things like Yelp, Amazon, and TripAdvisor reviews, eBay feedback scores, LinkedIn résumés, blogs, tweets, and so on. This data is very valuable and is the least problematic of the four. You make it available to everyone because you want to, and everyone benefits. The only problem is that the companies hosting the data don’t necessarily allow it to be downloaded in bulk for building models. They should. Today you can go to TripAdvisor and see the reviews and star ratings of particular hotels you’re considering, but what about a model of what makes a hotel good or bad in general, which you could use to rate hotels that currently have few or no reliable reviews? TripAdvisor could learn it, but what about a model of what makes a hotel good or bad <i>for you</i> ? This requires information about you that you may not want to share with TripAdvisor. What you’d like is a trusted party that combines the two types of data and gives you the results.</p>

<p class="noindent chinese">今天，你的数据可以有四种：你与所有人分享的数据，你与朋友或同事分享的数据，你与各种公司分享的数据（有意或无意地），以及你不分享的数据。第一种类型包括像 Yelp、Amazon 和 TripAdvisor 评论、eBay 反馈分数、LinkedIn 简历、博客、推特等等。这种数据非常有价值，是四种数据中问题最少的一种。你把它提供给每个人，因为你想这样做，而每个人都会受益。唯一的问题是，托管这些数据的公司不一定允许大量下载这些数据来建立模型。他们应该这样做。今天，你可以去 TripAdvisor，看到你正在考虑的特定酒店的评论和星级，但什么是使酒店好或坏的一般模型，你可以用它来评价那些目前很少或没有可靠评论的酒店？TripAdvisor 可以学习它，但<i>对你来说</i>，什么是使酒店好或坏的模型呢？这需要关于你的信息，你可能不想与 TripAdvisor 分享。你想要的是一个可信赖的一方，将这两类数据结合起来并给你结果。</p>

<p class="noindent english">The second kind of data should also be unproblematic, but it isn’t because it overlaps with the third. You share updates and pictures with your friends on Facebook, and they with you. But everyone shares their updates and pictures with Facebook. Lucky Facebook: it has a billion friends. Day by day, it learns a lot more about the world than any one person does. It would learn even more if it had better algorithms, and they are getting better every day, courtesy of us data scientists. Facebook’s main use for all this knowledge is to target ads to you. In return, it provides the infrastructure for your sharing. That’s the bargain you make when you use Facebook. As its learning algorithms improve, it gets more and more value out of the data, and some of that value returns to you in the form of more relevant ads and better service. The <a id="babilu_link-20"></a> only problem is that Facebook is also free to do things with the data and the models that are not in your interest, and you have no way to stop it.</p>

<p class="noindent chinese">第二种数据也应该是没有问题的，但它不是，因为它与第三种数据重叠了。你与你在 Facebook 上的朋友分享更新和图片，他们也与你分享。但每个人都会与 Facebook 分享他们的更新和图片。幸运的 Facebook：它有十亿个朋友。日复一日，它比任何一个人都更多地了解这个世界。如果它有更好的算法，它就会学到更多，而这些算法每天都在变得更好，这是我们数据科学家的功劳。Facebook 对所有这些知识的主要用途是向你投放广告。作为回报，它为你的分享提供基础设施。这是你使用 Facebook 时的交易。随着其学习算法的改进，它从数据中获得越来越多的价值，其中一些价值以更相关的广告和更好的服务的形式返回给你。唯一的问题是，Facebook 也可以自由地利用这些数据和模型做一些不符合你利益的事情，而你没有办法阻止它。</p>

<p class="noindent english">This problem pops up across the board with data you share with companies, which these days includes pretty much everything you do online as well as a lot of what you do offline. In case you haven’t noticed, there’s a mad race to gather data about you. Everybody loves your data, and no wonder: it’s the gateway to your world, your money, your vote, even your heart. But everyone has only a sliver of it. Google sees your searches, Amazon your online purchases, AT&amp;T your phone calls, Apple your music downloads, Safeway your groceries, Capital One your credit-card transactions. Companies like Acxiom collate and sell information about you, but if you inspect it (which in Acxiom’s case you can, at aboutthedata.com), it’s not much, and some of it is wrong. No one has anything even approaching a complete picture of you. That’s both good and bad. Good because if someone did, they’d have far too much power. Bad because as long as that’s the case there can be no 360-degree model of you. What you really want is a digital you that you’re the sole owner of and that others can access only on your terms.</p>

<p class="noindent chinese">这个问题在你与公司分享的数据中全面出现，如今，这包括你在网上做的几乎所有事情，以及你在网下做的很多事情。如果你没有注意到，现在有一场疯狂的竞赛来收集关于你的数据。每个人都喜欢你的数据，这也难怪：它是进入你的世界、你的钱、你的投票、甚至你的心的通道。但每个人都只拥有其中的一小部分。谷歌看到你的搜索，亚马逊看到你的网上购物，AT&amp;T 看到你的电话，苹果看到你的音乐下载，Safeway 看到你的杂货，Capital One 看到你的信用卡交易。像 Acxiom 这样的公司整理并出售关于你的信息，但如果你检查它（在 Acxiom 的情况下，你可以在 aboutthedata.com 上检查），它并不多，而且其中一些是错误的。没有人有任何东西甚至接近于对你的完整描述。这既是好事也是坏事。好是因为如果有人有，他们就会有太多的权力。坏的是，只要情况如此，就不可能有 360 度的你的模型。你真正想要的是一个数字的你，你是唯一的主人，别人只能按照你的条件访问。</p>

<p class="noindent english">The last type of data—data you don’t share—also has a problem, which is that maybe you should share it. Maybe it hasn’t occurred to you to do so, maybe there’s no easy way to, or maybe you just don’t want to. In the latter case, you should consider whether you have an ethical responsibility to share. One example we’ve seen is cancer patients, who can contribute to curing cancer by sharing their tumors’ genomes and treatment histories. But it goes well beyond that. All sorts of questions about society and policy can potentially be answered by learning from the data we generate in our daily lives. Social science is entering a golden age, where it finally has data commensurate with the complexity of the phenomena it studies, and the benefits to all of us could be enormous—provided the data is accessible to researchers, policy makers, and citizens. This does not mean letting others peek into your private life; it means letting them see the learned models, which should contain only statistical information. So between you and them there needs to be <a id="babilu_link-89"></a> an honest data broker that guarantees your data won’t be misused, but also that no free riders share the benefits without sharing the data.</p>

<p class="noindent chinese">最后一种数据 —— 你不分享的数据 —— 也有一个问题，那就是，也许你应该分享它。也许你还没有想到要这样做，也许没有简单的方法，或者你只是不想这样做。在后一种情况下，你应该考虑你是否有道德上的责任来分享。我们看到的一个例子是癌症患者，他们可以通过分享他们的肿瘤基因组和治疗历史来为治愈癌症作出贡献。但这远远不止这些。关于社会和政策的各种问题都有可能通过学习我们在日常生活中产生的数据而得到答案。社会科学正在进入一个黄金时代，它终于拥有了与它所研究的现象的复杂性相称的数据，而且对我们所有人的好处可能是巨大的 —— 只要研究人员、政策制定者和公民能够获得这些数据。这并不意味着让别人偷看你的私人生活；这意味着让他们看到所学到的模型，这些模型应该只包含统计信息。因此，在你和他们之间需要有一个诚实的数据经纪人，保证你的数据不会被滥用，但也要保证没有免费搭车者在不分享数据的情况下分享利益。</p>

<p class="noindent english">In sum, all four kinds of data sharing have problems. These problems all have a common solution: a new type of company that is to your data what your bank is to your money. Banks don’t steal your money (with rare exceptions). They’re supposed to invest it wisely, and your deposits are FDIC-insured. Many companies today offer to consolidate your data somewhere in the cloud, but they’re still a far cry from your personal data bank. If they’re cloud providers, they try to lock you in—a big no-no. (Imagine depositing your money with Bank of America and not knowing if you’ll be able to transfer it to Wells Fargo somewhere down the line.) Some startups offer to hoard your data and then mete it out to advertisers in return for discounts, but to me that misses the point. Sometimes you want to give information to advertisers for free because it’s in your interests, sometimes you don’t want to give it at all, and what to share when is a problem that only a good model of you can solve.</p>

<p class="noindent chinese">总而言之，所有这四种数据共享都有问题。这些问题都有一个共同的解决方案：一种新型的公司，它对你的数据就像你的银行对你的钱一样。银行不会偷你的钱（除了极少数例外）。他们应该明智地投资，而且你的存款有 FDIC 保险。今天，许多公司提供将你的数据整合到云的某个地方，但他们与你的个人数据库仍有很大差距。如果他们是云计算供应商，他们会试图锁定你 —— 这是大忌。（想象一下，你把钱存入美国银行，却不知道你是否能把钱转到富国银行的某个地方）。一些创业公司提供囤积你的数据，然后将其提供给广告商以换取折扣，但对我来说，这忽略了重点。有时你想免费给广告商提供信息，因为这符合你的利益，有时你根本不想给，什么时候分享是一个问题，只有一个好的你的模型可以解决。</p>

<p class="noindent english">The kind of company I’m envisaging would do several things in return for a subscription fee. It would anonymize your online interactions, routing them through its servers and aggregating them with its other users’. It would store all the data from all your life in one place—down to your 24/7 Google Glass video stream, if you ever get one. It would learn a complete model of you and your world and continually update it. And it would use the model on your behalf, always doing exactly what you would, to the best of the model’s ability. The company’s basic commitment to you is that your data and your model will never be used against your interests. Such a guarantee can never be foolproof—you yourself are not guaranteed to never do anything against your interests, after all. But the company’s life would depend on it as much as a bank’s depends on the guarantee that it won’t lose your money, so you should be able to trust it as much as you trust your bank.</p>

<p class="noindent chinese">我所设想的那种公司将做几件事来换取订阅费。它将对你的在线互动进行匿名处理，通过其服务器进行路由，并将它们与其他用户的互动进行汇总。它将把你生活中的所有数据储存在一个地方，甚至包括你的 24/7 谷歌眼镜视频流，如果你有一个的话。它将学习一个关于你和你的世界的完整模型，并不断地更新它。它将代表你使用这个模型，总是准确地做你想做的事，以模型的最佳能力。该公司对你的基本承诺是，你的数据和你的模型将永远不会被用来损害你的利益。这样的保证不可能是万无一失的，毕竟你自己也不能保证永远不做违背你利益的事情。但是，公司的生命将取决于它，就像银行取决于它不会失去你的钱的保证一样，所以你应该能够信任它，就像你信任你的银行一样。</p>

<p class="noindent english">A company like this could quickly become one of the most valuable in the world. As Alexis Madrigal of the <i>Atlantic</i> points out, today <a id="babilu_link-155"></a> your profile can be bought for half a cent or less, but the value of a user to the Internet advertising industry is more like $1,200 per year. Google’s sliver of your data is worth about $20, Facebook’s $5, and so on. Add to that all the slivers that no one has yet, and the fact that the whole is more than the sum of the parts—a model of you based on all your data is much better than a thousand models based on a thousand slivers—and we’re looking at easily over a trillion dollars per year for an economy the size of the United States. It doesn’t take a large cut of that to make a Fortune 500 company. If you decide to take up the challenge and wind up becoming a billionaire, remember where you first got the idea.</p>

<p class="noindent chinese">这样的公司可以迅速成为世界上最有价值的公司之一。正如《<i>大西洋</i>》杂志的亚历克西斯·马德里加尔所指出的，今天你的资料可以用半美分或更少的钱买到，但一个用户对互联网广告行业的价值更像是每年 1200 美元。谷歌对你的数据的价值约为 20 美元，Facebook 为 5 美元，以此类推。再加上所有还没有人拥有的部分，以及整体大于部分之和的事实 —— 基于所有数据的你的模型比基于一千块碎片的一千个模型要好得多 —— 我们正在寻找像美国这样的经济规模，每年轻松超过一万亿美元。要想成为财富 500 强公司，并不需要从中抽出很大一部分。如果你决定接受挑战并最终成为亿万富翁，请记住你最初是在哪里得到这个想法的。</p>

<p class="noindent english">Of course, some existing companies would love to host the digital you. Google, for example. Sergey Brin says that “we want Google to be the third half of your brain,” and some of Google’s acquisitions are probably not unrelated to how well their streams of user data complement its own. But, despite their head start, companies like Google and Facebook are not well suited to being your digital home because they have a conflict of interest. They earn a living by targeting ads, and so they have to balance your interests and the advertisers’. You wouldn’t let the first or second half of your brain have divided loyalties, so why would you let the third?</p>

<p class="noindent chinese">当然，一些现有的公司很愿意主持数字你。例如，谷歌。谢尔盖·布林说，“我们希望谷歌成为你大脑的第三部分”，而谷歌的一些收购可能与他们的用户数据流对其自身的补充程度不无关系。但是，尽管谷歌和 Facebook 这样的公司处于领先地位，它们并不适合成为你的数字家园，因为它们有利益冲突。他们通过定位广告谋生，因此他们必须平衡你和广告商的利益。你不会让你的大脑的第一或第二部分有分裂的忠诚，那么你为什么要让第三部分呢？</p>

<p class="noindent english">One possible showstopper is that the government may subpoena your data or even preventively jail you, <i>Minority Report</i> –style, if your model looks like a criminal’s. To forestall that, your data company can keep everything encrypted, with the key in your possession. (These days you can even compute over encrypted data without ever decrypting it.) Or you can keep it all in your hard disk at home, and the company just rents you the software.</p>

<p class="noindent chinese">一个可能的障碍是，如果你的模型看起来像罪犯的模型，政府可能会传唤你的数据，甚至预防性地将你关进监狱，《<i>少数派报告</i>》的风格。为了防止这种情况，你的数据公司可以把所有的东西都加密，钥匙在你手里。（如今，你甚至可以在加密的数据上进行计算而不需要解密。）或者你可以把所有的东西都放在家里的硬盘里，而公司只是把软件租给你。</p>

<p class="noindent english">If you don’t like the idea of a profit-making entity holding the keys to your kingdom, you can join a data union instead. (If there isn’t one in your neck of the cyberwoods yet, consider starting it.) The twentieth century needed labor unions to balance the power of workers and bosses. The twenty-first needs data unions for a similar reason. Corporations have a vastly greater ability to gather and use data than <a id="babilu_link-253"></a> individuals. This leads to an asymmetry in power, and the more valuable the data—the better and more useful the models that can be learned from it—the greater the asymmetry. A data union lets its members bargain on equal terms with companies about the use of their data. Perhaps labor unions can get the ball rolling, and shore up their membership, by starting data unions for their members. But labor unions are organized by occupation and location; data unions can be more flexible. Join up with people you have a lot in common with; the models learned will be more useful to you that way. Notice that being in a data union does not mean letting other members see your data; it just means letting everyone use the models learned from the pooled data. Data unions can also be your vehicle for telling politicians what you want. Your data can influence the world as much as your vote—or more—because you only go to the polls on election day. On all other days, your data is your vote. Stand up and be counted!</p>

<p class="noindent chinese">如果你不喜欢一个以盈利为目的的实体掌握着你的王国的钥匙，你可以加入一个数据联盟来代替。（如果在你的网络森林里还没有这样的工会，那就考虑建立吧）。二十世纪需要工会来平衡工人和老板的权力。二十一世纪也需要数据联盟，原因类似。与个人相比，公司在收集和使用数据方面的能力要大得多。这导致了权力的不对称，而数据越有价值 —— 从数据中可以学到更好、更有用的模型 —— 不对称就越大。数据联盟让其成员在平等的条件下与公司就其数据的使用进行谈判。也许工会可以通过为其成员建立数据联盟来实现这一目标，并巩固其成员地位。但工会是按职业和地点组织的；数据联盟可以更灵活。与你有很多共同点的人联合起来；这样学到的模型对你更有用。请注意，加入数据联盟并不意味着让其他成员看到你的数据；它只是意味着让每个人都使用从集合数据中学习的模型。数据联盟也可以成为你告诉政治家你想要什么的工具。你的数据可以像你的投票一样影响世界，甚至更多，因为你只在选举日去投票。在所有其他日子里，你的数据就是你的选票。站起来，被计算！</p>

<p class="noindent english">So far I haven’t uttered the word <i>privacy</i> . That’s not by accident. Privacy is only one aspect of the larger issue of data sharing, and if we focus on it to the detriment of the whole, as much of the debate to date has, we risk reaching the wrong conclusions. For example, laws that forbid using data for any purpose other than the originally intended one are extremely myopic. (Not a single chapter of <i>Freakonomics</i> could have been written under such a law.) When people have to trade off privacy against other benefits, as when filling out a profile on a website, the implied value of privacy that comes out is much lower than if you ask them abstract questions like “Do you care about your privacy?” But privacy debates are more often framed in terms of the latter. The European Union’s Court of Justice has decreed that people have the right to be forgotten, but they also have the right to remember, whether it’s with their neurons or a hard disk. So do companies, and up to a point, the interests of users, data gatherers, and advertisers are aligned. Wasted attention benefits no one, and better data makes better products. Privacy is not a zero-sum game, even though it’s often treated like one.</p>

<p class="noindent chinese">到目前为止，我还没有说过<i>隐私</i>这个词。这并不是偶然的。隐私只是数据共享这个大问题的一个方面，如果我们专注于它而损害了整体，就像迄今为止的大部分辩论那样，我们有可能得出错误的结论。例如，禁止将数据用于原定目的之外的任何其他目的的法律是非常近视的。（在这样的法律下，《<i>自由经济学</i>》的任何一章都不可能写出来）。当人们不得不在隐私与其他利益之间进行权衡时，比如在网站上填写个人资料时，得出的隐私隐含价值要比你问他们 “你关心你的隐私吗？” 这样的抽象问题低得多。但隐私问题的辩论更多是以后者为框架。欧盟法院已经宣布，人们有被遗忘的权利，但他们也有记忆的权利，无论是用他们的神经元还是用硬盘。公司也是如此，而且在一定程度上，用户、数据收集者和广告商的利益是一致的。浪费注意力对谁都没有好处，而更好的数据会带来更好的产品。隐私不是一个零和游戏，尽管它经常被当作一个零和游戏。</p>

<p class="noindent english">Companies that host the digital you and data unions are what a mature future of data in society looks like to me. Whether we’ll get there is <a id="babilu_link-190"></a> an open question. Today, most people are unaware of both how much data about them is being gathered and what the potential costs and benefits are. Companies seem content to continue doing it under the radar, terrified of a blowup. But sooner or later a blowup will happen, and in the ensuing fracas, draconian laws will be passed that in the end will serve no one. Better to foster awareness now and let everyone make their individual choices about what to share, what not, and how and where.</p>

<p class="noindent chinese">在我看来，托管数字你和数据联盟的公司就是社会中数据的成熟未来。我们是否能达到这个目标，是一个开放的问题。今天，大多数人都不知道有多少关于他们的数据正在被收集，以及潜在的成本和收益是什么。公司似乎满足于继续在雷达下做这件事，害怕发生爆炸。但是，爆炸迟早会发生，在随后的争吵中，严厉的法律将被通过，最终不会对任何人有利。最好现在就培养意识，让每个人对分享什么，不分享什么，以及如何分享和在哪里分享做出自己的选择。</p>

<h1 id="babilu_link-455"><b>A neural network stole my job</b></h1>

<h1 id="babilu_link-455"><b>一个神经网络偷走了我的工作</b></h1>

<p class="noindent english">How much of your brain does your job use? The more it does, the safer you are. In the early days of AI, the common view was that computers would replace blue-collar workers before white-collar ones, because white-collar work requires more brains. But that’s not quite how things turned out. Robots assemble cars, but they haven’t replaced construction workers. On the other hand, machine-learning algorithms have replaced credit analysts and direct marketers. As it turns out, evaluating credit applications is easier for machines than walking around a construction site without tripping, even though for humans it’s the other way around. The common theme is that narrowly defined tasks are easily learned from data, but tasks that require a broad combination of skills and knowledge aren’t. Most of your brain is devoted to vision and motion, which is a sign that walking around is much more complex than it seems; we just take it for granted because, having been honed to perfection by evolution, it’s mostly done subconsciously. The company Narrative Science has an AI system that can write pretty good summaries of baseball games, but not novels, because—<i>pace</i> George F. Will—there’s a lot more to life than to baseball games. Speech recognition is hard for computers because it’s hard to fill in the blanks—literally, the sounds speakers routinely elide—when you have no idea what the person is talking about. Algorithms can predict stock fluctuations but have no clue how they relate to politics. The more context a job requires, the less likely a computer will be able to do it soon. Common sense is <a id="babilu_link-170"></a> important not just because your mom taught you so, but because computers don’t have it.</p>

<p class="noindent chinese">你的工作使用了多少大脑？它用得越多，你就越安全。在人工智能的早期，人们普遍认为计算机会先于白领工人取代蓝领工人，因为白领工作需要更多大脑。但事情的结果并不完全如此。机器人组装汽车，但他们还没有取代建筑工人。另一方面，机器学习算法已经取代了信贷分析师和直销人员。事实证明，对机器来说，评估信贷申请比在建筑工地上走来走去而不被绊倒更容易，尽管对人类来说，这是一个相反的情况。共同的主题是，狭义的任务很容易从数据中学习，但需要广泛的技能和知识组合的任务却不是。你的大部分大脑都致力于视觉和运动，这表明走动比它看起来要复杂得多；我们只是认为这是理所当然的，因为经过进化的磨练，它大多是在下意识中完成的。叙事科学公司有一个人工智能系统，可以写出相当好的棒球比赛摘要，但不是小说，因为 ——<i>在</i>乔治·F·威尔看来，生活中有很多东西比棒球比赛更重要。语音识别对计算机来说很难，因为当你不知道对方在说什么的时候，很难填补空白 —— 从字面上看，就是说话人经常忽略的声音。算法可以预测股票波动，但不知道它们与政治有什么关系。一项工作需要的背景越多，计算机就越不可能很快做到这一点。常识是重要的，不仅仅是因为你的妈妈这样教你，而是因为计算机没有这个能力。</p>

<p class="noindent english">The best way to not lose your job is to automate it yourself. Then you’ll have time for all the parts of it that you didn’t before and that a computer won’t be able to do any time soon. (If there aren’t any, stay ahead of the curve and get a new job now.) If a computer has learned to do your job, don’t try to compete with it; harness it. H&amp;R Block is still in business, but tax preparers’ jobs are much less dreary than they used to be, now that computers do most of the grunge work. (OK, perhaps this is not the best example, given that the tax code’s exponential growth is one of the few that can hold its own against computing power’s exponential growth.) Think of big data as an extension of your senses and learning algorithms as an extension of your brain. The best chess players these days are so-called centaurs, half-man and half-program. The same is true in many other occupations, from stock analyst to baseball scout. It’s not man versus machine; it’s man with machine versus man without. Data and intuition are like horse and rider, and you don’t try to outrun a horse; you ride it.</p>

<p class="noindent chinese">不失去工作的最好办法是自己把它自动化。这样你就会有时间去做那些你以前没有做的，而电脑很快就不能做的部分。（如果没有，那就保持领先，现在就去找一份新的工作。）如果电脑已经学会了做你的工作，不要试图与它竞争；要利用它。H&amp;R Block 仍然在营业，但报税员的工作比以前要少得多，因为现在电脑已经做了大部分的繁琐工作。（好吧，也许这不是最好的例子，因为税法的指数级增长是少数几个能够抵御计算能力指数级增长的例子之一）。把大数据看作是你感官的延伸，把学习算法看作是你大脑的延伸。现在最好的国际象棋选手是所谓的半人马，一半是人，一半是程序。在许多其他职业中也是如此，从股票分析师到棒球侦察员。这不是人与机器的竞争，而是有机器的人与没有机器的人的竞争。数据和直觉就像马和骑手，你不要试图跑过一匹马；你要骑着它。</p>

<p class="noindent english">As technology progresses, an ever more intimate mix of human and machine takes shape. You’re hungry; Yelp suggests some good restaurants. You pick one; GPS gives you directions. You drive; car electronics does the low-level control. We are all cyborgs already. The real story of automation is not what it replaces but what it enables. Some professions disappear, but many more are born. Most of all, automation makes all sorts of things possible that would be way too expensive if done by humans. ATMs replaced some bank tellers, but mainly they let us withdraw money any time, anywhere. If pixels had to be colored one at a time by human animators, there would be no <i>Toy Story</i> and no video games.</p>

<p class="noindent chinese">随着技术的进步，人类和机器之间越来越亲密的结合逐渐形成。你饿了；Yelp 推荐了一些好餐馆。你选了一家，GPS 给你指路。你开车，汽车电子系统负责低级别的控制。我们都已经是电子人了。自动化的真正故事不是它取代了什么，而是它促成了什么。一些职业消失了，但更多的职业诞生了。最重要的是，自动化使各种事情成为可能，而这些事情如果由人类来做的话，成本会很高。自动取款机取代了一些银行出纳员，但主要是它们让我们随时随地取钱。如果像素必须由人类动画师一次一次地着色，就不会有《<i>玩具总动员</i>》，也不会有视频游戏。</p>

<p class="noindent english">Still, we can ask whether we’ll eventually run out of jobs for humans. I think not. Even if the day comes—and it won’t be soon—when computers and robots can do everything better, there will still be jobs for at least some of us. A robot may be able to do a perfect impersonation of a bartender, down to the small talk, but patrons may still prefer a <a id="babilu_link-274"></a> bartender they know is human, just because he is. Restaurants with human waiters will have extra cachet, just as handmade goods already do. People still go to the theater, ride horses, and sail, even though we have movies, cars, and motorboats. More importantly, some professionals will be truly irreplaceable because their jobs require the one thing that computers and robots by definition cannot have: the human experience. By that I don’t mean touchy-feely jobs, because touchy-feely is not hard to fake; witness the success of robo-pets. I mean the humanities, whose domain is precisely everything you can’t understand without the experience of being human. We worry that the humanities are in a death spiral, but they’ll rise from the ashes once other professions have been automated. The more everything is done cheaply by machines, the more valuable the humanist’s contribution will be.</p>

<p class="noindent chinese">不过，我们还是可以问，我们是否最终会失去人类的工作机会。我认为不会。即使有一天 —— 而且不会太早 —— 电脑和机器人能更好地完成所有工作，我们中至少还有一些人可以工作。一个机器人可能会完美地模仿酒保，甚至包括闲聊，但顾客可能仍然喜欢一个他们知道这个酒保是人类，只是因为他是。有人类服务员的餐厅将有额外的吸引力，就像手工制品已经做的那样。尽管我们有电影、汽车和摩托艇，人们仍然会去看戏、骑马和航海。更重要的是，一些专业人员将是真正不可替代的，因为他们的工作需要计算机和机器人定义上无法拥有的东西：人类的经验。我指的不是感性的工作，因为感性并不难伪造；见证机器人宠物的成功。我指的是人文学科，它的领域恰恰是你没有做人的经验就无法理解的一切。我们担心人文学科正处于死亡的漩涡中，但一旦其他行业被自动化，它们就会从灰烬中崛起。一切都是由机器廉价完成的，人文学者的贡献就越有价值。</p>

<p class="noindent english">Conversely, the long-term prospects of scientists are not the brightest, sadly. In the future, the only scientists may well be computer scientists, meaning computers doing science. The people formerly known as scientists (like me) will devote their lives to understanding the scientific advances made by computers. They won’t be noticeably less happy than before; after all, science was always a hobby to them. And one very important job for the technically minded will remain: keeping an eye on the computers. In fact, this will require more than engineers; ultimately, it may be the full-time occupation of all mankind to figure out what we want from the machines and make sure we’re getting it—more on this later in this chapter.</p>

<p class="noindent chinese">相反，可悲的是，科学家的长期前景不是最光明的。在未来，唯一的科学家很可能是计算机科学家，也就是计算机做科学。以前被称为科学家的人（如我）将把他们的生命投入到理解计算机所取得的科学进步中。他们不会明显地比以前更快乐；毕竟，科学对他们来说一直是一种爱好。而对于有技术头脑的人来说，一项非常重要的工作将继续存在：盯着计算机。事实上，这将需要比工程师更多的工作；最终，这可能是全人类的全职工作，以弄清我们从机器中想要什么，并确保我们得到它 —— 在本章后面会有更多这方面的内容。</p>

<p class="noindent english">In the meantime, as the boundary between automatable and nonautomatable jobs advances across the economic landscape, what we’ll likely see is unemployment creeping up, downward pressure on the wages of more and more professions, and increasing rewards for the fewer and fewer that can’t yet be automated. This is what’s already happening, of course, but it has much further to run. The transition will be tumultuous, but thanks to democracy, it will have a happy ending. (Hold on to your vote—it may be the most valuable thing you have.) When the unemployment rate rises above 50 percent, or even before, attitudes about redistribution will radically change. The newly <a id="babilu_link-216"></a> unemployed majority will vote for generous lifetime unemployment benefits and the sky-high taxes needed to fund them. These won’t break the bank because machines will do the necessary production. Eventually, we’ll start talking about the employment rate instead of the unemployment one and reducing it will be seen as a sign of progress. (“The US is falling behind. Our employment rate is still 23 percent.”) Unemployment benefits will be replaced by a basic income for everyone. Those of us who aren’t satisfied with it will be able to earn more, stupendously more, in the few remaining human occupations. Liberals and conservatives will still fight about the tax rate, but the goalposts will have permanently moved. With the total value of labor greatly reduced, the wealthiest nations will be those with the highest ratio of natural resources to population. (Move to Canada now.) For those of us not working, life will not be meaningless, any more than life on a tropical island where nature’s bounty meets all needs is meaningless. A gift economy will develop, of which the open-source software movement is a preview. People will seek meaning in human relationships, self-actualization, and spirituality, much as they do now. The need to earn a living will be a distant memory, another piece of humanity’s barbaric past that we rose above.</p>

<p class="noindent chinese">同时，随着可自动化工作和不可自动化工作之间的界限在整个经济领域的推进，我们可能会看到失业率逐渐上升，越来越多的职业的工资有下降的压力，而越来越少的还不能自动化的工作得到越来越多的奖励。当然，这就是已经发生的事情，但它还有很长的路要走。这一过渡将是动荡的，但由于民主，它将有一个快乐的结局。（当失业率上升到 50% 以上，甚至更早的时候，人们对再分配的态度将发生根本性的变化。新的大多数失业者将投票支持慷慨的终身失业福利和资助这些福利所需的高额税收。这些不会使银行破产，因为机器会进行必要的生产。最终，我们将开始讨论就业率而不是失业率，减少就业率将被视为进步的标志。（“美国正在落后，我们的就业率仍然是 23%。”）失业福利将被每个人的基本收入所取代。我们中那些不满足于此的人将能够赚取更多，令人难以置信的更多，在剩下的少数人类职业中。自由派和保守派仍将为税率而争吵，但目标杆将永久地移动。随着劳动力的总价值大大降低，最富有的国家将是那些自然资源与人口比例最高的国家。（对我们这些不工作的人来说，生活不会毫无意义，就像在热带岛屿上生活，大自然的恩赐满足了所有的需求一样毫无意义。礼物经济将会发展，开源软件运动就是一个预演。人们将在人际关系、自我实现和精神方面寻求意义，就像现在一样。谋生的需要将成为一个遥远的记忆，成为我们超越的人类野蛮历史的另一个部分。</p>

<h1 id="babilu_link-456"><b>War is not for humans</b></h1>

<h1 id="babilu_link-456"><b>战争不属于人类</b></h1>

<p class="noindent english">Soldiering is harder to automate than science, but it will be as well. One of the prime uses of robots is to do things that are too dangerous for humans, and fighting wars is about as dangerous as it gets. Robots already defuse bombs, and drones allow a platoon to see over the hill. Self-driving supply trucks and robotic mules are on the way. Soon we will need to decide whether robots are allowed to pull the trigger on their own. The argument for doing this is that we want to get humans out of harm’s way, and remote control is not viable in fast-moving, shoot-or-be-shot situations. The argument against is that robots don’t understand ethics, and so can’t be entrusted with life-or-death decisions. But we can teach them. The deeper question is whether we’re ready to.</p>

<p class="noindent chinese">军队比科学更难实现自动化，但它也将是如此。机器人的主要用途之一是做那些对人类来说太危险的事情，而打仗是最危险的事情。机器人已经可以拆除炸弹，无人机可以让一个排的人看到山的那边。自动驾驶的补给卡车和机器人骡子正在路上。很快，我们将需要决定是否允许机器人自己扣动扳机。支持这样做的理由是，我们想让人类远离危险，而在快速移动、不开枪就死的情况下，远程控制是不可行的。反对的理由是，机器人不懂道德，所以不能被委托做生死攸关的决定。但我们可以教他们。更深层次的问题是我们是否准备好了。</p>

<p class="noindent english"><a id="babilu_link-83"></a> It’s not hard to state general principles like military necessity, proportionality, and sparing civilians. But there’s a gulf between them and concrete actions, which the soldier’s judgment has to bridge. Asimov’s three laws of robotics quickly run into trouble when robots try to apply them in practice, as his stories memorably illustrate. General principles are usually contradictory, if not self-contradictory, and they have to be lest they turn all shades of gray into black and white. When does military necessity outweigh sparing civilians? There is no universal answer and no way to program a computer with all the eventualities. Machine learning, however, provides an alternative. First, teach the robot to recognize the relevant concepts, for example with data sets of situations where civilians were and were not spared, armed response was and was not proportional, and so on. Then give it a code of conduct in the form of rules involving these concepts. Finally, let the robot learn how to apply the code by observing humans: the soldier opened fire in this case but not in that case. By generalizing from these examples, the robot can learn an end-to-end model of ethical decision making, in the form of, say, a large MLN. Once the robot’s decisions agree with a human’s as often as one human agrees with another, the training is complete, meaning the model is ready for download into thousands of robot brains. Unlike humans, robots don’t lose their heads in the heat of combat. If a robot malfunctions, the manufacturer is responsible. If it makes a wrong call, its teachers are.</p>

<p class="noindent chinese">阐述军事必要性、相称性和保护平民等一般原则并不难。但在这些原则和具体行动之间存在着一道鸿沟，士兵的判断力必须弥合这道鸿沟。阿西莫夫的机器人三定律在机器人试图在实践中应用时，很快就会遇到麻烦，他的故事令人难忘地说明了这一点。一般原则通常是矛盾的，甚至是自相矛盾的，它们必须是矛盾的，以免把所有的灰色变成黑白。什么时候军事上的必要性胜过对平民的保护？没有普遍的答案，也没有办法用所有的可能性为计算机编程。然而，机器学习提供了一个替代方案。首先，教机器人识别相关的概念，例如，通过数据集，了解在哪些情况下平民得到了保护，哪些没有得到保护，武装反应是否相称，等等。然后以涉及这些概念的规则形式给它一个行为准则。最后，让机器人通过观察人类来学习如何应用该准则：士兵在这种情况下开火，但在那种情况下不开火。通过对这些例子的归纳，机器人可以学习一个端到端的道德决策模型，比如说，以一个大型 MLN 的形式。一旦机器人的决定与人类的决定达成一致，就像一个人与另一个人达成一致一样，训练就完成了，这意味着该模型已经准备好下载到成千上万的机器人大脑。与人类不同，机器人不会在激烈的战斗中失去理智。如果机器人出现故障，制造商要负责。如果它做出了错误的判断，它的老师就会。</p>

<p class="noindent english">The main problem with this scenario, as you may have already guessed, is that letting robots learn ethics by observing humans may not be such a good idea. The robot is liable to get seriously confused when it sees that humans’ actions often violate their ethical principles. We can clean up the training data by including only the examples where, say, a panel of ethicists agrees that the soldier made the right decision, and the panelists can also inspect and tweak the model postlearning to their satisfaction. Agreement may be hard to reach, however, particularly if the panel includes all the different kinds of people it should. Teaching ethics to robots, with their logical minds and lack of baggage, will force us to examine our assumptions and sort out our <a id="babilu_link-237"></a> contradictions. In this, as in many other areas, the greatest benefit of machine learning may ultimately be not what the machines learn but what we learn by teaching them.</p>

<p class="noindent chinese">这个方案的主要问题，你可能已经猜到了，就是让机器人通过观察人类来学习道德，可能不是一个好主意。当机器人看到人类的行为经常违反他们的道德原则时，它很可能会感到严重困惑。我们可以清理训练数据，只包括例如一个伦理学家小组同意士兵做出正确决定的例子，小组成员也可以检查和调整学习后的模型，使其满意。然而，协议可能很难达成，特别是如果小组包括所有不同种类的人，它应该。向机器人教授伦理学，由于他们的逻辑思维和缺乏包袱，将迫使我们检查我们的假设，并整理出我们的矛盾之处。在这一点上，就像在许多其他领域一样，机器学习的最大好处可能最终不是机器学到什么，而是我们通过教它们学到什么。</p>

<p class="noindent english">Another objection to robot armies is that they make war too easy. But if we unilaterally relinquish them, that could cost us the next war. The logical response, advocated by the United Nations and Human Rights Watch, is a treaty banning robot warfare, similar to the Geneva Protocol of 1925 banning chemical and biological warfare. This misses a crucial distinction, however. Chemical and biological warfare can only increase human suffering, but robot warfare can greatly decrease it. If a war is fought by machines, with humans only in command positions, no one is killed or wounded. Perhaps, then, what we should do, instead of outlawing robot soldiers, is—when we’re ready—outlaw human soldiers.</p>

<p class="noindent chinese">对机器人军队的另一个反对意见是，它们使战争变得过于容易。但如果我们单方面放弃它们，这可能会让我们在下一场战争中付出代价。联合国和人权观察组织倡导的合乎逻辑的回应是，制定一项禁止机器人战争的条约，类似于 1925 年禁止化学和生物战的日内瓦议定书。然而，这忽略了一个关键的区别。化学和生物战争只能增加人类的痛苦，但机器人战争可以大大减少人类的痛苦。如果一场战争是由机器进行的，人类只在指挥岗位上，就不会有人死亡或受伤。那么，也许我们应该做的，不是取缔机器人士兵，而是 —— 当我们准备好的时候，取缔人类士兵。</p>

<p class="noindent english">Robot armies may indeed make wars more likely, but they will also change the ethics of war. Shoot/don’t shoot dilemmas become much easier if the targets are other robots. The modern view of war as an unspeakable horror, to be engaged in only as a last resort, will give way to a more nuanced view of war as an orgy of destruction that leaves all sides impoverished and is best avoided but not at all costs. And if war is reduced to a competition to see who can destroy the most, then why not compete instead to create the most?</p>

<p class="noindent chinese">机器人军队可能确实使战争更有可能发生，但它们也将改变战争的伦理。如果目标是其他机器人，开枪/不开枪的难题就会变得容易得多。现代人认为战争是一种难以言喻的恐怖，只能作为最后的手段来参与，这将让位于一种更细微的观点，即战争是一场破坏的狂欢，让各方都陷入贫困，最好避免，但不是不惜一切代价。如果战争被简化为一场竞争，看谁能破坏得最多，那么为什么不改成竞争创造得最多呢？</p>

<p class="noindent english">In any case, banning robot warfare may not be viable. Far from banning drones—the precursors of tomorrow’s warbots—countries large and small are busy developing them, presumably because in their estimation the benefits outweigh the risks. As with any weapon, it’s safer to have robots than to trust the other side not to. If in future wars millions of kamikaze drones will destroy conventional armies in minutes, they’d better be our drones. If World War III will be over in seconds, as one side takes control of the other’s systems, we’d better have the smarter, faster, more resilient network. (Off-grid systems are not the answer: systems that aren’t networked can’t be hacked, but they can’t compete with networked systems, either.) And, on balance, a robot arms race may be a good thing, if it hastens the day when the Fifth Geneva <a id="babilu_link-78"></a> Convention bans humans in combat. War will always be with us, but the casualties of war need not be.</p>

<p class="noindent chinese">在任何情况下，禁止机器人战争可能是不可行的。大大小小的国家远没有禁止无人机 —— 未来战争机器人的前身 —— 而是在忙着开发它们，大概是因为他们估计其好处大于风险。与任何武器一样，拥有机器人比相信对方不拥有机器人更安全。如果在未来的战争中，数以百万计的神风特攻队无人机将在几分钟内摧毁常规军队，它们最好是我们的无人机。如果第三次世界大战将在几秒钟内结束，因为一方控制了另一方的系统，我们最好有更智能、更快速、更有弹性的网络。（离网系统不是答案：没有联网的系统不能被入侵，但它们也不能与联网系统竞争。）而且，总的来说，机器人军备竞赛可能是一件好事，如果它加速了《日内瓦第五公约》禁止人类参与战斗的那一天。战争将永远伴随着我们，但战争的伤亡不需要。</p>

<h1 id="babilu_link-457"><b>Google + Master Algorithm = Skynet?</b></h1>

<h1 id="babilu_link-457"><b>谷歌 + 主算法 = 天网？</b></h1>

<p class="noindent english">Of course, robot armies also raise a whole different specter. According to Hollywood, the future of humanity is to be snuffed out by a gargantuan AI and its vast army of machine minions. (Unless, of course, a plucky hero saves the day in the last five minutes of the movie.) Google already has the gargantuan hardware such an AI would need, and it’s recently acquired an army of robotics startups to go with it. If we drop the Master Algorithm into its servers, is it game over for humanity? Why yes, of course. It’s time to reveal my true agenda, with apologies to Tolkien:</p>

<p class="noindent chinese">当然，机器人军队也带来了一个完全不同的幽灵。根据好莱坞的说法，人类的未来将被一个巨大的人工智能及其庞大的机器奴才军队所扼杀。（当然，除非一个勇敢的英雄在电影的最后 5 分钟里拯救了世界）。谷歌已经拥有这样一个人工智能所需的巨大硬件，而且它最近还收购了一支机器人初创公司的军队。如果我们把主算法扔进它的服务器，人类的游戏就结束了吗？当然，是的。现在是时候揭示我的真实目的了，在此向托尔金表示歉意：</p>

<div>

<p class="noindent english"><i>Three Algorithms for the Scientists under the sky,</i></p>

<p class="noindent chinese"><i>天下科学家的三种算法。</i></p>

<p class="noindent english"><i>Seven for the Engineers in their halls of servers,</i></p>

<p class="noindent chinese"><i>工程师们在他们的服务器大厅里有七个，</i></p>

<p class="noindent english"><i>Nine for Mortal Businesses doomed to die,</i></p>

<p class="noindent chinese"><i>九个为凡人企业注定要死亡。</i></p>

<p class="noindent english"><i>One for the Dark AI on its dark throne,</i></p>

<p class="noindent chinese"><i>为黑暗王座上的黑暗人工智能准备一份，</i></p>

<p class="noindent english"><i>In the Land of Learning where the Data lies.</i></p>

<p class="noindent chinese"><i>在数据所在的学习之地。</i></p>

<p class="noindent english"><i>One Algorithm to rule them all, One Algorithm to find them,</i></p>

<p class="noindent chinese"><i>一个算法统治他们所有人，一个算法找到他们。</i></p>

<p class="noindent english"><i>One Algorithm to bring them all and in the darkness bind them,</i></p>

<p class="noindent chinese"><i>一个算法将他们全部带来，在黑暗中将他们捆绑起来，</i></p>

<p class="noindent english"><i>In the Land of Learning where the Data lies.</i></p>

<p class="noindent chinese"><i>在数据所在的学习之地。</i></p>

</div>

<p class="noindent english">Hahahaha! Seriously, though, should we worry that machines will take over? The signs seem ominous. With every passing year, computers don’t just do more of the world’s work; they make more of the decisions. Who gets credit, who buys what, who gets what job and what raise, which stocks will go up and down, how much insurance costs, where police officers patrol and therefore who gets arrested, how long their prison terms will be, who dates whom and therefore who will be born: machine-learned models already play a part in all of these. The point where we could turn off all our computers without causing the collapse of modern civilization has long passed. Machine learning is <a id="babilu_link-245"></a> the last straw: if computers can start programming themselves, all hope of controlling them is surely lost. Distinguished scientists like Stephen Hawking have called for urgent research on this issue before it’s too late.</p>

<p class="noindent chinese">哈哈哈哈！不过，说真的，我们应该担心机器会接管吗？这些迹象似乎是不祥之兆。随着时间的推移，计算机不只是做了世界上更多的工作；它们还做出了更多的决定。谁得到了信贷，谁买了什么，谁得到了什么工作，谁得到了什么加薪，哪些股票会上涨和下跌，保险费用是多少，警察在哪里巡逻，因此谁会被逮捕，他们的刑期会有多长，谁和谁约会，因此谁会出生：机器学习的模型已经在所有这些方面发挥作用。我们可以关闭所有的计算机而不造成现代文明的崩溃，这一点早已过去。机器学习是最后一根稻草：如果计算机可以开始为自己编程，那么控制它们的所有希望肯定会丧失。像斯蒂芬·霍金这样的杰出科学家已经呼吁在为时已晚之前紧急研究这个问题。</p>

<p class="noindent english">Relax. The chances that an AI equipped with the Master Algorithm will take over the world are <i>zero</i> . The reason is simple: unlike humans, computers don’t have a will of their own. They’re products of engineering, not evolution. Even an infinitely powerful computer would still be only an extension of our will and nothing to fear. Recall the three components of every learning algorithm: representation, evaluation, and optimization. The learner’s representation circumscribes what it can learn. Let’s make it a very powerful one, like Markov logic, so the learner can in principle learn anything. The optimizer then does everything in its power to maximize the evaluation function—no more and no less—and the evaluation function is <i>determined by us</i> . A more powerful computer will just optimize it better. There’s no risk of it getting out of control, even if it’s a genetic algorithm. A learned system that didn’t do what we want would be severely unfit and soon die out. In fact, it’s the systems that have even a slight edge in serving us better that will, generation after generation, multiply and take over the gene pool. Of course, if we’re so foolish as to deliberately program a computer to put itself above us, then maybe we’ll get what we deserve.</p>

<p class="noindent chinese">放轻松。配备了主算法的人工智能接管世界的可能性为<i>零</i>。原因很简单：与人类不同，计算机没有自己的意志。它们是工程的产物，而不是进化。即使是一个无限强大的计算机，也只是我们意志的延伸，没有什么好怕的。回顾每一种学习算法的三个组成部分：表示、评估和优化。学习者的表征限定了它可以学习的内容。让我们把它变成一个非常强大的表示，比如马尔可夫逻辑，所以学习者原则上可以学习任何东西。然后，优化器尽其所能使评价函数最大化 —— 不多也不少 —— 评价函数<i>由我们决定</i>。一台更强大的计算机只会把它优化得更好。即使是一个遗传算法，也不会有失控的风险。一个学习的系统如果不做我们想要的事情，就会严重不适应，很快就会消亡。事实上，正是那些在为我们提供更好服务方面具有哪怕是一点点优势的系统，将一代又一代地繁殖，并接管基因库。当然，如果我们如此愚蠢，故意给计算机编程，使其高于我们，那么也许我们会得到我们应得的东西。</p>

<p class="noindent english">The same reasoning applies to all AI systems because they all—explicitly or implicitly—have the same three components. They can vary what they do, even come up with surprising plans, but only in service of the goals we set them. A robot whose programmed goal is “make a good dinner” may decide to cook a steak, a bouillabaisse, or even a delicious new dish of its own creation, but it can’t decide to murder its owner any more than a car can decide to fly away. The purpose of AI systems is to solve NP-complete problems, which, as you may recall from <a href="#babilu_link-337">Chapter 2</a> , may take exponential time, but the solutions can always be checked efficiently. We should therefore welcome with open arms computers that are vastly more powerful than our brains, safe in the knowledge that our job is exponentially easier than theirs. They have to solve the problems; we just have to check that they did so to our satisfaction. AIs will <a id="babilu_link-226"></a> think fast what we think slow, and the world will be the better for it. I, for one, welcome our new robot underlings.</p>

<p class="noindent chinese">同样的推理适用于所有的人工智能系统，因为它们都 —— 明确地或隐含地 —— 具有相同的三个组成部分。它们可以改变它们所做的事情，甚至想出令人惊讶的计划，但只是为了服务于我们为它们设定的目标。一个机器人的编程目标是 “做一顿丰盛的晚餐”，它可以决定烹饪牛排、鱼汤，甚至是自己创造的美味新菜，但它不能决定谋杀它的主人，就像一辆汽车不能决定飞走一样。人工智能系统的目的是解决 NP 完备的问题，正如你可能记得的<a href="#babilu_link-337">第 2 章</a>，这可能需要指数级的时间，但解决方案总是可以被有效检查。因此，我们应该张开双臂欢迎那些比我们的大脑强大得多的计算机，因为我们知道我们的工作比他们的工作要容易得多。他们必须解决这些问题；我们只需检查他们是否做到了让我们满意。AI 将快速思考我们所思考的问题，而世界将因此变得更好。就我而言，我欢迎我们的新机器人下属。</p>

<p class="noindent english">It’s natural to worry about intelligent machines taking over because the only intelligent entities we know are humans and other animals, and they definitely have a will of their own. But there is no necessary connection between intelligence and autonomous will; or rather, intelligence and will may not inhabit the same body, provided there is a line of control between them. In <i>The Extended Phenotype</i> , Richard Dawkins shows how nature is replete with examples of an animal’s genes controlling more than its own body, from cuckoo eggs to beaver dams. Technology is the extended phenotype of man. This means we can continue to control it even if it becomes far more complex than we can understand.</p>

<p class="noindent chinese">担心智能机器接管是很自然的，因为我们所知道的唯一的智能实体是人类和其他动物，而且它们肯定有自己的意志。但是，智能和自主意志之间没有必然的联系；或者说，智能和意志可能不会居住在同一个身体里，只要它们之间有一条控制线。在《<i>扩展的表型</i>》中，理查德·道金斯展示了自然界中充满了动物的基因如何控制着比它自己的身体更多的东西的例子，从布谷鸟蛋到海狸坝。技术是人类的扩展表型。这意味着我们可以继续控制它，即使它变得比我们能理解的要复杂得多。</p>

<p class="noindent english">Picture two strands of DNA going for a swim in their private pool, aka a bacterium’s cytoplasm, two billion years ago. They’re pondering a momentous decision. “I’m worried, Diana,” says one. “If we start making multicellular creatures, will they take over?” Fast-forward to the twenty-first century, and DNA is still alive and well. Better than ever, in fact, with an increasing fraction living safely in bipedal organisms comprising trillions of cells. It’s been quite a ride for our tiny double-stranded friends since they made their momentous decision. Humans are their trickiest creation yet; we’ve invented things like contraception that let us have fun without spreading our DNA, and we have—or seem to have—free will. But it’s still DNA that shapes our notions of fun, and we use our free will to pursue pleasure and avoid pain, which, for the most part, still coincides with what’s best for our DNA’s survival. We may yet be DNA’s demise if we choose to transmute ourselves into silicon, but even then, it’s been a great two billion years. The decision we face today is similar: if we start making AIs—vast, interconnected, superhuman, unfathomable AIs—will they take over? Not any more than multicellular organisms took over from genes, vast and unfathomable as we may be to them. AIs are our survival machines, in the same way that we are our genes’.</p>

<p class="noindent chinese">想象一下，20 亿年前，两股 DNA 在它们的私人泳池（又称细菌的细胞质）中游泳。他们正在思考一个重大的决定。“我很担心，戴安娜”，一个说。“如果我们开始制造多细胞生物，它们会不会接管？” 快进到 21 世纪，DNA 仍然活得很好。事实上，它比以往任何时候都好，越来越多的部分安全地生活在由数万亿个细胞组成的双足生物体中。自从我们的小双链朋友做出重大决定以来，他们已经走过了相当长的一段路。人类是它们最棘手的创造物；我们已经发明了像避孕药这样的东西，让我们在不传播 DNA 的情况下获得乐趣，而且我们有 —— 或者似乎有自由意志。但仍然是 DNA 塑造了我们的乐趣概念，我们利用我们的自由意志来追求快乐和避免痛苦，在大多数情况下，这仍然与对我们的 DNA 的生存最有利的东西相吻合。如果我们选择将自己转化为硅，我们可能还会成为 DNA 的终结者，但即使如此，也已经经历了 20 亿年的伟大历程。我们今天面临的决定是类似的：如果我们开始制造人工智能 —— 庞大的、相互联系的、超人的、深不可测的人工智能 —— 它们会接管吗？不会，就像多细胞生物体取代基因一样，尽管我们对它们来说可能是庞大而深不可测的。AI 是我们的生存机器，就像我们是我们的基因一样。</p>

<p class="noindent english">This does not mean that there is nothing to worry about, however. The first big worry, as with any technology, is that AI could fall into the wrong hands. If a criminal or prankster programs an AI to take over the <a id="babilu_link-241"></a> world, we’d better have an AI police capable of catching it and erasing it before it gets too far. The best insurance policy against vast AIs gone amok is vaster AIs keeping the peace.</p>

<p class="noindent chinese">然而，这并不意味着没有什么可担心的。与任何技术一样，第一大担忧是人工智能可能落入坏人手中。如果一个罪犯或恶作剧者编程让人工智能接管我们最好有一个人工智能警察，能够在它走得太远之前抓到它并将其删除。防止庞大的人工智能失控的最好的保险政策是更大的人工智能保持和平。</p>

<p class="noindent english">The second worry is that humans will voluntarily surrender control. It starts with robot rights, which seem absurd to me but not to everyone. After all, we already give rights to animals, who never asked for them. Robot rights might seem like the logical next step in expanding the “circle of empathy.” Feeling empathy for robots is not hard, particularly if they’re designed to elicit it. Even Tamagotchi, Japanese “virtual pets” with all of three buttons and an LCD screen, do it quite successfully. The first humanoid consumer robot will set off a race to make more and more empathy-eliciting robots, because they’ll sell much better than the plain metal variety. Children raised by robot nannies will have a lifelong soft spot for kindly electronic friends. The “uncanny valley”—our discomfort with robots that are almost human but not quite—will be unknown to them because they grew up with robot mannerisms and maybe even adopted them as cool teenagers.</p>

<p class="noindent chinese">第二个担忧是，人类会自愿交出控制权。这要从机器人的权利说起，这在我看来是荒谬的，但对每个人来说却不是。毕竟，我们已经给了动物权利，它们从未要求过这些权利。机器人权利似乎是扩大 “同理心圈” 的合乎逻辑的下一个步骤。对机器人产生同情心并不难，尤其是当它们被设计成可以引起同情的时候。即使是 Tamagotchi，日本的 “虚拟宠物”，只有三个按钮和一个液晶屏幕，也做得相当成功。第一个人形消费机器人将掀起一场竞赛，制造越来越多的引起共鸣的机器人，因为它们将比普通的金属种类卖得更好。由机器人保姆抚养长大的孩子将对善良的电子朋友产生终生的爱慕。不思议谷" —— 我们对几乎是人类但又不是人类的机器人感到不适 —— 对他们来说将是未知的，因为他们是在机器人的举止中长大的，甚至可能是作为很酷的青少年而采用它们。</p>

<p class="noindent english">The next step in the insidious progression of AI control is letting them make all the decisions because they’re, well, so much smarter. Beware. They may be smarter, but they’re in the service of whoever designed their score functions. This is the “Wizard of Oz” problem. Your job in a world of intelligent machines is to keep making sure they do what you want, both at the input (setting the goals) and at the output (checking that you got what you asked for). If you don’t, somebody else will. Machines can help us figure out collectively what we want, but if you don’t participate, you lose out—just like democracy, only more so. Contrary to what we like to believe today, humans quite easily fall into obeying others, and any sufficiently advanced AI is indistinguishable from God. People won’t necessarily mind taking their marching orders from some vast oracular computer; the question is who oversees the overseer. Is AI the road to a more perfect democracy or to a more insidious dictatorship? The eternal vigil has just begun.</p>

<p class="noindent chinese">在人工智能控制的阴险进展中，下一步是让他们做出所有的决定，因为他们，嗯，聪明得多。请注意。它们可能更聪明，但它们是为设计其分数功能的人服务的。这就是 “绿野仙踪” 的问题。在智能机器的世界里，你的工作是不断确保它们做你想要的事情，包括输入（设定目标）和输出（检查你是否得到了你想要的东西）。如果你不这样做，别人就会这样做。机器可以帮助我们集体找出我们想要的东西，但如果你不参与，你就输了 —— 就像民主，只是更多。与我们今天愿意相信的相反，人类很容易陷入对他人的服从，任何足够先进的人工智能都与上帝无异。人们不一定介意从某个巨大的计算机那里接受他们的命令；问题是谁来监督这个监督者。人工智能是通往更完美的民主的道路，还是通往更阴险的独裁？永恒的守望才刚刚开始。</p>

<p class="noindent english">The third and perhaps biggest worry is that, like the proverbial genie, the machines will give us what we ask for instead of what we want. This <a id="babilu_link-58"></a> is not a hypothetical scenario; learning algorithms do it all the time. We train a neural network to recognize horses, but it learns instead to recognize brown patches, because all the horses in its training set happened to be brown. You just bought a watch, so Amazon recommends similar items: other watches, which are now the last thing you want to buy. If you examine all the decisions that computers make today—who gets credit, for example—you’ll find that they’re often needlessly bad. Yours would be too, if your brain was a support vector machine and all your knowledge of credit scoring came from perusing one lousy database. People worry that computers will get too smart and take over the world, but the real problem is that they’re too stupid and they’ve already taken over the world.</p>

<p class="noindent chinese">第三，也许是最大的担忧是，就像传说中的精灵一样，机器会给我们嘴里所要求的，而不是我们心里所想要的。这个并不是一个假设的场景；学习算法一直都在这样做。我们训练一个神经网络来识别马，但它却学会了识别棕色斑块，因为它的训练集中所有的马恰好都是棕色。你刚刚买了一块手表，所以亚马逊推荐了类似的商品：其他的手表，现在是你最不想买的东西。如果你研究一下今天计算机所做的所有决定 —— 例如，谁能得到信贷 —— 你会发现它们往往是不必要的坏事。如果你的大脑是一台支持向量机，而你对信用评分的所有知识都来自于浏览一个糟糕的数据库，你的也会是这样。人们担心计算机会变得太聪明并接管世界，但真正的问题是它们太愚蠢，而且它们已经接管了世界。</p>

<div>

</div>

<h1 id="babilu_link-458"><b>Evolution, part 2</b></h1>

<h1 id="babilu_link-458"><b>进化，第二部分</b></h1>

<p class="noindent english">Even if computers today are still not terribly smart, there’s no doubt that their intelligence is rapidly increasing. As early as 1965, I. J. Good, a British statistician and Alan Turing’s sidekick on the World War II Enigma code-breaking project, speculated on a coming intelligence explosion. Good pointed out that if we can design machines that are more intelligent than us, they should in turn be able to design machines that are more intelligent than them, and so on ad infinitum, leaving human intelligence far behind. In a 1993 essay, Vernor Vinge christened this “the Singularity.” The concept has been popularized most of all by Ray Kurzweil, who argues in <i>The Singularity Is Near</i> that not only is the Singularity inevitable, but the point where machine intelligence exceeds human intelligence—let’s call it the Turing point—will arrive within the next few decades.</p>

<p class="noindent chinese">即使今天的计算机仍然不是非常聪明，但毫无疑问，它们的智力正在迅速提高。早在 1965 年，英国统计学家、阿兰·图灵在二战英格玛密码破译项目中的跟班 I·J·古德就推测了即将到来的智能爆炸。古德指出，如果我们能够设计出比我们更聪明的机器，那么它们也应该能够设计出比它们更聪明的机器，以此类推，无穷无尽，将人类的智慧远远甩在后面。在 1993 年的一篇文章中，弗诺·文奇将其命名为 “奇点”。雷·库兹韦尔将这一概念推广得最广，他在《<i>奇点临近</i>》中认为，奇点不仅是不可避免的，而且机器智能超过人类智能的那一点 —— 让我们称之为图灵点 —— 将在未来几十年内到来。</p>

<p class="noindent english">Clearly, without machine learning—programs that design programs—the Singularity cannot happen. We also need sufficiently powerful hardware, but that’s coming along nicely. We’ll reach the Turing point soon after we invent the Master Algorithm. (I’m willing to bet Kurzweil a bottle of Dom Pérignon that this will happen before we reverse engineer the brain, his method of choice for bringing about <a id="babilu_link-281"></a> human-level AI.) <i>Pace</i> Kurzweil, this will not, however, lead to the Singularity. It will lead to something much more interesting.</p>

<p class="noindent chinese">显然，没有机器学习 —— 设计程序的程序 —— 奇点就不可能发生。我们还需要足够强大的硬件，但这正在顺利进行。在我们发明了主算法之后，我们将很快达到图灵点。（我愿意和库兹韦尔打赌，这将在我们对大脑进行逆向工程之前发生，他选择的方法是实现人类水平的人工智能）。然而，<i>按照</i>库兹韦尔的说法，这不会导致奇点的出现。它将导致更有趣的事情。</p>

<p class="noindent english">The term <i>singularity</i> comes from mathematics, where it denotes a point at which a function becomes infinite. For example, the function 1/<i>x</i> has a singularity when <i>x</i> is 0, because 1 divided by 0 is infinity. In physics, the quintessential example of a singularity is a black hole: a point of infinite density, where a finite amount of matter is crammed into infinitesimal space. The only problem with singularities is that they don’t really exist. (When did you last divide a cake among zero people, and each one got an infinite slice?) In physics, if a theory predicts something is infinite, something’s wrong with the theory. Case in point, general relativity presumably predicts that black holes have infinite density because it ignores quantum effects. Likewise, intelligence cannot continue to increase forever. Kurzweil acknowledges this, but points to a series of exponential curves in technology improvement (processor speed, memory capacity, etc.) and argues that the limits to this growth are so far away that we need not concern ourselves with them.</p>

<p class="noindent chinese"> <i>奇点</i>这个词来自数学，它表示一个函数变得无限大的点。例如，当 <i>x</i> 为 0 时，函数 1/<i>x</i> 有一个奇点，因为 1 除以 0 就是无穷大。在物理学中，奇点的典型例子是黑洞：一个密度无限大的点，有限数量的物质被塞进无限小的空间。奇点的唯一问题是，它们并不真正存在。（你最后一次把蛋糕分给 0 个人，而每个人都得到了无限大的一片是什么时候？）在物理学中，如果一个理论预测某些东西是无限的，那么这个理论就有问题。例如，广义相对论推测黑洞的密度是无限的，因为它忽略了量子效应。同样地，智力也不可能永远持续增长。库兹韦尔承认这一点，但他指出技术改进的一系列指数曲线（处理器速度、内存容量等），并认为这种增长的极限非常遥远，我们不需要关心它们。</p>

<p class="noindent english">Kurzweil is overfitting. He correctly faults other people for always extrapolating linearly—seeing straight lines instead of curves—but then falls prey to a more exotic malady: seeing exponentials everywhere. In curves that are flat—nothing happening—he sees exponentials that have not taken off yet. But technology improvement curves are not exponentials; they are S curves, our good friends from <a href="#babilu_link-338">Chapter 4</a> . The early part of an S curve is easy to mistake for an exponential, but then they quickly diverge. Most of Kurzweil’s curves are consequences of Moore’s law, which is on its last legs. Kurzweil argues that other technologies will take the place of semiconductors and S curve will pile on S curve, each steeper than the previous one, but this is speculation. He goes even further to claim that the entire history of life on Earth, not just human technology, shows exponentially accelerating progress, but this perception is at least partly due to a parallax effect: things that are closer seem to move faster. Trilobites in the heat of the Cambrian explosion could be forgiven for believing in exponentially accelerating progress, but then there was a big slowdown. A Tyrannosaurus Ray would <a id="babilu_link-272"></a> probably have proposed a law of accelerating body size. Eukaryotes (us) evolve more slowly than prokaryotes (bacteria). Far from accelerating smoothly, evolution proceeds in fits and starts.</p>

<p class="noindent chinese">库兹韦尔是过度拟合。他正确地指责其他人总是以线性方式推断 —— 看到直线而不是曲线 —— 但随后又陷入了一种更奇特的弊病：到处看到指数。在那些平坦的曲线中，他看到了尚未起飞的指数。但技术改进曲线不是指数；它们是 S 曲线，即我们<a href="#babilu_link-338">第四章</a>中的好朋友。S 曲线的早期部分很容易被误认为是指数，但随后它们很快就会出现分歧。库兹韦尔的大多数曲线都是摩尔定律的结果，而摩尔定律已经到了最后的阶段。库兹韦尔认为，其他技术将取代半导体，S 曲线将堆积在 S 曲线上，每一条都比前一条更陡峭，但这只是猜测。他甚至进一步声称，地球上的整个生命史，不仅仅是人类技术，显示出指数级的加速进步，但这种感觉至少部分是由于视差效应：距离较近的事物似乎发展更快。在寒武纪大爆炸的热潮中，三叶虫相信指数级加速的进步是可以被原谅的，但后来有一个很大的放缓。霸王龙雷可能会提出身体大小加速的规律。真核生物（我们）比原核生物（细菌）进化得更慢。进化远非平稳加速，而是在一开始就进行。</p>

<p class="noindent english">To sidestep the problem that infinitely dense points don’t exist, Kurzweil proposes to instead equate the Singularity with a black hole’s event horizon, the region within which gravity is so strong that not even light can escape. Similarly, he says, the Singularity is the point beyond which technological evolution is so fast that humans cannot predict or understand what will happen. If that’s what the Singularity is, then we’re already inside it. We can’t predict in advance what a learner will come up with, and often we can’t even understand it in retrospect. As a matter of fact, we’ve always lived in a world that we only partly understood. The main difference is that our world is now partly created by us, which is surely an improvement. The world beyond the Turing point will not be incomprehensible to us, any more than the Pleistocene was. We’ll focus on what we can understand, as we always have, and call the rest random (or divine).</p>

<p class="noindent chinese">为了回避无限密集点不存在的问题，库兹韦尔提议将奇点等同于黑洞的事件视界，即引力强大到连光都无法逃脱的区域。他说，同样地，奇点是指技术演进如此之快，以至于人类无法预测或理解将发生什么。如果这就是奇点，那么我们已经在它里面了。我们无法提前预测学习者会想出什么，而且往往我们甚至无法在回顾中理解它。事实上，我们一直生活在一个我们只能部分理解的世界里。主要的区别是，我们的世界现在有一部分是由我们创造的，这肯定是一种进步。图灵点之后的世界对我们来说不会是不可理解的，就像更新世那样。我们将专注于我们能够理解的东西，就像我们一直以来所做的那样，并将其余的东西称为随机的（或神圣的）。</p>

<p class="noindent english">The trajectory we’re on is not a singularity but a phase transition. Its critical point—the Turing point—will come when machine learning overtakes the natural variety. Natural learning itself has gone through three phases: evolution, the brain, and culture. Each is a product of the previous one, and each learns faster. Machine learning is the logical next stage of this progression. Computer programs are the fastest replicators on Earth: copying them takes only a fraction of a second. But creating them is slow, if it has to be done by humans. Machine learning removes that bottleneck, leaving a final one: the speed at which humans can absorb change. This too will eventually be removed, but not because we’ll decide to hand things off to our “mind children,” as Hans Moravec calls them, and go gently into the good night. Humans are not a dying twig on the tree of life. On the contrary, we’re about to start branching.</p>

<p class="noindent chinese">我们所处的轨迹不是一个奇点，而是一个阶段性过渡。它的临界点 —— 图灵点 —— 将在机器学习超越自然种类时出现。自然学习本身已经经历了三个阶段：进化、大脑和文化。每个阶段都是前一个阶段的产物，而且每个阶段的学习速度都更快。机器学习是这一进程中合乎逻辑的下一个阶段。计算机程序是地球上最快的复制者：复制它们只需要一秒钟的时间。但是，如果必须由人类来创造它们，那就很慢了。机器学习消除了这个瓶颈，留下了最后一个瓶颈：人类吸收变化的速度。这一点最终也会被消除，但不是因为我们会决定把事情交给我们的 “心灵的孩子”，就像汉斯·莫拉维克所说的那样，然后轻轻地进入良夜。人类不是生命之树上垂死的树枝。恰恰相反，我们即将开始分化。</p>

<p class="noindent english">In the same way that culture coevolved with larger brains, we will coevolve with our creations. We always have: humans would be physically different if we had not invented fire or spears. We are <i>Homo technicus</i> as much as <i>Homo sapiens</i> . But a model of the cell of the kind I envisaged <a id="babilu_link-261"></a> in the last chapter will allow something entirely new: computers that design cells based on the parameters we give them, in the same way that silicon compilers design microchips based on their functional specifications. The corresponding DNA can then be synthesized and inserted into a “generic” cell, transforming it into the desired one. Craig Venter, the genome pioneer, has already taken the first steps in this direction. At first we will use this power to fight disease: a new pathogen is identified, the cure is immediately found, and your immune system downloads it from the Internet. <i>Health problems</i> becomes an oxymoron. Then DNA design will let people at last have the body they want, ushering in an age of affordable beauty, in William Gibson’s memorable words. And then <i>Homo technicus</i> will evolve into a myriad different intelligent species, each with its own niche, a whole new biosphere as different from today’s as today’s is from the primordial ocean.</p>

<p class="noindent chinese">就像文化与更大的大脑共同进化一样，我们将与我们的创造物共同进化。我们总是这样：如果我们没有发明火或长矛，人类在身体上会有所不同。我们和智人一样，都是<i>技术型智人</i>（Homo technicus）。但是，我在上一章中设想的那种细胞模型将允许一些全新的东西：计算机可以根据我们提供的参数设计细胞，就像硅编译器根据功能规格设计微芯片一样。然后可以合成相应的 DNA 并插入一个 “通用” 细胞，将其转化为所需的细胞。基因组先驱克雷格·文特尔已经朝着这个方向迈出了第一步。起初，我们将利用这种力量来对抗疾病：一个新的病原体被发现，治疗方法立即被找到，你的免疫系统会从互联网上下载它。<i>健康问题</i>变成了一个矛盾的概念。然后，DNA 设计将让人们最终拥有他们想要的身体，用威廉·吉布森令人难忘的话语，迎来了一个负担得起的美丽时代。然后<i>技术智人</i>将进化成无数不同的智能物种，每个物种都有自己的利基，一个全新的生物圈，就像今天的生物圈与原始海洋一样不同。</p>

<p class="noindent english">Many people worry that human-directed evolution will permanently split the human race into a class of genetic haves and one of have-nots. This strikes me as a singular failure of imagination. Natural evolution did not result in just two species, one subservient to the other, but in an infinite variety of creatures and intricate ecosystems. Why would artificial evolution, building on it but less constrained, do so?</p>

<p class="noindent chinese">许多人担心，人类主导的进化将永久地将人类分裂成一个基因富足的阶层和一个基因匮乏的阶层。这让我觉得是想象力的一个奇特的失败。自然进化的结果并不是只有两个物种，一个服从另一个，而是产生了无穷无尽的生物和复杂的生态系统。为什么在自然进化的基础上但不那么受限制的人工进化也会这样做？</p>

<p class="noindent english">Like all phase transitions, this one will eventually taper off too. Overcoming a bottleneck does not mean the sky is the limit; it means the next bottleneck is the limit, even if we don’t see it yet. Other transitions will follow, some large, some small, some soon, some not for a long time. But the next thousand years could well be the most amazing in the life of planet Earth.</p>

<p class="noindent chinese">像所有的相变一样，这个相变最终也会渐渐消失。克服一个瓶颈并不意味着天空就是极限；它意味着下一个瓶颈就是极限，即使我们还没有看到它。其他的转变会接踵而至，有的大，有的小，有的很快，有的很长时间内都不会出现。但是，未来的一千年很可能是地球生命中最令人惊讶的时期。</p>

</section>

</div>

</div>

<div id="babilu_link-331">

<div>

<section id="babilu_link-346">

<h1><a id="babilu_link-49"></a> <a href="#babilu_link-332">Epilogue</a></h1>

<h1><a id="babilu_link-49"></a> <a href="#babilu_link-332">后记</a></h1>

<p class="noindent english">So now you know the secrets of machine learning. The engine that turns data into knowledge is no longer a black box: you know how the magic happens and what it can and can’t do. You’ve met the complexity monster, the overfitting problem, the curse of dimensionality, and the exploration-exploitation dilemma. You know in broad outline what Google, Facebook, Amazon, and all the rest do with the data you generously give them every day and why they can find stuff for you, filter out spam, and keep improving their offerings. You’ve seen what’s brewing in the world’s machine-learning research labs, and you have a ringside seat to the future they’re helping to bring about. You’ve met the five tribes of machine learning and their master algorithms: symbolists and inverse deduction; connectionists and backpropagation; evolutionaries and genetic algorithms; Bayesians and probabilistic inference; analogizers and support vector machines. And because you’ve traveled over a vast territory, negotiated the border crossings, and climbed the high peaks, you have a better view of the landscape than even many machine learners, who toil daily in the fields. You can see the common themes running through the land like an underground river, and you know how the five master algorithms, superficially so different, are really just five facets of a single universal learner.</p>

<p class="noindent chinese">所以现在你知道了机器学习的秘密。将数据转化为知识的引擎不再是一个黑盒子：你知道魔法是如何发生的，它能做什么，不能做什么。你已经遇到了复杂性怪物、过拟合问题、维度的诅咒以及探索·开发的困境。你大致知道谷歌、脸书、亚马逊和所有其他公司每天都在用你慷慨提供的数据做什么，以及为什么他们能为你找到东西，过滤掉垃圾邮件，并不断改进他们的产品。你已经看到了世界上的机器学习研究实验室正在酝酿的东西，你有一个环形座位，可以看到他们正在帮助实现的未来。你已经见到了机器学习的五个部落和他们的主要算法：符号主义者和逆向推理；连接主义者和反向传播；进化主义者和遗传算法；贝叶斯主义者和概率推理；模拟者和支持向量机。因为你已经走过了广阔的领土，经过了边境口岸，爬上了高高的山峰，你甚至比许多机器学习者更清楚地看到了风景，他们每天都在田间劳作。你可以看到像地下河一样贯穿这片土地的共同主题，你知道表面上如此不同的五种主算法，实际上只是一个单一的通用学习器的五个方面。</p>

<p class="noindent english"><a id="babilu_link-273"></a> But the journey is far from over. We don’t have the Master Algorithm yet, just a glimpse of what it might look like. What if something fundamental is still missing, something all of us in the field, steeped in its history, can’t see? We need new ideas, and ideas that are not just variations on the ones we already have. That’s why I wrote this book: to start you thinking. I teach an evening class on machine learning at the University of Washington. In 2007, soon after the Netflix Prize was announced, I proposed it as one of the class projects. Jeff Howbert, a student in the class, got hooked and continued to work on it after the class was over. He wound up being a member of one of the two winning teams, two years after learning about machine learning for the first time. Now it’s your turn. To learn more about machine learning, check out the section on further readings at the end of the book. Download some data sets from the UCI repository (archive.ics.uci.edu/ml/) and start playing. When you’re ready, check out Kaggle.com, a whole website dedicated to running machine-learning competitions, and pick one or two to enter. Of course, it’ll be more fun if you recruit a friend or two to work with you. If you’re hooked, like Jeff was, and wind up becoming a professional data scientist, welcome to the most fascinating job in the world. If you find yourself dissatisfied with today’s learners, invent new ones—or just do it for fun. My fondest wish is that your reaction to this book will be like my reaction to the first AI book I read, over twenty years ago: there’s so much to do here, I don’t know where to start. If one day you invent the Master Algorithm, please don’t run to the patent office with it. Open-source it. The Master Algorithm is too important to be owned by any one person or organization. Its applications will multiply faster than you can license it. But if you decide instead to do a startup, remember to give a share in it to every man, woman, and child on Earth.</p>

<p class="noindent chinese">但旅程还远未结束。我们还没有掌握主算法，只是瞥见了它可能的样子。如果还缺少一些根本性的东西，而我们所有在这个领域的人，在这个领域的历史中都看不到的东西，那该怎么办？我们需要新的想法，而且这些想法不仅仅是我们已有的想法的变种。这就是我写这本书的原因：让你开始思考。我在华盛顿大学教授机器学习的夜校课程。2007 年，在 Netflix 奖宣布后不久，我提出将其作为课堂项目之一。班上的学生杰夫·豪伯特迷上了这个项目，并在下课后继续研究。他最终成为两个获胜团队中的一员，这是他第一次学习机器知识的两年后。现在轮到你了。要了解更多关于机器学习的知识，请查看书末的进一步阅读部分。从 UCI 资源库（archive.ics.uci.edu/ml/）下载一些数据集，然后开始玩。当你准备好了，看看 Kaggle.com，这是一个专门举办机器学习竞赛的网站，挑选一两个来参加。当然，如果你招募一两个朋友和你一起工作，会更有趣。如果你像杰夫一样被迷住了，并最终成为一名专业的数据科学家，欢迎来到世界上最迷人的工作。如果你发现自己对今天的学习者不满意，那就发明新的学习者，或者只是为了好玩而做。我最美好的愿望是，你对这本书的反应会像我二十多年前读第一本人工智能书时的反应一样：这里有太多的事情要做，我不知道从哪里开始。如果有一天你发明了主算法，请不要带着它跑到专利局。把它开放出来吧。主算法太重要了，不能被任何一个人或组织所拥有。它的应用会比你的授权更快。但是，如果你决定做一个初创公司，请记得给地球上的每一个男人、女人和孩子分享它。</p>

<p class="noindent english">Whether you read this book out of curiosity or professional interest, I hope you will share what you’ve learned with your friends and colleagues. Machine learning touches the lives of every one of us, and it’s up to all of us to decide what we want to do with it. Armed with your new understanding of machine learning, you’re in a much better <a id="babilu_link-307"></a> position to think about issues like privacy and data sharing, the future of work, robot warfare, and the promise and peril of AI; and the more of us have this understanding, the more likely we’ll avoid the pitfalls and find the right paths. That’s the other big reason I wrote this book. The statistician knows that prediction is hard, especially about the future, and the computer scientist knows that the best way to predict the future is to invent it, but the unexamined future is not worth inventing.</p>

<p class="noindent chinese">无论你是出于好奇还是职业兴趣而阅读本书，我都希望你能与你的朋友和同事分享你所学到的东西。机器学习触及我们每一个人的生活，而我们所有人都要决定我们要用它做什么。带着对机器学习的新理解，你在思考隐私和数据共享、未来工作、机器人战争以及人工智能的承诺和危险等问题时，；我们中越多的人有这种理解，我们就越有可能避免陷阱，找到正确的道路。这是我写这本书的另一个重要原因。统计学家知道预测是困难的，尤其是对未来的预测，计算机科学家知道预测未来的最好方法是发明它，但未经检验的未来不值得发明。</p>

<p class="noindent english">Thanks for letting me be your guide. I’d like to give you a parting gift. Newton said that he felt like a boy playing on the seashore, picking up a pebble here and a shell there while the great ocean of truth lay undiscovered before him. Three hundred years later, we’ve gathered an amazing collection of pebbles and shells, but the great undiscovered ocean still stretches into the distance, sparkling with promise. The gift is a boat—machine learning—and it’s time to set sail.</p>

<p class="noindent chinese">谢谢你让我成为你的向导。我想给你一份临别礼物。牛顿说，他觉得自己就像一个在海边玩耍的男孩，在这里捡到一块鹅卵石，在那里捡到一个贝壳，而伟大的真理海洋就在他面前未被发现。三百年后，我们已经收集了大量的鹅卵石和贝壳，但未被发现的伟大海洋仍然延伸到远方，闪耀着希望。礼物是一艘船 —— 机器学习 —— 是时候起航了。</p>

</section>

</div>

</div>

<div id="babilu_link-320">

<div>

<section id="babilu_link-347">

<h1><a id="babilu_link-459"></a> <a href="#babilu_link-321">Acknowledgments</a></h1>

<p class="noindent">First of all, I thank my companions in scientific adventure: students, collaborators, colleagues, and everyone in the machine-learning community. This is your book as much as mine. I hope you will forgive my many oversimplifications and omissions, and the somewhat fanciful way in which parts of the book are written.</p>

<p class="noindent">I’m grateful to everyone who read and commented on drafts of the book at various stages, including Mike Belfiore, Thomas Dietterich, Tiago Domingos, Oren Etzioni, Abe Friesen, Rob Gens, Alon Halevy, David Israel, Henry Kautz, Chloé Kiddon, Gary Marcus, Ray Mooney, Kevin Murphy, Franzi Roesner, and Ben Taskar. Thanks also to everyone who gave me pointers, information, or help of various kinds, including Tom Griffiths, David Heckerman, Hannah Hickey, Albert-László Barabási, Yann LeCun, Barbara Mones, Mike Morgan, Peter Norvig, Judea Pearl, Gregory Piatetsky-Shapiro, and Sebastian Seung.</p>

<p class="noindent">I’m lucky to work in a very special place, the University of Washington’s Department of Computer Science and Engineering. I’m also grateful to Josh Tenenbaum, and to everyone in his group, for hosting the sabbatical at MIT during which I started this book. Thanks to Jim Levine, my indefatigable agent, for drinking the Kool-Aid (as he put it) and spreading the word; and to everyone at Levine Greenberg Rostan. Thanks to TJ Kelleher, my amazing editor, for helping make this a better book, chapter by chapter, line by line; and to everyone at Basic Books.</p>

<p class="noindent">I’m indebted to the organizations that have funded my research over the years, including ARO, DARPA, FCT, NSF, ONR, Ford, Google, IBM, Kodak, Yahoo, and the Sloan Foundation.</p>

<p class="noindent">Last and most, I thank my family for their love and support.</p>

</section>

</div>

</div>

<div id="babilu_link-15">

<div>

<div>

<div>

<p class="noindent"><strong><b>Discover Your Next Great Read</b></strong></p>

<p class="noindent">Get sneak peeks, book recommendations, and news about your favorite authors.</p>

</div>

<div><a href="https://discover.hachettebookgroup.com/?ref=9780465061921&amp;discp=100">Tap here to learn more</a> .

</div>

</div>

<div>

<p class="noindent">

<img alt="Basic Books logo" src="images/000013.jpg"/></p>

</div>

</div>

</div>

<div id="babilu_link-357">

<div>

<section id="babilu_link-348">

<h1></h1>

<div>

<div>

<img alt="image" src="images/000010.jpg"/>

</div>

</div>

<p class="noindent"><a id="babilu_link-460"></a> <strong><b>PEDRO DOMINGOS</b></strong> is a professor of computer science at the University of Washington. He is a winner of the SIGKDD Innovation Award, the highest honor in data science. A fellow of the Association for the Advancement of Artificial Intelligence, he lives near Seattle.</p>

</section>

</div>

</div>

<div id="babilu_link-333">

<div>

<section id="babilu_link-349">

<h1><b><a id="babilu_link-461"><b></b></a> <a href="#babilu_link-334"><b>Praise for <b><i>The Master Algorithm</i></b></b></a></b></h1>

<p class="noindent">“With terms like ‘machine learning’ and ‘Big Data’ regularly making headlines, there is no shortage of hype-filled business books on the subject. There are also textbooks that are too technical to be accessible. For those in the middle—from executives to college students—this is the ideal book, showing how and why things really work without the heavy math. Unlike other books that proclaim a bright future, this one actually gives you what you need to understand the changes that are coming.”</p>

<p class="noindent">—Peter Norvig, director of research, Google, and coauthor of <i>Artificial Intelligence</i></p>

<p class="noindent">“Starting with the audacious claim that all knowledge can be derived from data by a single ‘master algorithm,’ Domingos takes the reader on a fast-paced journey through the brave new world of machine learning. Writing breezily but with deep authority, Domingos is the perfect tour guide from whom you will learn everything you need to know about this exciting field, and a surprising amount about science and philosophy as well.”</p>

<p class="noindent">—Duncan Watts, principal researcher, Microsoft Research, and author of <i>Six Degrees</i> and <i>Everything Is Obvious</i></p>

<p class="noindent">“[<i>The Master Algorithm</i> ] does a good job of examining the field’s five main techniques…. The subject is meaty and the author… has a knack for introducing concepts at the right moment.”</p>

<p class="noindent">—<i>Economist</i></p>

<p class="noindent">“Domingos is a genial and amusing guide, who sneaks us around the backstage areas of the science in order to witness the sometimes personal (and occasionally acrimonious) tenor of research on the subject in recent decades…. This is a highly inclusive book, aimed at a wide range of readers from the merely curious to those who might be interested in pursuing a career in the field. Descriptions and discussions are presented with a commendable lack of jargon and the examples are clear and accessible.”</p>

<p class="noindent">—<i>Times Higher Education</i></p>

<p class="noindent"><a id="babilu_link-462"></a> “[An] interesting work.”</p>

<p class="noindent">—<i>CHOICE Reviews</i></p>

<p class="noindent">“An exhilarating venture into groundbreaking computer science.”</p>

<p class="noindent">—<i>Booklist</i> , starred review</p>

<p class="noindent">“[An] enthusiastic but not dumbed-down introduction to machine learning…. [L]ucid and consistently informative…. With wit, vision, and scholarship, Domingos describes how these scientists are creating programs that allow a computer to teach itself. Readers… will discover fascinating insights.”</p>

<p class="noindent">—<i>Kirkus Reviews</i></p>

<p class="noindent">“[<i>The Master Algorithm</i> ] opens the doorway to a world many of us never see or think about, though it has a tremendous impact on our daily lives.”</p>

<p class="noindent">—Shelf Awareness for Readers</p>

<p class="noindent">“This book is a must-have to learn machine learning without equations. It will help you get the big picture of the several learning paradigms. Finally, the provocative idea is not only intriguing, but also very well argued.”</p>

<p class="noindent">—Data Mining Research</p>

<p class="noindent">“If you are interested in a crash course on the enigmatic field of machine learning and the challenges for AI practitioners that lie ahead, this book is a great read.”</p>

<p class="noindent">—TechCast Global</p>

<p class="noindent">“This book is a sheer pleasure, mixed with education. I am recommending it to all my students, those who studied machine learning, those who are about to do it, and those who are about to teach it. The author succeeds not only in presenting an accurate and entertaining journey through the methodological ideas behind machine learning, but also in embedding those ideas in a colorful tapestry of philosophical questions concerning the ultimate capacity of man to emulate itself. A must-read for both realists and futurists.”</p>

<p class="noindent">—Judea Pearl, professor of computer science, UCLA, and winner of the A. M. Turing Award</p>

<p class="noindent">“Machine learning is the single most transformative technology that will shape our lives over the next fifteen years. This book is a must-read—a bold and beautifully written new framework for looking into the future.”</p>

<p class="noindent">—Geoffrey Moore, author of <i>Crossing the Chasm</i></p>

<p class="noindent"><a id="babilu_link-463"></a> “Machine learning is a fascinating world never before glimpsed by outsiders. Pedro Domingos initiates you to the mysterious languages spoken by its five tribes, and invites you to join in his plan to unite them, creating the most powerful technology our civilization has ever seen.”</p>

<p class="noindent">—Sebastian Seung, professor, Princeton, and author of <i>Connectome</i></p>

<p class="noindent">“A delightful book by one of the leading experts in the field. If you wonder how AI will change your life, read this book.”</p>

<p class="noindent">—Sebastian Thrun, research professor, Stanford, Google Fellow, and inventor of the Self-Driving Car</p>

<p class="noindent">“This is an incredibly important and useful book. Machine learning is already critical to your life and work, and will only become more so. Finally, Pedro Domingos has written about it in a clear and understandable fashion.”</p>

<p class="noindent">—Thomas H. Davenport, distinguished professor, Babson College, and author of <i>Competing on Analytics</i> and <i>Big Data @ Work</i></p>

<p class="noindent">“Machine learning, known in commercial use as predictive analytics, is changing the world. This riveting, far-reaching, and inspiring book introduces the deep scientific concepts to even non-technical readers, and yet also satisfies experts with a fresh, profound perspective that reveals the most promising research directions. It’s a rare gem indeed.”</p>

<p class="noindent">—Eric Siegel, founder, Predictive Analytics World, and author of <i>Predictive Analytics</i></p>

</section>

</div>

</div>

<div id="babilu_link-324">

<div>

<section id="babilu_link-350">

<h1><a id="babilu_link-290"></a> <a href="#babilu_link-325">Further Readings</a></h1>

<p class="noindent">If this book whetted your appetite for machine learning and the issues surrounding it, you’ll find many suggestions in this section. Its aim is not to be comprehensive but to provide an entrance to machine learning’s garden of forking paths (as Borges put it). Wherever possible, I chose books and articles appropriate for the general reader. Technical publications, which require at least some computational, statistical, or mathematical background, are marked with an asterisk (*). Even these, however, often have large sections accessible to the general reader. I didn’t list volume, issue, or page numbers, since the web renders them superfluous; likewise for publishers’ locations.</p>

<p class="noindent">If you’d like to learn more about machine learning in general, one good place to start is online courses. Of these, the closest in content to this book is, not coincidentally, the one I teach (http://www.cs.washington.edu/homes/pedrod/class). Two other options are Andrew Ng’s course (www.coursera.org/course/ml) and Yaser Abu-Mostafa’s (http://work.caltech.edu/telecourse.html). The next step is to read a textbook. The closest to this book, and one of the most accessible, is Tom Mitchell’s <i>Machine Learning*</i> (McGraw-Hill, 1997). More up-to-date, but also more mathematical, are Kevin Murphy’s <i>Machine Learning: A Probabilistic Perspective*</i> (MIT Press, 2012), Chris Bishop’s <i>Pattern Recognition and Machine Learning*</i> (Springer, 2006), and <i>An Introduction to Statistical Learning with Applications in R</i> ,* by Gareth James, Daniela Witten, Trevor Hastie, and Rob Tibshirani (Springer, 2013). My article “A few useful things to know about machine learning” (<i>Communications of the ACM</i> , 2012) summarizes some of the “folk knowledge” of machine learning <a id="babilu_link-43"></a> that textbooks often leave implicit and was one of the starting points for this book. If you know how to program and are itching to give machine learning a try, you can start from a number of open-source packages, such as Weka (www.cs.waikato.ac.nz/ml/weka). The two main machine-learning journals are <i>Machine Learning</i> and the <i>Journal of Machine Learning Research</i> . Leading machine-learning conferences, with yearly proceedings, include the International Conference on Machine Learning, the Conference on Neural Information Processing Systems, and the International Conference on Knowledge Discovery and Data Mining. A large number of machine-learning talks are available on http://videolectures.net. The www.KDnuggets.com website is a one-stop shop for machine-learning resources, and you can sign up for its newsletter to keep up-to-date with the latest developments.</p>

<div>

</div>

<h1 id="babilu_link-464">Prologue</h1>

<p class="noindent">An early list of examples of machine learning’s impact on daily life can be found in “Behind-the-scenes data mining,” by George John (<i>SIGKDD Explorations</i> , 1999), which was also the inspiration for the “day-in-the-life” paragraphs of the prologue. Eric Siegel’s book <i>Predictive Analytics</i> (Wiley, 2013) surveys a large number of machine-learning applications. The term <i>big data</i> was popularized by the McKinsey Global Institute’s 2011 report <i>Big Data: The Next Frontier for Innovation, Competition, and Productivity</i> . Many of the issues raised by big data are discussed in <i>Big Data: A Revolution That Will Change How We Live, Work, and Think</i> , by Viktor Mayer-Schönberger and Kenneth Cukier (Houghton Mifflin Harcourt, 2013). The textbook I learned AI from is <i>Artificial Intelligence</i> ,* by Elaine Rich (McGraw-Hill, 1983). A current one is <i>Artificial Intelligence: A Modern Approach</i> , by Stuart Russell and Peter Norvig (3rd ed., Prentice Hall, 2010). Nils Nilsson’s <i>The Quest for Artificial Intelligence</i> (Cambridge University Press, 2010) tells the story of AI from its earliest days.</p>

<div>

</div>

<h1 id="babilu_link-465">Chapter One</h1>

<p class="noindent"><i>Nine Algorithms That Changed the Future</i> , by John MacCormick (Princeton University Press, 2012), describes some of the most important algorithms in computer science, with a chapter on machine learning. <i>Algorithms</i> ,* by Sanjoy Dasgupta, Christos Papadimitriou, and Umesh Vazirani (McGraw-Hill, 2008), is a concise introductory textbook on the subject. <i>The Pattern on the Stone</i> , by Danny Hillis (Basic Books, 1998), explains how computers work. Walter Isaacson recounts the lively history of computer science in <i>The Innovators</i> (Simon &amp; Schuster, 2014).</p>

<p class="noindent">“Spreadsheet data manipulation using examples,”* by Sumit Gulwani, William Harris, and Rishabh Singh (<i>Communications of the ACM</i> , 2012), is an example of <a id="babilu_link-32"></a> how computers can program themselves by observing users. <i>Competing on Analytics</i> , by Tom Davenport and Jeanne Harris (HBS Press, 2007), is an introduction to the use of predictive analytics in business. <i>In the Plex</i> , by Steven Levy (Simon &amp; Schuster, 2011), describes at a high level how Google’s technology works. Carl Shapiro and Hal Varian explain the network effect in <i>Information Rules</i> (HBS Press, 1999). Chris Anderson does the same for the long-tail phenomenon in <i>The Long Tail</i> (Hyperion, 2006).</p>

<p class="noindent">The transformation of science by data-intensive computing is surveyed in <i>The Fourth Paradigm</i> , edited by Tony Hey, Stewart Tansley, and Kristin Tolle (Microsoft Research, 2009). “Machine science,” by James Evans and Andrey Rzhetsky (<i>Science</i> , 2010), discusses some of the different ways computers can make scientific discoveries. <i>Scientific Discovery: Computational Explorations of the Creative Processes</i> ,* by Pat Langley et al. (MIT Press, 1987), describes a series of approaches to automating the discovery of scientific laws. The SKICAT project is described in “From digitized images to online catalogs,” by Usama Fayyad, George Djorgovski, and Nicholas Weir (<i>AI Magazine</i> , 1996). “Machine learning in drug discovery and development,”* by Niki Wale (<i>Drug Development Research</i> , 2001), gives an overview of just that. Adam, the robot scientist, is described in “The automation of science,” by Ross King et al. (<i>Science</i> , 2009).</p>

<p class="noindent">Sasha Issenberg’s <i>The Victory Lab</i> (Broadway Books, 2012) dissects the use of data analysis in politics. “How President Obama’s campaign used big data to rally individual votes,” by the same author (<i>MIT Technology Review</i> , 2013), tells the story of its greatest success to date. Nate Silver’s <i>The Signal and the Noise</i> (Penguin Press, 2012) has a chapter on his poll aggregation method.</p>

<p class="noindent">Robot warfare is the theme of P. W. Singer’s <i>Wired for War</i> (Penguin, 2009). <i>Cyber War</i> , by Richard Clarke and Robert Knake (Ecco, 2012), sounds the alarm on cyberwar. My work on combining machine learning with game theory to defeat adversaries, which started as a class project, is described in “Adversarial classification,”* by Nilesh Dalvi et al. (<i>Proceedings of the Tenth International Conference on Knowledge Discovery and Data Mining</i> , 2004). <i>Predictive Policing</i> , by Walter Perry et al. (Rand, 2013), is a guide to the use of analytics in police work.</p>

<div>

</div>

<h1 id="babilu_link-466">Chapter Two</h1>

<p class="noindent">The ferret brain rewiring experiments are described in “Visual behaviour mediated by retinal projections directed to the auditory pathway,” by Laurie von Melchner, Sarah Pallas, and Mriganka Sur (<i>Nature</i> , 2000). Ben Underwood’s story is told in “Seeing with sound,” by Joanna Moorhead (<i>Guardian</i> , 2007), and at www.ben underwood.com. Otto Creutzfeldt makes the case that the cortex is one algorithm in “Generality of the functional structure of the neocortex” (<i>Naturwissenschaften</i> , <a id="babilu_link-191"></a> 1977), as does Vernon Mountcastle in “An organizing principle for cerebral function: The unit model and the distributed system,” in <i>The Mindful Brain</i> , edited by Gerald Edelman and Vernon Mountcastle (MIT Press, 1978). Gary Marcus, Adam Marblestone, and Tom Dean make the case against in “The atoms of neural computation” (<i>Science</i> , 2014).</p>

<p class="noindent">“The unreasonable effectiveness of data,” by Alon Halevy, Peter Norvig, and Fernando Pereira (<i>IEEE Intelligent Systems</i> , 2009), argues for machine learning as the new discovery paradigm. Benoît Mandelbrot explores the fractal geometry of nature in the eponymous book* (Freeman, 1982). James Gleick’s <i>Chaos</i> (Viking, 1987) discusses and depicts the Mandelbrot set. The Langlands program, a research effort that seeks to unify different subfields of mathematics, is described in <i>Love and Math</i> , by Edward Frenkel (Basic Books, 2014). <i>The Golden Ticket</i> , by Lance Fortnow (Princeton University Press, 2013), is an introduction to NP-completeness and the P = NP problem. <i>The Annotated Turing</i> ,* by Charles Petzold (Wiley, 2008), explains Turing machines by revisiting Turing’s original paper on them.</p>

<p class="noindent">The Cyc project is described in “Cyc: Toward programs with common sense,”* by Douglas Lenat et al. (<i>Communications of the ACM</i> , 1990). Peter Norvig discusses Noam Chomsky’s criticisms of statistical learning in “On Chomsky and the two cultures of statistical learning” (http://norvig.com/chomsky.html). Jerry Fodor’s <i>The Modularity of Mind</i> (MIT Press, 1983) summarizes his views on how the mind works. “What big data will never explain,” by Leon Wieseltier (<i>New Republic</i> , 2013), and “Pundits, stop sounding ignorant about data,” by Andrew McAfee (<i>Harvard Business Review</i> , 2013), give a flavor of the controversy surrounding what big data can and can’t do. Daniel Kahneman explains why algorithms often beat intuitions in Chapter 21 of <i>Thinking, Fast and Slow</i> (Farrar, Straus and Giroux, 2011). David Patterson makes the case for the role of computing and data in the fight against cancer in “Computer scientists may have what it takes to help cure cancer” (<i>New York Times</i> , 2011).</p>

<p class="noindent">More on the various tribes’ paths to the Master Algorithm in the corresponding sections below.</p>

<div>

</div>

<h1 id="babilu_link-467">Chapter Three</h1>

<p class="noindent">Hume’s classic formulation of the problem of induction appears in Volume I of <i>A Treatise of Human Nature</i> (1739). David Wolpert derives his “no free lunch” theorem for induction in “The lack of a priori distinctions between learning algorithms”* (<i>Neural Computation</i> , 1996). I discuss the importance of prior knowledge in machine learning in “Toward knowledge-rich data mining”* (<i>Data Mining and Knowledge Discovery</i> , 2007) and misinterpretations of Occam’s razor in “The role <a id="babilu_link-136"></a> of Occam’s razor in knowledge discovery”* (<i>Data Mining and Knowledge Discovery</i> , 1999). Overfitting is one of the main themes of <i>The Signal and the Noise</i> , by Nate Silver (Penguin Press, 2012), who calls it “the most important scientific problem you’ve never heard of.” “Why most published research findings are false,”* by John Ioannidis (<i>PLoS Medicine</i> , 2005), discusses the problem of mistaking chance findings for true ones in science. Yoav Benjamini and Yosef Hochberg propose a way to combat it in “Controlling the false discovery rate: A practical and powerful approach to multiple testing”* (<i>Journal of the Royal Statistical Society, Series B</i> , 1995). The bias-variance decomposition is presented in “Neural networks and the bias/variance dilemma,” by Stuart Geman, Elie Bienenstock, and René Doursat (<i>Neural Computation</i> , 1992). <strong><b>“</b></strong> Machine learning as an experimental science,” by Pat Langley (<i>Machine Learning</i> , 1988), discusses the role of experimentation in machine learning.</p>

<p class="noindent">William Stanley Jevons first proposed viewing induction as the inverse of deduction in <i>The Principles of Science</i> (1874). The paper “Machine learning of first-order predicates by inverting resolution,”* by Steve Muggleton and Wray Buntine (<i>Proceedings of the Fifth International Conference on Machine Learning</i> , 1988), initiated the use of inverse deduction in machine learning. The book <i>Relational Data Mining</i> ,* edited by Sašo Džeroski and Nada Lavrač (Springer, 2001), is an introduction to the field of inductive logic programming, where inverse deduction is studied. “The CN2 Induction Algorithm,”* by Peter Clark and Tim Niblett (<i>Machine Learning</i> , 1989), summarizes some of the main Michalski-style rule induction algorithms. The rule-mining approach used by retailers is described in “Fast algorithms for mining association rules,”* by Rakesh Agrawal and Ramakrishnan Srikant (<i>Proceedings of the Twentieth International Conference on Very Large Databases</i> , 1994). An example of rule induction for cancer prediction is described in “Carcinogenesis predictions using inductive logic programming,” by Ashwin Srinivasan, Ross King, Stephen Muggleton, and Michael Sternberg (<i>Intelligent Data Analysis in Medicine and Pharmacology</i> , 1997).</p>

<p class="noindent">The two leading decision tree learners are presented in <i>C4.5: Programs for Machine Learning</i> ,* by J. Ross Quinlan (Morgan Kaufmann, 1992), and <i>Classification and Regression Trees</i> ,* by Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone (Chapman and Hall, 1984). “Real-time human pose recognition in parts from single depth images,”* by Jamie Shotton et al. (<i>Communications of the ACM</i> , 2013), explains how Microsoft’s Kinect uses decision trees to track gamers’ motions. “Competing approaches to predicting Supreme Court decision making,” by Andrew Martin et al. (<i>Perspectives on Politics</i> , 2004), describes how decision trees beat legal experts at predicting Supreme Court votes and shows the decision tree for Justice Sandra Day O’Connor.</p>

<p class="noindent"><a id="babilu_link-101"></a> Allen Newell and Herbert Simon formulated the hypothesis that all intelligence is symbol manipulation in “Computer science as empirical enquiry: Symbols and search” (<i>Communications of the ACM</i> , 1976). David Marr proposed his three levels of information processing in <i>Vision</i> * (Freeman, 1982). <i>Machine Learning: An Artificial Intelligence Approach</i> ,* edited by Ryszard Michalski, Jaime Carbonell, and Tom Mitchell (Tioga, 1983), gives a snapshot of the early days of symbolist research in machine learning. “Connectionist AI, symbolic AI, and the brain,”* by Paul Smolensky (<i>Artificial Intelligence Review</i> , 1987), gives a connectionist view of symbolist models.</p>

<div>

</div>

<h1 id="babilu_link-468">Chapter Four</h1>

<p class="noindent">Sebastian Seung’s <i>Connectome</i> (Houghton Mifflin Harcourt, 2012) is an accessible introduction to neuroscience, connectomics, and the daunting challenge of reverse engineering the brain. <i>Parallel Distributed Processing</i> ,* edited by David Rumelhart, James McClelland, and the PDP research group (MIT Press, 1986), is the bible of connectionism in its 1980s heyday. <i>Neurocomputing</i> ,* edited by James Anderson and Edward Rosenfeld (MIT Press, 1988), collates many of the classic connectionist papers, including: McCulloch and Pitts on the first models of neurons; Hebb on Hebb’s rule; Rosenblatt on perceptrons; Hopfield on Hopfield networks; Ackley, Hinton, and Sejnowski on Boltzmann machines; Sejnowski and Rosenberg on NETtalk; and Rumelhart, Hinton, and Williams on backpropagation. “Efficient backprop,”* by Yann LeCun, Léon Bottou, Genevieve Orr, and Klaus-Robert Müller, in <i>Neural Networks: Tricks of the Trade</i> , edited by Genevieve Orr and Klaus-Robert Müller (Springer, 1998), explains some of the main tricks needed to make backprop work.</p>

<p class="noindent"><i>Neural Networks in Finance and Investing</i> ,* edited by Robert Trippi and Efraim Turban (McGraw-Hill, 1992), is a collection of articles on financial applications of neural networks. “Life in the fast lane: The evolution of an adaptive vehicle control system,” by Todd Jochem and Dean Pomerleau (<i>AI Magazine</i> , 1996), describes the ALVINN self-driving car project. Paul Werbos’s PhD thesis is <i>Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences*</i> (Harvard University, 1974). Arthur Bryson and Yu-Chi Ho describe their early version of backprop in <i>Applied Optimal Control</i> * (Blaisdell, 1969).</p>

<p class="noindent"><i>Learning Deep Architectures for AI</i> ,* by Yoshua Bengio (Now, 2009), is a brief introduction to deep learning. The problem of error signal diffusion in backprop is described in “Learning long-term dependencies with gradient descent is difficult,”* by Yoshua Bengio, Patrice Simard, and Paolo Frasconi (<i>IEEE Transactions on Neural Networks</i> , 1994). “How many computers to identify a cat? 16,000,” by <a id="babilu_link-208"></a> John Markoff (<i>New York Times</i> , 2012), reports on the Google Brain project and its results. Convolutional neural networks, the current deep learning champion, are described in “Gradient-based learning applied to document recognition,”* by Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner (<i>Proceedings of the IEEE</i> , 1998). “The $1.3B quest to build a supercomputer replica of a human brain,” by Jonathon Keats (<i>Wired</i> , 2013), describes the European Union’s brain modeling project. “The NIH BRAIN Initiative,” by Thomas Insel, Story Landis, and Francis Collins (<i>Science</i> , 2013), describes the BRAIN initiative.</p>

<p class="noindent">Steven Pinker summarizes the symbolists’ criticisms of connectionist models in Chapter 2 of <i>How the Mind Works</i> (Norton, 1997). Seymour Papert gives his take on the debate in “One AI or Many?” (<i>Daedalus</i> , 1988). <i>The Birth of the Mind</i> , by Gary Marcus (Basic Books, 2004), explains how evolution could give rise to the human brain’s complex abilities.</p>

<div>

</div>

<h1 id="babilu_link-469">Chapter Five</h1>

<p class="noindent">“Evolutionary robotics,” by Josh Bongard (<i>Communications of the ACM</i> , 2013), surveys the work of Hod Lipson and others on evolving robots. <i>Artificial Life</i> , by Steven Levy (Vintage, 1993), gives a tour of the digital zoo, from computer-created animals in virtual worlds to genetic algorithms. Chapter 5 of <i>Complexity</i> , by Mitch Waldrop (Touchstone, 1992), tells the story of John Holland and the first few decades of research on genetic algorithms. <i>Genetic Algorithms in Search, Optimization, and Machine Learning</i> ,* by David Goldberg (Addison-Wesley, 1989), is the standard introduction to genetic algorithms.</p>

<p class="noindent">Niles Eldredge and Stephen Jay Gould propose their theory of punctuated equilibria in “Punctuated equilibria: An alternative to phyletic gradualism,” in <i>Models in Paleobiology</i> , edited by T. J. M. Schopf (Freeman, 1972). Richard Dawkins critiques it in Chapter 9 of <i>The Blind Watchmaker</i> (Norton, 1986). The exploration-exploitation dilemma is discussed in Chapter 2 of <i>Reinforcement Learning</i> ,* by Richard Sutton and Andrew Barto (MIT Press, 1998). John Holland proposes his solution, and much else, in <i>Adaptation in Natural and Artificial Systems</i> * (University of Michigan Press, 1975).</p>

<p class="noindent">John Koza’s <i>Genetic Programming</i> * (MIT Press, 1992) is the key reference on this paradigm. An evolved robot soccer team is described in “Evolving team <i>Darwin United</i> ,”* by David Andre and Astro Teller, in <i>RoboCup-98: Robot Soccer World Cup II</i> , edited by Minoru Asada and Hiroaki Kitano (Springer, 1999). <i>Genetic Programming III</i> ,* by John Koza, Forrest Bennett III, David Andre, and Martin Keane (Morgan Kaufmann, 1999), includes many examples of evolved electronic circuits. Danny Hillis argues that parasites are good for evolution in “Co-evolving parasites <a id="babilu_link-110"></a> improve simulated evolution as an optimization procedure”* (<i>Physica D</i> , 1990). Adi Livnat, Christos Papadimitriou, Jonathan Dushoff, and Marcus Feldman propose that sex optimizes mixability in “A mixability theory of the role of sex in evolution”* (<i>Proceedings of the National Academy of Sciences</i> , 2008). Kevin Lang’s paper comparing genetic programming and hill climbing is “Hill climbing beats genetic search on a Boolean circuit synthesis problem of Koza’s”* (<i>Proceedings of the Twelfth International Conference on Machine Learning</i> , 1995). Koza’s reply is “A response to the ML-95 paper entitled…”* (unpublished; online at www.genetic-programming.com/jktahoe24page.html).</p>

<p class="noindent">James Baldwin proposed the eponymous effect in “A new factor in evolution” (<i>American Naturalist</i> , 1896). Geoff Hinton and Steven Nowlan describe their implementation of it in “How learning can guide evolution”* (<i>Complex Systems</i> , 1987). The Baldwin effect was the theme of a 1996 special issue* of the journal <i>Evolutionary Computation</i> edited by Peter Turney, Darrell Whitley, and Russell Anderson.</p>

<p class="noindent">The distinction between descriptive and normative theories was articulated by John Neville Keynes in <i>The Scope and Method of Political Economy</i> (Macmillan, 1891).</p>

<div>

</div>

<h1 id="babilu_link-470">Chapter Six</h1>

<p class="noindent">Sharon Bertsch McGrayne tells the history of Bayesianism, from Bayes and Laplace to the present, in <i>The Theory That Would Not Die</i> (Yale University Press, 2011). <i>A First Course in Bayesian Statistical Methods</i> ,* by Peter Hoff (Springer, 2009), is an introduction to Bayesian statistics.</p>

<p class="noindent">The Naïve Bayes algorithm is first mentioned in <i>Pattern Classification and Scene Analysis</i> ,* by Richard Duda and Peter Hart (Wiley, 1973). Milton Friedman argues for oversimplified theories in “The methodology of positive economics,” which appears in <i>Essays in Positive Economics</i> (University of Chicago Press, 1966). The use of Naïve Bayes in spam filtering is described in “Stopping spam,” by Joshua Goodman, David Heckerman, and Robert Rounthwaite (<i>Scientific American</i> , 2005). “Relevance weighting of search terms,”* by Stephen Robertson and Karen Sparck Jones (<i>Journal of the American Society for Information Science</i> , 1976), explains the use of Naïve Bayes–like methods in information retrieval.</p>

<p class="noindent">“First links in the Markov chain,” by Brian Hayes (<i>American Scientist</i> , 2013), recounts Markov’s invention of the eponymous chains. “Large language models in machine translation,”* by Thorsten Brants et al. (<i>Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</i> , 2007), explains how Google Translate works. <a id="babilu_link-118"></a> “The PageRank citation ranking: Bringing order to the Web,”* by Larry Page, Sergey Brin, Rajeev Motwani, and Terry Winograd (Stanford University technical report, 1998), describes the PageRank algorithm and its interpretation as a random walk over the web. <i>Statistical Language Learning</i> ,* by Eugene Charniak (MIT Press, 1996), explains how hidden Markov models work. <i>Statistical Methods for Speech Recognition</i> ,* by Fred Jelinek (MIT Press, 1997), describes their application to speech recognition. The story of HMM-style inference in communication is told in “The Viterbi algorithm: A personal history,” by David Forney (unpublished; online at arxiv.org/pdf/cs/0504020v2.pdf). <i>Bioinformatics: The Machine Learning Approach</i> ,* by Pierre Baldi and Søren Brunak (2nd ed., MIT Press, 2001), is an introduction to the use of machine learning in biology, including HMMs. “Engineers look to Kalman filtering for guidance,” by Barry Cipra (<i>SIAM News</i> , 1993), is a brief introduction to Kalman filters, their history, and their applications.</p>

<p class="noindent">Judea Pearl’s pioneering work on Bayesian networks appears in his book <i>Probabilistic Reasoning in Intelligent Systems</i> * (Morgan Kaufmann, 1988). “Bayesian networks without tears,”* by Eugene Charniak (<i>AI Magazine</i> , 1991), is a largely nonmathematical introduction to them. “Probabilistic interpretation for MYCIN’s certainty factors,”* by David Heckerman (<i>Proceedings of the Second Conference on Uncertainty in Artificial Intelligence</i> , 1986), explains when sets of rules with confidence estimates are and aren’t a reasonable approximation to Bayesian networks. “Module networks: Identifying regulatory modules and their condition-specific regulators from gene expression data,” by Eran Segal et al. (<i>Nature Genetics</i> , 2003), is an example of using Bayesian networks to model gene regulation. “Microsoft virus fighter: Spam may be more difficult to stop than HIV,” by Ben Paynter (<i>Fast Company</i> , 2012), tells how David Heckerman took inspiration from spam filters and used Bayesian networks to design a potential AIDS vaccine. The probabilistic or “noisy” OR is explained in Pearl’s book.* “Probabilistic diagnosis using a reformulation of the INTERNIST-1/QMR knowledge base,” by M. A. Shwe et al. (Parts I and II, <i>Methods of Information in Medicine</i> , 1991), describes a noisy-OR Bayesian network for medical diagnosis. Google’s Bayesian network for ad placement is described in Section 26.5.4 of Kevin Murphy’s <i>Machine Learning*</i> (MIT Press, 2012). Microsoft’s player rating system is described in “TrueSkill<sup>TM</sup> : A Bayesian skill rating system,”* by Ralf Herbrich, Tom Minka, and Thore Graepel (<i>Advances in Neural Information Processing Systems 19</i> , 2007).</p>

<p class="noindent"><i>Modeling and Reasoning with Bayesian Networks</i> ,* by Adnan Darwiche (Cambridge University Press, 2009), explains the main algorithms for inference in Bayesian networks. The January/February 2000 issue* of <i>Computing in Science and Engineering</i> , edited by Jack Dongarra and Francis Sullivan, has articles on the top ten algorithms of the twentieth century, including MCMC. “Stanley: The robot <a id="babilu_link-19"></a> that won the DARPA Grand Challenge,” by Sebastian Thrun et al. (<i>Journal of Field Robotics</i> , 2006), explains how the eponymous self-driving car works. “Bayesian networks for data mining,”* by David Heckerman <i>(Data Mining and Knowledge Discovery</i> , 1997), summarizes the Bayesian approach to learning and explains how to learn Bayesian networks from data. “Gaussian processes: A replacement for supervised neural networks?,”* by David MacKay (NIPS tutorial notes, 1997; online at www.inference.eng.cam.ac.uk/mackay/gp.pdf), gives a flavor of how the Bayesians co-opted NIPS.</p>

<p class="noindent">The need for weighting the word probabilities in speech recognition is discussed in Section 9.6 of <i>Speech and Language Processing</i> ,* by Dan Jurafsky and James Martin (2nd ed., Prentice Hall, 2009). My paper on Naïve Bayes, with Mike Pazzani, is “On the optimality of the simple Bayesian classifier under zero-one loss”* (<i>Machine Learning</i> , 1997; expanded journal version of the 1996 conference paper). Judea Pearl’s book,* mentioned above, discusses Markov networks along with Bayesian networks. Markov networks in computer vision are the subject of <i>Markov Random Fields for Vision and Image Processing</i> ,* edited by Andrew Blake, Pushmeet Kohli, and Carsten Rother (MIT Press, 2011). Markov networks that maximize conditional likelihood were introduced in “Conditional random fields: Probabilistic models for segmenting and labeling sequence data,”* by John Lafferty, Andrew McCallum, and Fernando Pereira (<i>International Conference on Machine Learning</i> , 2001).</p>

<p class="noindent">The history of attempts to combine probability and logic is surveyed in a 2003 special issue* of the <i>Journal of Applied Logic</i> devoted to the subject, edited by Jon Williamson and Dov Gabbay. “From knowledge bases to decision models,”* by Michael Wellman, John Breese, and Robert Goldman (<i>Knowledge Engineering Review</i> , 1992), discusses some of the early AI approaches to the problem.</p>

<div>

</div>

<h1 id="babilu_link-471">Chapter Seven</h1>

<p class="noindent">Frank Abagnale details his exploits in his autobiography, <i>Catch Me If You Can</i> , cowritten with Stan Redding (Grosset &amp; Dunlap, 1980). The original technical report on the nearest-neighbor algorithm by Evelyn Fix and Joe Hodges is “Discriminatory analysis: Nonparametric discrimination: Consistency properties”* (USAF School of Aviation Medicine, 1951). <i>Nearest Neighbor (NN) Norms</i> ,* edited by Belur Dasarathy (IEEE Computer Society Press, 1991), collects many of the key papers in this area. Locally linear regression is surveyed in “Locally weighted learning,”* by Chris Atkeson, Andrew Moore, and Stefan Schaal (<i>Artificial Intelligence Review</i> , 1997). The first collaborative filtering system based on nearest neighbors is described in “GroupLens: An open architecture for collaborative filtering <a id="babilu_link-168"></a> of netnews,”* by Paul Resnick et al. (<i>Proceedings of the 1994 ACM Conference on Computer-Supported Cooperative Work</i> , 1994). Amazon’s collaborative filtering algorithm is described in “Amazon.com recommendations: Item-to-item collaborative filtering,”* by Greg Linden, Brent Smith, and Jeremy York (<i>IEEE Internet Computing</i> , 2003). (See Chapter 8’s further readings for Netflix’s.) Recommender systems’ contribution to Amazon and Netflix sales is referenced in, among others, Mayer-Schönberger and Cukier’s <i>Big Data</i> and Siegel’s <i>Predictive Analytics</i> (cited earlier). The 1967 paper by Tom Cover and Peter Hart on nearest-neighbor’s error rate is “Nearest neighbor pattern classification”* (<i>IEEE Transactions on Information Theory</i>).</p>

<p class="noindent">The curse of dimensionality is discussed in Section 2.5 of <i>The Elements of Statistical Learning</i> ,* by Trevor Hastie, Rob Tibshirani, and Jerry Friedman (2nd ed., Springer, 2009). “Wrappers for feature subset selection,”* by Ron Kohavi and George John (<i>Artificial Intelligence</i> , 1997), compares attribute selection methods. “Similarity metric learning for a variable-kernel classifier,”* by David Lowe (<i>Neural Computation</i> , 1995), is an example of a feature weighting algorithm.</p>

<p class="noindent">“Support vector machines and kernel methods: The new generation of learning machines,”* by Nello Cristianini and Bernhard Schölkopf (<i>AI Magazine</i> , 2002), is a mostly nonmathematical introduction to SVMs. The paper that started the SVM revolution was “A training algorithm for optimal margin classifiers,”* by Bernhard Boser, Isabel Guyon, and Vladimir Vapnik (<i>Proceedings of the Fifth Annual Workshop on Computational Learning Theory</i> , 1992). The first paper applying SVMs to text classification was “Text categorization with support vector machines,”* by Thorsten Joachims (<i>Proceedings of the Tenth European Conference on Machine Learning</i> , 1998). Chapter 5 of <i>An Introduction to Support Vector Machines</i> ,* by Nello Cristianini and John Shawe-Taylor (Cambridge University Press, 2000), is a brief introduction to constrained optimization in the context of SVMs.</p>

<p class="noindent"><i>Case-Based Reasoning</i> ,* by Janet Kolodner (Morgan Kaufmann, 1993), is a textbook on the subject. “Using case-based retrieval for customer technical support,”* by Evangelos Simoudis (<i>IEEE Expert</i> , 1992), explains its application to help desks. IPsoft’s Eliza is described in “Rise of the software machines” (<i>Economist</i> , 2013) and on the company’s website. Kevin Ashley explores case-based legal reasoning in <i>Modeling Legal Arguments</i> * (MIT Press, 1991). David Cope summarizes his approach to automated music composition in “Recombinant music: Using the computer to explore musical style” (<i>IEEE Computer</i> , 1991). Dedre Gentner proposed structure mapping in “Structure mapping: A theoretical framework for analogy”* (<i>Cognitive Science</i> , 1983). “The man who would teach machines to think,” by James Somers (<i>Atlantic</i> , 2013), discusses Douglas Hofstadter’s views on AI.</p>

<p class="noindent"><a id="babilu_link-233"></a> The RISE algorithm is described in my paper “Unifying instance-based and rule-based induction”* (<i>Machine Learning</i> , 1996).</p>

<div>

</div>

<h1 id="babilu_link-472">Chapter Eight</h1>

<p class="noindent"><i>The Scientist in the Crib</i> , by Alison Gopnik, Andy Meltzoff, and Pat Kuhl (Harper, 1999), summarizes psychologists’ discoveries about how babies and young children learn.</p>

<p class="noindent">The <i>k</i> -means algorithm was originally proposed by Stuart Lloyd at Bell Labs in 1957, in a technical report entitled “Least squares quantization in PCM”* (which later appeared as a paper in the <i>IEEE Transactions on Information Theory</i> in 1982). The original paper on the EM algorithm is “Maximum likelihood from incomplete data via the EM algorithm,”* by Arthur Dempster, Nan Laird, and Donald Rubin (<i>Journal of the Royal Statistical Society B</i> , 1977). Hierarchical clustering and other methods are described in <i>Finding Groups in Data: An Introduction to Cluster Analysis</i> ,* by Leonard Kaufman and Peter Rousseeuw (Wiley, 1990).</p>

<p class="noindent">Principal-component analysis is one of the oldest techniques in machine learning and statistics, having been first proposed by Karl Pearson in 1901 in the paper “On lines and planes of closest fit to systems of points in space”* (<i>Philosophical Magazine</i>). The type of dimensionality reduction used to grade SAT essays was introduced by Scott Deerwester et al. in the paper “Indexing by latent semantic analysis”* (<i>Journal of the American Society for Information Science</i> , 1990). Yehuda Koren, Robert Bell, and Chris Volinsky explain how Netflix-style collaborative filtering works in “Matrix factorization techniques for recommender systems”* (<i>IEEE Computer</i> , 2009). The Isomap algorithm was introduced in “A global geometric framework for nonlinear dimensionality reduction,”* by Josh Tenenbaum, Vin de Silva, and John Langford (<i>Science</i> , 2000).</p>

<p class="noindent"><i>Reinforcement Learning: An Introduction</i> ,* by Rich Sutton and Andy Barto (MIT Press, 1998), is the standard textbook on the subject. <i>Universal Artificial Intelligence</i> ,* by Marcus Hutter (Springer, 2005), is an attempt at a general theory of reinforcement learning. Arthur Samuel’s pioneering research on learning to play checkers is described in his paper “Some studies in machine learning using the game of checkers”* (<i>IBM Journal of Research and Development</i> , 1959). This paper also marks one of the earliest appearances in print of the term <i>machine learning</i> . Chris Watkins’s formulation of the reinforcement learning problem appeared in his PhD thesis <i>Learning from Delayed Rewards</i> * (Cambridge University, 1989). DeepMind’s reinforcement learner for video games is described in “Human-level control through deep reinforcement learning,”* by Volodymyr Mnih et al. (<i>Nature</i> , 2015).</p>

<p class="noindent"><a id="babilu_link-22"></a> Paul Rosenbloom retells the development of chunking in “A cognitive odyssey: From the power law of practice to a general learning mechanism and beyond” (<i>Tutorials in Quantitative Methods for Psychology</i> , 2006). A/B testing and other online experimentation techniques are explained in “Practical guide to controlled experiments on the Web: Listen to your customers not to the HiPPO,”* by Ron Kohavi, Randal Henne, and Dan Sommerfield (<i>Proceedings of the Thirteenth International Conference on Knowledge Discovery and Data Mining</i> , 2007). Uplift modeling, a multidimensional generalization of A/B testing, is the subject of Chapter 7 of Eric Siegel’s <i>Predictive Analytics</i> (Wiley, 2013).</p>

<p class="noindent"><i>Introduction to Statistical Relational Learning</i> ,* edited by Lise Getoor and Ben Taskar (MIT Press, 2007), surveys the main approaches in this area. My work with Matt Richardson on modeling word of mouth is summarized in “Mining social networks for viral marketing” (<i>IEEE Intelligent Systems</i> , 2005).</p>

<div>

</div>

<h1 id="babilu_link-473">Chapter Nine</h1>

<p class="noindent"><i>Model Ensembles: Foundations and Algorithms</i> ,* by Zhi-Hua Zhou (Chapman and Hall, 2012), is an introduction to metalearning. The original paper on stacking is “Stacked generalization,”* by David Wolpert (<i>Neural Networks</i> , 1992). Leo Breiman introduced bagging in “Bagging predictors”* (<i>Machine Learning</i> , 1996) and random forests in “Random forests”* (<i>Machine Learning</i> , 2001). Boosting is described in “Experiments with a new boosting algorithm,” by Yoav Freund and Rob Schapire (<i>Proceedings of the Thirteenth International Conference on Machine Learning</i> , 1996).</p>

<p class="noindent">“I, Algorithm,” by Anil Ananthaswamy (<i>New Scientist</i> , 2011), chronicles the road to combining logic and probability in AI. <i>Markov Logic: An Interface Layer for Artificial Intelligence</i> ,* which I cowrote with Daniel Lowd (Morgan &amp; Claypool, 2009), is an introduction to Markov logic networks. The Alchemy website, http://alchemy.cs.washington.edu, also includes tutorials, videos, MLNs, data sets, publications, pointers to other systems, and so on. An MLN for robot mapping is described in “Hybrid Markov logic networks,”* by Jue Wang and Pedro Domingos (<i>Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence</i> , 2008). Thomas Dietterich and Xinlong Bao describe the use of MLNs in DARPA’s PAL project in “Integrating multiple learning components through Markov logic”* (<i>Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence</i> , 2008). “Extracting semantic networks from text via relational clustering,”* by Stanley Kok and Pedro Domingos (<i>Proceedings of the Nineteenth European Conference on Machine Learning</i> , 2008), describes how we used MLNs to learn a semantic network from the Web.</p>

<p class="noindent"><a id="babilu_link-166"></a> Efficient MLNs with hierarchical class and part structure are described in “Learning and inference in tractable probabilistic knowledge bases,”* by Mathias Niepert and Pedro Domingos (<i>Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence</i> , 2015). Google’s approach to parallel gradient descent is described in “Large-scale distributed deep networks,”* by Jeff Dean et al. (<i>Advances in Neural Information Processing Systems 25</i> , 2012). “A general framework for mining massive data streams,”* by Pedro Domingos and Geoff Hulten (<i>Journal of Computational and Graphical Statistics</i> , 2003), summarizes our sampling-based method for learning from open-ended data streams. The FuturICT project is the subject of “The machine that would predict the future,” by David Weinberger (<i>Scientific American</i> , 2011).</p>

<p class="noindent">“Cancer: The march on malignancy” (<i>Nature</i> supplement, 2014) surveys the current state of the war on cancer. “Using patient data for personalized cancer treatments,” by Chris Edwards (<i>Communications of the ACM</i> , 2014), describes the early stages of what could grow into CanceRx. “Simulating a living cell,” by Markus Covert (<i>Scientific American</i> , 2014), explains how his group built a computer model of a whole infectious bacterium. “Breakthrough Technologies 2015: Internet of DNA,” by Antonio Regalado (<i>MIT Technology Review</i> , 2015), reports on the work of the Global Alliance for Genomics and Health. Cancer Commons is described in “Cancer: A Computational Disease that AI Can Cure,” by Jay Tenenbaum and Jeff Shrager (<i>AI Magazine</i> , 2011).</p>

<div>

</div>

<h1 id="babilu_link-474">Chapter Ten</h1>

<p class="noindent">“Love, actuarially,” by Kevin Poulsen (<i>Wired</i> , 2014), tells the story of how one man used machine learning to find love on the OkCupid dating site. <i>Dataclysm</i> , by Christian Rudder (Crown, 2014), mines OkCupid’s data for sundry insights. <i>Total Recall</i> , by Gordon Moore and Jim Gemmell (Dutton, 2009), explores the implications of digitally recording everything we do. <i>The Naked Future</i> , by Patrick Tucker (Current, 2014), surveys the use and abuse of data for prediction in our world. Craig Mundie argues for a balanced approach to data collection and use in “Privacy pragmatism” (<i>Foreign Affairs</i> , 2014). <i>The Second Machine Age</i> , by Erik Brynjolfsson and Andrew McAfee (Norton, 2014), discusses how progress in AI will shape the future of work and the economy. “World War R,” by Chris Baraniuk (<i>New Scientist</i> , 2014) reports on the debate surrounding the use of robots in battle. “Transcending complacency on superintelligent machines,” by Stephen Hawking et al. (<i>Huffington Post</i> , 2014), argues that now is the time to worry about AI’s risks. Nick Bostrom’s <i>Superintelligence</i> (Oxford University Press, 2014) considers those dangers and what to do about them.</p>

<p class="noindent"><i><a id="babilu_link-247"><i></i></a> A Brief History of Life</i> , by Richard Hawking (Random Penguin, 1982), summarizes the quantum leaps of evolution in the eons BC. (Before Computers. Just kidding.) <i>The Singularity Is Near</i> , by Ray Kurzweil (Penguin, 2005), is your guide to the transhuman future. Joel Garreau considers three different scenarios for how human-directed evolution will unfold in <i>Radical Evolution</i> (Broadway Books, 2005). In <i>What Technology Wants</i> (Penguin, 2010), Kevin Kelly argues that technology is the continuation of evolution by other means. <i>Darwin Among the Machines</i> , by George Dyson (Basic Books, 1997), chronicles the evolution of technology and speculates on where it will lead. Craig Venter explains how his team synthesized a living cell in <i>Life at the Speed of Light</i> (Viking, 2013).</p>

</section>

</div>

</div>

<div id="babilu_link-16">

<div>

<section id="babilu_link-351">

<h1><a id="babilu_link-475"></a> <a href="#babilu_link-17">Index</a></h1>

<p class="noindent">Abagnale, Frank, Jr., <a href="#babilu_link-18">177</a> , <a href="#babilu_link-19">306</a></p>

<p class="noindent">aboutthedata.com, <a href="#babilu_link-20">272</a></p>

<p class="noindent">A/B testing, <a href="#babilu_link-21">227</a> , <a href="#babilu_link-22">309</a></p>

<p class="noindent">Accuracy, <a href="#babilu_link-23">75–79</a> , <a href="#babilu_link-24">87</a> , <a href="#babilu_link-25">241</a> , <a href="#babilu_link-26">243</a></p>

<p class="noindent">Ackley, David, <a href="#babilu_link-27">103</a></p>

<p class="noindent">Action potentials, <a href="#babilu_link-28">95–96</a> , <a href="#babilu_link-29">104–105</a></p>

<p class="noindent">Acxiom, <a href="#babilu_link-20">272</a></p>

<p class="noindent">Adam, the robot scientist, <a href="#babilu_link-30">16</a> , <a href="#babilu_link-31">84</a> , <a href="#babilu_link-32">299</a></p>

<p class="noindent">Adaptive systems, <a href="#babilu_link-33">8</a> . <i>See also</i> Machine learning</p>

<p class="noindent">AdSense system, <a href="#babilu_link-34">160</a></p>

<p class="noindent">AI. <i>See</i> Artificial intelligence (AI)</p>

<p class="noindent">AIDS vaccine, Bayesian networks and, <a href="#babilu_link-35">159–160</a></p>

<p class="noindent">Alchemy, <a href="#babilu_link-36">246–259</a> , <a href="#babilu_link-22">309</a></p>

<p class="noindent">Markov logic networks and, <a href="#babilu_link-36">246–250</a></p>

<p class="noindent">shortcomings, <a href="#babilu_link-37">255–259</a></p>

<p class="noindent">tribes of machine learning and, <a href="#babilu_link-38">250–255</a></p>

<p class="noindent">alchemy.cs.washington.edu, <a href="#babilu_link-38">250</a></p>

<p class="noindent">Algorithms</p>

<p class="noindent">classifiers, <a href="#babilu_link-39">86–87</a></p>

<p class="noindent">complexity monster and, <a href="#babilu_link-40">5–6</a></p>

<p class="noindent">defined, <a href="#babilu_link-41">1</a></p>

<p class="noindent">designing, <a href="#babilu_link-42">4–5</a></p>

<p class="noindent">further readings, <a href="#babilu_link-43">298–299</a></p>

<p class="noindent">genetic, <a href="#babilu_link-44">122–128</a></p>

<p class="noindent">overview, <a href="#babilu_link-41">1–6</a></p>

<p class="noindent">structure mapping, <a href="#babilu_link-45">199–200</a></p>

<p class="noindent"><i>See also</i> Machine learning; <i>individual algorithms</i></p>

<p class="noindent">AlphaDog, <a href="#babilu_link-46">21</a></p>

<p class="noindent">Amazon, <a href="#babilu_link-47">198</a> , <a href="#babilu_link-48">266</a> , <a href="#babilu_link-49">291</a></p>

<p class="noindent">A/B testing and, <a href="#babilu_link-21">227</a></p>

<p class="noindent">data gathering, <a href="#babilu_link-50">211</a> , <a href="#babilu_link-51">271</a> , <a href="#babilu_link-20">272</a></p>

<p class="noindent">machine learning and, <a href="#babilu_link-52">11</a> , <a href="#babilu_link-53">12</a></p>

<p class="noindent">Mechanical Turk, <a href="#babilu_link-54">14</a></p>

<p class="noindent">recommendations, <a href="#babilu_link-53">12–13</a> , <a href="#babilu_link-55">42</a> , <a href="#babilu_link-56">184</a> , <a href="#babilu_link-57">268</a> , <a href="#babilu_link-58">286</a></p>

<p class="noindent">Analogical reasoning, <a href="#babilu_link-59">179</a> , <a href="#babilu_link-60">197</a></p>

<p class="noindent">Analogizers, <a href="#babilu_link-61">51</a> , <a href="#babilu_link-62">53</a> , <a href="#babilu_link-63">54</a> , <a href="#babilu_link-64">172–173</a></p>

<p class="noindent">Alchemy and, <a href="#babilu_link-65">253–254</a></p>

<p class="noindent">case-based reasoning, <a href="#babilu_link-60">197–200</a></p>

<p class="noindent">dimensionality, <a href="#babilu_link-66">186–190</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-67">240–241</a></p>

<p class="noindent">nearest-neighbor algorithm, <a href="#babilu_link-68">178–186</a></p>

<p class="noindent">similarity and, <a href="#babilu_link-59">179</a></p>

<p class="noindent">support vector machines, <a href="#babilu_link-62">53</a> , <a href="#babilu_link-69">190–196</a></p>

<p class="noindent">symbolists <i>vs.,</i> <a href="#babilu_link-70">200–202</a></p>

<p class="noindent">Analogy, <a href="#babilu_link-71">175–179</a> , <a href="#babilu_link-60">197–200</a></p>

<p class="noindent">AND gate, <a href="#babilu_link-72">96</a></p>

<p class="noindent">AND operation, <a href="#babilu_link-73">2</a></p>

<p class="noindent"><i><a id="babilu_link-476"><i></i></a> Anna Karenina</i> (Tolstoy), <a href="#babilu_link-74">66</a></p>

<p class="noindent">Apple, <a href="#babilu_link-20">272</a></p>

<p class="noindent">Aristotle, <a href="#babilu_link-75">58</a> , <a href="#babilu_link-76">64</a> , <a href="#babilu_link-77">72</a> , <a href="#babilu_link-68">178</a> , <a href="#babilu_link-26">243</a></p>

<p class="noindent">Artificial intelligence (AI)</p>

<p class="noindent">human control of, <a href="#babilu_link-78">282–284</a></p>

<p class="noindent">knowledge engineers and, <a href="#babilu_link-79">35–36</a></p>

<p class="noindent">machine learning and, <a href="#babilu_link-33">8</a> , <a href="#babilu_link-80">89–90</a></p>

<p class="noindent">ASIC (application-specific integrated circuit) design, <a href="#babilu_link-81">49</a></p>

<p class="noindent">Asimov, Isaac, <a href="#babilu_link-82">232</a> , <a href="#babilu_link-83">280</a></p>

<p class="noindent">Assumptions</p>

<p class="noindent">ill-posed problem and, <a href="#babilu_link-76">64</a></p>

<p class="noindent">of learners, <a href="#babilu_link-84">44</a></p>

<p class="noindent">learning from finite data and, <a href="#babilu_link-85">24–25</a></p>

<p class="noindent">prior, <a href="#babilu_link-86">174</a></p>

<p class="noindent">simplifying to reduce number of probabilities, <a href="#babilu_link-87">150</a></p>

<p class="noindent">symbolists and, <a href="#babilu_link-88">61–62</a></p>

<p class="noindent"><i>Atlantic</i> (magazine), <a href="#babilu_link-89">273–274</a></p>

<p class="noindent">AT&amp;T, <a href="#babilu_link-20">272</a></p>

<p class="noindent">Attribute selection, <a href="#babilu_link-66">186–187</a> , <a href="#babilu_link-90">188–189</a></p>

<p class="noindent">Attribute weights, <a href="#babilu_link-91">189</a></p>

<p class="noindent">Auditory cortex, <a href="#babilu_link-92">26</a></p>

<p class="noindent">Autoencoder, <a href="#babilu_link-93">116–118</a></p>

<p class="noindent">Automation, machine learning and, <a href="#babilu_link-94">10</a></p>

<p class="noindent">Automaton, <a href="#babilu_link-95">123</a></p>

<p class="noindent"><i>The Average American</i> (O’Keefe), <a href="#babilu_link-96">206</a></p>

<p class="noindent">Average member, <a href="#babilu_link-96">206</a></p>

<p class="noindent">Axon, <a href="#babilu_link-28">95</a></p>

<p class="noindent">Babbage, Charles, <a href="#babilu_link-97">28</a></p>

<p class="noindent">Backpropagation (backprop), <a href="#babilu_link-98">52</a> , <a href="#babilu_link-29">104</a> , <a href="#babilu_link-99">107–111</a> , <a href="#babilu_link-100">115</a> , <a href="#babilu_link-101">302</a></p>

<p class="noindent">Alchemy and, <a href="#babilu_link-102">252</a></p>

<p class="noindent">genetic algorithms <i>vs.,</i> <a href="#babilu_link-103">128</a></p>

<p class="noindent">neural networks and, <a href="#babilu_link-104">112–114</a></p>

<p class="noindent">reinforcement learning and, <a href="#babilu_link-105">222</a></p>

<p class="noindent">Bagging, <a href="#babilu_link-106">238</a></p>

<p class="noindent">Baldwin, J. M., <a href="#babilu_link-107">138–139</a></p>

<p class="noindent">Baldwin effect, <a href="#babilu_link-108">139</a> , <a href="#babilu_link-109">140</a> , <a href="#babilu_link-110">304</a></p>

<p class="noindent">Bandit problems, <a href="#babilu_link-111">129–130</a></p>

<p class="noindent">Barto, Andy, <a href="#babilu_link-112">221</a></p>

<p class="noindent">Bayes, Thomas, <a href="#babilu_link-113">144–145</a></p>

<p class="noindent">Bayesian learning, <a href="#babilu_link-114">166–170</a> , <a href="#babilu_link-86">174–175</a></p>

<p class="noindent">Bayesian methods, cell model and, <a href="#babilu_link-115">114</a></p>

<p class="noindent">Bayesian model averaging, <a href="#babilu_link-114">166–167</a></p>

<p class="noindent">Bayesian models, tweaking probabilities, <a href="#babilu_link-116">170–173</a></p>

<p class="noindent">Bayesian networks, <a href="#babilu_link-85">24</a> , <a href="#babilu_link-117">156–161</a> , <a href="#babilu_link-118">305–306</a></p>

<p class="noindent">Alchemy and, <a href="#babilu_link-38">250</a></p>

<p class="noindent">gene regulation and, <a href="#babilu_link-35">159</a></p>

<p class="noindent">inference problem and, <a href="#babilu_link-119">161–166</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-67">240</a> , <a href="#babilu_link-120">245</a></p>

<p class="noindent">relational learning and, <a href="#babilu_link-121">231</a></p>

<p class="noindent">Bayesians, <a href="#babilu_link-61">51</a> , <a href="#babilu_link-98">52–53</a> , <a href="#babilu_link-63">54</a> , <a href="#babilu_link-122">143–175</a></p>

<p class="noindent">Alchemy and, <a href="#babilu_link-65">253</a></p>

<p class="noindent">further reading, <a href="#babilu_link-110">304–305</a></p>

<p class="noindent">hidden Markov model, <a href="#babilu_link-123">154–155</a></p>

<p class="noindent"><i>If</i> … <i>then</i> … rules and, <a href="#babilu_link-124">155–156</a></p>

<p class="noindent">inference problem, <a href="#babilu_link-119">161–166</a></p>

<p class="noindent">learning and, <a href="#babilu_link-114">166–170</a></p>

<p class="noindent">logic and probability and, <a href="#babilu_link-125">173–175</a></p>

<p class="noindent">Markov chain, <a href="#babilu_link-126">153–155</a></p>

<p class="noindent">Markov networks, <a href="#babilu_link-116">170–173</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-67">240–241</a> , <a href="#babilu_link-127">242</a></p>

<p class="noindent">medical diagnosis and, <a href="#babilu_link-128">149–150</a></p>

<p class="noindent">models and, <a href="#babilu_link-128">149–153</a></p>

<p class="noindent">nature and, <a href="#babilu_link-129">141</a></p>

<p class="noindent">probabilistic inference and, <a href="#babilu_link-98">52</a> , <a href="#babilu_link-62">53</a></p>

<p class="noindent"><i>See also</i> Bayesian networks</p>

<p class="noindent">Bayes’ theorem, <a href="#babilu_link-130">31–32</a> , <a href="#babilu_link-98">52–53</a> , <a href="#babilu_link-122">143–149</a> , <a href="#babilu_link-65">253</a></p>

<p class="noindent">Beam search, <a href="#babilu_link-131">135</a></p>

<p class="noindent">“Beer and diapers” rule, <a href="#babilu_link-132">69–70</a></p>

<p class="noindent">Belief, probability and, <a href="#babilu_link-128">149</a></p>

<p class="noindent">Belief propagation, <a href="#babilu_link-119">161–164</a> , <a href="#babilu_link-127">242</a> , <a href="#babilu_link-65">253</a></p>

<p class="noindent">Bell Labs, <a href="#babilu_link-69">190</a></p>

<p class="noindent">Bellman, Richard, <a href="#babilu_link-90">188</a> , <a href="#babilu_link-133">220</a></p>

<p class="noindent">Bellman’s equation, <a href="#babilu_link-133">220</a></p>

<p class="noindent">Berkeley, George, <a href="#babilu_link-75">58</a></p>

<p class="noindent">Berlin, Isaiah, <a href="#babilu_link-134">41</a></p>

<p class="noindent">Bias, <a href="#babilu_link-135">78–79</a></p>

<p class="noindent">Bias-free learning, futility of, <a href="#babilu_link-76">64</a></p>

<p class="noindent">Bias-variance decomposition, <a href="#babilu_link-136">301</a></p>

<p class="noindent"><i>The Bible Code</i> (Drosnin), <a href="#babilu_link-77">72</a></p>

<p class="noindent">Big data, <a href="#babilu_link-46">21</a></p>

<p class="noindent">A/B testing and, <a href="#babilu_link-21">227</a></p>

<p class="noindent">algorithms and, <a href="#babilu_link-137">7</a></p>

<p class="noindent">clustering and, <a href="#babilu_link-96">206–207</a></p>

<p class="noindent"><a id="babilu_link-477"></a> relational learning and, <a href="#babilu_link-82">232–233</a></p>

<p class="noindent">science, machine learning, and, <a href="#babilu_link-54">14–16</a></p>

<p class="noindent">scientific truth and, <a href="#babilu_link-138">40</a></p>

<p class="noindent">Big-data systems, <a href="#babilu_link-139">258</a></p>

<p class="noindent">Bing, <a href="#babilu_link-53">12</a></p>

<p class="noindent">Biology, learning algorithms and, <a href="#babilu_link-140">15</a></p>

<p class="noindent">Black swans, <a href="#babilu_link-141">38–39</a> , <a href="#babilu_link-142">158</a> , <a href="#babilu_link-82">232</a></p>

<p class="noindent"><i>The Black Swan</i> (Taleb), <a href="#babilu_link-141">38</a></p>

<p class="noindent">Blessing of nonuniformity, <a href="#babilu_link-91">189</a></p>

<p class="noindent">Board games, reinforcement learning and, <a href="#babilu_link-143">219</a></p>

<p class="noindent">Bohr, Niels, <a href="#babilu_link-68">178</a> , <a href="#babilu_link-45">199</a></p>

<p class="noindent">Boltzmann distribution, <a href="#babilu_link-27">103–104</a></p>

<p class="noindent">Boltzmann machines, <a href="#babilu_link-27">103–104</a> , <a href="#babilu_link-144">117</a> , <a href="#babilu_link-38">250</a></p>

<p class="noindent">Boole, George, <a href="#babilu_link-29">104</a> , <a href="#babilu_link-71">175</a></p>

<p class="noindent">Boolean circuits, <a href="#babilu_link-95">123</a> , <a href="#babilu_link-145">136</a></p>

<p class="noindent">Boolean variable, <a href="#babilu_link-128">149</a></p>

<p class="noindent">Boosting, <a href="#babilu_link-106">238</a></p>

<p class="noindent">Borges, Jorge Luis, <a href="#babilu_link-146">71</a></p>

<p class="noindent">Box, George, <a href="#babilu_link-147">151</a></p>

<p class="noindent">Brahe, Tycho, <a href="#babilu_link-54">14</a> , <a href="#babilu_link-148">131</a></p>

<p class="noindent">Brahe phase of science, <a href="#babilu_link-149">39–40</a></p>

<p class="noindent">Brain</p>

<p class="noindent">learning algorithms and, <a href="#babilu_link-92">26–28</a></p>

<p class="noindent">mapping, <a href="#babilu_link-150">118</a></p>

<p class="noindent">number of connections in, <a href="#babilu_link-151">94–95</a></p>

<p class="noindent">reverse engineering the, <a href="#babilu_link-98">52</a> , <a href="#babilu_link-101">302</a></p>

<p class="noindent">S curves and, <a href="#babilu_link-152">105</a></p>

<p class="noindent">simulating with computer, <a href="#babilu_link-28">95</a></p>

<p class="noindent">spin glasses and, <a href="#babilu_link-153">102–103</a></p>

<p class="noindent">BRAIN initiative, <a href="#babilu_link-150">118</a></p>

<p class="noindent">Breiman, Leo, <a href="#babilu_link-106">238</a></p>

<p class="noindent">Brin, Sergey, <a href="#babilu_link-154">55</a> , <a href="#babilu_link-21">227</a> , <a href="#babilu_link-155">274</a></p>

<p class="noindent">Bryson, Arthur, <a href="#babilu_link-156">113</a></p>

<p class="noindent">Bucket brigade algorithm, <a href="#babilu_link-157">127</a></p>

<p class="noindent">Building blocks, <a href="#babilu_link-103">128–129</a> , <a href="#babilu_link-158">134</a></p>

<p class="noindent">Buntine, Wray, <a href="#babilu_link-159">80</a></p>

<p class="noindent">Burglar alarms, Bayesian networks and, <a href="#babilu_link-160">157–158</a></p>

<p class="noindent">Burks, Arthur, <a href="#babilu_link-95">123</a></p>

<p class="noindent">Burns, Bob, <a href="#babilu_link-96">206</a></p>

<p class="noindent">Business, machine learning and, <a href="#babilu_link-94">10–13</a></p>

<p class="noindent"><i>C. elegans</i> , <a href="#babilu_link-150">118</a></p>

<p class="noindent">Cajal, Santiago Ramón y, <a href="#babilu_link-161">93–94</a></p>

<p class="noindent">Caltech, <a href="#babilu_link-116">170</a></p>

<p class="noindent">CancerCommons.org, <a href="#babilu_link-162">261</a></p>

<p class="noindent">Cancer cure</p>

<p class="noindent">algorithm for, <a href="#babilu_link-62">53–54</a></p>

<p class="noindent">Bayesian learning and, <a href="#babilu_link-86">174</a></p>

<p class="noindent">inverse deduction and, <a href="#babilu_link-163">83–85</a></p>

<p class="noindent">Markov logic network and, <a href="#babilu_link-164">249</a></p>

<p class="noindent">program for (CanceRx), <a href="#babilu_link-165">259–261</a> , <a href="#babilu_link-166">310</a></p>

<p class="noindent">Cancer diagnosis, <a href="#babilu_link-129">141</a></p>

<p class="noindent">Cancer drugs</p>

<p class="noindent">predicting efficacy of, <a href="#babilu_link-163">83–84</a></p>

<p class="noindent">relational learning and models for, <a href="#babilu_link-167">233</a></p>

<p class="noindent">selection of, <a href="#babilu_link-134">41–42</a></p>

<p class="noindent">CanceRx, <a href="#babilu_link-165">259–261</a> , <a href="#babilu_link-166">310</a></p>

<p class="noindent">Capital One, <a href="#babilu_link-20">272</a></p>

<p class="noindent">Carbonell, Jaime, <a href="#babilu_link-132">69</a></p>

<p class="noindent">Carnap, Rudolf, <a href="#babilu_link-71">175</a></p>

<p class="noindent">Cars</p>

<p class="noindent">driverless, <a href="#babilu_link-156">113</a> , <a href="#babilu_link-114">166</a> , <a href="#babilu_link-64">172</a> , <a href="#babilu_link-19">306</a></p>

<p class="noindent">learning to drive, <a href="#babilu_link-156">113</a></p>

<p class="noindent">Case-based reasoning, <a href="#babilu_link-47">198</a> , <a href="#babilu_link-168">307</a></p>

<p class="noindent"><i>Catch Me If You Can</i> (film), <a href="#babilu_link-18">177</a></p>

<p class="noindent">Cause and effect, Bayes’ theorem and, <a href="#babilu_link-169">145–149</a></p>

<p class="noindent">Cell</p>

<p class="noindent">model of, <a href="#babilu_link-115">114–115</a></p>

<p class="noindent">relational learning and workings of, <a href="#babilu_link-167">233</a></p>

<p class="noindent">Cell assembly, <a href="#babilu_link-151">94</a></p>

<p class="noindent">Cell phone, hidden Markov models and, <a href="#babilu_link-124">155</a></p>

<p class="noindent">Centaurs, <a href="#babilu_link-170">277</a></p>

<p class="noindent">Central Dogma, <a href="#babilu_link-163">83</a></p>

<p class="noindent">Cerebellum, <a href="#babilu_link-171">27</a> , <a href="#babilu_link-150">118</a></p>

<p class="noindent">Chance, Bayes and, <a href="#babilu_link-169">145</a></p>

<p class="noindent">Chaos, study of, <a href="#babilu_link-172">30</a></p>

<p class="noindent">Checkers-playing program, <a href="#babilu_link-143">219</a></p>

<p class="noindent">Cholera outbreak, London’s, <a href="#babilu_link-173">182–183</a></p>

<p class="noindent">Chomsky, Noam, <a href="#babilu_link-174">36–38</a></p>

<p class="noindent">Chrome, <a href="#babilu_link-48">266</a></p>

<p class="noindent">Chunking, <a href="#babilu_link-175">223–227</a> , <a href="#babilu_link-176">254</a> , <a href="#babilu_link-22">309</a></p>

<p class="noindent">Circuit design, genetic programming and, <a href="#babilu_link-131">135–136</a></p>

<p class="noindent">Classes, <a href="#babilu_link-39">86–87</a> , <a href="#babilu_link-177">209</a> , <a href="#babilu_link-178">257</a></p>

<p class="noindent"><a id="babilu_link-478"></a> Classifiers, <a href="#babilu_link-39">86–87</a> , <a href="#babilu_link-157">127</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-67">240</a></p>

<p class="noindent">Naïve Bayes, <a href="#babilu_link-147">151–153</a></p>

<p class="noindent">nearest-neighbor algorithm and, <a href="#babilu_link-179">183</a></p>

<p class="noindent">Clinton, Bill, <a href="#babilu_link-180">18</a></p>

<p class="noindent">Clustering, <a href="#babilu_link-181">205–210</a> , <a href="#babilu_link-176">254</a> , <a href="#babilu_link-178">257</a></p>

<p class="noindent">hierarchical, <a href="#babilu_link-182">210</a></p>

<p class="noindent">Cluster prototypes, <a href="#babilu_link-183">207–208</a></p>

<p class="noindent">Clusters, <a href="#babilu_link-181">205–210</a></p>

<p class="noindent">“Cocktail party” problem, <a href="#babilu_link-184">215</a></p>

<p class="noindent">Cognition, theory of, <a href="#babilu_link-185">226</a></p>

<p class="noindent">Coin toss, <a href="#babilu_link-186">63</a> , <a href="#babilu_link-187">130</a> , <a href="#babilu_link-188">167–168</a></p>

<p class="noindent">Collaborative filtering systems, <a href="#babilu_link-179">183–184</a> , <a href="#babilu_link-19">306–307</a></p>

<p class="noindent">Columbus test, <a href="#babilu_link-156">113</a></p>

<p class="noindent">Combinatorial explosion, <a href="#babilu_link-189">73–74</a></p>

<p class="noindent">Commoner, Barry, <a href="#babilu_link-142">158</a></p>

<p class="noindent">Commonsense reasoning, <a href="#babilu_link-79">35</a> , <a href="#babilu_link-150">118–119</a> , <a href="#babilu_link-169">145</a> , <a href="#babilu_link-190">276–277</a> , <a href="#babilu_link-191">300</a></p>

<p class="noindent">Complexity monster, <a href="#babilu_link-40">5–6</a> , <a href="#babilu_link-137">7</a> , <a href="#babilu_link-192">43</a> , <a href="#babilu_link-36">246</a></p>

<p class="noindent">Compositionality, <a href="#babilu_link-193">119</a></p>

<p class="noindent">Computational biologists, use of hidden Markov models, <a href="#babilu_link-124">155</a></p>

<p class="noindent">Computers</p>

<p class="noindent">decision making and, <a href="#babilu_link-78">282–286</a></p>

<p class="noindent">evolution of, <a href="#babilu_link-58">286–289</a></p>

<p class="noindent">human interaction with, <a href="#babilu_link-194">264–267</a></p>

<p class="noindent">as learners, <a href="#babilu_link-195">45</a></p>

<p class="noindent">logic and, <a href="#babilu_link-73">2</a></p>

<p class="noindent">S curves and, <a href="#babilu_link-152">105</a></p>

<p class="noindent">as sign of Master Algorithm, <a href="#babilu_link-196">34</a></p>

<p class="noindent">simulating brain using, <a href="#babilu_link-28">95</a></p>

<p class="noindent">as unifier, <a href="#babilu_link-197">236</a></p>

<p class="noindent">writing own programs, <a href="#babilu_link-198">6</a></p>

<p class="noindent">Computer science, Master Algorithm and, <a href="#babilu_link-199">32–34</a></p>

<p class="noindent">Computer vision, Markov networks and, <a href="#babilu_link-64">172</a></p>

<p class="noindent">Concepts, <a href="#babilu_link-200">67</a></p>

<p class="noindent">conjunctive, <a href="#babilu_link-74">66–68</a></p>

<p class="noindent">set of rules and, <a href="#babilu_link-201">68–69</a></p>

<p class="noindent">sets of, <a href="#babilu_link-39">86–87</a></p>

<p class="noindent">Conceptual model, <a href="#babilu_link-84">44</a> , <a href="#babilu_link-202">152</a></p>

<p class="noindent">Conditional independence, <a href="#babilu_link-160">157–158</a></p>

<p class="noindent">Conditional probabilities, <a href="#babilu_link-120">245</a></p>

<p class="noindent">Conditional random fields, <a href="#babilu_link-64">172</a> , <a href="#babilu_link-19">306</a></p>

<p class="noindent">Conference on Neural Information Processing Systems (NIPS), <a href="#babilu_link-116">170</a> , <a href="#babilu_link-64">172</a></p>

<p class="noindent">Conjunctive concepts, <a href="#babilu_link-203">65–68</a> , <a href="#babilu_link-204">74</a></p>

<p class="noindent">Connectionists/connectionism, <a href="#babilu_link-61">51</a> , <a href="#babilu_link-98">52</a> , <a href="#babilu_link-63">54</a> , <a href="#babilu_link-161">93–119</a></p>

<p class="noindent">Alchemy and, <a href="#babilu_link-102">252</a></p>

<p class="noindent">autoencoder and, <a href="#babilu_link-93">116–118</a></p>

<p class="noindent">backpropagation and, <a href="#babilu_link-98">52</a> , <a href="#babilu_link-99">107–111</a></p>

<p class="noindent">Boltzmann machine and, <a href="#babilu_link-27">103–104</a></p>

<p class="noindent">cell model, <a href="#babilu_link-115">114–115</a></p>

<p class="noindent">connectomics, <a href="#babilu_link-150">118–119</a></p>

<p class="noindent">deep learning and, <a href="#babilu_link-100">115</a></p>

<p class="noindent">further reading, <a href="#babilu_link-101">302–303</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-67">240–241</a></p>

<p class="noindent">nature and, <a href="#babilu_link-205">137–142</a></p>

<p class="noindent">neural networks and, <a href="#babilu_link-104">112–114</a></p>

<p class="noindent">perceptron, <a href="#babilu_link-72">96–101</a> , <a href="#babilu_link-99">107–108</a></p>

<p class="noindent">S curves and, <a href="#babilu_link-29">104–107</a></p>

<p class="noindent">spin glasses and, <a href="#babilu_link-153">102–103</a></p>

<p class="noindent">symbolist learning <i>vs.,</i> <a href="#babilu_link-206">91</a> , <a href="#babilu_link-151">94–95</a></p>

<p class="noindent">Connectomics, <a href="#babilu_link-150">118–119</a></p>

<p class="noindent">Consciousness, <a href="#babilu_link-72">96</a></p>

<p class="noindent"><i>Consilience</i> (Wilson), <a href="#babilu_link-130">31</a></p>

<p class="noindent">Constrained optimization, <a href="#babilu_link-207">193–195</a> , <a href="#babilu_link-25">241</a> , <a href="#babilu_link-127">242</a></p>

<p class="noindent">Constraints, support vector machines and, <a href="#babilu_link-207">193–195</a></p>

<p class="noindent">Convolutional neural networks, <a href="#babilu_link-144">117–119</a> , <a href="#babilu_link-208">303</a></p>

<p class="noindent">Cope, David, <a href="#babilu_link-45">199</a> , <a href="#babilu_link-168">307</a></p>

<p class="noindent">Cornell University, Creative Machines Lab, <a href="#babilu_link-209">121–122</a></p>

<p class="noindent">Cortex, <a href="#babilu_link-150">118</a> , <a href="#babilu_link-107">138</a></p>

<p class="noindent">unity of, <a href="#babilu_link-92">26–28</a> , <a href="#babilu_link-32">299–300</a></p>

<p class="noindent">Counterexamples, <a href="#babilu_link-200">67</a></p>

<p class="noindent">Cover, Tom, <a href="#babilu_link-210">185</a></p>

<p class="noindent">Crawlers, <a href="#babilu_link-33">8–9</a></p>

<p class="noindent">Creative Machines Lab, <a href="#babilu_link-209">121–122</a></p>

<p class="noindent">Credit-assignment problem, <a href="#babilu_link-211">101</a> , <a href="#babilu_link-153">102</a> , <a href="#babilu_link-29">104</a> , <a href="#babilu_link-99">107</a> , <a href="#babilu_link-157">127</a></p>

<p class="noindent">Crick, Francis, <a href="#babilu_link-44">122</a> , <a href="#babilu_link-197">236</a></p>

<p class="noindent">Crossover, <a href="#babilu_link-212">124–125</a> , <a href="#babilu_link-158">134–136</a> , <a href="#babilu_link-25">241</a> , <a href="#babilu_link-26">243</a></p>

<p class="noindent"><a id="babilu_link-479"></a> Curse of dimensionality, <a href="#babilu_link-66">186–190</a> , <a href="#babilu_link-213">196</a> , <a href="#babilu_link-214">201</a> , <a href="#babilu_link-168">307</a></p>

<p class="noindent">Cyber Command, <a href="#babilu_link-215">19</a></p>

<p class="noindent">Cyberwar, <a href="#babilu_link-215">19–21</a> , <a href="#babilu_link-216">279–282</a> , <a href="#babilu_link-32">299</a> , <a href="#babilu_link-166">310</a></p>

<p class="noindent">Cyc project, <a href="#babilu_link-79">35</a> , <a href="#babilu_link-191">300</a></p>

<p class="noindent">DARPA, <a href="#babilu_link-46">21</a> , <a href="#babilu_link-217">37</a> , <a href="#babilu_link-156">113</a> , <a href="#babilu_link-209">121</a> , <a href="#babilu_link-37">255</a></p>

<p class="noindent">Darwin, Charles, <a href="#babilu_link-97">28</a> , <a href="#babilu_link-172">30</a> , <a href="#babilu_link-148">131</a> , <a href="#babilu_link-218">235</a></p>

<p class="noindent">algorithm, <a href="#babilu_link-44">122–128</a></p>

<p class="noindent">analogy and, <a href="#babilu_link-68">178</a></p>

<p class="noindent">Hume and, <a href="#babilu_link-75">58</a></p>

<p class="noindent">on lack of mathematical ability, <a href="#babilu_link-157">127</a></p>

<p class="noindent">on selective breeding, <a href="#babilu_link-95">123–124</a></p>

<p class="noindent">variation and, <a href="#babilu_link-212">124</a></p>

<p class="noindent">Data</p>

<p class="noindent">accuracy of held-out, <a href="#babilu_link-23">75–76</a></p>

<p class="noindent">Bayes’ theorem and, <a href="#babilu_link-130">31–32</a></p>

<p class="noindent">control of, <a href="#babilu_link-195">45</a></p>

<p class="noindent">first principal component of the, <a href="#babilu_link-219">214</a></p>

<p class="noindent">human intuition and, <a href="#babilu_link-149">39</a></p>

<p class="noindent">learning from finite, <a href="#babilu_link-85">24–25</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-220">25–26</a></p>

<p class="noindent">patterns in, <a href="#babilu_link-221">70–75</a></p>

<p class="noindent">sciences and complex, <a href="#babilu_link-54">14</a></p>

<p class="noindent">as strategic asset for business, <a href="#babilu_link-222">13</a></p>

<p class="noindent">theory and, <a href="#babilu_link-223">46</a></p>

<p class="noindent"><i>See also</i> Big data; Overfitting; Personal data</p>

<p class="noindent">Database engine, <a href="#babilu_link-81">49–50</a></p>

<p class="noindent">Databases, <a href="#babilu_link-33">8</a> , <a href="#babilu_link-224">9</a></p>

<p class="noindent">Data mining, <a href="#babilu_link-33">8</a> , <a href="#babilu_link-189">73</a> , <a href="#babilu_link-82">232–233</a> , <a href="#babilu_link-43">298</a> , <a href="#babilu_link-19">306</a> . <i>See also</i> Machine learning</p>

<p class="noindent">Data science, <a href="#babilu_link-33">8</a> . <i>See also</i> Machine learning</p>

<p class="noindent">Data scientist, <a href="#babilu_link-224">9</a></p>

<p class="noindent">Data sharing, <a href="#babilu_link-225">270–276</a></p>

<p class="noindent">Data unions, <a href="#babilu_link-155">274–275</a></p>

<p class="noindent">Dawkins, Richard, <a href="#babilu_link-226">284</a></p>

<p class="noindent">Decision making, artificial intelligence and, <a href="#babilu_link-78">282–286</a></p>

<p class="noindent">Decision theory, <a href="#babilu_link-227">165</a></p>

<p class="noindent">Decision tree induction, <a href="#babilu_link-228">85–89</a></p>

<p class="noindent">Decision tree learners, <a href="#babilu_link-85">24</a> , <a href="#babilu_link-136">301</a></p>

<p class="noindent">Decision trees, <a href="#babilu_link-85">24</a> , <a href="#babilu_link-228">85–90</a> , <a href="#babilu_link-229">181–182</a> , <a href="#babilu_link-90">188</a> , <a href="#babilu_link-230">237–238</a></p>

<p class="noindent">Deduction</p>

<p class="noindent">induction as inverse of, <a href="#babilu_link-159">80–83</a> , <a href="#babilu_link-136">301</a></p>

<p class="noindent">Turing machine and, <a href="#babilu_link-196">34</a></p>

<p class="noindent">Deductive reasoning, <a href="#babilu_link-159">80–81</a></p>

<p class="noindent">Deep learning, <a href="#babilu_link-29">104</a> , <a href="#babilu_link-100">115–118</a> , <a href="#babilu_link-64">172</a> , <a href="#babilu_link-231">195</a> , <a href="#babilu_link-25">241</a> , <a href="#babilu_link-101">302</a></p>

<p class="noindent">DeepMind, <a href="#babilu_link-105">222</a></p>

<p class="noindent">Democracy, machine learning and, <a href="#babilu_link-180">18–19</a></p>

<p class="noindent">Dempster, Arthur, <a href="#babilu_link-177">209</a></p>

<p class="noindent">Dendrites, <a href="#babilu_link-28">95</a></p>

<p class="noindent">Descartes, René, <a href="#babilu_link-75">58</a> , <a href="#babilu_link-76">64</a></p>

<p class="noindent">Descriptive theories, normative theories <i>vs.,</i> <a href="#babilu_link-129">141–142</a> , <a href="#babilu_link-110">304</a></p>

<p class="noindent">Determinism, Laplace and, <a href="#babilu_link-169">145</a></p>

<p class="noindent">Developmental psychology, <a href="#babilu_link-232">203–204</a> , <a href="#babilu_link-233">308</a></p>

<p class="noindent">DiCaprio, Leonardo, <a href="#babilu_link-18">177</a></p>

<p class="noindent">Diderot, Denis, <a href="#babilu_link-186">63</a></p>

<p class="noindent">Diffusion equation, <a href="#babilu_link-172">30</a></p>

<p class="noindent">Dimensionality, curse of, <a href="#babilu_link-66">186–190</a> , <a href="#babilu_link-168">307</a></p>

<p class="noindent">Dimensionality reduction, <a href="#babilu_link-91">189–190</a> , <a href="#babilu_link-50">211–215</a> , <a href="#babilu_link-37">255</a></p>

<p class="noindent">nonlinear, <a href="#babilu_link-184">215–217</a></p>

<p class="noindent"><i>Dirty Harry</i> (film), <a href="#babilu_link-203">65</a></p>

<p class="noindent">Disney animators, S curves and, <a href="#babilu_link-234">106</a></p>

<p class="noindent">“Divide and conquer” algorithm, <a href="#babilu_link-235">77–78</a> , <a href="#babilu_link-159">80</a> , <a href="#babilu_link-236">81</a> , <a href="#babilu_link-24">87</a></p>

<p class="noindent">DNA sequencers, <a href="#babilu_link-31">84</a></p>

<p class="noindent">Downweighting attributes, <a href="#babilu_link-91">189</a></p>

<p class="noindent">Driverless cars, <a href="#babilu_link-33">8</a> , <a href="#babilu_link-156">113</a> , <a href="#babilu_link-114">166</a> , <a href="#babilu_link-64">172</a> , <a href="#babilu_link-19">306</a></p>

<p class="noindent">Drones, <a href="#babilu_link-46">21</a> , <a href="#babilu_link-237">281</a></p>

<p class="noindent">Drugs, <a href="#babilu_link-140">15</a> , <a href="#babilu_link-134">41–42</a> , <a href="#babilu_link-163">83</a> . <i>See also</i> Cancer drugs</p>

<p class="noindent">Duhigg, Charles, <a href="#babilu_link-175">223</a></p>

<p class="noindent">Dynamic programming, <a href="#babilu_link-133">220</a></p>

<p class="noindent">Eastwood, Clint, <a href="#babilu_link-203">65</a></p>

<p class="noindent">Echolocation, <a href="#babilu_link-92">26</a> , <a href="#babilu_link-32">299</a></p>

<p class="noindent">Eddington, Arthur, <a href="#babilu_link-23">75</a></p>

<p class="noindent">Effect, law of, <a href="#babilu_link-238">218</a></p>

<p class="noindent">eHarmony, <a href="#babilu_link-239">265</a></p>

<p class="noindent">Eigenfaces, <a href="#babilu_link-184">215</a></p>

<p class="noindent">80/20 rule, <a href="#babilu_link-192">43</a></p>

<p class="noindent">Einstein, Albert, <a href="#babilu_link-23">75</a> , <a href="#babilu_link-70">200</a></p>

<p class="noindent"><a id="babilu_link-480"></a> Eldredge, Niles, <a href="#babilu_link-157">127</a></p>

<p class="noindent">Electronic circuits, genetic programming and, <a href="#babilu_link-240">133–134</a></p>

<p class="noindent">Eliza (help desk), <a href="#babilu_link-47">198</a></p>

<p class="noindent">EM (expectation maximization) algorithm, <a href="#babilu_link-177">209–210</a></p>

<p class="noindent">Emotions, learning and, <a href="#babilu_link-238">218</a></p>

<p class="noindent">Empathy-eliciting robots, <a href="#babilu_link-241">285</a></p>

<p class="noindent">Empiricists, <a href="#babilu_link-242">57–58</a></p>

<p class="noindent">Employment, effect of machine learning on, <a href="#babilu_link-190">276–279</a></p>

<p class="noindent">Enlightenment, rationalism <i>vs.</i> empiricism, <a href="#babilu_link-75">58</a></p>

<p class="noindent">Entropy, <a href="#babilu_link-24">87</a></p>

<p class="noindent">Epinions, <a href="#babilu_link-121">231</a></p>

<p class="noindent">Equations, <a href="#babilu_link-42">4</a> , <a href="#babilu_link-243">50</a></p>

<p class="noindent"><i>Essay on Population</i> (Malthus), <a href="#babilu_link-68">178</a> , <a href="#babilu_link-218">235</a></p>

<p class="noindent">Ethics, robot armies and, <a href="#babilu_link-83">280–281</a></p>

<p class="noindent"><i>Eugene Onegin</i> (Pushkin), <a href="#babilu_link-126">153–154</a></p>

<p class="noindent">“Explaining away” phenomenon, <a href="#babilu_link-244">163</a></p>

<p class="noindent">Evaluation</p>

<p class="noindent">learning algorithms and, <a href="#babilu_link-245">283</a></p>

<p class="noindent">Markov logic networks and, <a href="#babilu_link-164">249</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-246">239</a> , <a href="#babilu_link-25">241</a> , <a href="#babilu_link-26">243</a></p>

<p class="noindent">Evolution, <a href="#babilu_link-97">28–29</a> , <a href="#babilu_link-209">121–142</a></p>

<p class="noindent">Baldwinian, <a href="#babilu_link-108">139</a></p>

<p class="noindent">Darwin’s algorithm, <a href="#babilu_link-44">122–128</a></p>

<p class="noindent">human-directed, <a href="#babilu_link-58">286–289</a> , <a href="#babilu_link-247">311</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-97">28–29</a></p>

<p class="noindent">of robots, <a href="#babilu_link-209">121–122</a> , <a href="#babilu_link-205">137</a> , <a href="#babilu_link-208">303</a></p>

<p class="noindent">role of sex in, <a href="#babilu_link-158">134–137</a></p>

<p class="noindent">technological, <a href="#babilu_link-145">136–137</a></p>

<p class="noindent"><i>See also</i> Genetic algorithms</p>

<p class="noindent">Evolutionaries, <a href="#babilu_link-61">51</a> , <a href="#babilu_link-98">52</a> , <a href="#babilu_link-63">54</a></p>

<p class="noindent">Alchemy and, <a href="#babilu_link-102">252–253</a></p>

<p class="noindent">exploration-exploitation dilemma, <a href="#babilu_link-103">128–130</a> , <a href="#babilu_link-112">221</a></p>

<p class="noindent">further reading, <a href="#babilu_link-208">303–304</a></p>

<p class="noindent">genetic programming and, <a href="#babilu_link-98">52</a></p>

<p class="noindent">Holland and, <a href="#babilu_link-157">127</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-67">240–241</a></p>

<p class="noindent">nature and, <a href="#babilu_link-205">137–139</a></p>

<p class="noindent">Evolutionary computation, <a href="#babilu_link-209">121–142</a></p>

<p class="noindent">Evolutionary robotics, <a href="#babilu_link-209">121–122</a> , <a href="#babilu_link-208">303</a></p>

<p class="noindent">Exclusive-OR function (XOR), <a href="#babilu_link-248">100–101</a> , <a href="#babilu_link-104">112</a> , <a href="#babilu_link-231">195</a></p>

<p class="noindent">Exploration-exploitation dilemma, <a href="#babilu_link-103">128–130</a> , <a href="#babilu_link-112">221</a></p>

<p class="noindent">Exponential function, machine learning and, <a href="#babilu_link-189">73–74</a></p>

<p class="noindent"><i>The Extended Phenotype</i> (Dawkins), <a href="#babilu_link-226">284</a></p>

<p class="noindent">Facebook, <a href="#babilu_link-84">44</a> , <a href="#babilu_link-49">291</a></p>

<p class="noindent">data and, <a href="#babilu_link-54">14</a> , <a href="#babilu_link-155">274</a></p>

<p class="noindent">facial recognition technology, <a href="#babilu_link-59">179–180</a></p>

<p class="noindent">machine learning and, <a href="#babilu_link-52">11</a></p>

<p class="noindent">relational learning and, <a href="#babilu_link-249">230</a></p>

<p class="noindent">sharing via, <a href="#babilu_link-51">271–272</a></p>

<p class="noindent">Facial identification, <a href="#babilu_link-59">179–180</a> , <a href="#babilu_link-173">182</a></p>

<p class="noindent">False discovery rate, <a href="#babilu_link-235">77</a> , <a href="#babilu_link-136">301</a></p>

<p class="noindent">Farming, as analogy for machine learning, <a href="#babilu_link-198">6–7</a></p>

<p class="noindent">Feature selection, <a href="#babilu_link-90">188–189</a></p>

<p class="noindent">Feature template, <a href="#babilu_link-250">248</a></p>

<p class="noindent">Feature weighting, <a href="#babilu_link-91">189</a></p>

<p class="noindent">Ferret brain rewiring, <a href="#babilu_link-92">26</a> , <a href="#babilu_link-32">299</a></p>

<p class="noindent">Feynman, Richard, <a href="#babilu_link-42">4</a></p>

<p class="noindent">Filter bubble, <a href="#babilu_link-225">270</a></p>

<p class="noindent">Filtering spam, rule for, <a href="#babilu_link-251">125–127</a></p>

<p class="noindent">First principal component of the data, <a href="#babilu_link-219">214</a></p>

<p class="noindent">Fisher, Ronald, <a href="#babilu_link-44">122</a></p>

<p class="noindent">Fitness</p>

<p class="noindent">Fisher on, <a href="#babilu_link-44">122</a></p>

<p class="noindent">in genetic programming, <a href="#babilu_link-252">132</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-26">243</a></p>

<p class="noindent">neural learning and, <a href="#babilu_link-107">138–139</a></p>

<p class="noindent">sex and, <a href="#babilu_link-131">135</a></p>

<p class="noindent">Fitness function, <a href="#babilu_link-95">123–124</a></p>

<p class="noindent">Fitness maximum, genetic algorithms and, <a href="#babilu_link-157">127–128</a> , <a href="#babilu_link-111">129</a></p>

<p class="noindent">Fix, Evelyn, <a href="#babilu_link-68">178–179</a> , <a href="#babilu_link-66">186</a></p>

<p class="noindent">Fodor, Jerry, <a href="#babilu_link-141">38</a></p>

<p class="noindent">Forecasting, S curves and, <a href="#babilu_link-234">106</a></p>

<p class="noindent">Foundation Medicine, <a href="#babilu_link-134">41</a> , <a href="#babilu_link-162">261</a></p>

<p class="noindent"><i>Foundation</i> (Asimov), <a href="#babilu_link-82">232</a></p>

<p class="noindent">Fractal geometry, <a href="#babilu_link-172">30</a> , <a href="#babilu_link-191">300</a></p>

<p class="noindent"><i>Freakonomics</i> (Dubner &amp; Levitt), <a href="#babilu_link-253">275</a></p>

<p class="noindent"><a id="babilu_link-481"></a> Frequentist interpretation of probability, <a href="#babilu_link-128">149</a></p>

<p class="noindent">Freund, Yoav, <a href="#babilu_link-106">238</a></p>

<p class="noindent">Friedman, Milton, <a href="#babilu_link-147">151</a></p>

<p class="noindent">Frontiers, <a href="#babilu_link-210">185</a> , <a href="#babilu_link-254">187</a> , <a href="#babilu_link-255">191</a> , <a href="#babilu_link-213">196</a></p>

<p class="noindent">“Funes the Memorious” (Borges), <a href="#babilu_link-146">71</a></p>

<p class="noindent">Futility of bias-free learning, <a href="#babilu_link-76">64</a></p>

<p class="noindent">FuturICT project, <a href="#babilu_link-139">258</a></p>

<p class="noindent">Galileo, <a href="#babilu_link-54">14</a> , <a href="#babilu_link-77">72</a></p>

<p class="noindent">Galois, Évariste, <a href="#babilu_link-70">200</a></p>

<p class="noindent">Game theory, machine learning and, <a href="#babilu_link-256">20</a></p>

<p class="noindent">Gaming, reinforcement learning and, <a href="#babilu_link-105">222</a></p>

<p class="noindent">Gates, Bill, <a href="#babilu_link-257">22</a> , <a href="#babilu_link-154">55</a> , <a href="#babilu_link-202">152</a></p>

<p class="noindent">GECCO (Genetic and Evolutionary Computing Conference), <a href="#babilu_link-145">136</a></p>

<p class="noindent">Gene expression microarrays, <a href="#babilu_link-31">84–85</a></p>

<p class="noindent">Generalizations, choosing, <a href="#babilu_link-258">60</a> , <a href="#babilu_link-88">61</a></p>

<p class="noindent">Generative model, Bayesian network as, <a href="#babilu_link-35">159</a></p>

<p class="noindent">Gene regulation, Bayesian networks and, <a href="#babilu_link-35">159</a></p>

<p class="noindent">Genetic algorithms, <a href="#babilu_link-44">122–128</a></p>

<p class="noindent">Alchemy and, <a href="#babilu_link-102">252</a></p>

<p class="noindent">backpropagation <i>vs.,</i> <a href="#babilu_link-103">128</a></p>

<p class="noindent">building blocks and, <a href="#babilu_link-103">128–129</a> , <a href="#babilu_link-158">134</a></p>

<p class="noindent">schemas, <a href="#babilu_link-111">129</a></p>

<p class="noindent">survival of the fittest programs, <a href="#babilu_link-148">131–134</a></p>

<p class="noindent"><i>The Genetical Theory of Natural Selection</i> (Fisher), <a href="#babilu_link-44">122</a></p>

<p class="noindent">Genetic programming, <a href="#babilu_link-98">52</a> , <a href="#babilu_link-148">131–133</a> , <a href="#babilu_link-67">240</a> , <a href="#babilu_link-259">244</a> , <a href="#babilu_link-120">245</a> , <a href="#babilu_link-102">252</a> , <a href="#babilu_link-208">303–304</a></p>

<p class="noindent">sex and, <a href="#babilu_link-158">134–137</a></p>

<p class="noindent"><i>Genetic Programming</i> (Koza), <a href="#babilu_link-145">136</a></p>

<p class="noindent">Genetic search, <a href="#babilu_link-25">241</a> , <a href="#babilu_link-26">243</a> , <a href="#babilu_link-164">249</a></p>

<p class="noindent">Genome, poverty of, <a href="#babilu_link-171">27</a></p>

<p class="noindent">Gentner, Dedre, <a href="#babilu_link-45">199</a></p>

<p class="noindent">Ghani, Rayid, <a href="#babilu_link-260">17</a></p>

<p class="noindent"><i>The Ghost Map</i> (Johnson), <a href="#babilu_link-173">182–183</a></p>

<p class="noindent">Gibson, William, <a href="#babilu_link-261">289</a></p>

<p class="noindent">Gift economy, <a href="#babilu_link-216">279</a></p>

<p class="noindent">Gleevec, <a href="#babilu_link-31">84</a></p>

<p class="noindent">Global Alliance for Genomics and Health, <a href="#babilu_link-162">261</a></p>

<p class="noindent"><i>Gödel, Escher, Bach</i> (Hofstadter), <a href="#babilu_link-70">200</a></p>

<p class="noindent">Good, I. J., <a href="#babilu_link-58">286</a></p>

<p class="noindent">Google, <a href="#babilu_link-224">9</a> , <a href="#babilu_link-84">44</a> , <a href="#babilu_link-49">291</a></p>

<p class="noindent">A/B testing and, <a href="#babilu_link-21">227</a></p>

<p class="noindent">AdSense system, <a href="#babilu_link-34">160</a></p>

<p class="noindent">communication with learner, <a href="#babilu_link-48">266–267</a></p>

<p class="noindent">data gathering, <a href="#babilu_link-20">272</a></p>

<p class="noindent">DeepMind and, <a href="#babilu_link-105">222</a></p>

<p class="noindent">knowledge graph, <a href="#babilu_link-37">255</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-78">282</a></p>

<p class="noindent">Naïve Bayes and, <a href="#babilu_link-202">152</a></p>

<p class="noindent">PageRank and, <a href="#babilu_link-123">154</a> , <a href="#babilu_link-118">305</a></p>

<p class="noindent">problem of induction and, <a href="#babilu_link-88">61</a></p>

<p class="noindent">relational learning and, <a href="#babilu_link-21">227–228</a></p>

<p class="noindent">search results, <a href="#babilu_link-222">13</a></p>

<p class="noindent">value of data, <a href="#babilu_link-155">274</a></p>

<p class="noindent">value of learning algorithms, <a href="#babilu_link-94">10</a> , <a href="#babilu_link-53">12</a></p>

<p class="noindent">Google Brain network, <a href="#babilu_link-144">117</a></p>

<p class="noindent">Google Translate, <a href="#babilu_link-123">154</a> , <a href="#babilu_link-110">304</a></p>

<p class="noindent">Gould, Stephen Jay, <a href="#babilu_link-157">127</a></p>

<p class="noindent">GPS, <a href="#babilu_link-262">212–214</a> , <a href="#babilu_link-263">216</a> , <a href="#babilu_link-170">277</a></p>

<p class="noindent">Gradient descent, <a href="#babilu_link-264">109–110</a> , <a href="#babilu_link-265">171</a> , <a href="#babilu_link-91">189</a> , <a href="#babilu_link-207">193</a> , <a href="#babilu_link-25">241</a> , <a href="#babilu_link-26">243</a> , <a href="#babilu_link-164">249</a> , <a href="#babilu_link-102">252</a> , <a href="#babilu_link-178">257–258</a></p>

<p class="noindent">Grammars, formal, <a href="#babilu_link-174">36–37</a></p>

<p class="noindent">Grandmother cell, perceptron and, <a href="#babilu_link-266">99–100</a></p>

<p class="noindent">Graphical models, <a href="#babilu_link-67">240</a> , <a href="#babilu_link-120">245–250</a></p>

<p class="noindent">Graphical user interfaces, <a href="#babilu_link-197">236</a></p>

<p class="noindent"><i>The Guns of August</i> (Tuchman), <a href="#babilu_link-68">178</a></p>

<p class="noindent">Handwritten digit recognition, <a href="#babilu_link-91">189</a> , <a href="#babilu_link-231">195</a></p>

<p class="noindent">Hart, Peter, <a href="#babilu_link-210">185</a></p>

<p class="noindent">Hawking, Stephen, <a href="#babilu_link-267">47</a> , <a href="#babilu_link-245">283</a></p>

<p class="noindent">Hawkins, Jeff, <a href="#babilu_link-97">28</a> , <a href="#babilu_link-150">118</a></p>

<p class="noindent">Hebb, Donald, <a href="#babilu_link-161">93</a> , <a href="#babilu_link-151">94</a></p>

<p class="noindent">Hebb’s rule, <a href="#babilu_link-161">93</a> , <a href="#babilu_link-151">94</a> , <a href="#babilu_link-28">95</a></p>

<p class="noindent">Heckerman, David, <a href="#babilu_link-147">151–152</a> , <a href="#babilu_link-35">159–160</a></p>

<p class="noindent">Held-out data, accuracy of, <a href="#babilu_link-23">75–76</a></p>

<p class="noindent">Help desks, <a href="#babilu_link-47">198</a></p>

<p class="noindent">Hemingway, Ernest, <a href="#babilu_link-234">106</a></p>

<p class="noindent">Heraclitus, <a href="#babilu_link-268">48</a></p>

<p class="noindent"><a id="babilu_link-482"></a> Hidden Markov model (HMM), <a href="#babilu_link-123">154–155</a> , <a href="#babilu_link-35">159</a> , <a href="#babilu_link-182">210</a> , <a href="#babilu_link-118">305</a></p>

<p class="noindent">Hierarchical structure, Markov logic network with, <a href="#babilu_link-269">256–257</a></p>

<p class="noindent">Hill climbing, <a href="#babilu_link-131">135</a> , <a href="#babilu_link-145">136</a> , <a href="#babilu_link-270">169</a> , <a href="#babilu_link-91">189</a> , <a href="#babilu_link-102">252</a></p>

<p class="noindent">Hillis, Danny, <a href="#babilu_link-131">135</a></p>

<p class="noindent">Hinton, Geoff, <a href="#babilu_link-27">103</a> , <a href="#babilu_link-29">104</a> , <a href="#babilu_link-104">112</a> , <a href="#babilu_link-100">115</a> , <a href="#babilu_link-205">137</a> , <a href="#babilu_link-108">139</a></p>

<p class="noindent"><i>The Hitchhiker’s Guide to the Galaxy</i> (Adams), <a href="#babilu_link-187">130</a></p>

<p class="noindent">HIV testing, Bayes’ theorem and, <a href="#babilu_link-271">147–148</a></p>

<p class="noindent">HMM. <i>See</i> Hidden Markov model (HMM)</p>

<p class="noindent">Ho, Yu-Chi, <a href="#babilu_link-156">113</a></p>

<p class="noindent">Hodges, Joe, <a href="#babilu_link-68">178–179</a> , <a href="#babilu_link-66">186</a></p>

<p class="noindent">Hofstadter, Douglas, <a href="#babilu_link-70">200</a></p>

<p class="noindent">Holland, John, <a href="#babilu_link-44">122–128</a> , <a href="#babilu_link-111">129</a> , <a href="#babilu_link-187">130</a> , <a href="#babilu_link-148">131</a> , <a href="#babilu_link-158">134</a></p>

<p class="noindent"><i>Homo technicus,</i> <a href="#babilu_link-272">288–289</a></p>

<p class="noindent">Hopfield, John, <a href="#babilu_link-153">102–103</a> , <a href="#babilu_link-116">170</a></p>

<p class="noindent">Hopfield networks, <a href="#babilu_link-27">103</a> , <a href="#babilu_link-93">116</a> , <a href="#babilu_link-101">302</a></p>

<p class="noindent">Horning, J. J., <a href="#babilu_link-174">36–37</a></p>

<p class="noindent">Howbert, Jeff, <a href="#babilu_link-273">292</a></p>

<p class="noindent"><i>How to Create a Mind</i> (Kurzweil), <a href="#babilu_link-97">28</a></p>

<p class="noindent">H&amp;R Block, <a href="#babilu_link-170">277</a></p>

<p class="noindent">Hubble, Edwin, <a href="#babilu_link-54">14–15</a></p>

<p class="noindent">Human complexity</p>

<p class="noindent">as complexity monster, <a href="#babilu_link-40">5</a></p>

<p class="noindent">machine learning and, <a href="#babilu_link-139">258–259</a></p>

<p class="noindent">Human control of artificial intelligence, <a href="#babilu_link-78">282–286</a></p>

<p class="noindent">Human-directed evolution, <a href="#babilu_link-58">286–289</a> , <a href="#babilu_link-247">311</a></p>

<p class="noindent">Human intuition, data and, <a href="#babilu_link-149">39</a></p>

<p class="noindent">Humanities, machine learning and, <a href="#babilu_link-274">278</a></p>

<p class="noindent">Human Rights Watch, <a href="#babilu_link-237">281</a></p>

<p class="noindent">Hume, David, <a href="#babilu_link-75">58–59</a> , <a href="#babilu_link-275">62</a> , <a href="#babilu_link-186">63</a> , <a href="#babilu_link-161">93</a> , <a href="#babilu_link-68">178</a> , <a href="#babilu_link-191">300–301</a></p>

<p class="noindent">Hume’s problem of induction, <a href="#babilu_link-75">58–59</a> , <a href="#babilu_link-169">145</a> , <a href="#babilu_link-270">169</a> , <a href="#babilu_link-60">197</a> , <a href="#babilu_link-276">251</a></p>

<p class="noindent">Humie Awards, <a href="#babilu_link-158">134</a></p>

<p class="noindent">Hunt, Earl, <a href="#babilu_link-277">88</a></p>

<p class="noindent">Hyperplanes, <a href="#babilu_link-278">98</a> , <a href="#babilu_link-248">100</a> , <a href="#babilu_link-231">195</a> , <a href="#babilu_link-213">196</a></p>

<p class="noindent">Hyperspace, <a href="#babilu_link-99">107–111</a> , <a href="#babilu_link-254">187</a></p>

<p class="noindent">Hypotheses</p>

<p class="noindent">Bayesians and, <a href="#babilu_link-113">144</a> , <a href="#babilu_link-188">167–168</a></p>

<p class="noindent">machine learning and, <a href="#babilu_link-222">13–15</a></p>

<p class="noindent">overfitting and, <a href="#babilu_link-189">73–75</a></p>

<p class="noindent">preference for simpler, <a href="#babilu_link-235">77–78</a></p>

<p class="noindent">Red Queen, <a href="#babilu_link-131">135</a></p>

<p class="noindent">testing, <a href="#babilu_link-222">13–15</a> , <a href="#babilu_link-81">49</a></p>

<p class="noindent">IBM, <a href="#babilu_link-222">13</a> , <a href="#babilu_link-217">37</a> , <a href="#babilu_link-143">219</a></p>

<p class="noindent">ICML. <i>See</i> International Conference on Machine Learning (ICML)</p>

<p class="noindent"><i>If</i> … <i>then</i> … rules, <a href="#babilu_link-201">68–71</a> , <a href="#babilu_link-31">84–85</a> , <a href="#babilu_link-251">125–127</a> , <a href="#babilu_link-252">132</a> , <a href="#babilu_link-202">152</a> , <a href="#babilu_link-124">155–156</a> , <a href="#babilu_link-214">201–202</a> , <a href="#babilu_link-259">244–245</a> , <a href="#babilu_link-176">254</a></p>

<p class="noindent">Ill-posed problem, <a href="#babilu_link-76">64</a></p>

<p class="noindent">Immortality, genetic algorithms and, <a href="#babilu_link-279">126</a></p>

<p class="noindent">Incognito mode, <a href="#babilu_link-48">266</a></p>

<p class="noindent">Income, basic guaranteed, <a href="#babilu_link-216">279</a></p>

<p class="noindent">Independent-component analysis, <a href="#babilu_link-184">215</a></p>

<p class="noindent">Indexers, <a href="#babilu_link-33">8</a> , <a href="#babilu_link-224">9</a></p>

<p class="noindent">Indifference, principle of, <a href="#babilu_link-169">145</a></p>

<p class="noindent">Induction</p>

<p class="noindent">decision tree, <a href="#babilu_link-228">85–89</a></p>

<p class="noindent">further readings, <a href="#babilu_link-191">300–302</a></p>

<p class="noindent">as inverse of deduction, <a href="#babilu_link-159">80–83</a> , <a href="#babilu_link-136">301</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-196">34</a></p>

<p class="noindent">Newton’s rules of, <a href="#babilu_link-203">65–66</a></p>

<p class="noindent">problem of, <a href="#babilu_link-280">59–62</a></p>

<p class="noindent">Inductive logic programming. <i>See</i> Inverse deduction</p>

<p class="noindent">Inductivist turkey, <a href="#babilu_link-88">61</a></p>

<p class="noindent">Inference</p>

<p class="noindent">Alchemy and, <a href="#babilu_link-269">256–257</a></p>

<p class="noindent">Bayesian networks and, <a href="#babilu_link-119">161–166</a></p>

<p class="noindent">Information, cyberwar and, <a href="#babilu_link-215">19–20</a></p>

<p class="noindent">Information gain, <a href="#babilu_link-24">87</a> , <a href="#babilu_link-90">188</a></p>

<p class="noindent">Information processing systems, study of, <a href="#babilu_link-80">89</a></p>

<p class="noindent">Information Revolution, <a href="#babilu_link-224">9</a></p>

<p class="noindent">Instance-based learning, <a href="#babilu_link-214">201–202</a> , <a href="#babilu_link-38">250</a></p>

<p class="noindent">Institute of Control Sciences, <a href="#babilu_link-69">190</a></p>

<p class="noindent">Intelligence</p>

<p class="noindent">computers and, <a href="#babilu_link-79">35</a> , <a href="#babilu_link-58">286</a> , <a href="#babilu_link-281">287</a></p>

<p class="noindent">symbolists and, <a href="#babilu_link-98">52</a> , <a href="#babilu_link-80">89</a> , <a href="#babilu_link-101">302</a></p>

<p class="noindent"><a id="babilu_link-483"></a> International Conference on Machine Learning (ICML), <a href="#babilu_link-145">136</a></p>

<p class="noindent">Internet, <a href="#babilu_link-121">231</a> , <a href="#babilu_link-197">236</a></p>

<p class="noindent">Intuition, evidence and, <a href="#babilu_link-149">39</a></p>

<p class="noindent">Inverse deduction, <a href="#babilu_link-98">52</a> , <a href="#babilu_link-159">80–83</a> , <a href="#babilu_link-282">90–91</a> , <a href="#babilu_link-136">301</a></p>

<p class="noindent">Alchemy and, <a href="#babilu_link-102">252</a></p>

<p class="noindent">cell model and, <a href="#babilu_link-100">115</a></p>

<p class="noindent">computational intensiveness of, <a href="#babilu_link-228">85</a></p>

<p class="noindent">cure for cancer and, <a href="#babilu_link-163">83–85</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-282">90</a> , <a href="#babilu_link-25">241</a> , <a href="#babilu_link-127">242–243</a></p>

<p class="noindent">Newton’s principle and, <a href="#babilu_link-283">82–83</a></p>

<p class="noindent">shortcomings of, <a href="#babilu_link-206">91</a></p>

<p class="noindent">IPsoft, <a href="#babilu_link-47">198</a></p>

<p class="noindent">Irrelevant attributes, nearest neighbor algorithm and, <a href="#babilu_link-66">186–187</a> , <a href="#babilu_link-90">188–189</a></p>

<p class="noindent">Isomap, <a href="#babilu_link-284">217</a> , <a href="#babilu_link-37">255</a> , <a href="#babilu_link-233">308</a></p>

<p class="noindent">Iterative search, <a href="#babilu_link-97">28</a></p>

<p class="noindent">Jackel, Larry, <a href="#babilu_link-231">195</a></p>

<p class="noindent">James, William, <a href="#babilu_link-161">93</a> , <a href="#babilu_link-68">178</a> , <a href="#babilu_link-181">205</a></p>

<p class="noindent">Java, <a href="#babilu_link-42">4</a></p>

<p class="noindent">Jelinek, Fred, <a href="#babilu_link-217">37</a></p>

<p class="noindent">Jesus, <a href="#babilu_link-113">144</a></p>

<p class="noindent">Jevons, William Stanley, <a href="#babilu_link-159">80</a></p>

<p class="noindent">Johnson, Steven, <a href="#babilu_link-173">182–183</a></p>

<p class="noindent">Jordan, Michael, <a href="#babilu_link-285">164</a> , <a href="#babilu_link-116">170</a></p>

<p class="noindent">Junction trees, <a href="#babilu_link-244">163</a></p>

<p class="noindent">Kaggle.com, <a href="#babilu_link-273">292</a></p>

<p class="noindent">Kahneman, Daniel, <a href="#babilu_link-129">141</a></p>

<p class="noindent">Kalman filter, <a href="#babilu_link-124">155</a> , <a href="#babilu_link-118">305</a></p>

<p class="noindent">Kant, Immanuel, <a href="#babilu_link-68">178</a></p>

<p class="noindent">Kekulè, August, <a href="#babilu_link-68">178</a></p>

<p class="noindent">Kennedy, John F., <a href="#babilu_link-174">36</a> , <a href="#babilu_link-18">177–178</a> , <a href="#babilu_link-173">182</a></p>

<p class="noindent">Kepler, Johannes, <a href="#babilu_link-203">65</a> , <a href="#babilu_link-148">131</a></p>

<p class="noindent">laws of, <a href="#babilu_link-138">40</a> , <a href="#babilu_link-203">65</a> , <a href="#babilu_link-148">131</a></p>

<p class="noindent">Kepler phase of science, <a href="#babilu_link-149">39–40</a></p>

<p class="noindent">Kernels, <a href="#babilu_link-286">192</a> , <a href="#babilu_link-213">196</a> , <a href="#babilu_link-26">243</a> , <a href="#babilu_link-168">307</a></p>

<p class="noindent">Keyword matching, <a href="#babilu_link-256">20</a></p>

<p class="noindent">Kinect, <a href="#babilu_link-277">88</a> , <a href="#babilu_link-230">237</a> , <a href="#babilu_link-106">238</a></p>

<p class="noindent">Kipling, Rudyard, <a href="#babilu_link-201">68</a></p>

<p class="noindent"><i>k</i> -means algorithm, <a href="#babilu_link-287">208</a> , <a href="#babilu_link-182">210</a> , <a href="#babilu_link-233">308</a></p>

<p class="noindent"><i>k</i> -nearest-neighbor algorithm, <a href="#babilu_link-179">183</a></p>

<p class="noindent">Knowledge, <a href="#babilu_link-33">8</a> , <a href="#babilu_link-98">52</a> , <a href="#babilu_link-76">64</a></p>

<p class="noindent">unity of, <a href="#babilu_link-130">31</a></p>

<p class="noindent">Knowledge acquisition bottleneck, <a href="#babilu_link-80">89–90</a></p>

<p class="noindent">Knowledge-based system, <a href="#babilu_link-80">89–90</a></p>

<p class="noindent">Knowledge discovery, <a href="#babilu_link-33">8</a> . <i>See also</i> Machine learning</p>

<p class="noindent">Knowledge engineering</p>

<p class="noindent">machine learning and, <a href="#babilu_link-153">102</a></p>

<p class="noindent">symbolist learning and, <a href="#babilu_link-282">90</a></p>

<p class="noindent">Knowledge engineers, <a href="#babilu_link-276">251</a></p>

<p class="noindent">machiner learners <i>vs.,</i> <a href="#babilu_link-196">34–38</a></p>

<p class="noindent">Knowledge graph, <a href="#babilu_link-37">255</a></p>

<p class="noindent">Koza, John, <a href="#babilu_link-148">131</a> , <a href="#babilu_link-252">132</a> , <a href="#babilu_link-158">134</a> , <a href="#babilu_link-145">136</a></p>

<p class="noindent">Krugman, Paul, <a href="#babilu_link-82">232</a></p>

<p class="noindent">Kurzweil, Ray, <a href="#babilu_link-97">28</a> , <a href="#babilu_link-163">83</a> , <a href="#babilu_link-66">186</a> , <a href="#babilu_link-58">286–289</a></p>

<p class="noindent">Laird, John, <a href="#babilu_link-185">226</a></p>

<p class="noindent">Laird, Nan, <a href="#babilu_link-177">209</a></p>

<p class="noindent">Landmine example, support vector machines and, <a href="#babilu_link-286">192–193</a></p>

<p class="noindent">Lang, Kevin, <a href="#babilu_link-145">136</a></p>

<p class="noindent">Langlands program, <a href="#babilu_link-191">300</a></p>

<p class="noindent">Language learning, <a href="#babilu_link-174">36–37</a></p>

<p class="noindent">Laplace, Pierre-Simon de, <a href="#babilu_link-113">144–145</a></p>

<p class="noindent">Latent semantic analysis, <a href="#babilu_link-184">215</a> , <a href="#babilu_link-233">308</a></p>

<p class="noindent">Law of effect, <a href="#babilu_link-238">218</a></p>

<p class="noindent">Law of similarity, <a href="#babilu_link-68">178</a></p>

<p class="noindent">Lazy learning, <a href="#babilu_link-288">180–182</a></p>

<p class="noindent">Learning</p>

<p class="noindent">across problem domains, <a href="#babilu_link-45">199</a></p>

<p class="noindent">by association, <a href="#babilu_link-161">93–94</a></p>

<p class="noindent">Bayesian, <a href="#babilu_link-113">144</a> , <a href="#babilu_link-114">166–170</a> , <a href="#babilu_link-86">174–175</a></p>

<p class="noindent">children’s, <a href="#babilu_link-232">203–204</a> , <a href="#babilu_link-233">308</a></p>

<p class="noindent">deep, <a href="#babilu_link-29">104</a> , <a href="#babilu_link-100">115–118</a> , <a href="#babilu_link-64">172</a> , <a href="#babilu_link-231">195</a> , <a href="#babilu_link-25">241</a></p>

<p class="noindent">instance-based, <a href="#babilu_link-214">201–202</a></p>

<p class="noindent">knowledge and, <a href="#babilu_link-76">64</a></p>

<p class="noindent">lazy, <a href="#babilu_link-288">180–182</a></p>

<p class="noindent">as problem solving, <a href="#babilu_link-185">226</a></p>

<p class="noindent">reinforcement, <a href="#babilu_link-238">218–223</a> , <a href="#babilu_link-233">308</a></p>

<p class="noindent">rule-based, <a href="#babilu_link-132">69–70</a> , <a href="#babilu_link-214">201–202</a></p>

<p class="noindent">statistical, <a href="#babilu_link-217">37</a> , <a href="#babilu_link-289">228</a> , <a href="#babilu_link-290">297</a> , <a href="#babilu_link-191">300</a> , <a href="#babilu_link-168">307</a></p>

<p class="noindent">statistical relational, <a href="#babilu_link-22">309</a></p>

<p class="noindent">supervised, <a href="#babilu_link-177">209</a> , <a href="#babilu_link-219">214</a> , <a href="#babilu_link-133">220</a> , <a href="#babilu_link-105">222</a> , <a href="#babilu_link-185">226</a></p>

<p class="noindent">unsupervised, <a href="#babilu_link-232">203–233</a></p>

<p class="noindent"><a id="babilu_link-484"></a> Learning algorithms (learners)</p>

<p class="noindent">Alchemy, <a href="#babilu_link-38">250–259</a></p>

<p class="noindent">children’s learning and, <a href="#babilu_link-232">203–204</a></p>

<p class="noindent">control of, <a href="#babilu_link-195">45</a></p>

<p class="noindent">curse of dimensionality and, <a href="#babilu_link-90">188</a></p>

<p class="noindent">empirical evaluation of, <a href="#babilu_link-291">76</a></p>

<p class="noindent">evaluation and, <a href="#babilu_link-245">283</a></p>

<p class="noindent">interaction with, <a href="#babilu_link-194">264–267</a></p>

<p class="noindent">machine learning and, <a href="#babilu_link-198">6–10</a></p>

<p class="noindent">optimization and, <a href="#babilu_link-245">283</a></p>

<p class="noindent">prediction and, <a href="#babilu_link-149">39</a></p>

<p class="noindent">representation and, <a href="#babilu_link-245">283</a></p>

<p class="noindent">speed and, <a href="#babilu_link-108">139–142</a></p>

<p class="noindent">as superpredators, <a href="#babilu_link-33">8–9</a></p>

<p class="noindent">variety of tasks undertaken by, <a href="#babilu_link-292">23–25</a></p>

<p class="noindent"><i>See also individual algorithms</i></p>

<p class="noindent">LeCun, Yann, <a href="#babilu_link-156">113</a> , <a href="#babilu_link-231">195</a></p>

<p class="noindent">Lehman Brothers, <a href="#babilu_link-234">106</a></p>

<p class="noindent">Leibniz, Gottfried, <a href="#babilu_link-75">58</a> , <a href="#babilu_link-76">64</a> , <a href="#babilu_link-71">175</a> , <a href="#babilu_link-47">198</a></p>

<p class="noindent">Lenat, Doug, <a href="#babilu_link-79">35</a></p>

<p class="noindent">Lewis, Michael, <a href="#babilu_link-149">39</a></p>

<p class="noindent">Linear regression, <a href="#babilu_link-140">15</a> , <a href="#babilu_link-243">50</a> , <a href="#babilu_link-156">113</a> , <a href="#babilu_link-173">182</a> , <a href="#babilu_link-219">214</a> , <a href="#babilu_link-19">306</a></p>

<p class="noindent">LinkedIn, <a href="#babilu_link-293">269</a> , <a href="#babilu_link-51">271</a></p>

<p class="noindent">Lipson, Hod, <a href="#babilu_link-209">121</a></p>

<p class="noindent">Local minimum, <a href="#babilu_link-153">102</a> , <a href="#babilu_link-27">103</a> , <a href="#babilu_link-294">110–111</a></p>

<p class="noindent">Local optima, <a href="#babilu_link-295">111</a> , <a href="#babilu_link-103">128</a></p>

<p class="noindent">Locally weighted regression, <a href="#babilu_link-173">182</a> , <a href="#babilu_link-19">306</a></p>

<p class="noindent">Locke, John, <a href="#babilu_link-75">58</a> , <a href="#babilu_link-161">93</a> , <a href="#babilu_link-68">178</a></p>

<p class="noindent">Logic, <a href="#babilu_link-243">50</a> , <a href="#babilu_link-296">33</a> , <a href="#babilu_link-81">49</a> , <a href="#babilu_link-159">80–81</a></p>

<p class="noindent">Bayesians and, <a href="#babilu_link-125">173</a></p>

<p class="noindent">computers and, <a href="#babilu_link-73">2</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-67">240</a> , <a href="#babilu_link-259">244</a> , <a href="#babilu_link-120">245–246</a></p>

<p class="noindent">probability and, <a href="#babilu_link-125">173–175</a> , <a href="#babilu_link-120">245–246</a> , <a href="#babilu_link-19">306</a> , <a href="#babilu_link-22">309</a></p>

<p class="noindent">unified with graphical models, <a href="#babilu_link-120">245–250</a></p>

<p class="noindent">Logical inference, Alchemy and, <a href="#babilu_link-269">256</a></p>

<p class="noindent">Logic gates, <a href="#babilu_link-72">96</a></p>

<p class="noindent">Logistic curve. <i>See</i> S curves</p>

<p class="noindent">Long-tail phenomenon, <a href="#babilu_link-53">12</a> , <a href="#babilu_link-32">299</a></p>

<p class="noindent">Long-term potentiation, <a href="#babilu_link-171">27</a></p>

<p class="noindent">Loopy belief propagation, <a href="#babilu_link-244">163–164</a> , <a href="#babilu_link-121">231</a></p>

<p class="noindent">Lorenz, Konrad, <a href="#babilu_link-107">138</a></p>

<p class="noindent">Low-pass filter, <a href="#babilu_link-240">133</a></p>

<p class="noindent">Machine learners, knowledge engineers <i>vs.,</i> <a href="#babilu_link-196">34–38</a></p>

<p class="noindent">Machine learning, <a href="#babilu_link-198">6–10</a></p>

<p class="noindent">analogy and, <a href="#babilu_link-68">178–179</a></p>

<p class="noindent">bias and variance and, <a href="#babilu_link-135">78–79</a></p>

<p class="noindent">big data and, <a href="#babilu_link-140">15–16</a></p>

<p class="noindent">business and, <a href="#babilu_link-94">10–13</a></p>

<p class="noindent">chunking, <a href="#babilu_link-175">223–227</a></p>

<p class="noindent">clustering, <a href="#babilu_link-181">205–210</a></p>

<p class="noindent">dimensionality reduction, <a href="#babilu_link-50">211–217</a></p>

<p class="noindent">effect on employment, <a href="#babilu_link-190">276–279</a></p>

<p class="noindent">exponential function and, <a href="#babilu_link-189">73–74</a></p>

<p class="noindent">fitness function and, <a href="#babilu_link-95">123</a></p>

<p class="noindent">further readings, <a href="#babilu_link-290">297–298</a></p>

<p class="noindent">future of, <a href="#babilu_link-46">21–22</a></p>

<p class="noindent">impact on daily life, <a href="#babilu_link-43">298</a></p>

<p class="noindent">effect on employment, <a href="#babilu_link-190">276–279</a></p>

<p class="noindent">meta-learning, <a href="#babilu_link-230">237–239</a></p>

<p class="noindent">nature <i>vs.</i> nurture debate and, <a href="#babilu_link-297">29</a> , <a href="#babilu_link-205">137–139</a></p>

<p class="noindent">Newton’s principle and, <a href="#babilu_link-203">65–66</a></p>

<p class="noindent">planetary-scale, <a href="#babilu_link-269">256–259</a></p>

<p class="noindent">politics and, <a href="#babilu_link-30">16–19</a></p>

<p class="noindent">principal-component analysis, <a href="#babilu_link-50">211–217</a></p>

<p class="noindent">problem of unpredictability and, <a href="#babilu_link-141">38–40</a></p>

<p class="noindent">reinforcement learning, <a href="#babilu_link-238">218–223</a> , <a href="#babilu_link-185">226–227</a></p>

<p class="noindent">relational learning, <a href="#babilu_link-21">227–233</a></p>

<p class="noindent">relationship to artificial intelligence, <a href="#babilu_link-33">8</a></p>

<p class="noindent">science and, <a href="#babilu_link-222">13–16</a> , <a href="#babilu_link-218">235–236</a></p>

<p class="noindent">significance tests and, <a href="#babilu_link-291">76–77</a></p>

<p class="noindent">as technology, <a href="#babilu_link-197">236–237</a></p>

<p class="noindent">Turing point and, <a href="#babilu_link-58">286</a> , <a href="#babilu_link-272">288</a></p>

<p class="noindent">war and, <a href="#babilu_link-215">19–21</a> , <a href="#babilu_link-216">279–282</a></p>

<p class="noindent"><i>See also</i> Algorithms</p>

<p class="noindent">Machine-learning problem, <a href="#babilu_link-88">61–62</a> , <a href="#babilu_link-264">109–110</a></p>

<p class="noindent">Machine-translation systems, <a href="#babilu_link-123">154</a></p>

<p class="noindent">MacKay, David, <a href="#babilu_link-116">170</a></p>

<p class="noindent">Madrigal, Alexis, <a href="#babilu_link-89">273–274</a></p>

<p class="noindent"><a id="babilu_link-485"></a> Malthus, Thomas, <a href="#babilu_link-68">178</a> , <a href="#babilu_link-218">235</a></p>

<p class="noindent">Manchester Institute of Biotechnology, <a href="#babilu_link-30">16</a></p>

<p class="noindent">Mandelbrot set, <a href="#babilu_link-172">30</a> , <a href="#babilu_link-191">300</a></p>

<p class="noindent">Margins, <a href="#babilu_link-286">192–194</a> , <a href="#babilu_link-213">196</a> , <a href="#babilu_link-25">241</a> , <a href="#babilu_link-127">242</a> , <a href="#babilu_link-26">243</a> , <a href="#babilu_link-168">307</a></p>

<p class="noindent">Markov, Andrei, <a href="#babilu_link-126">153</a></p>

<p class="noindent">Markov chain Monte Carlo (MCMC), <a href="#babilu_link-285">164–165</a> , <a href="#babilu_link-188">167</a> , <a href="#babilu_link-116">170</a> , <a href="#babilu_link-121">231</a> , <a href="#babilu_link-25">241</a> , <a href="#babilu_link-127">242</a> , <a href="#babilu_link-65">253</a> , <a href="#babilu_link-269">256</a></p>

<p class="noindent">Markov chains, <a href="#babilu_link-126">153–155</a> , <a href="#babilu_link-35">159</a> , <a href="#babilu_link-110">304–305</a></p>

<p class="noindent">Markov logic. <i>See</i> Markov logic networks (MLNs)</p>

<p class="noindent">Markov logic networks (MLNs), <a href="#babilu_link-36">246–259</a> , <a href="#babilu_link-22">309–310</a></p>

<p class="noindent">classes and, <a href="#babilu_link-178">257</a></p>

<p class="noindent">complexity and, <a href="#babilu_link-139">258–259</a></p>

<p class="noindent">parts and, <a href="#babilu_link-269">256–257</a></p>

<p class="noindent">with hierarchical structure, <a href="#babilu_link-269">256–257</a></p>

<p class="noindent"><i>See also</i> Alchemy</p>

<p class="noindent">Markov networks, <a href="#babilu_link-265">171–172</a> , <a href="#babilu_link-298">229</a> , <a href="#babilu_link-67">240</a> , <a href="#babilu_link-120">245</a> , <a href="#babilu_link-65">253</a> , <a href="#babilu_link-19">306</a></p>

<p class="noindent">Marr, David, <a href="#babilu_link-80">89</a></p>

<p class="noindent">Marr’s three levels, <a href="#babilu_link-80">89</a></p>

<p class="noindent">Master Algorithm, <a href="#babilu_link-246">239–246</a></p>

<p class="noindent">Alchemy and, <a href="#babilu_link-38">250–259</a></p>

<p class="noindent">Bayes’ theorem and, <a href="#babilu_link-299">148</a></p>

<p class="noindent">brain as, <a href="#babilu_link-92">26–28</a></p>

<p class="noindent">CanceRx, <a href="#babilu_link-165">259–261</a></p>

<p class="noindent">candidates that fail as, <a href="#babilu_link-268">48–50</a></p>

<p class="noindent">chunking and, <a href="#babilu_link-185">226</a></p>

<p class="noindent">complexity of, <a href="#babilu_link-138">40–41</a></p>

<p class="noindent">as composite picture of current and future learners, <a href="#babilu_link-300">263–264</a></p>

<p class="noindent">computer science and, <a href="#babilu_link-199">32–34</a></p>

<p class="noindent">equation, <a href="#babilu_link-243">50</a></p>

<p class="noindent">evolution and, <a href="#babilu_link-97">28–29</a></p>

<p class="noindent">five tribes and, <a href="#babilu_link-61">51–55</a></p>

<p class="noindent">future and, <a href="#babilu_link-273">292</a></p>

<p class="noindent">goal of, <a href="#babilu_link-149">39</a></p>

<p class="noindent">Google and, <a href="#babilu_link-78">282</a></p>

<p class="noindent">invention of, <a href="#babilu_link-220">25–26</a></p>

<p class="noindent">Markov logic networks and, <a href="#babilu_link-197">236–250</a></p>

<p class="noindent">meta-learning and, <a href="#babilu_link-230">237–239</a></p>

<p class="noindent">physics and, <a href="#babilu_link-297">29–31</a></p>

<p class="noindent">practical applications of, <a href="#babilu_link-134">41–45</a></p>

<p class="noindent">statistics and, <a href="#babilu_link-130">31–32</a></p>

<p class="noindent">symbolism and, <a href="#babilu_link-282">90–91</a></p>

<p class="noindent">theory of everything and, <a href="#babilu_link-223">46–48</a></p>

<p class="noindent">Turing point and, <a href="#babilu_link-58">286</a> , <a href="#babilu_link-272">288</a></p>

<p class="noindent">as unifier of machine learning, <a href="#babilu_link-230">237</a></p>

<p class="noindent">unity of knowledge and, <a href="#babilu_link-130">31</a></p>

<p class="noindent">Match.com, <a href="#babilu_link-53">12</a> , <a href="#babilu_link-239">265</a></p>

<p class="noindent">Matrix factorization for recommendation systems, <a href="#babilu_link-184">215</a></p>

<p class="noindent">Maximum likelihood principle, <a href="#babilu_link-114">166–167</a> , <a href="#babilu_link-301">168</a></p>

<p class="noindent">Maxwell, James Clerk, <a href="#babilu_link-218">235</a></p>

<p class="noindent">McCulloch, Warren, <a href="#babilu_link-72">96</a></p>

<p class="noindent">McKinsey Global Institute, <a href="#babilu_link-224">9</a></p>

<p class="noindent">MCMC. <i>See</i> Markov chain Monte Carlo (MCMC)</p>

<p class="noindent">Means-ends analysis, <a href="#babilu_link-302">225</a></p>

<p class="noindent">Mechanical Turk, <a href="#babilu_link-54">14</a></p>

<p class="noindent">Medical data, sharing of, <a href="#babilu_link-20">272–273</a></p>

<p class="noindent">Medical diagnosis, <a href="#babilu_link-292">23</a> , <a href="#babilu_link-271">147</a> , <a href="#babilu_link-128">149–150</a> , <a href="#babilu_link-34">160</a> , <a href="#babilu_link-270">169</a> , <a href="#babilu_link-289">228–229</a> , <a href="#babilu_link-250">248–249</a></p>

<p class="noindent">Memorization, <a href="#babilu_link-268">48</a></p>

<p class="noindent">Memory, time as principal component of, <a href="#babilu_link-284">217</a></p>

<p class="noindent">Mencken, H. L., <a href="#babilu_link-249">230</a></p>

<p class="noindent">Mendeleev, Dmitri, <a href="#babilu_link-218">235</a></p>

<p class="noindent">Meta-learning, <a href="#babilu_link-230">237–239</a> , <a href="#babilu_link-37">255</a> , <a href="#babilu_link-22">309</a></p>

<p class="noindent">Methane/methanol, <a href="#babilu_link-60">197–198</a></p>

<p class="noindent">Michalski, Ryszard, <a href="#babilu_link-132">69</a> , <a href="#babilu_link-221">70</a> , <a href="#babilu_link-282">90</a></p>

<p class="noindent">Michelangelo, <a href="#babilu_link-73">2</a></p>

<p class="noindent">Microprocessor, <a href="#babilu_link-268">48–49</a> , <a href="#babilu_link-197">236</a></p>

<p class="noindent">Microsoft, <a href="#babilu_link-224">9</a> , <a href="#babilu_link-257">22</a></p>

<p class="noindent">Kinect, <a href="#babilu_link-277">88</a> , <a href="#babilu_link-230">237</a> , <a href="#babilu_link-106">238</a></p>

<p class="noindent">Windows, <a href="#babilu_link-53">12</a> , <a href="#babilu_link-240">133</a> , <a href="#babilu_link-303">224</a></p>

<p class="noindent">Xbox Live, <a href="#babilu_link-34">160–161</a></p>

<p class="noindent">Microsoft Research, <a href="#babilu_link-202">152</a></p>

<p class="noindent">Military robots, <a href="#babilu_link-46">21</a> , <a href="#babilu_link-216">279–282</a> , <a href="#babilu_link-32">299</a> , <a href="#babilu_link-166">310</a></p>

<p class="noindent">Mill, John Stuart, <a href="#babilu_link-161">93</a></p>

<p class="noindent">Miller, George, <a href="#babilu_link-303">224</a></p>

<p class="noindent">Minsky, Marvin, <a href="#babilu_link-79">35</a> , <a href="#babilu_link-141">38</a> , <a href="#babilu_link-248">100–101</a> , <a href="#babilu_link-153">102</a> , <a href="#babilu_link-294">110</a> , <a href="#babilu_link-104">112</a> , <a href="#babilu_link-156">113</a></p>

<p class="noindent">Mitchell, Tom, <a href="#babilu_link-76">64</a> , <a href="#babilu_link-132">69</a> , <a href="#babilu_link-282">90</a></p>

<p class="noindent"><a id="babilu_link-486"></a> Mixability, <a href="#babilu_link-131">135</a></p>

<p class="noindent">MLNs. <i>See</i> Markov logic networks (MLNs)</p>

<p class="noindent"><i>Moby Dick</i> (Melville), <a href="#babilu_link-77">72</a></p>

<p class="noindent">Model ensembles, <a href="#babilu_link-230">237</a> -239</p>

<p class="noindent">Molecular biology, data and, <a href="#babilu_link-54">14</a></p>

<p class="noindent"><i>Moneyball</i> (Lewis), <a href="#babilu_link-149">39</a></p>

<p class="noindent">Mooney, Ray, <a href="#babilu_link-291">76</a></p>

<p class="noindent">Moore’s law, <a href="#babilu_link-281">287</a></p>

<p class="noindent">Moravec, Hans, <a href="#babilu_link-272">288</a></p>

<p class="noindent">Muggleton, Steve, <a href="#babilu_link-159">80</a></p>

<p class="noindent">Multilayer perceptron, <a href="#babilu_link-304">108–111</a></p>

<p class="noindent">autoencoder, <a href="#babilu_link-93">116–118</a></p>

<p class="noindent">Bayesian, <a href="#babilu_link-116">170</a></p>

<p class="noindent">driving a car and, <a href="#babilu_link-156">113</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-259">244</a></p>

<p class="noindent">NETtalk system, <a href="#babilu_link-104">112</a></p>

<p class="noindent">reinforcement learning and, <a href="#babilu_link-105">222</a></p>

<p class="noindent">support vector machines and, <a href="#babilu_link-231">195</a></p>

<p class="noindent">Music composition, case-based reasoning and, <a href="#babilu_link-45">199</a></p>

<p class="noindent">Music Genome Project, <a href="#babilu_link-265">171</a></p>

<p class="noindent">Mutation, <a href="#babilu_link-212">124</a> , <a href="#babilu_link-158">134–135</a> , <a href="#babilu_link-25">241</a> , <a href="#babilu_link-102">252</a></p>

<p class="noindent">Naïve Bayes classifier, <a href="#babilu_link-147">151–153</a> , <a href="#babilu_link-265">171</a> , <a href="#babilu_link-110">304</a></p>

<p class="noindent">Bayesian networks and, <a href="#babilu_link-142">158–159</a></p>

<p class="noindent">clustering and, <a href="#babilu_link-177">209</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-120">245</a></p>

<p class="noindent">medical diagnosis and, <a href="#babilu_link-292">23</a></p>

<p class="noindent">relational learning and, <a href="#babilu_link-289">228–229</a></p>

<p class="noindent">spam filters and, <a href="#babilu_link-292">23–24</a></p>

<p class="noindent">text classification and, <a href="#babilu_link-231">195–196</a></p>

<p class="noindent">Narrative Science, <a href="#babilu_link-190">276</a></p>

<p class="noindent">National Security Agency (NSA), <a href="#babilu_link-215">19–20</a> , <a href="#babilu_link-82">232</a></p>

<p class="noindent">Natural selection, <a href="#babilu_link-97">28–29</a> , <a href="#babilu_link-172">30</a> , <a href="#babilu_link-98">52</a></p>

<p class="noindent">as algorithm, <a href="#babilu_link-95">123–128</a></p>

<p class="noindent">Nature</p>

<p class="noindent">Bayesians and, <a href="#babilu_link-129">141</a></p>

<p class="noindent">evolutionaries and, <a href="#babilu_link-205">137–142</a></p>

<p class="noindent">symbolists and, <a href="#babilu_link-129">141</a></p>

<p class="noindent"><i>Nature</i> (journal), <a href="#babilu_link-92">26</a></p>

<p class="noindent">Nature <i>vs.</i> nurture debate, machine learning and, <a href="#babilu_link-297">29</a> , <a href="#babilu_link-205">137–139</a></p>

<p class="noindent">Neal, Radford, <a href="#babilu_link-116">170</a></p>

<p class="noindent">Nearest-neighbor algorithms, <a href="#babilu_link-85">24</a> , <a href="#babilu_link-68">178–186</a> , <a href="#babilu_link-305">202</a> , <a href="#babilu_link-19">306–307</a></p>

<p class="noindent">dimensionality and, <a href="#babilu_link-66">186–190</a></p>

<p class="noindent">Negative examples, <a href="#babilu_link-200">67</a></p>

<p class="noindent">Netflix, <a href="#babilu_link-53">12–13</a> , <a href="#babilu_link-179">183–184</a> , <a href="#babilu_link-184">215</a> , <a href="#babilu_link-230">237</a> , <a href="#babilu_link-48">266</a></p>

<p class="noindent">Netflix Prize, <a href="#babilu_link-106">238</a> , <a href="#babilu_link-273">292</a></p>

<p class="noindent">Netscape, <a href="#babilu_link-224">9</a></p>

<p class="noindent">NETtalk system, <a href="#babilu_link-104">112</a></p>

<p class="noindent">Network effect, <a href="#babilu_link-53">12</a> , <a href="#babilu_link-32">299</a></p>

<p class="noindent">Neumann, John von, <a href="#babilu_link-77">72</a> , <a href="#babilu_link-95">123</a></p>

<p class="noindent">Neural learning, fitness and, <a href="#babilu_link-107">138–139</a></p>

<p class="noindent">Neural networks, <a href="#babilu_link-266">99</a> , <a href="#babilu_link-248">100</a> , <a href="#babilu_link-104">112–114</a> , <a href="#babilu_link-44">122</a> , <a href="#babilu_link-306">204</a></p>

<p class="noindent">convolutional, <a href="#babilu_link-144">117–118</a> , <a href="#babilu_link-101">302–303</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-67">240</a> , <a href="#babilu_link-259">244</a> , <a href="#babilu_link-120">245</a></p>

<p class="noindent">reinforcement learning and, <a href="#babilu_link-105">222</a></p>

<p class="noindent">spin glasses and, <a href="#babilu_link-153">102–103</a></p>

<p class="noindent">Neural network structure, Baldwin effect and, <a href="#babilu_link-108">139</a></p>

<p class="noindent">Neurons</p>

<p class="noindent">action potentials and, <a href="#babilu_link-28">95–96</a> , <a href="#babilu_link-29">104–105</a></p>

<p class="noindent">Hebb’s rule and, <a href="#babilu_link-161">93–94</a></p>

<p class="noindent">McCulloch-Pitts model of, <a href="#babilu_link-72">96–97</a></p>

<p class="noindent">processing in brain and, <a href="#babilu_link-151">94–95</a></p>

<p class="noindent"><i>See also</i> Perceptron</p>

<p class="noindent">Neuroscience, Master Algorithm and, <a href="#babilu_link-92">26–28</a></p>

<p class="noindent">Newell, Allen, <a href="#babilu_link-303">224–226</a> , <a href="#babilu_link-101">302</a></p>

<p class="noindent">Newhouse, Neil, <a href="#babilu_link-260">17</a></p>

<p class="noindent">Newman, Mark, <a href="#babilu_link-34">160</a></p>

<p class="noindent">Newton, Isaac, <a href="#babilu_link-307">293</a></p>

<p class="noindent">attribute selection, <a href="#babilu_link-91">189</a></p>

<p class="noindent">laws of, <a href="#babilu_link-42">4</a> , <a href="#babilu_link-54">14</a> , <a href="#babilu_link-140">15</a> , <a href="#babilu_link-223">46</a> , <a href="#babilu_link-218">235</a></p>

<p class="noindent">rules of induction, <a href="#babilu_link-203">65–66</a> , <a href="#babilu_link-236">81</a> , <a href="#babilu_link-283">82</a></p>

<p class="noindent">Newtonian determinism, Laplace and, <a href="#babilu_link-169">145</a></p>

<p class="noindent">Newton phase of science, <a href="#babilu_link-149">39–400</a></p>

<p class="noindent"><i>New York Times</i> (newspaper), <a href="#babilu_link-100">115</a> , <a href="#babilu_link-144">117</a></p>

<p class="noindent">Ng, Andrew, <a href="#babilu_link-144">117</a> , <a href="#babilu_link-290">297</a></p>

<p class="noindent">Nietzche, Friedrich, <a href="#babilu_link-68">178</a></p>

<p class="noindent">NIPS. <i>See</i> Conference on Neural Information Processing Systems ((NIPS)</p>

<p class="noindent">“No free lunch” theorem, <a href="#babilu_link-280">59</a> , <a href="#babilu_link-275">62–65</a> , <a href="#babilu_link-221">70–71</a></p>

<p class="noindent"><a id="babilu_link-487"></a> “No Hands Across America,” <a href="#babilu_link-156">113</a></p>

<p class="noindent">Noise, <a href="#babilu_link-189">73</a> , <a href="#babilu_link-206">91</a> , <a href="#babilu_link-124">155</a></p>

<p class="noindent">Nonlinear dimensionality reduction, <a href="#babilu_link-184">215–217</a></p>

<p class="noindent">Nonlinear models, <a href="#babilu_link-140">15</a> , <a href="#babilu_link-104">112–114</a></p>

<p class="noindent">Nonuniformity, <a href="#babilu_link-91">189–190</a></p>

<p class="noindent">NOR gate, <a href="#babilu_link-81">49</a></p>

<p class="noindent">Normal distributions, <a href="#babilu_link-254">187–188</a> , <a href="#babilu_link-182">210</a></p>

<p class="noindent">Normative theories, descriptive theories <i>vs.,</i> <a href="#babilu_link-129">141–142</a> , <a href="#babilu_link-110">304</a></p>

<p class="noindent">Norvig, Peter, <a href="#babilu_link-202">152</a></p>

<p class="noindent">NOT gate, <a href="#babilu_link-72">96</a></p>

<p class="noindent">NOT operation, <a href="#babilu_link-73">2</a></p>

<p class="noindent">Nowlan, Steven, <a href="#babilu_link-108">139</a></p>

<p class="noindent">NP-completeness, <a href="#babilu_link-199">32–34</a> , <a href="#babilu_link-153">102</a></p>

<p class="noindent">NSA. <i>See</i> National Security Agency (NSA)</p>

<p class="noindent">Nurture, nature <i>vs.,</i> <a href="#babilu_link-297">29</a> , <a href="#babilu_link-205">137–139</a></p>

<p class="noindent">Obama, Barack, <a href="#babilu_link-260">17</a></p>

<p class="noindent">Objective reality, Bayesians and, <a href="#babilu_link-188">167</a></p>

<p class="noindent">Occam’s razor, <a href="#babilu_link-235">77–78</a> , <a href="#babilu_link-213">196</a> , <a href="#babilu_link-191">300–301</a></p>

<p class="noindent">OkCupid, <a href="#babilu_link-239">265</a> , <a href="#babilu_link-293">269</a> , <a href="#babilu_link-166">310</a></p>

<p class="noindent">O’Keefe, Kevin, <a href="#babilu_link-96">206</a></p>

<p class="noindent"><i>On Intelligence</i> (Hawkins), <a href="#babilu_link-97">28</a> , <a href="#babilu_link-150">118</a></p>

<p class="noindent">Online analytical processing, <a href="#babilu_link-33">8</a></p>

<p class="noindent">Online dating, <a href="#babilu_link-239">265–266</a> , <a href="#babilu_link-293">269</a> , <a href="#babilu_link-166">310</a></p>

<p class="noindent">Open-source movement, <a href="#babilu_link-195">45</a> , <a href="#babilu_link-216">279</a> , <a href="#babilu_link-273">292</a></p>

<p class="noindent">Optimization, <a href="#babilu_link-172">30–31</a> , <a href="#babilu_link-296">33</a> , <a href="#babilu_link-264">109</a> , <a href="#babilu_link-131">135</a> , <a href="#babilu_link-246">239</a> , <a href="#babilu_link-25">241</a> , <a href="#babilu_link-245">283</a></p>

<p class="noindent">constrained, <a href="#babilu_link-207">193–195</a></p>

<p class="noindent">O’Reilly, Tim, <a href="#babilu_link-224">9</a></p>

<p class="noindent"><i>The Organization of Behavior</i> (Hebb), <a href="#babilu_link-161">93</a></p>

<p class="noindent">OR gate, <a href="#babilu_link-72">96</a></p>

<p class="noindent"><i>The Origin of Species</i> (Darwin), <a href="#babilu_link-97">28</a> , <a href="#babilu_link-95">123</a></p>

<p class="noindent">OR operation, <a href="#babilu_link-73">2</a></p>

<p class="noindent">Overfitting, <a href="#babilu_link-280">59</a> , <a href="#babilu_link-221">70–75</a> , <a href="#babilu_link-279">126</a> , <a href="#babilu_link-270">169</a> , <a href="#babilu_link-136">301</a></p>

<p class="noindent">avoiding, <a href="#babilu_link-291">76–77</a></p>

<p class="noindent">hypotheses and, <a href="#babilu_link-189">73–75</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-26">243</a></p>

<p class="noindent">nearest-neighbor algorithm and, <a href="#babilu_link-179">183</a></p>

<p class="noindent">noise and, <a href="#babilu_link-189">73</a></p>

<p class="noindent">singularity and, <a href="#babilu_link-281">287</a></p>

<p class="noindent">support vector machines and, <a href="#babilu_link-213">196</a></p>

<p class="noindent">P = NP question, <a href="#babilu_link-296">33–34</a></p>

<p class="noindent">PAC learning, <a href="#babilu_link-204">74–75</a></p>

<p class="noindent">Page, Larry, <a href="#babilu_link-154">55</a> , <a href="#babilu_link-123">154</a> , <a href="#babilu_link-21">227</a></p>

<p class="noindent">PageRank algorithm, <a href="#babilu_link-123">154</a> , <a href="#babilu_link-118">305</a></p>

<p class="noindent">PAL (Personalized Assistant that Learns) project, <a href="#babilu_link-37">255</a></p>

<p class="noindent">Pandora, <a href="#babilu_link-265">171</a></p>

<p class="noindent">Papadimitriou, Christos, <a href="#babilu_link-131">135</a></p>

<p class="noindent">Papert, Seymour, <a href="#babilu_link-248">100–101</a> , <a href="#babilu_link-153">102</a> , <a href="#babilu_link-294">110</a> , <a href="#babilu_link-104">112</a> , <a href="#babilu_link-156">113</a></p>

<p class="noindent">Parallax effect, <a href="#babilu_link-281">287</a></p>

<p class="noindent">Parallel processing, <a href="#babilu_link-178">257–258</a></p>

<p class="noindent">Parasites, <a href="#babilu_link-131">135</a></p>

<p class="noindent">Pascal, Blaise, <a href="#babilu_link-186">63</a></p>

<p class="noindent">Pattern recognition, <a href="#babilu_link-33">8</a> . <i>See also</i> Machine learning</p>

<p class="noindent">Patterns in data, <a href="#babilu_link-221">70–75</a></p>

<p class="noindent">PCA. <i>See</i> Principal-component analysis (PCA)</p>

<p class="noindent">Pearl, Judea, <a href="#babilu_link-117">156–157</a> , <a href="#babilu_link-244">163</a> , <a href="#babilu_link-118">305</a></p>

<p class="noindent"><i>Pensées</i> (Pascal), <a href="#babilu_link-186">63</a></p>

<p class="noindent">Pentagon, <a href="#babilu_link-215">19</a> , <a href="#babilu_link-217">37</a></p>

<p class="noindent">Perceptron, <a href="#babilu_link-72">96–101</a> , <a href="#babilu_link-304">108–111</a> , <a href="#babilu_link-202">152</a> , <a href="#babilu_link-239">265</a> . <i>See also</i> Multilayer perceptron</p>

<p class="noindent"><i>Perceptrons</i> (Minsky &amp; Papert), <a href="#babilu_link-248">100–101</a> , <a href="#babilu_link-156">113</a></p>

<p class="noindent">Personal data</p>

<p class="noindent">ethical responsibility to share some types of, <a href="#babilu_link-20">272–273</a></p>

<p class="noindent">as model, <a href="#babilu_link-308">267–270</a></p>

<p class="noindent">professional management of, <a href="#babilu_link-89">273–276</a></p>

<p class="noindent">sharing or not, <a href="#babilu_link-225">270–276</a></p>

<p class="noindent">types of, <a href="#babilu_link-51">271–273</a></p>

<p class="noindent">value of, <a href="#babilu_link-155">274</a></p>

<p class="noindent">Phase transitions, <a href="#babilu_link-152">105–107</a> , <a href="#babilu_link-272">288</a></p>

<p class="noindent">Physical symbol system hypothesis, <a href="#babilu_link-80">89</a></p>

<p class="noindent">Physics, <a href="#babilu_link-297">29–31</a> , <a href="#babilu_link-223">46–47</a> , <a href="#babilu_link-243">50</a></p>

<p class="noindent">Pitts, Walter, <a href="#babilu_link-72">96</a></p>

<p class="noindent">Planetary-scale machine learning, <a href="#babilu_link-269">256–259</a></p>

<p class="noindent">Planets, computing duration of year of, <a href="#babilu_link-148">131–133</a></p>

<p class="noindent">Plato, <a href="#babilu_link-75">58</a></p>

<p class="noindent">Point mutation, <a href="#babilu_link-212">124</a></p>

<p class="noindent">Poisson’s equation, <a href="#babilu_link-172">30</a></p>

<p class="noindent"><a id="babilu_link-488"></a> Policing, predictive, <a href="#babilu_link-256">20</a></p>

<p class="noindent">Politics, machine learning and, <a href="#babilu_link-30">16–19</a> , <a href="#babilu_link-32">299</a></p>

<p class="noindent">Positive examples, <a href="#babilu_link-200">67</a> , <a href="#babilu_link-132">69</a></p>

<p class="noindent">Posterior probability, <a href="#babilu_link-309">146–147</a> , <a href="#babilu_link-25">241</a> , <a href="#babilu_link-127">242</a> , <a href="#babilu_link-26">243</a> , <a href="#babilu_link-164">249</a></p>

<p class="noindent">Poverty of the stimulus argument, <a href="#babilu_link-174">36–37</a></p>

<p class="noindent">Power law of practice, <a href="#babilu_link-303">224–225</a></p>

<p class="noindent"><i>The Power of Habit</i> (Duhigg), <a href="#babilu_link-175">223</a></p>

<p class="noindent">Practice</p>

<p class="noindent">learning and, <a href="#babilu_link-175">223</a></p>

<p class="noindent">power law of, <a href="#babilu_link-303">224–225</a></p>

<p class="noindent">Predictive analytics, <a href="#babilu_link-33">8</a> . <i>See also</i> Machine learning</p>

<p class="noindent">Predictive policing, <a href="#babilu_link-256">20</a></p>

<p class="noindent">Presidential election, machine learning and 2012, <a href="#babilu_link-30">16–19</a></p>

<p class="noindent">Principal-component analysis (PCA), <a href="#babilu_link-50">211–217</a> , <a href="#babilu_link-37">255</a> , <a href="#babilu_link-233">308</a></p>

<p class="noindent"><i>Principia</i> (Newton), <a href="#babilu_link-203">65</a></p>

<p class="noindent">Principal components of the data, <a href="#babilu_link-219">214</a></p>

<p class="noindent">Principle of association, <a href="#babilu_link-161">93</a></p>

<p class="noindent">Principle of indifference, <a href="#babilu_link-169">145</a></p>

<p class="noindent">Principle of insufficient reason, <a href="#babilu_link-169">145</a></p>

<p class="noindent"><i>Principles of Psychology</i> (James), <a href="#babilu_link-161">93</a></p>

<p class="noindent">Prior probability, <a href="#babilu_link-309">146–147</a></p>

<p class="noindent">Privacy, personal data and, <a href="#babilu_link-253">275</a></p>

<p class="noindent">Probabilistic inference, <a href="#babilu_link-98">52</a> , <a href="#babilu_link-62">53</a> , <a href="#babilu_link-119">161–166</a> , <a href="#babilu_link-127">242</a> , <a href="#babilu_link-269">256</a> , <a href="#babilu_link-118">305</a></p>

<p class="noindent">Probability</p>

<p class="noindent">applied to poetry, <a href="#babilu_link-126">153–154</a></p>

<p class="noindent">Bayesian networks and, <a href="#babilu_link-117">156–158</a></p>

<p class="noindent">Bayesians and meaning of, <a href="#babilu_link-128">149</a> , <a href="#babilu_link-270">169–170</a></p>

<p class="noindent">Bayes’ theorem and, <a href="#babilu_link-169">145–149</a></p>

<p class="noindent">estimating, <a href="#babilu_link-299">148–149</a></p>

<p class="noindent">frequentist interpretation of, <a href="#babilu_link-128">149</a></p>

<p class="noindent">logic and, <a href="#babilu_link-125">173–175</a> , <a href="#babilu_link-120">245–246</a> , <a href="#babilu_link-19">306</a> , <a href="#babilu_link-22">309</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-120">245–246</a></p>

<p class="noindent">posterior, <a href="#babilu_link-309">146–147</a></p>

<p class="noindent">prior, <a href="#babilu_link-309">146–147</a></p>

<p class="noindent">Probability theory, Laplace and, <a href="#babilu_link-169">145</a></p>

<p class="noindent"><i>Probably Approximately Correct</i> (Valiant), <a href="#babilu_link-23">75</a></p>

<p class="noindent">Problem solving</p>

<p class="noindent">learning as, <a href="#babilu_link-185">226</a></p>

<p class="noindent">theory of, <a href="#babilu_link-302">225</a></p>

<p class="noindent">Procedures, learners and, <a href="#babilu_link-33">8</a></p>

<p class="noindent">Programming by example, <a href="#babilu_link-43">298</a></p>

<p class="noindent">Programming, machine learning <i>vs.,</i> <a href="#babilu_link-137">7–8</a></p>

<p class="noindent">Programs, <a href="#babilu_link-42">4</a></p>

<p class="noindent">computers writing own, <a href="#babilu_link-198">6</a></p>

<p class="noindent">survival of the fittest, <a href="#babilu_link-148">131–134</a></p>

<p class="noindent">Program trees, <a href="#babilu_link-148">131–133</a></p>

<p class="noindent">Prolog programming language, <a href="#babilu_link-102">252–253</a></p>

<p class="noindent">Punctuated equilibria, <a href="#babilu_link-157">127</a> , <a href="#babilu_link-208">303</a></p>

<p class="noindent">Pushkin, Alexander, <a href="#babilu_link-126">153</a></p>

<p class="noindent">Python, <a href="#babilu_link-42">4</a></p>

<p class="noindent">Quinlan, J. Ross, <a href="#babilu_link-277">88</a> , <a href="#babilu_link-282">90</a></p>

<p class="noindent">Random forest, <a href="#babilu_link-106">238</a></p>

<p class="noindent">Rationalists, <a href="#babilu_link-242">57–58</a></p>

<p class="noindent">Reasoning, <a href="#babilu_link-242">57–58</a></p>

<p class="noindent">analogical, <a href="#babilu_link-59">179</a> , <a href="#babilu_link-60">197</a></p>

<p class="noindent">case-based, <a href="#babilu_link-60">197–200</a> , <a href="#babilu_link-168">307</a></p>

<p class="noindent">transistors and, <a href="#babilu_link-73">2</a></p>

<p class="noindent">Recommendation systems, <a href="#babilu_link-53">12–13</a> , <a href="#babilu_link-55">42</a> , <a href="#babilu_link-179">183–185</a> , <a href="#babilu_link-57">268</a> , <a href="#babilu_link-58">286</a></p>

<p class="noindent">Redistribution of income, <a href="#babilu_link-274">278–279</a></p>

<p class="noindent">Red Queen hypothesis, <a href="#babilu_link-131">135</a></p>

<p class="noindent">Reinforcement learning, <a href="#babilu_link-238">218–223</a> , <a href="#babilu_link-185">226–227</a> , <a href="#babilu_link-176">254</a> , <a href="#babilu_link-233">308</a></p>

<p class="noindent">Relational databases, <a href="#babilu_link-197">236</a></p>

<p class="noindent">Relational learning, <a href="#babilu_link-21">227–233</a> , <a href="#babilu_link-176">254</a></p>

<p class="noindent">Representation</p>

<p class="noindent">learning algorithms and, <a href="#babilu_link-245">283</a></p>

<p class="noindent">Markov logic networks and, <a href="#babilu_link-164">249</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-246">239–240</a> , <a href="#babilu_link-25">241</a> , <a href="#babilu_link-26">243</a></p>

<p class="noindent">Retailers, sets of rules and stocking, <a href="#babilu_link-132">69–70</a></p>

<p class="noindent">Rewards of states, <a href="#babilu_link-238">218–222</a></p>

<p class="noindent">Richardson, Matt, <a href="#babilu_link-121">231</a> , <a href="#babilu_link-36">246</a></p>

<p class="noindent">Ridiculograms, <a href="#babilu_link-34">160</a></p>

<p class="noindent">Ridley, Matt, <a href="#babilu_link-131">135</a></p>

<p class="noindent">RISE algorithm, <a href="#babilu_link-214">201–202</a> , <a href="#babilu_link-233">308</a></p>

<p class="noindent">Robotic Park, <a href="#babilu_link-209">121</a></p>

<p class="noindent"><a id="babilu_link-489"></a> Robot rights, <a href="#babilu_link-241">285</a></p>

<p class="noindent">Robots</p>

<p class="noindent">empathy-eliciting, <a href="#babilu_link-241">285</a></p>

<p class="noindent">evolution of, <a href="#babilu_link-209">121–22</a> , <a href="#babilu_link-205">137</a> , <a href="#babilu_link-208">303</a></p>

<p class="noindent">genetic programming and, <a href="#babilu_link-240">133</a></p>

<p class="noindent">housebots, <a href="#babilu_link-55">42–43</a> , <a href="#babilu_link-238">218</a> , <a href="#babilu_link-37">255</a></p>

<p class="noindent">military, <a href="#babilu_link-215">19–21</a> , <a href="#babilu_link-216">279–282</a> , <a href="#babilu_link-32">299</a> , <a href="#babilu_link-166">310</a></p>

<p class="noindent">probabilistic inference and, <a href="#babilu_link-114">166</a></p>

<p class="noindent">Romney, Mitt, <a href="#babilu_link-260">17</a></p>

<p class="noindent">Rosenberg, Charles, <a href="#babilu_link-104">112</a></p>

<p class="noindent">Rosenblatt, Frank, <a href="#babilu_link-310">97</a> , <a href="#babilu_link-266">99</a> , <a href="#babilu_link-248">100</a> , <a href="#babilu_link-156">113</a></p>

<p class="noindent">Rosenbloom, Paul, <a href="#babilu_link-303">224–226</a></p>

<p class="noindent">Rove, Karl, <a href="#babilu_link-260">17</a></p>

<p class="noindent">Rubin, Donald, <a href="#babilu_link-177">209</a></p>

<p class="noindent">Rule-based learning, <a href="#babilu_link-132">69–70</a> , <a href="#babilu_link-214">201–202</a></p>

<p class="noindent">Rule mining, <a href="#babilu_link-136">301</a></p>

<p class="noindent">Rule of succession, <a href="#babilu_link-169">145–146</a></p>

<p class="noindent">Rules</p>

<p class="noindent">filtering spam, <a href="#babilu_link-251">125–127</a></p>

<p class="noindent">induction of, <a href="#babilu_link-236">81–82</a></p>

<p class="noindent">instances and, <a href="#babilu_link-214">201</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-259">244</a></p>

<p class="noindent">sets of, <a href="#babilu_link-201">68–71</a> , <a href="#babilu_link-282">90</a> , <a href="#babilu_link-206">91</a></p>

<p class="noindent"><i>See also If… then…</i> rules</p>

<p class="noindent">Rumelhart, David, <a href="#babilu_link-104">112</a></p>

<p class="noindent">Russell, Bertrand, <a href="#babilu_link-88">61</a></p>

<p class="noindent">Rutherford, Ernest, <a href="#babilu_link-197">236</a></p>

<p class="noindent">Safeway, <a href="#babilu_link-20">272</a></p>

<p class="noindent">Saffo, Paul, <a href="#babilu_link-234">106</a></p>

<p class="noindent">Sahami, Mehran, <a href="#babilu_link-147">151–152</a></p>

<p class="noindent">Saint Paul, <a href="#babilu_link-113">144</a></p>

<p class="noindent">Sampling principle, <a href="#babilu_link-139">258</a></p>

<p class="noindent">Samuel, Arthur, <a href="#babilu_link-143">219</a></p>

<p class="noindent">Sander, Emmanuel, <a href="#babilu_link-70">200</a></p>

<p class="noindent">Satisfiability of a logical formula, <a href="#babilu_link-296">33–34</a> , <a href="#babilu_link-234">106</a></p>

<p class="noindent">Schapire, Rob, <a href="#babilu_link-106">238</a></p>

<p class="noindent">Schemas, <a href="#babilu_link-111">129</a></p>

<p class="noindent">Science</p>

<p class="noindent">analogy and, <a href="#babilu_link-68">178</a></p>

<p class="noindent">effect of machine learning on jobs in, <a href="#babilu_link-274">278</a></p>

<p class="noindent">frequentism and, <a href="#babilu_link-188">167</a></p>

<p class="noindent">machine learning and, <a href="#babilu_link-222">13–16</a> , <a href="#babilu_link-218">235–236</a> , <a href="#babilu_link-32">299</a></p>

<p class="noindent">phases of, <a href="#babilu_link-149">39–40</a></p>

<p class="noindent"><i>The Sciences of the Artificial</i> (Simon), <a href="#babilu_link-134">41</a></p>

<p class="noindent">S curves, <a href="#babilu_link-29">104–107</a> , <a href="#babilu_link-295">111</a> , <a href="#babilu_link-164">249</a> , <a href="#babilu_link-102">252</a> , <a href="#babilu_link-281">287</a></p>

<p class="noindent">Search engines, <a href="#babilu_link-224">9</a> , <a href="#babilu_link-202">152</a> , <a href="#babilu_link-21">227–228</a></p>

<p class="noindent">Sejnowski, Terry, <a href="#babilu_link-27">103</a> , <a href="#babilu_link-104">112</a></p>

<p class="noindent">Selective breeding, genetic algorithms and, <a href="#babilu_link-95">123–124</a></p>

<p class="noindent">Self-driving cars. <i>See</i> Driverless cars</p>

<p class="noindent">Self-organizing systems, <a href="#babilu_link-33">8</a> . <i>See also</i> Machine learning</p>

<p class="noindent">Semantic network, <a href="#babilu_link-37">255</a> , <a href="#babilu_link-22">309</a></p>

<p class="noindent">Sets of classes, <a href="#babilu_link-39">86–87</a></p>

<p class="noindent">Sets of concepts, <a href="#babilu_link-39">86–87</a></p>

<p class="noindent">Sets of rules, <a href="#babilu_link-201">68–70</a> , <a href="#babilu_link-282">90</a> , <a href="#babilu_link-206">91</a></p>

<p class="noindent">power of, <a href="#babilu_link-221">70–71</a></p>

<p class="noindent">Sex, <a href="#babilu_link-212">124–126</a> , <a href="#babilu_link-158">134–137</a></p>

<p class="noindent">Shannon, Claude, <a href="#babilu_link-41">1–2</a></p>

<p class="noindent">Shavlik, Jude, <a href="#babilu_link-291">76</a></p>

<p class="noindent">Sigmoid curve. <i>See</i> S curves</p>

<p class="noindent">Significance tests, <a href="#babilu_link-24">87</a></p>

<p class="noindent">Silver, Nate, <a href="#babilu_link-260">17</a> , <a href="#babilu_link-106">238</a></p>

<p class="noindent">Similarity, <a href="#babilu_link-68">178</a> , <a href="#babilu_link-59">179</a></p>

<p class="noindent">Similarity measures, <a href="#babilu_link-286">192</a> , <a href="#babilu_link-60">197–200</a> , <a href="#babilu_link-183">207</a></p>

<p class="noindent">Simon, Herbert, <a href="#babilu_link-134">41</a> , <a href="#babilu_link-302">225–226</a> , <a href="#babilu_link-101">302</a></p>

<p class="noindent">Simultaneous localization and mapping (SLAM), <a href="#babilu_link-114">166</a></p>

<p class="noindent">Singularity, <a href="#babilu_link-97">28</a> , <a href="#babilu_link-66">186</a> , <a href="#babilu_link-58">286–289</a> , <a href="#babilu_link-247">311</a></p>

<p class="noindent"><i>The Singularity Is Near</i> (Kurzweil), <a href="#babilu_link-58">286</a></p>

<p class="noindent">Siri, <a href="#babilu_link-217">37</a> , <a href="#babilu_link-124">155</a> , <a href="#babilu_link-119">161–162</a> , <a href="#babilu_link-227">165</a> , <a href="#babilu_link-64">172</a> , <a href="#babilu_link-37">255</a></p>

<p class="noindent">SKICAT (sky image cataloging and analysis tool), <a href="#babilu_link-140">15</a> , <a href="#babilu_link-32">299</a></p>

<p class="noindent">Skills, learners and, <a href="#babilu_link-33">8</a> , <a href="#babilu_link-284">217–227</a></p>

<p class="noindent">Skynet, <a href="#babilu_link-78">282–286</a></p>

<p class="noindent">Sloan Digital Sky Survey, <a href="#babilu_link-140">15</a></p>

<p class="noindent">Smith, Adam, <a href="#babilu_link-75">58</a></p>

<p class="noindent">Snow, John, <a href="#babilu_link-179">183</a></p>

<p class="noindent">Soar, chunking in, <a href="#babilu_link-185">226</a></p>

<p class="noindent">Social networks, information propagation in, <a href="#babilu_link-121">231</a></p>

<p class="noindent"><i>The Society of Mind</i> (Minsky), <a href="#babilu_link-79">35</a></p>

<p class="noindent">Space complexity, <a href="#babilu_link-40">5</a></p>

<p class="noindent"><a id="babilu_link-490"></a> Spam filters, <a href="#babilu_link-292">23–24</a> , <a href="#babilu_link-147">151–152</a> , <a href="#babilu_link-301">168–169</a> , <a href="#babilu_link-265">171</a></p>

<p class="noindent">Sparse autoencoder, <a href="#babilu_link-144">117</a></p>

<p class="noindent">Speech recognition, <a href="#babilu_link-124">155</a> , <a href="#babilu_link-116">170–172</a> , <a href="#babilu_link-190">276</a> , <a href="#babilu_link-19">306</a></p>

<p class="noindent">Speed, learning algorithms and, <a href="#babilu_link-108">139–142</a></p>

<p class="noindent">Spin glasses, brain and, <a href="#babilu_link-153">102–103</a></p>

<p class="noindent">Spinoza, Baruch, <a href="#babilu_link-75">58</a></p>

<p class="noindent">Squared error, <a href="#babilu_link-25">241</a> , <a href="#babilu_link-26">243</a></p>

<p class="noindent">Stacked autoencoder, <a href="#babilu_link-144">117</a></p>

<p class="noindent">Stacking, <a href="#babilu_link-106">238</a> , <a href="#babilu_link-37">255</a> , <a href="#babilu_link-22">309</a></p>

<p class="noindent">States, value of, <a href="#babilu_link-143">219–221</a></p>

<p class="noindent">Statistical algorithms, <a href="#babilu_link-33">8</a></p>

<p class="noindent">Statistical learning, <a href="#babilu_link-217">37</a> , <a href="#babilu_link-289">228</a> , <a href="#babilu_link-290">297</a> , <a href="#babilu_link-191">300</a> , <a href="#babilu_link-168">307</a></p>

<p class="noindent">Statistical modeling, <a href="#babilu_link-33">8</a> . <i>See also</i> Machine learning</p>

<p class="noindent">Statistical relational learning, <a href="#babilu_link-21">227–233</a> , <a href="#babilu_link-176">254</a> , <a href="#babilu_link-22">309</a></p>

<p class="noindent">Statistical significance tests, <a href="#babilu_link-291">76–77</a></p>

<p class="noindent">Statistics, Master Algorithm and, <a href="#babilu_link-130">31–32</a></p>

<p class="noindent">Stock market predictions, neural networks and, <a href="#babilu_link-104">112</a> , <a href="#babilu_link-101">302</a></p>

<p class="noindent">Stream mining, <a href="#babilu_link-139">258</a></p>

<p class="noindent">String theory, <a href="#babilu_link-223">46–47</a></p>

<p class="noindent">Structure mapping, <a href="#babilu_link-45">199–200</a> , <a href="#babilu_link-176">254</a> , <a href="#babilu_link-168">307</a></p>

<p class="noindent">Succession, rule of, <a href="#babilu_link-169">145–146</a></p>

<p class="noindent"><i>The Sun Also Rises</i> (Hemingway), <a href="#babilu_link-234">106</a></p>

<p class="noindent">Supervised learning, <a href="#babilu_link-177">209</a> , <a href="#babilu_link-219">214</a> , <a href="#babilu_link-133">220</a> , <a href="#babilu_link-105">222</a> , <a href="#babilu_link-185">226</a></p>

<p class="noindent">Support vector machines (SVMs), <a href="#babilu_link-62">53</a> , <a href="#babilu_link-59">179</a> , <a href="#babilu_link-69">190–196</a> , <a href="#babilu_link-67">240</a> , <a href="#babilu_link-127">242</a> , <a href="#babilu_link-259">244</a> , <a href="#babilu_link-120">245</a> , <a href="#babilu_link-176">254</a> , <a href="#babilu_link-168">307</a></p>

<p class="noindent">Support vectors, <a href="#babilu_link-255">191–193</a> , <a href="#babilu_link-213">196</a> , <a href="#babilu_link-26">243–244</a></p>

<p class="noindent"><i>Surfaces and Essences</i> (Hofstadter &amp; Sander), <a href="#babilu_link-70">200</a></p>

<p class="noindent">Survival of the fittest programs, <a href="#babilu_link-148">131–134</a></p>

<p class="noindent">Sutton, Rich, <a href="#babilu_link-112">221</a> , <a href="#babilu_link-175">223</a></p>

<p class="noindent">SVMs. <i>See</i> Support vector machines (SVMs)</p>

<p class="noindent">Symbolists/symbolism, <a href="#babilu_link-61">51</a> , <a href="#babilu_link-98">52</a> , <a href="#babilu_link-63">54</a> , <a href="#babilu_link-242">57–91</a></p>

<p class="noindent">accuracy and, <a href="#babilu_link-23">75–79</a></p>

<p class="noindent">Alchemy and, <a href="#babilu_link-276">251–252</a></p>

<p class="noindent">analogizers <i>vs.,</i> <a href="#babilu_link-70">200–202</a></p>

<p class="noindent">assumptions and, <a href="#babilu_link-76">64</a></p>

<p class="noindent">conjunctive concepts, <a href="#babilu_link-203">65–68</a></p>

<p class="noindent">connectionists <i>vs.,</i> <a href="#babilu_link-206">91</a> , <a href="#babilu_link-151">94–95</a></p>

<p class="noindent">decision tree induction, <a href="#babilu_link-228">85–89</a></p>

<p class="noindent">further reading, <a href="#babilu_link-191">300–302</a></p>

<p class="noindent">hill climbing and, <a href="#babilu_link-131">135</a></p>

<p class="noindent">Hume and, <a href="#babilu_link-75">58–59</a></p>

<p class="noindent">induction and, <a href="#babilu_link-159">80–83</a></p>

<p class="noindent">intelligence and, <a href="#babilu_link-98">52</a> , <a href="#babilu_link-80">89</a></p>

<p class="noindent">inverse deduction and, <a href="#babilu_link-98">52</a> , <a href="#babilu_link-283">82–85</a> , <a href="#babilu_link-206">91</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-67">240–241</a> , <a href="#babilu_link-127">242–243</a></p>

<p class="noindent">nature and, <a href="#babilu_link-129">141</a></p>

<p class="noindent">“no free lunch” theorem, <a href="#babilu_link-275">62–65</a></p>

<p class="noindent">overfitting, <a href="#babilu_link-221">70–75</a></p>

<p class="noindent">probability and, <a href="#babilu_link-125">173</a></p>

<p class="noindent">problem of induction, <a href="#babilu_link-280">59–62</a></p>

<p class="noindent">sets of rules, <a href="#babilu_link-201">68–70</a></p>

<p class="noindent">Taleb, Nassim, <a href="#babilu_link-141">38</a> , <a href="#babilu_link-142">158</a></p>

<p class="noindent">Tamagotchi, <a href="#babilu_link-241">285</a></p>

<p class="noindent">Technology</p>

<p class="noindent">machine learning as, <a href="#babilu_link-197">236–237</a></p>

<p class="noindent">sex and evolution of, <a href="#babilu_link-145">136–137</a></p>

<p class="noindent">trends in, <a href="#babilu_link-46">21–22</a></p>

<p class="noindent">Terrorists, data mining to catch, <a href="#babilu_link-82">232–233</a></p>

<p class="noindent">Test set accuracy, <a href="#babilu_link-23">75–76</a> , <a href="#babilu_link-135">78–79</a></p>

<p class="noindent">Tetris, <a href="#babilu_link-199">32–33</a></p>

<p class="noindent">Text classification, support vector machines and, <a href="#babilu_link-231">195–196</a></p>

<p class="noindent">Thalamus, <a href="#babilu_link-171">27</a></p>

<p class="noindent">Theory, defined, <a href="#babilu_link-223">46</a></p>

<p class="noindent">Theory of cognition, <a href="#babilu_link-185">226</a></p>

<p class="noindent">Theory of everything, Master Algorithm and, <a href="#babilu_link-223">46–48</a></p>

<p class="noindent">Theory of intelligence, <a href="#babilu_link-79">35</a></p>

<p class="noindent">Theory of problem solving, <a href="#babilu_link-302">225</a></p>

<p class="noindent"><i>Thinking, Fast and Slow</i> (Kahneman), <a href="#babilu_link-129">141</a></p>

<p class="noindent">Thorndike, Edward, <a href="#babilu_link-238">218</a></p>

<p class="noindent"><i>Through the Looking Glass</i> (Carroll), <a href="#babilu_link-131">135</a></p>

<p class="noindent">Tic-tac-toe, algorithm for, <a href="#babilu_link-311">3–4</a></p>

<p class="noindent">Time, as principal component of memory, <a href="#babilu_link-284">217</a></p>

<p class="noindent"><a id="babilu_link-491"></a> Time complexity, <a href="#babilu_link-40">5</a></p>

<p class="noindent"><i>The Tipping Point</i> (Gladwell), <a href="#babilu_link-152">105–106</a></p>

<p class="noindent">Tolstoy, Leo, <a href="#babilu_link-74">66</a></p>

<p class="noindent">Training set accuracy, <a href="#babilu_link-23">75–76</a> , <a href="#babilu_link-312">79</a></p>

<p class="noindent">Transistors, <a href="#babilu_link-41">1–2</a></p>

<p class="noindent">Treaty banning robot warfare, <a href="#babilu_link-237">281</a></p>

<p class="noindent">Truth, Bayesians and, <a href="#babilu_link-188">167</a></p>

<p class="noindent">Turing, Alan, <a href="#babilu_link-196">34</a> , <a href="#babilu_link-79">35</a> , <a href="#babilu_link-58">286</a></p>

<p class="noindent">Turing Award, <a href="#babilu_link-23">75</a> , <a href="#babilu_link-117">156</a></p>

<p class="noindent">Turing machine, <a href="#babilu_link-196">34</a> , <a href="#babilu_link-38">250</a></p>

<p class="noindent">Turing point, Singularity and, <a href="#babilu_link-58">286</a> , <a href="#babilu_link-272">288</a></p>

<p class="noindent">Turing test, <a href="#babilu_link-240">133–134</a></p>

<p class="noindent">“Turning the Bayesian crank,” <a href="#babilu_link-128">149</a></p>

<p class="noindent">UCI repository of data sets, <a href="#babilu_link-291">76</a></p>

<p class="noindent">Uncertainty, <a href="#babilu_link-98">52</a> , <a href="#babilu_link-282">90</a> , <a href="#babilu_link-122">143–175</a></p>

<p class="noindent">Unconstrained optimization, <a href="#babilu_link-207">193–194</a> . <i>See also</i> Gradient descent</p>

<p class="noindent">Underwood, Ben, <a href="#babilu_link-92">26</a> , <a href="#babilu_link-32">299</a></p>

<p class="noindent">Unemployment, machine learning and, <a href="#babilu_link-274">278–279</a></p>

<p class="noindent">Unified inference algorithm, <a href="#babilu_link-269">256</a></p>

<p class="noindent">United Nations, <a href="#babilu_link-237">281</a></p>

<p class="noindent">US Patent and Trademark Office, <a href="#babilu_link-240">133</a></p>

<p class="noindent">Universal learning algorithm. <i>See</i> Master Algorithm</p>

<p class="noindent">Universal Turing machine, <a href="#babilu_link-196">34</a></p>

<p class="noindent">Uplift modeling, <a href="#babilu_link-22">309</a></p>

<p class="noindent">Valiant, Leslie, <a href="#babilu_link-23">75</a></p>

<p class="noindent">Value of states, <a href="#babilu_link-143">219–221</a></p>

<p class="noindent">Vapnik, Vladimir, <a href="#babilu_link-69">190</a> , <a href="#babilu_link-286">192</a> , <a href="#babilu_link-207">193</a> , <a href="#babilu_link-231">195</a></p>

<p class="noindent">Variance, <a href="#babilu_link-135">78–79</a></p>

<p class="noindent">Variational inference, <a href="#babilu_link-285">164</a> , <a href="#babilu_link-116">170</a></p>

<p class="noindent">Venter, Craig, <a href="#babilu_link-261">289</a></p>

<p class="noindent">Vinge, Vernor, <a href="#babilu_link-58">286</a></p>

<p class="noindent">Virtual machines, <a href="#babilu_link-197">236</a></p>

<p class="noindent">Visual cortex, <a href="#babilu_link-92">26</a></p>

<p class="noindent">Viterbi algorithm, <a href="#babilu_link-227">165</a> , <a href="#babilu_link-118">305</a></p>

<p class="noindent">Voronoi diagrams, <a href="#babilu_link-229">181</a> , <a href="#babilu_link-179">183</a></p>

<p class="noindent">Wake-sleep algorithm, <a href="#babilu_link-27">103–104</a></p>

<p class="noindent">Walmart, <a href="#babilu_link-52">11</a> , <a href="#babilu_link-132">69–70</a></p>

<p class="noindent">War, cyber-, <a href="#babilu_link-215">19–21</a> , <a href="#babilu_link-216">279–282</a> , <a href="#babilu_link-32">299</a> , <a href="#babilu_link-166">310</a></p>

<p class="noindent"><i>War of the Worlds</i> (radio program), <a href="#babilu_link-117">156</a></p>

<p class="noindent">Watkins, Chris, <a href="#babilu_link-112">221</a> , <a href="#babilu_link-175">223</a></p>

<p class="noindent">Watson, James, <a href="#babilu_link-44">122</a> , <a href="#babilu_link-197">236</a></p>

<p class="noindent">Watson, Thomas J., Sr., <a href="#babilu_link-143">219</a></p>

<p class="noindent">Watson (computer), <a href="#babilu_link-217">37</a> , <a href="#babilu_link-55">42–43</a> , <a href="#babilu_link-143">219</a> , <a href="#babilu_link-230">237</a> , <a href="#babilu_link-106">238</a></p>

<p class="noindent">Wave equation, <a href="#babilu_link-172">30</a></p>

<p class="noindent">Web 2.0, <a href="#babilu_link-46">21</a></p>

<p class="noindent">Web advertising, <a href="#babilu_link-94">10–11</a> , <a href="#babilu_link-34">160</a> , <a href="#babilu_link-118">305</a></p>

<p class="noindent">Weighted <i>k</i> -nearest-neighbor algorithm, <a href="#babilu_link-179">183–185</a> , <a href="#babilu_link-69">190</a></p>

<p class="noindent">Weights</p>

<p class="noindent">attribute, <a href="#babilu_link-91">189</a></p>

<p class="noindent">backpropagation and, <a href="#babilu_link-295">111</a></p>

<p class="noindent">Master Algorithm and, <a href="#babilu_link-127">242</a></p>

<p class="noindent">meta-learning and, <a href="#babilu_link-230">237–238</a></p>

<p class="noindent">perceptron’s, <a href="#babilu_link-310">97–99</a></p>

<p class="noindent">relational learning and, <a href="#babilu_link-298">229</a></p>

<p class="noindent">of support vectors, <a href="#babilu_link-286">192–193</a></p>

<p class="noindent">Welles, Orson, <a href="#babilu_link-117">156</a></p>

<p class="noindent">Werbos, Paul, <a href="#babilu_link-156">113</a></p>

<p class="noindent">Wigner, Eugene, <a href="#babilu_link-297">29</a></p>

<p class="noindent">Will, George F., <a href="#babilu_link-190">276</a></p>

<p class="noindent">Williams, Ronald, <a href="#babilu_link-104">112</a></p>

<p class="noindent">Wilson, E. O., <a href="#babilu_link-130">31</a></p>

<p class="noindent">Windows, <a href="#babilu_link-53">12</a> , <a href="#babilu_link-240">133</a> , <a href="#babilu_link-303">224</a></p>

<p class="noindent"><i>Wired</i> (magazine), <a href="#babilu_link-239">265</a></p>

<p class="noindent">Wizard of Oz problem, <a href="#babilu_link-241">285</a></p>

<p class="noindent">Wolpert, David, <a href="#babilu_link-275">62</a> , <a href="#babilu_link-106">238</a></p>

<p class="noindent">Word of mouth, <a href="#babilu_link-121">231</a></p>

<p class="noindent">Xbox Live, <a href="#babilu_link-34">160–161</a></p>

<p class="noindent">XOR. <i>See</i> Exclusive-OR function (XOR)</p>

<p class="noindent">Yahoo, <a href="#babilu_link-94">10</a></p>

<p class="noindent">Yelp, <a href="#babilu_link-51">271</a> , <a href="#babilu_link-170">277</a></p>

<p class="noindent">YouTube, <a href="#babilu_link-48">266</a></p>

<p class="noindent">Zuckerberg, Mark, <a href="#babilu_link-154">55</a></p>

</section>

</div>

</div>

</body></html>